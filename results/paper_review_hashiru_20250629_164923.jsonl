{"paper_id": "lMgFRIILVB", "prompt": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success\n\n1 INTRODUCTION: Graph neural networks (GNN) (Defferrard et al., 2016; Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017) are popular methods for various graph-based tasks such as node classification, clustering, or link prediction. GNNs learn low-dimensional representations of nodes by combining the information of node features and a graph structure, providing rich semantic information to solve each task by end-to-end training. There are a huge number of recent variants of GNNs - which one should a practitioner use? Which are the strong and weak points of each variant? Could we design a variant, that has all the strong points, and none of the weak ones? In response, we propose SlenderGNN, a new linear GNN for node classification on both homophily and heterophily graphs, and even on graphs with no network effects. As we elaborate in Section 4, SlenderGNN is based on several, careful design decisions (like inclusion of structural information, feature concatenation, etc.) to combine the strengths of prior GNNs, while remedying their pitfalls. As a result, SlenderGNN shows notable advantages compared to both linear and nonlinear GNNs: \u2022 Accurate on semi-supervised learning, on both real-world and synthetic datasets, almost always winning or tying in the first place (see Table 2 and Table 3). \u2022 Robust, being able to handle datasets with homophily; with heterophily; with no network effects; even without tuning any hyperparameters on the propagator function. \u2022 Slender, using a few, carefully chosen features, and imposing sparsity on top of that. \u2022 Interpretable, thanks to being linear, and thanks to the sparsity of features that we impose (see Figure 1c for details). This paper has two additional contributions that shed light to the strong and weak points of various GNN models: \u2022 GNNLIN framework: Using linearization, GNNLIN encompasses recent GNN models and highlights the similarities and differences among them \u2013 see Table 1 (and Section 3). \u2022 Sanity checks: Based on GNNLIN, we devise diverse scenarios (homophily, heterophily, block-communities, bipartite-graph communities, etc), which easily reveal the strong and weak points of each GNN variant \u2013 see Figure 1b and Table 2 (and Section 5). Reproducibility: Our code is available at https://bit.ly/3fhWJfK along with our datasets for \u2018sanity checks\u2019 and real-world datasets of homophily and heterophily graphs. 2 PROBLEM DEFINITION AND RELATED WORKS: We introduce the problem definition of semi-supervised node classification, symbols frequently used in this paper, and related works on graph neural networks (GNN). Problem definition We define the problem of semi-supervised node classification as follows: \u2022 Given An undirected graph G = (A,X), where A \u2208 Rn\u00d7n is an adjacency matrix, X \u2208 Rn\u00d7d is a node feature matrix, n is the number of nodes, and d is the number of features \u2022 Given Labels y \u2208 {1, \u00b7 \u00b7 \u00b7 , c}m for m nodes, where m \u226a n, and c is the number of classes. \u2022 Predict the unknown classes of n\u2212m test nodes in G. We use the following symbols to represent modified adjacency matrices. A\u0303 = A+I is the adjacency matrix with self-loops. D\u0303 = A\u03031n\u00d71 is the diagonal degree matrix of A\u0303, where 1n\u00d71 is the matrix of size n\u00d7 1 filled with ones. A\u0303sym = D\u0303\u22121/2A\u0303D\u0303\u22121/2 is the symmetrically normalized A\u0303. Similarly, Asym = D \u22121/2AD\u22121/2 is also a symmetrically normalized A without self-loops. There are other types of normalization Arow = D\u22121A and Acol = AD\u22121 (and accordingly A\u0303row and A\u0303col), which we call row and column normalization, respectively, based on the direction of normalization. Graph neural networks There are a massive number of GNNs proposed with different motivations and insights. Recent surveys (Zhou et al., 2020; Wu et al., 2021) on GNNs introduce a way to categorize existing models into a few groups including spectral models (Defferrard et al., 2016; Kipf & Welling, 2017), sampling-based models (Hamilton et al., 2017; Ying et al., 2018), attention-based models (Velickovic et al., 2018; Kim & Oh, 2021; Brody et al., 2022), and deep models with residual connections (Li et al., 2019; Chen et al., 2020). GNNs are often fused with graphical inference for fully exploiting observed labels at test time (Yoo et al., 2019; Huang et al., 2021). Decoupled GNNs separate the two major functionalities of GNNs: node-wise feature transformation and feature propagation through the graph (Klicpera et al., 2019a;b; Chien et al., 2021). This allows one to propagate node-level information to distant neighbors without causing the oversmoothing issue. Linear graph neural networks Linear GNNs have been studied widely in literature to improve the robustness and generalizability of GNNs. Wu et al. (2019) proposed SGC, which removes the nonlinear activation functions from GCN (Kipf & Welling, 2017) and reduces the propagator function to a simple matrix multiplication. Wang et al. (2021) and Zhu & Koniusz (2021) improved SGC by manually adjusting the strength of self-loops with hyperparameters, which allows one to increase the number of propagation steps. Li et al. (2022) proposed G2CN, which improves the performance of DGC (Wang et al., 2021) on heterophily graphs by combining multiple propagation settings (i.e. bandwidths). The main limitation of such approaches is the high complexity of propagator functions, which impairs robustness and interpretability even with the linearity with given features. Spectral analysis of GNNs The problem to compare the propagator functions of different GNNs is related to the spectral analysis (Balcilar et al., 2021; Zhu & Koniusz, 2021; Li et al., 2022), which understands different graph convolution operators in the spectral view. However, our linearizationbased approach has two notable advantages. First, linearization is generally applicable even to GNN models whose feature transformation and propagation are mixed. Second, linearization gives a clear view on how the features interact with the graph structure and make predictions. To the best of our knowledge, linearization-based comparison between GNNs has not been studied in literature. 3 PROPOSED FRAMEWORK: GNNLIN Propagator Functions ... Figure 2: Linearization through GNNLIN. We propose GNNLIN, a framework that generalizes existing linear graph neural network (GNN) and provides a way of systematic linearization for existing GNN models (see Figure 2). Our observations from linearization motivate us to propose SlenderGNN, a new GNN model elaborated in Section 4. 3.1 DEFINITION OF GNNLIN: Definition 1 (LR). Given a feature X \u2208 Rn\u00d7d and a label y \u2208 Rm, where m \u2264 n is the number of observations, let Y \u2208 Rm\u00d7c be the one-hot representation of y, and yij be the (i, j)-th element in Y. Then, logistic regression (LR) finds an optimal weight matrix W\u2217 \u2208 Rd\u00d7c as follows: LR(X,y) = argmax W l\u2211 i=1 c\u2211 j=1 yij log y\u0302ij where y\u0302ij = exp(w\u22a4\u00b7jxi)\u2211c k=1 exp(w \u22a4 \u00b7kxi) , (1) and w\u00b7j is the j-th column of W. We omit the bias term for brevity without loss of generality. Definition 2 (Linearized GNN). Given a graph G = (A,X), let f(\u00b7; \u03b8) be a node classifier function to predict the labels of all nodes in G as y\u0302 = f(A,X; \u03b8), where \u03b8 is the set of learnable parameters. Then, f is linearized if \u03b8 = {W} and an optimal weight matrix W\u2217 \u2208 Rh\u00d7c is given as W\u2217 = LR(P(A,X),y), (2) where P is a feature propagator function that is linear with X and contains no learnable parameters, and P(A,X) \u2208 Rn\u00d7h. We ignore the bias term for brevity without loss of generality. Definition 3 (GNNLIN). Let f(\u00b7; \u03b8) be a (nonlinear) GNN. GNNLIN is to represent f as a linearized GNN by replacing all (nonlinear) activation functions in f with the identity function and deriving a variant f \u2032 that is at least as expressive as f but contains no parameters in P . GNNLIN represents the characteristic of a GNN as the linear feature propagation function P , which transforms raw features X by utilizing A. Lemma 1 shows that GNNLIN generalizes existing linear GNNs. Logistic regression is also represented by GNNLIN with the identity P(A,X) = X. Lemma 1. Existing linear models such as SGC (Wu et al., 2019), DGC (Wang et al., 2021), S2GC (Zhu & Koniusz, 2021), and G2CN (Li et al., 2022) are represented by GNNLIN. Proof. The proof is given in Appendix A.1. \u25a0 The first few rows of Table 1 show the representation of linear models as GNNLIN. DGC and S2GC improve SGC by controlling the strength of self-loops with new hyperparameters T and \u03b1. G2CN is the latest linear model that achieves highest accuracy, but has the most complex propagator function with many hyperparameters K, N , Ti, and bi. Such an increase of complexity limits the robustness and interpretability of decisions, which are the main advantages of linear models. 3.2 LINEARIZATION OF EXISTING MODELS: We apply GNNLIN to represent existing GNNs in the linearized forms in Table 1, which allows us to understand and compare all linear and nonlinear GNNs in the form of propagator functions. Decoupled models PPNP, APPNP (Klicpera et al., 2019a), GDC (Klicpera et al., 2019b), and GPR-GNN (Chien et al., 2021) are decoupled GNNs that separate feature transformation and propagation stages. PPNP runs Personalized PageRank on the node features, and APPNP approximates PPNP with K steps of message propagation. GDC generalizes APPNP by increasing the value of K to \u221e and sparsifies the propagator matrix S. GPR-GNN also generalizes APPNP to avoid the usage of \u03b1 by learning a weight for each component, resulting in the concatenation of multiple different features. We provide details of linearization in Appendix A.2. Coupled models We linearize coupled GNNs including ChebNet (Defferrard et al., 2016), GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), and GCNII (Chen et al., 2020). The linearized version of GCN is the same as SGC, since the motivation of SGC is to linearize GCN for better scalability and robustness. Although their motivations are different, the linearized versions of ChebNet, GraphSAGE, and GCNII are similar to linearized GPR-GNN in that features propagated by different steps are combined by concatenation. We provide the details in Appendix A.3. Attention models Attention-based GNNs (Velickovic et al., 2018; Kim & Oh, 2021; Brody et al., 2022) are a popular category of GNNs to learn the importance of each edge based on X and learnable parameters. Thus, it is not straightforward to linearize them by the GNNLIN framework: even if we assume the learnable parameters in P as fixed hyperparameters, P is at least quadratic with X, since X participates in computing the new adjacency matrix that is multiplied again with X. Nevertheless, we perform incomplete linearization of attention-based models for completeness, and represent the results with the \u2018**\u2019 symbol in Table 1. Detailed processes are given in Appendix A.4. The linearized version of GAT uses a different adjacency matrix for each step k, which is the main difference from other linearized models. It is noteworthy that DA-GNN (Liu et al., 2020), proposed as a decoupled model in their paper, is an attention-based model in our analysis. This is because X participates in computing the new adjacency matrix, making P quadratic as in GAT. 3.3 DISTINGUISHING FACTORS AND PAIN POINTS: Table 1 shows that many existing GNNs have similar or even identical propagator functions after we linearize them. We present the main factors that make a difference between those models and detect pain points that none of the existing models in Table 1 addresses well. Distinguishing Factor 1 (Combination of features). How should we combine the node features, the immediate neighbors\u2019 features, and the k-step-away neighbors\u2019 features? GNNs propagate information by multiplying the feature X with (a variant of) the adjacency matrix A multiple times. Three approaches prevail in Table 1: (1) no aggregation (SGC, DGC, G2CN, and GAT), (2) summation (S2GC, APPNP, and GDC), and (3) concatenation (GPR-GNN, GraphSAGE, and GCNII). The simplest approach is to use only the one with K steps (i.e. no aggregation), while the other two approaches aim to combine the information of multiple steps. Distinguishing Factor 2 (Modification of adjacency matrices). How should we normalize or modify the adjacency matrix to better represent the connectivity information? The three prevailing choices are (1) symmetric vs. row normalization, (2) the strength of self-loops, including making zero self-loops, and (3) static vs. dynamic adjustment based on the given features. Most models use the symmetric normalization A\u0303sym with self-loops, but some variants avoid selfloops and use either row normalization Arow or symmetric one Asym. Recent models such as DGC, G2CN, and GCNII determine the weight of self-loops with hyperparameters, since strong self-loops allow one to increase the value of K for distant propagation. Finally, attention-based models learn the elements in A based on node features, making propagator functions quadratic with X. Distinguishing Factor 3 (Dealing with heterophily graphs). How should we consider heterophily if the direct neighbors are likely to have the information of different labels? In the case of heterophily, using the features of immediate neighbors can actually hurt performance, and several GNN models do suffer under heterophily. Observed ways to handle it are (1) using the square of A as the main unit, (2) learning different weights for different steps, and (3) making small or no self-loops in A. The idea of such approaches is to avoid or downplay the effect of immediate (and odd-step-away) neighbors. Self-loops hurt under heterophily, as they force to have information of all intermediate neighbors by acting as implicit summation of transformed features. Pain Point 1 (Hyperparameters in propagators). The hyperparameters in a propagator function P impairs the interpretability of the weight matrix W, and forces one to re-compute the transformed feature for every new choice during hyperparameter search. The last column of Table 1 summarizes the hyperparameters in each P , which make the following limitations in terms of linear models. First, the interpretability of the weight matrix W is impaired, because it is learned on the transformed feature P(A,X) whose meaning changes arbitrarily by the choice of hyperparameters. Second, P(A,X) should be computed for each choice of hyperparameters, while it can be easily cached for searching hyperparameters outside P . Pain Point 2 (Failure on noisy features). All models in Table 1 depend on the node feature matrix X, and cannot fully exploit the adjacency matrix A if the given features are noisy. Real-world datasets often contain noisy features, and the graph structure A plays an essential role in the performance of node classification in such cases. That is, a desirable property for a robust model is to adaptively emphasize important features or disregard noisy ones to maximize its generalization performance. However, the models in Table 1 lack such a functionality. 4 PROPOSED METHOD - SlenderGNN Based on the insights from the comprehensive linearization, we propose SlenderGNN, a novel linear GNN that addresses the limitations of existing GNN models with a simple propagator function. The propagator function P of SlenderGNN is given as follows: P(A,X) = U\ufe38\ufe37\ufe37\ufe38 Structure \u2225 g(X)\ufe38 \ufe37\ufe37 \ufe38 Node features \u2225 g(A2rowX)\ufe38 \ufe37\ufe37 \ufe38 2-step neighbors \u2225 g(A\u03032symX)\ufe38 \ufe37\ufe37 \ufe38 Neighbors (3) where g(\u00b7) is the principal component analysis (PCA) for the orthogonalization of each component, followed by an L2 normalization, and U \u2208 Rn\u00d7r contains r-dimensional structural features derived by running the low-rank support vector decomposition (SVD) on the matrix A. The goals of SlenderGNN are to maximize (a) the robustness to graphs of various properties and (b) the interpretability of learned weights. We emphasize robustness, since in semi-supervised learning, high accuracy on real-world datasets is acquired naturally from robustness. We introduce the design decisions D1-D5, that lead us to propose Equation 3 as our approach to achieve our goals. D1 is to achieve both goals, D2 and D3 are for robustness, and D4 and D5 are designed for interpretability. In response to Pain Point 1, Equation 3 is designed to contain no hyperparameters to tune. D1: Concatenation-based linear method The main principle of SlenderGNN is to make linear decisions to acquire robustness and interpretability at the same time. Recent works on linear GNNs improve the limited expressiveness of linear decisions by designing more complex P . On the other hand, Table 1 suggests that many GNNs concatenate the features of heterogeneous information, and it leads to combining rich information of a graph structure even remaining the low complexity of P . The concatenation also allows us to ignore meaningless components in the transformed features and to interpret the learned weights as the importances of given components in each dataset. D2: Structural features (for robustness) In response to Pain Point 2 where features are missing, noisy, or useless for classification, then we have to resort to the adjacency matrix A ignoring X. At the same time, it is not effective to use raw A in P , considering that it is a large sparse matrix. We thus adopt low-rank SVD with rank r to extract structural features U. The value of r is automatically selected to keep 90% of the energy of A, where the sum of the largest r squared singular values divided by the squared Frobenius norm of A is approximately 0.9. When the graph is large, we set r to be d for the size consistency of components (see D5). D3: Combining different normalizations (for robustness) The symmetric normalization A\u0303sym with self-loops is known to perform well in many cases, but has two limitations. First, the self-loops in A\u0303Ksym force one to consider all intermediate nodes until the K-hop neighbors, even in heterophily graphs where the direct neighbors should be avoided. Second, the neighboring features are rescaled based on the node degrees during an aggregation. As a solution, we combine different normalization methods in feature transformation: A\u0303sym and Arow, where the row-normalized matrix Arow does not include self-loops. Arow effectively summarizes the information of K-hop neighbors preserving the scales of features, acquiring better signals on heterophily graphs. D4: Keeping the low power (for interpretability) Existing linear GNNs use a very large value of K \u2265 50, which determines the power of a (transformed) adjacency matrix A. This is effective in distant propagation of information, but severely limits the interpretability of the learned weight W: for example, one cannot tell where the information comes in a graph when features are transformed with A50X. Therefore, we propose to keep the small value of K = 2 for the interpretability, while taking the information of two-hop neighbors is essential for many cases. D5: Orthogonalization and sparsification (for interpretability) We use two reliable methods to improve the interpretability of SlenderGNN: dimensionality reduction by PCA and regularization by group LASSO. First, we run PCA on each component independently to orthogonalize given features and to improve the consistency of learned weights. Second, we apply group LASSO to learn sparse weights on the component level, while preserving the relative magnitude of each element. To make the consistency between components, we force all components to have the same dimensionality by selecting r features from each component when adopting PCA. 5 PROPOSED SANITY CHECKS ON POSSIBLE GRAPH SCENARIOS: We propose sanity checks to evaluate the robustness of GNNs to various scenarios of node classification and to observe their strengths and weaknesses in different settings. Table 2: SlenderGNN passes all sanity checks. Accuracy of all models on sanity checks; three groups of scenarios: (left) only features X are informative; (middle) only connectivity A is informative; (right) both are informative. The three letters at the second row represent the cases of X, A, and y, respectively: S (semantic X), R (random X), T (structural X), U (uniform A), C (clustered A), B (bipartite A), I (individual y), O (homophily y), and E (heterophily y). The colors , , represent the top three methods within 2\u03c3 of the best accuracy at each setting, and represents accuracy below 2\u03c3 of the third-best accuracy. Model MI(X,y) > 0, i.e., X helps MI(A,y) > 0 MI(X,A,y) > 0, i.e., both X and A help (S, U, I) (S, C, I) (S, B, I) (R, C, O) (R, B, E) (T, C, O) (T, B, E) (S, C, O) (S, B, E) Uniform Uniform Uniform Homophily Heterophily Homophily Heterophily Homophily Heterophily LR 83.7\u00b10.6 83.7\u00b10.6 83.7\u00b10.6 24.2\u00b10.7 24.2\u00b10.7 71.4\u00b10.9 66.8\u00b12.2 83.4\u00b10.6 83.4\u00b10.6 SGC 44.6\u00b19.8 43.0\u00b19.0 45.1\u00b110. 64.3\u00b10.7 50.2\u00b114. 87.1\u00b10.6 84.3\u00b10.5 93.9\u00b10.9 91.5\u00b10.5 DGC 63.8\u00b11.0 66.3\u00b10.9 65.9\u00b11.2 50.5\u00b113. 26.0\u00b10.9 88.6\u00b11.0 45.3\u00b11.3 96.2\u00b10.4 54.0\u00b10.6 S2GC 79.9\u00b10.6 79.8\u00b11.1 80.1\u00b11.0 38.5\u00b112. 25.4\u00b10.9 88.4\u00b11.0 67.9\u00b11.5 95.9\u00b10.6 78.0\u00b10.5 G2CN 25.2\u00b10.3 25.3\u00b10.1 25.0\u00b10.2 24.2\u00b11.1 25.0\u00b10.1 88.5\u00b11.0 88.6\u00b11.2 24.3\u00b11.1 50.7\u00b131. GCN 36.3\u00b13.5 33.2\u00b12.4 35.7\u00b13.5 46.7\u00b18.0 43.7\u00b11.9 83.3\u00b11.3 72.2\u00b11.7 91.2\u00b11.2 80.3\u00b13.9 SAGE 80.3\u00b11.1 79.0\u00b11.2 79.8\u00b10.7 31.1\u00b10.7 34.6\u00b12.1 83.9\u00b10.8 81.3\u00b10.7 94.4\u00b10.5 94.4\u00b10.9 GCNII 73.5\u00b11.2 73.2\u00b10.9 73.6\u00b10.9 30.7\u00b10.7 27.1\u00b11.3 84.2\u00b10.8 69.0\u00b11.4 90.6\u00b10.9 80.4\u00b11.2 APPNP 66.0\u00b12.6 65.4\u00b12.7 64.2\u00b11.6 30.3\u00b11.2 25.2\u00b10.7 71.2\u00b14.9 43.8\u00b12.0 83.2\u00b13.8 58.7\u00b14.5 GPR-GNN 73.4\u00b10.4 73.5\u00b11.3 73.9\u00b10.7 74.6\u00b10.7 65.9\u00b12.1 89.9\u00b10.6 87.6\u00b11.2 95.0\u00b11.1 91.9\u00b11.1 GAT 32.7\u00b15.5 30.6\u00b13.0 33.8\u00b16.8 42.6\u00b14.8 36.8\u00b15.7 64.0\u00b15.7 55.6\u00b16.8 68.5\u00b17.1 67.0\u00b112. SlenderGNN 81.0\u00b11.1 81.0\u00b10.9 81.2\u00b11.0 87.1\u00b11.4 89.2\u00b11.2 88.1\u00b10.5 88.9\u00b10.7 94.4\u00b10.6 93.9\u00b10.5 Graph scenarios We categorize possible scenarios of node classification based on the characteristics of node features X, a graph structure A, and node labels y. We describe only the main ideas, leaving exact definitions of such scenarios to Appendix B.1. \u2022 Edges A: We consider three cases: uniform (no communities), clustered (block-diagonal), and bipartite. The uniform case means that every element aij is determined independently of the other edges in A. In the clustered case, nodes having common neighbors are likely to make more edges, while it is the opposite in the bipartite case. \u2022 Labels y: We consider three cases: individual (no network effects), homophily, and heterophily. In the homophily case, adjacent nodes are likely to have the same label, while it is the opposite in the heterophily case. \u2022 Features X: We consider three cases: random, structural, and semantic. The random case means that each feature is determined independently of all other variables. In the structural case, features give information of the graph structure, and in the semantic case, they directly provide useful information for node labels. Unlike the cases of edges and labels, which are mutually exclusive, features can be both structural and semantic at the same time. Feasibility Although there exist a total of 27 combinations for (X,A,y), not all of them are possible to implement; for example, either homophily or heterophily y is not compatible with uniform A. After removing the infeasible combinations of variables, we categorize the remaining choices based on the mutual information (MI) between variables. Implementation For generating synthetic graphs, we divide all nodes into c clusters, where c = 4 in our experiments, and determine the edge densities in each cluster and between different clusters based on the property of A. Details on implementation are described in Appendix B.2. Results of sanity checks Table 2 shows the results of sanity checks for our SlenderGNN and all baseline models (details of the baselines in Section 6). We run each experiment five times and report the average and standard deviation. We assume four target classes of nodes, and thus the accuracy of random guessing is 25%. Logistic regression (LR) fails when MI(X,y) = 0, since it uses only X for classification. LR shows relatively lower accuracy with structural features, since they are not a perfect indicator for node labels, and they need to be refined through A for better performance. It is clear that our SlenderGNN is the only approach that passes all sanity checks. The four components in SlenderGNN are carefully designed to maximize its robustness for various graph scenarios. Most GNNs work well in the cases (S, C, O) and (T, C, O), where X is either semantic or structural, A is clustered, and y is homophily, since many real-world datasets that recent works on GNNs use in their experiments follow such assumptions. However, many GNNs fail when a graph is generated with different assumptions, as we summarize as follows: \u2022 No network effects: In the cases of (S, ?, I), where ? is a placeholder, only a few models such as SAGE and GPR-GNN perform well. This is because MI(A,y) = 0 in such cases, and models are required to focus on raw features X ignoring A. \u2022 Meaningless features: In the cases of (R, C, O) and (R, B, E), models are required to do the opposite since MI(X,y) = 0: models should focus on A, ignoring X. Since there is no approach that explicitly uses A, all baselines show low accuracy. \u2022 Heterophily graphs: In the cases of (?, B, E), where ? is a placeholder, the labels follow heterophily. Models like G2CN, SAGE, and GPR-GNN perform well in these cases, since they address the heterophily in their designs (details in Section 3.3). 6 EXPERIMENTS:  6.1 EXPERIMENTAL SETTINGS: Datasets We use 5 homophily and 4 heterophily datasets, which were used in previous works on node classification (Chien et al., 2021; Pei et al., 2020). Cora, CiteSeer, and PubMed (Sen et al., 2008; Yang et al., 2016) are homophily citation graphs between research articles. Computers and Photo (Shchur et al., 2018) are homophily Amazon co-purchase graphs between items. Chameleon and Squirrel are heterophily Wikipedia graphs (Rozemberczki et al., 2021), connected by links between pages. Actor (Tang et al., 2009) is a heterophily graph connected by co-occurrence of actors on the same Wikipedia pages. Penn94 (Traud et al., 2012) is a heterophily graph of gender relations in a social network. We make the heterophily graphs undirected, as done in (Chien et al., 2021). Competitors We compare our SlenderGNN with various types of graph neural networks (GNN). Logistic regression (LR) is the simplest baseline, which does not use graph information. SGC (Wu et al., 2019), DGC (Wang et al., 2021), S2GC (Zhu & Koniusz, 2021), and G2CN (Li et al., 2022) are linear GNNs that perform as our main competitors. GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), GCNII (Chen et al., 2020) are coupled GNN models popular in literature. APPNP (Klicpera et al., 2019a) and GPR-GNN (Chien et al., 2021) are decoupled GNNs, and GAT (Velickovic et al., 2018) is an attention-based model. We perform hyperparameter search based on those reported in their original papers. Refer to Appendix C for details. Experimental setup We perform semi-supervised node classification by dividing all nodes in a graph by the 2.5%/2.5%/95% ratio into training, validation, and testing data. We perform five runs of each experiment with different random seeds and report the average and standard deviation. All hyperparameter search and early stopping are done based on validation accuracy for each run. 6.2 EXPERIMENTAL RESULTS: Accuracy on real-world graphs In Table 3, SlenderGNN is compared against linear as well as the state-of-the-art nonlinear GNNs on 9 real-world datasets (5 homophily and 4 heterophily graphs). We report the accuracy in Table 3 where , , represent top three methods within 2\u03c3 of the best method, and represents accuracy below 2\u03c3 of the third-best method. Our proposed SlenderGNN outperforms all the competitors in 2 homophily and 3 heterophily datasets, and shows competitive results in the rest (7 out of 9 times among top 3 methods). Moreover, it is the only model without red cells, which demonstrates the robustness and generality of SlenderGNN. SlenderGNN is also slender, where we report the number of parameters in Table 6 in the appendix. Compared with linear models, SlenderGNN achieves the best trade-off between the model size and two benefits, namely accuracy and interpretability; compared with nonlinear models, it has much fewer parameters, while performing no worse than them. Ablation study We report the ablation study in Table 4, by removing each of the essential modules of SlenderGNN. The results show that SlenderGNN is most robust and effective with all the modules. Interpretability of learned weights As shown in Figure 1c, we illustrate the learned weights of SlenderGNN in our proposed \u2018sanity checks\u2019, where the ground truths are known. In the graphs without network effects, SlenderGNN successfully assigns the highest weights to the node features Model Cora CiteSeer PubMed Computers Photo Chameleon Squirrel Actor Penn94:  Model Cora CiteSeer PubMed Computers Photo Chameleon Squirrel Actor Penn94: g(X). When the features are meaningless, it ignores the features and puts most of the attention on the extracted structural features U. In the cases that structural features are useful, but not enough to correctly identify the labels, it gives the highest weights to the propagated features g(A\u03032symX) and g(A2rowX) in homophily and heterophily graphs, respectively, perfectly fitting our design choices. For the cases that the features themselves are already good for the classification, SlenderGNN gives both the node and propagated features large weights to achieve higher accuracy. 7 CONCLUSION: The main contribution of this work is SlenderGNN, which has a long list of desirable properties: \u2022 Accurate: On both real-world and synthetic datasets, SlenderGNN exceeds the accuracy of state-of-the-art linear GNNs and it matches or exceeds the accuracy of nonlinear GNNs with 8\u00d7 \u2013 64\u00d7 fewer parameters. \u2022 Robust: SlenderGNN can handle the case of homophily, heterophily, and no-network-effect (randomly connected) graphs, as well as the case of no meaningful features. \u2022 Slender: SlenderGNN has few, but useful parameters. \u2022 Interpretable: SlenderGNN can easily justify its decisions (see Figure 1c). Additional contributions include: \u2022 The GNNLIN framework, that illuminates the fundamental similarities and differences of popular GNN variants (see Table 1). \u2022 The \u2018sanity checks\u2019, which highlight the strengths and weaknesses of each GNN method. Reproducibility: Our source code and \u2018sanity checks\u2019 are available at https://bit.ly/3fhWJfK. A LINEARIZATION PROCESSES: We present detailed processes to linearize various graph neural networks (GNN). Refer to Section 2 of the main paper for frequently used symbols. Linearization is done in two steps. First, we replace all activation functions with the identity function from the original definition of each GNN. Second, if the resulting layer function contains learnable parameters, we devise a replacement that is at least as expressive as the given function but containing no learnable parameters. This is because our goal of linearization is not just deriving a linear function with respect to X, but understanding GNNs in relation to logistic regression based on our GNNLIN framework. We ignore the bias terms of linear layers for simplicity, without loss of generality. A.1 PROOF OF LEMMA 1: REPRESENTING LINEAR MODELS WITH GNNLIN: We prove Lemma 1 by representing each linear GNN with GNNLIN. Let K \u2265 0 be a hyperparameter that determines the number of propagation steps in every GNN. A.1.1 SGC (WU ET AL., 2019) SGC directly fits the definition of linearization with the following propagator function: P(A,X) = A\u0303KsymX. (4) A.1.2 DGC (WANG ET AL., 2021) DGC has two variants, DGC-Euler and DGC-DK, which have different propagator functions. We focus on DGC-Euler, which is used as the main model in their experiments. DGC is similar to SGC, except that it controls the strength of self-loops as follows: P(A,X) = [(1\u2212 T/K)I+ (T/K)A\u0303sym]KX, (5) where T > 0 is a hyperparameter. The self-loops become stronger if T is closer to 0. A.1.3 S2GC (ZHU & KONIUSZ, 2021) S2GC computes the summation of features propagated with different numbers of steps: P(A,X) = K\u2211 k=1 (\u03b1I+ (1\u2212 \u03b1)A\u0303ksym)X. (6) The original formulation divides the added features by K, which can be safely ignored considering that the weight matrix W is multiplied to the transformed feature for classification. A.1.4 G2CN (LI ET AL., 2022) Since G2CN does not provide an explicit formulation of the propagator function, we need to derive it. First, the parameterized version P \u2032 of the propagator function is given as follows: P \u2032(A,X; {\u03b8i}Ni=1) = N\u2211 i=1 \u03b8iHi,K , (7) where \u03b8i is a learnable parameter. The k-th feature representation Hi,k is recursively defined as Hi,k = Hi,k\u22121 \u2212 Ti K (L\u2212 biI)2Hi,k\u22121 = Hi,k\u22121 \u2212 Ti K ((bi \u2212 1)I+Asym)2Hi,k\u22121 = [I\u2212 Ti K ((bi \u2212 1)I+Asym)2]Hi,k\u22121 Hi,0 = X, (8) where N , Ti, and bi are hyperparameters, and L = I \u2212 Asym is the normalized Laplacian matrix. Since the transformed features are combined with a learnable parameter \u03b8i in Equation 7, we make a propagator function P that contains no learnable parameters as follows: P(A,X) = N \u2225 i=1 Hi,K = N \u2225 i=1 [I\u2212 Ti K ((bi \u2212 1)I+Asym)2]KX. (9) A.2 LINEARIZATION OF DECOUPLED GNNS: The propagation in decoupled GNNs is done on the abstract representations of node features, which are typically generated by multilayer perceptrons (MLP). Linearization starts with replacing MLPs with linear projections, and removes additional nonlinearity in the process of propagation. A.2.1 PPNP (KLICPERA ET AL., 2019A): The lineraization of PPNP is straightforward, since the authors present a closed-form representation of the propagator function. If we remove the activation function, we have the following: P(A,X) = (In \u2212 (1\u2212 \u03b1)A\u0303sym)\u22121X, (10) where 0 < \u03b1 < 1 is a hyperparameter that controls the weight of self-loops. A.2.2 APPNP (KLICPERA ET AL., 2019A): We assume that the initial node representation is created by a single linear layer of XW, where W is a weight matrix. Then, the k-th representation matrix Hk is represented as follows: Hk = (1\u2212 \u03b1)A\u0303symHk\u22121 + \u03b1XW, (11) where 0 < \u03b1 < 1 is a hyperparameter. The closed-form representation of HK is given as follows: HK = ( [(1\u2212 \u03b1)A\u0303sym]K + \u03b1 K\u22121\u2211 k=0 [(1\u2212 \u03b1)A\u0303sym]k ) XW. (12) We safely remove the weight matrix W, which is redundant, and get the fiinal representation: P(A,X) = [ K\u22121\u2211 k=0 \u03b1(1\u2212 \u03b1)kA\u0303ksym + (1\u2212 \u03b1)KA\u0303Ksym]X (13) A.2.3 GDC (KLICPERA ET AL., 2019B) GDC generalizes APPNP and presents various forms of the propagation function. We pick the most representative one given in the paper, which is directly related to APPNP. The unnormalized version of the propagation matrix S\u2032 is given as follows: S\u2032 = \u221e\u2211 k=0 \u03b1(1\u2212 \u03b1)kA\u0303ksym, (14) and then it is normalized and sparsified as S = sparsify(S\u0303\u2032sym). (15) S\u0303\u2032sym represents adding self-loops and applying the symmetric normalization to S \u2032. The paper gives two approaches for sparsification, which are a) removing elements smaller than \u03f5, which is given as a hyperparameter, and b) selecting the top k neighbors for each node. With any choice, the function P is simply given as follows: P(A,X) = SX. (16) A.2.4 GPR-GNN (CHIEN ET AL., 2021) We assume that the initial node representation is created by a single linear layer of XW, where W is a weight matrix. Then, we have the following propagator function P \u2032 with parameters: P \u2032(A,X; {\u03b8k}Kk=0) = K\u2211 k=0 \u03b8kA\u0303 k symX, (17) where \u03b8k is a parameter that is learned together with W. We replace the summation with concatenation to remove the learnable parameters from P \u2032 and get the following: P(A,X) = K \u2225 k=0 A\u0303ksymX. (18) A.3 LINEARIZATION OF COUPLED GNNS:  A.3.1 CHEBNET (DEFFERRARD ET AL., 2016): Let Hk be the k-th node representation matrix, and L = I \u2212 Asym be the graph Laplacian matrix normalized symmetrically. Then, the propagator function of ChebNet with parameters \u03b8 is P \u2032(A,X; \u03b8) = K\u22121\u2211 k=0 \u03b8kHk, (19) where the recurrence relation is given as follows with the intial terms: H0 = X H1 = LX = \u2212AsymX+X \u00b7 \u00b7 \u00b7 Hk = 2(L\u2212 I)Hk\u22121 \u2212Hk\u22122 = \u22122AsymHk\u22121 \u2212Hk\u22122. (20) Based on the recurrence relation, the closed-form representation of Hk is given as Hk = akA k symX+ ak\u22121A k\u22121 symX+ \u00b7 \u00b7 \u00b7+ a0X, (21) where a0, \u00b7 \u00b7 \u00b7 , ak are constants. Since we have K free parameters \u03b80, \u00b7 \u00b7 \u00b7 , \u03b8K corresponding to the K terms in the representation matrix HK , we safely rewrite the propagator function as P \u2032(A,X; \u03b8) = K\u22121\u2211 k=0 \u03b8kA k symX. (22) Each value of k has a free parameter \u03b8k. Thus, we generalize it as follows: P(A,X) = K\u22121 \u2225 k=0 AksymX. (23) A.3.2 GRAPHSAGE (HAMILTON ET AL., 2017): We assume the mean aggregator of GraphSAGE. By replacing the activation function as the identity function, each layer F of GraphSAGE is linearized as follows: F(X) = XW1 +ArowXW2, (24) where Arow = D\u22121A represents the mean operator in the aggregation, and W1 and W2 are learnable weight matrices in the layer. If we apply a chain of two layers, where the weight matrices of the second layer are represented as W3 and W4, we get the following: F(F(X)) = XW1W3 +ArowX(W1W2 +W2W3) +A2rowXW2W4 (25) = XWa +ArowXWb +A 2 rowXWc, (26) where we redefine the weight matrices without loss of generality as Wa = W1W3 (27) Wb = W1W2 +W2W3 (28) Wc = W2W4. (29) If we generalize it into K layers, we get the following: FK(X) = K\u2211 k=1 AkrowXWk. (30) Note that a different weight matrix Wk is applied to each layer k. This is equivalent to concatenating the transformed features of all layers and learning a single large weight matrix in training. A.3.3 GCNII (CHEN ET AL., 2020) After replacing the activation function with the identity function, the l-th layer Fl of GCNII is given as follows: Fl(H) = ((1\u2212 \u03b1l)A\u0303symH+ \u03b1lX))((1\u2212 \u03b2l)I+ \u03b2lWl), (31) where \u03b1l and \u03b2l are hyperparameters, and Wl is a weight matrix. The second term is equivalent to Wl regardless of the value of \u03b2l, since Wl is a free parameter. We also set \u03b1l to a constant \u03b1 which is the same for every layer l, following the original paper (Chen et al., 2020).1 Then, the equation is simplified as Fl(H) = ((1\u2212 \u03b1)A\u0303symH+ \u03b1X)Wl. (32) If we apply a chain of two layers l and l + 1, we get the following: Fl+1(Fl(H)) = ((1\u2212 \u03b1)A\u0303sym((1\u2212 \u03b1)A\u0303symH+ \u03b1X)Wl + \u03b1X)Wl+1 (33) = (1\u2212 \u03b1)2A\u03032symHWlWl+1 + \u03b1(1\u2212 \u03b1)A\u0303symXWlWl+1 + \u03b1XWl+1 (34) = (1\u2212 \u03b1)2A\u03032symHW\u2032l + \u03b1(1\u2212 \u03b1)A\u0303symXW\u2032l + \u03b1XWl+1, (35) where W\u2032l = WlWl+1, which is also a free parameter. If we generalize it into K layers, we get the following: FK(X) = (1\u2212 \u03b1)K\u22121((1\u2212 \u03b1)A\u0303Ksym + \u03b1A\u0303K\u22121sym )XWK + \u03b1 K\u22122\u2211 k=0 (1\u2212 \u03b1)kA\u0303ksymXWk. (36) We safely remove the constant from each term, which can be included in the weight matrix: FK(X) = ((1\u2212 \u03b1)A\u0303Ksym + \u03b1A\u0303K\u22121sym )XWK + K\u22122\u2211 k=0 A\u0303ksymXWk. (37) We replace the summation operators between terms having different weight matrices with concatenation operators, having the final propagator function P as follows: P(A,X) = K\u22122 \u2225 k=0 A\u0303ksymX \u2225 ((1\u2212 \u03b1)A\u0303Ksym + \u03b1A\u0303K\u22121sym )X (38) A.4 LINEARIZATION OF ATTENTION GNNS: A.4.1 DA-GNN (LIU ET AL., 2020) We assume that the initial node representation is created by a single linear layer of XW, where W is a weight matrix. Then, the k-th representation matrix Hk is represented as follows: Hk = A\u0303 k symXW. (39) 1\u03b1 is set to 0.1 in the original paper of GCNII (Chen et al., 2020). DA-GNN computes the weighted sum of representations for all k \u2208 [0,K], where the weight values are determined also from the representation matrices: P(A,X) = K\u2211 k=0 diag(Hks)Hk = K\u2211 k=0 diag(A\u0303ksymXWs)A\u0303 k symXW. (40) where s is a learnable weight vector. We safely remove the last W and rewrite Ws as w. Then, we have the final representation of the propagator function: P(A,X) = K\u2211 k=0 diag(A\u0303ksymXw)A\u0303 k symX. (41) A.4.2 GAT (VELICKOVIC ET AL., 2018): We apply the following changes to linearize GAT, whose linearization is not straightforward due to the nonlinearity in the attention function: 1. We replace the activation functions between layers with the identity functions. 2. We simplify the attention function \u03b1ij = exp(eij)/ \u2211 k exp(eik) as eij . 3. We remove the LeaklyReLU function in the computation of eij . 4. We assume the single-head attention. The edge weight eij , which is the (i, j)-th element of the propagator matrix, is defined as follows: eij = a \u22a4 dst(W \u22a4xi) + a \u22a4 src(W \u22a4xj), (42) where xi and xj are feature vectors of length d for node i and j, respectively, W is a d\u00d7 c learnable weight matrix, and adst and asrc are learnable weight vectors of length c. Then, we derive the initial form of a linearized GAT layer as follows: H = [diag(XWadst)A\u0303+ A\u0303diag(XWasrc)]XW. (43) Since all adst, asrc, and W are free parameters, we generalize it as follows: H = [diag(Xwdst)A\u0303+ A\u0303diag(Xwsrc)]XW, (44) where wdst and wsrc are learnable vectors of length m that replace adst and asrc, respectively. B DETAILS ON SANITY CHECKS:  B.1 FORMAL DEFINITIONS OF GRAPH SCENARIOS: We categorize all possible scenarios of node classification based on the characteristics of node features X, a graph structure A, and node labels y. We denote by Aij and Yi the random variables for edge (i, j) between nodes i and j and label yi of node i, respectively. Edges For the adjacency matrix A, we consider the following three cases: \u2022 Uniform: P (Aki = 1 | Aij = Ajk = 1) = P (Aki = 1) \u2022 Clustered: P (Aki = 1 | Aij = Ajk = 1) > P (Aki = 1) \u2022 Bipartite: P (Aki = 1 | Aij = Ajk = 1) < P (Aki = 1) The uniform case represents that every edge is determined independently of the others, and the graph structure gives no useful information for node classification. In the clustered case, it is more likely that nodes having common neighbors make more connections in a graph. In the bipartite case, nodes make more connections with those sharing no common neighbors. Labels For the labels in y, we consider the following three cases: \u2022 Individual: P (Yi = y | Aij = 1, Yj = y) = P (Yi = y) \u2022 Homophily: P (Yi = y | Aij = 1, Yj = y) > P (Yi = y) \u2022 Heterophily: P (Yi = y | Aij = 1, Yj = y) < P (Yi = y) The individual case means that the label of a node is independent of the labels of its neighbors. This is the case when the graph structure works as noise information with regard to classification. In the homophily case, adjacent nodes are likely to have the same label. It is the most popular assumption of GNNs for node classification. In the heterophily case, adjacent nodes are likely to have different labels, which is not as common as homophily but often observed in real-world graphs. Features For the feature matrix X, we consider the following three cases based on A and y. We use the notation of p(\u00b7) since the features are typically modeled as continuous variables. \u2022 Random: p(xi,xj | yi, yj , aij) = p(xi,xj) \u2022 Structural: p(xi,xj | yi, yj , aij) \u0338= p(xi,xj | yi, yj) \u2022 Semantic: p(xi,xj | yi, yj , aij) \u0338= p(xi,xj | aij) The random case represents that features are determined independently of the graph and labels. In this case, node features do not give useful information for classification, but work as unique indices of nodes like one-hot embeddings if the dimensionality d of features is large. In the structural case, features are correlated with the graph structure, but not directly with labels. Such features give useful information for classification only if the graph structure and labels are related. In the semantic case, features directly provide useful information for predicting labels. Note that features can be semantic and structural at the same time by satisfying both of the conditions. B.2 IMPLEMENTATION OF GRAPH SCENARIOS There are various ways to generate synthetic graphs satisfying the definitions of different scenarios. One can use a synthetic graph generator designed to create more plausible graphs (Leskovec et al., 2010; Baraba\u0301si & Albert, 1999), but we choose the simplest one to focus on the mutual information, rather than the other characteristics of synthetic graphs such as the degree distribution. Structure We assume that the number of structural clusters is the same as the number c of labels for the alignment with label information. We divide all nodes into c groups and then decide the edge densities for intra- and inter-connections of groups based on the structural type: uniform, clustered, and bipartite. We use a hyperparameter \u03f5a to determine the noise level in the case of homophily or heterophily: for example, if \u03f5a = 0, the graph has a full block-diagonal adjacency matrix in the case of homophily. The expected number of edges is the same for all three cases, and we set \u03f5a = 0 in all of our experiments. A notable characteristic of a bipartite structure A is that A2 has a clustered structure. If we create inter-group connections for every possible pair of different groups, even noise-free A with \u03f5a = 0 makes noisy A2 with inter-group connections. For better consistency, we set the number of classes to an even number in our experiments, randomly pick paired classes such as (1, 3) and (2, 4) when c = 4, for example, and create inter-group connections only for the chosen pairs. In this way, we create a non-diagonal block-permutation matrix A when \u03f5a = 0, and the noise level of A2, which has a clustered structure, is solely controlled by the noise level of A. Labels In the case of individual y, we determine the label of each node uniformly at random, with no consideration of the graph structure. In the case of homophily or heterophily y, we assign labels based on the groups of nodes assumed by the graph structure. That is, nodes in the same group have the same label, and thus the group index itself works as the label y. In this way, we force homophily y for clustered A, and heterophily y for bipartite A. The degree of homophily (or heterophily) is also determined by the noise level \u03f5a of the graph structure. Features We basically assume that every feature element is sampled from a uniform distribution. Thus, in the random case, we sample each element from the uniform distribution U(0, 1) between 0 and 1. In the structural case, we run low-rank support vector decomposition (SVD) (Halko et al., 2011) to make X have structural information. Given U\u03a3V\u22a4 \u2248 A from low-rank SVD, we take U and normalize each feature element to have the zero-mean and unit-variance. The rank r in the SVD is determined as a hyperparameter; higher r captures the structure better, but can give noisy information. We also apply the ReLU function to U to make them positive. In the semantic case, we randomly pick c representative vectors {vk}ck=1 from the uniform distribution, which correspond to the c different classes. Then, for each node i with label y, we sample a feature vector such that argmaxk x \u22a4vk = y. In this way, we have random vectors having sufficient semantic information for the classification of labels, with a guarantee that the perfect linear decision boundaries can be drawn in the feature space X at the training time. C REPRODUCIBILITY: We perform row-normalization on the node features of all datasets as done in most studies on GNNs. We report the hyperparameters used for a grid search in Table 5, which is done for every split of data. The dimensions of hidden layers are all set to 64, and the probabilities of dropout layers are all set to 0.5. For the linear models, we use L-BFGS as the optimizer for training 100 epochs with patience 5; for the nonlinear ones, we use ADAM and train them for 1000 epochs with patience 200. In SlenderGNN, there are only 2 hyperparameters, where wd1 is the weight of LASSO, and wd2 is the weight of group LASSO. It is worth noting that, when searching the hyperparameters, SlenderGNN does not need to recompute the features, while most of the linear methods need to do so because of including one or more hyperparameters in the features that need to be precomputed.", "agent_review": [{"role": "user", "content": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success\n\n1 INTRODUCTION: Graph neural networks (GNN) (Defferrard et al., 2016; Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017) are popular methods for various graph-based tasks such as node classification, clustering, or link prediction. GNNs learn low-dimensional representations of nodes by combining the information of node features and a graph structure, providing rich semantic information to solve each task by end-to-end training. There are a huge number of recent variants of GNNs - which one should a practitioner use? Which are the strong and weak points of each variant? Could we design a variant, that has all the strong points, and none of the weak ones? In response, we propose SlenderGNN, a new linear GNN for node classification on both homophily and heterophily graphs, and even on graphs with no network effects. As we elaborate in Section 4, SlenderGNN is based on several, careful design decisions (like inclusion of structural information, feature concatenation, etc.) to combine the strengths of prior GNNs, while remedying their pitfalls. As a result, SlenderGNN shows notable advantages compared to both linear and nonlinear GNNs: \u2022 Accurate on semi-supervised learning, on both real-world and synthetic datasets, almost always winning or tying in the first place (see Table 2 and Table 3). \u2022 Robust, being able to handle datasets with homophily; with heterophily; with no network effects; even without tuning any hyperparameters on the propagator function. \u2022 Slender, using a few, carefully chosen features, and imposing sparsity on top of that. \u2022 Interpretable, thanks to being linear, and thanks to the sparsity of features that we impose (see Figure 1c for details). This paper has two additional contributions that shed light to the strong and weak points of various GNN models: \u2022 GNNLIN framework: Using linearization, GNNLIN encompasses recent GNN models and highlights the similarities and differences among them \u2013 see Table 1 (and Section 3). \u2022 Sanity checks: Based on GNNLIN, we devise diverse scenarios (homophily, heterophily, block-communities, bipartite-graph communities, etc), which easily reveal the strong and weak points of each GNN variant \u2013 see Figure 1b and Table 2 (and Section 5). Reproducibility: Our code is available at https://bit.ly/3fhWJfK along with our datasets for \u2018sanity checks\u2019 and real-world datasets of homophily and heterophily graphs. 2 PROBLEM DEFINITION AND RELATED WORKS: We introduce the problem definition of semi-supervised node classification, symbols frequently used in this paper, and related works on graph neural networks (GNN). Problem definition We define the problem of semi-supervised node classification as follows: \u2022 Given An undirected graph G = (A,X), where A \u2208 Rn\u00d7n is an adjacency matrix, X \u2208 Rn\u00d7d is a node feature matrix, n is the number of nodes, and d is the number of features \u2022 Given Labels y \u2208 {1, \u00b7 \u00b7 \u00b7 , c}m for m nodes, where m \u226a n, and c is the number of classes. \u2022 Predict the unknown classes of n\u2212m test nodes in G. We use the following symbols to represent modified adjacency matrices. A\u0303 = A+I is the adjacency matrix with self-loops. D\u0303 = A\u03031n\u00d71 is the diagonal degree matrix of A\u0303, where 1n\u00d71 is the matrix of size n\u00d7 1 filled with ones. A\u0303sym = D\u0303\u22121/2A\u0303D\u0303\u22121/2 is the symmetrically normalized A\u0303. Similarly, Asym = D \u22121/2AD\u22121/2 is also a symmetrically normalized A without self-loops. There are other types of normalization Arow = D\u22121A and Acol = AD\u22121 (and accordingly A\u0303row and A\u0303col), which we call row and column normalization, respectively, based on the direction of normalization. Graph neural networks There are a massive number of GNNs proposed with different motivations and insights. Recent surveys (Zhou et al., 2020; Wu et al., 2021) on GNNs introduce a way to categorize existing models into a few groups including spectral models (Defferrard et al., 2016; Kipf & Welling, 2017), sampling-based models (Hamilton et al., 2017; Ying et al., 2018), attention-based models (Velickovic et al., 2018; Kim & Oh, 2021; Brody et al., 2022), and deep models with residual connections (Li et al., 2019; Chen et al., 2020). GNNs are often fused with graphical inference for fully exploiting observed labels at test time (Yoo et al., 2019; Huang et al., 2021). Decoupled GNNs separate the two major functionalities of GNNs: node-wise feature transformation and feature propagation through the graph (Klicpera et al., 2019a;b; Chien et al., 2021). This allows one to propagate node-level information to distant neighbors without causing the oversmoothing issue. Linear graph neural networks Linear GNNs have been studied widely in literature to improve the robustness and generalizability of GNNs. Wu et al. (2019) proposed SGC, which removes the nonlinear activation functions from GCN (Kipf & Welling, 2017) and reduces the propagator function to a simple matrix multiplication. Wang et al. (2021) and Zhu & Koniusz (2021) improved SGC by manually adjusting the strength of self-loops with hyperparameters, which allows one to increase the number of propagation steps. Li et al. (2022) proposed G2CN, which improves the performance of DGC (Wang et al., 2021) on heterophily graphs by combining multiple propagation settings (i.e. bandwidths). The main limitation of such approaches is the high complexity of propagator functions, which impairs robustness and interpretability even with the linearity with given features. Spectral analysis of GNNs The problem to compare the propagator functions of different GNNs is related to the spectral analysis (Balcilar et al., 2021; Zhu & Koniusz, 2021; Li et al., 2022), which understands different graph convolution operators in the spectral view. However, our linearizationbased approach has two notable advantages. First, linearization is generally applicable even to GNN models whose feature transformation and propagation are mixed. Second, linearization gives a clear view on how the features interact with the graph structure and make predictions. To the best of our knowledge, linearization-based comparison between GNNs has not been studied in literature. 3 PROPOSED FRAMEWORK: GNNLIN Propagator Functions ... Figure 2: Linearization through GNNLIN. We propose GNNLIN, a framework that generalizes existing linear graph neural network (GNN) and provides a way of systematic linearization for existing GNN models (see Figure 2). Our observations from linearization motivate us to propose SlenderGNN, a new GNN model elaborated in Section 4. 3.1 DEFINITION OF GNNLIN: Definition 1 (LR). Given a feature X \u2208 Rn\u00d7d and a label y \u2208 Rm, where m \u2264 n is the number of observations, let Y \u2208 Rm\u00d7c be the one-hot representation of y, and yij be the (i, j)-th element in Y. Then, logistic regression (LR) finds an optimal weight matrix W\u2217 \u2208 Rd\u00d7c as follows: LR(X,y) = argmax W l\u2211 i=1 c\u2211 j=1 yij log y\u0302ij where y\u0302ij = exp(w\u22a4\u00b7jxi)\u2211c k=1 exp(w \u22a4 \u00b7kxi) , (1) and w\u00b7j is the j-th column of W. We omit the bias term for brevity without loss of generality. Definition 2 (Linearized GNN). Given a graph G = (A,X), let f(\u00b7; \u03b8) be a node classifier function to predict the labels of all nodes in G as y\u0302 = f(A,X; \u03b8), where \u03b8 is the set of learnable parameters. Then, f is linearized if \u03b8 = {W} and an optimal weight matrix W\u2217 \u2208 Rh\u00d7c is given as W\u2217 = LR(P(A,X),y), (2) where P is a feature propagator function that is linear with X and contains no learnable parameters, and P(A,X) \u2208 Rn\u00d7h. We ignore the bias term for brevity without loss of generality. Definition 3 (GNNLIN). Let f(\u00b7; \u03b8) be a (nonlinear) GNN. GNNLIN is to represent f as a linearized GNN by replacing all (nonlinear) activation functions in f with the identity function and deriving a variant f \u2032 that is at least as expressive as f but contains no parameters in P . GNNLIN represents the characteristic of a GNN as the linear feature propagation function P , which transforms raw features X by utilizing A. Lemma 1 shows that GNNLIN generalizes existing linear GNNs. Logistic regression is also represented by GNNLIN with the identity P(A,X) = X. Lemma 1. Existing linear models such as SGC (Wu et al., 2019), DGC (Wang et al., 2021), S2GC (Zhu & Koniusz, 2021), and G2CN (Li et al., 2022) are represented by GNNLIN. Proof. The proof is given in Appendix A.1. \u25a0 The first few rows of Table 1 show the representation of linear models as GNNLIN. DGC and S2GC improve SGC by controlling the strength of self-loops with new hyperparameters T and \u03b1. G2CN is the latest linear model that achieves highest accuracy, but has the most complex propagator function with many hyperparameters K, N , Ti, and bi. Such an increase of complexity limits the robustness and interpretability of decisions, which are the main advantages of linear models. 3.2 LINEARIZATION OF EXISTING MODELS: We apply GNNLIN to represent existing GNNs in the linearized forms in Table 1, which allows us to understand and compare all linear and nonlinear GNNs in the form of propagator functions. Decoupled models PPNP, APPNP (Klicpera et al., 2019a), GDC (Klicpera et al., 2019b), and GPR-GNN (Chien et al., 2021) are decoupled GNNs that separate feature transformation and propagation stages. PPNP runs Personalized PageRank on the node features, and APPNP approximates PPNP with K steps of message propagation. GDC generalizes APPNP by increasing the value of K to \u221e and sparsifies the propagator matrix S. GPR-GNN also generalizes APPNP to avoid the usage of \u03b1 by learning a weight for each component, resulting in the concatenation of multiple different features. We provide details of linearization in Appendix A.2. Coupled models We linearize coupled GNNs including ChebNet (Defferrard et al., 2016), GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), and GCNII (Chen et al., 2020). The linearized version of GCN is the same as SGC, since the motivation of SGC is to linearize GCN for better scalability and robustness. Although their motivations are different, the linearized versions of ChebNet, GraphSAGE, and GCNII are similar to linearized GPR-GNN in that features propagated by different steps are combined by concatenation. We provide the details in Appendix A.3. Attention models Attention-based GNNs (Velickovic et al., 2018; Kim & Oh, 2021; Brody et al., 2022) are a popular category of GNNs to learn the importance of each edge based on X and learnable parameters. Thus, it is not straightforward to linearize them by the GNNLIN framework: even if we assume the learnable parameters in P as fixed hyperparameters, P is at least quadratic with X, since X participates in computing the new adjacency matrix that is multiplied again with X. Nevertheless, we perform incomplete linearization of attention-based models for completeness, and represent the results with the \u2018**\u2019 symbol in Table 1. Detailed processes are given in Appendix A.4. The linearized version of GAT uses a different adjacency matrix for each step k, which is the main difference from other linearized models. It is noteworthy that DA-GNN (Liu et al., 2020), proposed as a decoupled model in their paper, is an attention-based model in our analysis. This is because X participates in computing the new adjacency matrix, making P quadratic as in GAT. 3.3 DISTINGUISHING FACTORS AND PAIN POINTS: Table 1 shows that many existing GNNs have similar or even identical propagator functions after we linearize them. We present the main factors that make a difference between those models and detect pain points that none of the existing models in Table 1 addresses well. Distinguishing Factor 1 (Combination of features). How should we combine the node features, the immediate neighbors\u2019 features, and the k-step-away neighbors\u2019 features? GNNs propagate information by multiplying the feature X with (a variant of) the adjacency matrix A multiple times. Three approaches prevail in Table 1: (1) no aggregation (SGC, DGC, G2CN, and GAT), (2) summation (S2GC, APPNP, and GDC), and (3) concatenation (GPR-GNN, GraphSAGE, and GCNII). The simplest approach is to use only the one with K steps (i.e. no aggregation), while the other two approaches aim to combine the information of multiple steps. Distinguishing Factor 2 (Modification of adjacency matrices). How should we normalize or modify the adjacency matrix to better represent the connectivity information? The three prevailing choices are (1) symmetric vs. row normalization, (2) the strength of self-loops, including making zero self-loops, and (3) static vs. dynamic adjustment based on the given features. Most models use the symmetric normalization A\u0303sym with self-loops, but some variants avoid selfloops and use either row normalization Arow or symmetric one Asym. Recent models such as DGC, G2CN, and GCNII determine the weight of self-loops with hyperparameters, since strong self-loops allow one to increase the value of K for distant propagation. Finally, attention-based models learn the elements in A based on node features, making propagator functions quadratic with X. Distinguishing Factor 3 (Dealing with heterophily graphs). How should we consider heterophily if the direct neighbors are likely to have the information of different labels? In the case of heterophily, using the features of immediate neighbors can actually hurt performance, and several GNN models do suffer under heterophily. Observed ways to handle it are (1) using the square of A as the main unit, (2) learning different weights for different steps, and (3) making small or no self-loops in A. The idea of such approaches is to avoid or downplay the effect of immediate (and odd-step-away) neighbors. Self-loops hurt under heterophily, as they force to have information of all intermediate neighbors by acting as implicit summation of transformed features. Pain Point 1 (Hyperparameters in propagators). The hyperparameters in a propagator function P impairs the interpretability of the weight matrix W, and forces one to re-compute the transformed feature for every new choice during hyperparameter search. The last column of Table 1 summarizes the hyperparameters in each P , which make the following limitations in terms of linear models. First, the interpretability of the weight matrix W is impaired, because it is learned on the transformed feature P(A,X) whose meaning changes arbitrarily by the choice of hyperparameters. Second, P(A,X) should be computed for each choice of hyperparameters, while it can be easily cached for searching hyperparameters outside P . Pain Point 2 (Failure on noisy features). All models in Table 1 depend on the node feature matrix X, and cannot fully exploit the adjacency matrix A if the given features are noisy. Real-world datasets often contain noisy features, and the graph structure A plays an essential role in the performance of node classification in such cases. That is, a desirable property for a robust model is to adaptively emphasize important features or disregard noisy ones to maximize its generalization performance. However, the models in Table 1 lack such a functionality. 4 PROPOSED METHOD - SlenderGNN Based on the insights from the comprehensive linearization, we propose SlenderGNN, a novel linear GNN that addresses the limitations of existing GNN models with a simple propagator function. The propagator function P of SlenderGNN is given as follows: P(A,X) = U\ufe38\ufe37\ufe37\ufe38 Structure \u2225 g(X)\ufe38 \ufe37\ufe37 \ufe38 Node features \u2225 g(A2rowX)\ufe38 \ufe37\ufe37 \ufe38 2-step neighbors \u2225 g(A\u03032symX)\ufe38 \ufe37\ufe37 \ufe38 Neighbors (3) where g(\u00b7) is the principal component analysis (PCA) for the orthogonalization of each component, followed by an L2 normalization, and U \u2208 Rn\u00d7r contains r-dimensional structural features derived by running the low-rank support vector decomposition (SVD) on the matrix A. The goals of SlenderGNN are to maximize (a) the robustness to graphs of various properties and (b) the interpretability of learned weights. We emphasize robustness, since in semi-supervised learning, high accuracy on real-world datasets is acquired naturally from robustness. We introduce the design decisions D1-D5, that lead us to propose Equation 3 as our approach to achieve our goals. D1 is to achieve both goals, D2 and D3 are for robustness, and D4 and D5 are designed for interpretability. In response to Pain Point 1, Equation 3 is designed to contain no hyperparameters to tune. D1: Concatenation-based linear method The main principle of SlenderGNN is to make linear decisions to acquire robustness and interpretability at the same time. Recent works on linear GNNs improve the limited expressiveness of linear decisions by designing more complex P . On the other hand, Table 1 suggests that many GNNs concatenate the features of heterogeneous information, and it leads to combining rich information of a graph structure even remaining the low complexity of P . The concatenation also allows us to ignore meaningless components in the transformed features and to interpret the learned weights as the importances of given components in each dataset. D2: Structural features (for robustness) In response to Pain Point 2 where features are missing, noisy, or useless for classification, then we have to resort to the adjacency matrix A ignoring X. At the same time, it is not effective to use raw A in P , considering that it is a large sparse matrix. We thus adopt low-rank SVD with rank r to extract structural features U. The value of r is automatically selected to keep 90% of the energy of A, where the sum of the largest r squared singular values divided by the squared Frobenius norm of A is approximately 0.9. When the graph is large, we set r to be d for the size consistency of components (see D5). D3: Combining different normalizations (for robustness) The symmetric normalization A\u0303sym with self-loops is known to perform well in many cases, but has two limitations. First, the self-loops in A\u0303Ksym force one to consider all intermediate nodes until the K-hop neighbors, even in heterophily graphs where the direct neighbors should be avoided. Second, the neighboring features are rescaled based on the node degrees during an aggregation. As a solution, we combine different normalization methods in feature transformation: A\u0303sym and Arow, where the row-normalized matrix Arow does not include self-loops. Arow effectively summarizes the information of K-hop neighbors preserving the scales of features, acquiring better signals on heterophily graphs. D4: Keeping the low power (for interpretability) Existing linear GNNs use a very large value of K \u2265 50, which determines the power of a (transformed) adjacency matrix A. This is effective in distant propagation of information, but severely limits the interpretability of the learned weight W: for example, one cannot tell where the information comes in a graph when features are transformed with A50X. Therefore, we propose to keep the small value of K = 2 for the interpretability, while taking the information of two-hop neighbors is essential for many cases. D5: Orthogonalization and sparsification (for interpretability) We use two reliable methods to improve the interpretability of SlenderGNN: dimensionality reduction by PCA and regularization by group LASSO. First, we run PCA on each component independently to orthogonalize given features and to improve the consistency of learned weights. Second, we apply group LASSO to learn sparse weights on the component level, while preserving the relative magnitude of each element. To make the consistency between components, we force all components to have the same dimensionality by selecting r features from each component when adopting PCA. 5 PROPOSED SANITY CHECKS ON POSSIBLE GRAPH SCENARIOS: We propose sanity checks to evaluate the robustness of GNNs to various scenarios of node classification and to observe their strengths and weaknesses in different settings. Table 2: SlenderGNN passes all sanity checks. Accuracy of all models on sanity checks; three groups of scenarios: (left) only features X are informative; (middle) only connectivity A is informative; (right) both are informative. The three letters at the second row represent the cases of X, A, and y, respectively: S (semantic X), R (random X), T (structural X), U (uniform A), C (clustered A), B (bipartite A), I (individual y), O (homophily y), and E (heterophily y). The colors , , represent the top three methods within 2\u03c3 of the best accuracy at each setting, and represents accuracy below 2\u03c3 of the third-best accuracy. Model MI(X,y) > 0, i.e., X helps MI(A,y) > 0 MI(X,A,y) > 0, i.e., both X and A help (S, U, I) (S, C, I) (S, B, I) (R, C, O) (R, B, E) (T, C, O) (T, B, E) (S, C, O) (S, B, E) Uniform Uniform Uniform Homophily Heterophily Homophily Heterophily Homophily Heterophily LR 83.7\u00b10.6 83.7\u00b10.6 83.7\u00b10.6 24.2\u00b10.7 24.2\u00b10.7 71.4\u00b10.9 66.8\u00b12.2 83.4\u00b10.6 83.4\u00b10.6 SGC 44.6\u00b19.8 43.0\u00b19.0 45.1\u00b110. 64.3\u00b10.7 50.2\u00b114. 87.1\u00b10.6 84.3\u00b10.5 93.9\u00b10.9 91.5\u00b10.5 DGC 63.8\u00b11.0 66.3\u00b10.9 65.9\u00b11.2 50.5\u00b113. 26.0\u00b10.9 88.6\u00b11.0 45.3\u00b11.3 96.2\u00b10.4 54.0\u00b10.6 S2GC 79.9\u00b10.6 79.8\u00b11.1 80.1\u00b11.0 38.5\u00b112. 25.4\u00b10.9 88.4\u00b11.0 67.9\u00b11.5 95.9\u00b10.6 78.0\u00b10.5 G2CN 25.2\u00b10.3 25.3\u00b10.1 25.0\u00b10.2 24.2\u00b11.1 25.0\u00b10.1 88.5\u00b11.0 88.6\u00b11.2 24.3\u00b11.1 50.7\u00b131. GCN 36.3\u00b13.5 33.2\u00b12.4 35.7\u00b13.5 46.7\u00b18.0 43.7\u00b11.9 83.3\u00b11.3 72.2\u00b11.7 91.2\u00b11.2 80.3\u00b13.9 SAGE 80.3\u00b11.1 79.0\u00b11.2 79.8\u00b10.7 31.1\u00b10.7 34.6\u00b12.1 83.9\u00b10.8 81.3\u00b10.7 94.4\u00b10.5 94.4\u00b10.9 GCNII 73.5\u00b11.2 73.2\u00b10.9 73.6\u00b10.9 30.7\u00b10.7 27.1\u00b11.3 84.2\u00b10.8 69.0\u00b11.4 90.6\u00b10.9 80.4\u00b11.2 APPNP 66.0\u00b12.6 65.4\u00b12.7 64.2\u00b11.6 30.3\u00b11.2 25.2\u00b10.7 71.2\u00b14.9 43.8\u00b12.0 83.2\u00b13.8 58.7\u00b14.5 GPR-GNN 73.4\u00b10.4 73.5\u00b11.3 73.9\u00b10.7 74.6\u00b10.7 65.9\u00b12.1 89.9\u00b10.6 87.6\u00b11.2 95.0\u00b11.1 91.9\u00b11.1 GAT 32.7\u00b15.5 30.6\u00b13.0 33.8\u00b16.8 42.6\u00b14.8 36.8\u00b15.7 64.0\u00b15.7 55.6\u00b16.8 68.5\u00b17.1 67.0\u00b112. SlenderGNN 81.0\u00b11.1 81.0\u00b10.9 81.2\u00b11.0 87.1\u00b11.4 89.2\u00b11.2 88.1\u00b10.5 88.9\u00b10.7 94.4\u00b10.6 93.9\u00b10.5 Graph scenarios We categorize possible scenarios of node classification based on the characteristics of node features X, a graph structure A, and node labels y. We describe only the main ideas, leaving exact definitions of such scenarios to Appendix B.1. \u2022 Edges A: We consider three cases: uniform (no communities), clustered (block-diagonal), and bipartite. The uniform case means that every element aij is determined independently of the other edges in A. In the clustered case, nodes having common neighbors are likely to make more edges, while it is the opposite in the bipartite case. \u2022 Labels y: We consider three cases: individual (no network effects), homophily, and heterophily. In the homophily case, adjacent nodes are likely to have the same label, while it is the opposite in the heterophily case. \u2022 Features X: We consider three cases: random, structural, and semantic. The random case means that each feature is determined independently of all other variables. In the structural case, features give information of the graph structure, and in the semantic case, they directly provide useful information for node labels. Unlike the cases of edges and labels, which are mutually exclusive, features can be both structural and semantic at the same time. Feasibility Although there exist a total of 27 combinations for (X,A,y), not all of them are possible to implement; for example, either homophily or heterophily y is not compatible with uniform A. After removing the infeasible combinations of variables, we categorize the remaining choices based on the mutual information (MI) between variables. Implementation For generating synthetic graphs, we divide all nodes into c clusters, where c = 4 in our experiments, and determine the edge densities in each cluster and between different clusters based on the property of A. Details on implementation are described in Appendix B.2. Results of sanity checks Table 2 shows the results of sanity checks for our SlenderGNN and all baseline models (details of the baselines in Section 6). We run each experiment five times and report the average and standard deviation. We assume four target classes of nodes, and thus the accuracy of random guessing is 25%. Logistic regression (LR) fails when MI(X,y) = 0, since it uses only X for classification. LR shows relatively lower accuracy with structural features, since they are not a perfect indicator for node labels, and they need to be refined through A for better performance. It is clear that our SlenderGNN is the only approach that passes all sanity checks. The four components in SlenderGNN are carefully designed to maximize its robustness for various graph scenarios. Most GNNs work well in the cases (S, C, O) and (T, C, O), where X is either semantic or structural, A is clustered, and y is homophily, since many real-world datasets that recent works on GNNs use in their experiments follow such assumptions. However, many GNNs fail when a graph is generated with different assumptions, as we summarize as follows: \u2022 No network effects: In the cases of (S, ?, I), where ? is a placeholder, only a few models such as SAGE and GPR-GNN perform well. This is because MI(A,y) = 0 in such cases, and models are required to focus on raw features X ignoring A. \u2022 Meaningless features: In the cases of (R, C, O) and (R, B, E), models are required to do the opposite since MI(X,y) = 0: models should focus on A, ignoring X. Since there is no approach that explicitly uses A, all baselines show low accuracy. \u2022 Heterophily graphs: In the cases of (?, B, E), where ? is a placeholder, the labels follow heterophily. Models like G2CN, SAGE, and GPR-GNN perform well in these cases, since they address the heterophily in their designs (details in Section 3.3). 6 EXPERIMENTS:  6.1 EXPERIMENTAL SETTINGS: Datasets We use 5 homophily and 4 heterophily datasets, which were used in previous works on node classification (Chien et al., 2021; Pei et al., 2020). Cora, CiteSeer, and PubMed (Sen et al., 2008; Yang et al., 2016) are homophily citation graphs between research articles. Computers and Photo (Shchur et al., 2018) are homophily Amazon co-purchase graphs between items. Chameleon and Squirrel are heterophily Wikipedia graphs (Rozemberczki et al., 2021), connected by links between pages. Actor (Tang et al., 2009) is a heterophily graph connected by co-occurrence of actors on the same Wikipedia pages. Penn94 (Traud et al., 2012) is a heterophily graph of gender relations in a social network. We make the heterophily graphs undirected, as done in (Chien et al., 2021). Competitors We compare our SlenderGNN with various types of graph neural networks (GNN). Logistic regression (LR) is the simplest baseline, which does not use graph information. SGC (Wu et al., 2019), DGC (Wang et al., 2021), S2GC (Zhu & Koniusz, 2021), and G2CN (Li et al., 2022) are linear GNNs that perform as our main competitors. GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), GCNII (Chen et al., 2020) are coupled GNN models popular in literature. APPNP (Klicpera et al., 2019a) and GPR-GNN (Chien et al., 2021) are decoupled GNNs, and GAT (Velickovic et al., 2018) is an attention-based model. We perform hyperparameter search based on those reported in their original papers. Refer to Appendix C for details. Experimental setup We perform semi-supervised node classification by dividing all nodes in a graph by the 2.5%/2.5%/95% ratio into training, validation, and testing data. We perform five runs of each experiment with different random seeds and report the average and standard deviation. All hyperparameter search and early stopping are done based on validation accuracy for each run. 6.2 EXPERIMENTAL RESULTS: Accuracy on real-world graphs In Table 3, SlenderGNN is compared against linear as well as the state-of-the-art nonlinear GNNs on 9 real-world datasets (5 homophily and 4 heterophily graphs). We report the accuracy in Table 3 where , , represent top three methods within 2\u03c3 of the best method, and represents accuracy below 2\u03c3 of the third-best method. Our proposed SlenderGNN outperforms all the competitors in 2 homophily and 3 heterophily datasets, and shows competitive results in the rest (7 out of 9 times among top 3 methods). Moreover, it is the only model without red cells, which demonstrates the robustness and generality of SlenderGNN. SlenderGNN is also slender, where we report the number of parameters in Table 6 in the appendix. Compared with linear models, SlenderGNN achieves the best trade-off between the model size and two benefits, namely accuracy and interpretability; compared with nonlinear models, it has much fewer parameters, while performing no worse than them. Ablation study We report the ablation study in Table 4, by removing each of the essential modules of SlenderGNN. The results show that SlenderGNN is most robust and effective with all the modules. Interpretability of learned weights As shown in Figure 1c, we illustrate the learned weights of SlenderGNN in our proposed \u2018sanity checks\u2019, where the ground truths are known. In the graphs without network effects, SlenderGNN successfully assigns the highest weights to the node features Model Cora CiteSeer PubMed Computers Photo Chameleon Squirrel Actor Penn94:  Model Cora CiteSeer PubMed Computers Photo Chameleon Squirrel Actor Penn94: g(X). When the features are meaningless, it ignores the features and puts most of the attention on the extracted structural features U. In the cases that structural features are useful, but not enough to correctly identify the labels, it gives the highest weights to the propagated features g(A\u03032symX) and g(A2rowX) in homophily and heterophily graphs, respectively, perfectly fitting our design choices. For the cases that the features themselves are already good for the classification, SlenderGNN gives both the node and propagated features large weights to achieve higher accuracy. 7 CONCLUSION: The main contribution of this work is SlenderGNN, which has a long list of desirable properties: \u2022 Accurate: On both real-world and synthetic datasets, SlenderGNN exceeds the accuracy of state-of-the-art linear GNNs and it matches or exceeds the accuracy of nonlinear GNNs with 8\u00d7 \u2013 64\u00d7 fewer parameters. \u2022 Robust: SlenderGNN can handle the case of homophily, heterophily, and no-network-effect (randomly connected) graphs, as well as the case of no meaningful features. \u2022 Slender: SlenderGNN has few, but useful parameters. \u2022 Interpretable: SlenderGNN can easily justify its decisions (see Figure 1c). Additional contributions include: \u2022 The GNNLIN framework, that illuminates the fundamental similarities and differences of popular GNN variants (see Table 1). \u2022 The \u2018sanity checks\u2019, which highlight the strengths and weaknesses of each GNN method. Reproducibility: Our source code and \u2018sanity checks\u2019 are available at https://bit.ly/3fhWJfK. A LINEARIZATION PROCESSES: We present detailed processes to linearize various graph neural networks (GNN). Refer to Section 2 of the main paper for frequently used symbols. Linearization is done in two steps. First, we replace all activation functions with the identity function from the original definition of each GNN. Second, if the resulting layer function contains learnable parameters, we devise a replacement that is at least as expressive as the given function but containing no learnable parameters. This is because our goal of linearization is not just deriving a linear function with respect to X, but understanding GNNs in relation to logistic regression based on our GNNLIN framework. We ignore the bias terms of linear layers for simplicity, without loss of generality. A.1 PROOF OF LEMMA 1: REPRESENTING LINEAR MODELS WITH GNNLIN: We prove Lemma 1 by representing each linear GNN with GNNLIN. Let K \u2265 0 be a hyperparameter that determines the number of propagation steps in every GNN. A.1.1 SGC (WU ET AL., 2019) SGC directly fits the definition of linearization with the following propagator function: P(A,X) = A\u0303KsymX. (4) A.1.2 DGC (WANG ET AL., 2021) DGC has two variants, DGC-Euler and DGC-DK, which have different propagator functions. We focus on DGC-Euler, which is used as the main model in their experiments. DGC is similar to SGC, except that it controls the strength of self-loops as follows: P(A,X) = [(1\u2212 T/K)I+ (T/K)A\u0303sym]KX, (5) where T > 0 is a hyperparameter. The self-loops become stronger if T is closer to 0. A.1.3 S2GC (ZHU & KONIUSZ, 2021) S2GC computes the summation of features propagated with different numbers of steps: P(A,X) = K\u2211 k=1 (\u03b1I+ (1\u2212 \u03b1)A\u0303ksym)X. (6) The original formulation divides the added features by K, which can be safely ignored considering that the weight matrix W is multiplied to the transformed feature for classification. A.1.4 G2CN (LI ET AL., 2022) Since G2CN does not provide an explicit formulation of the propagator function, we need to derive it. First, the parameterized version P \u2032 of the propagator function is given as follows: P \u2032(A,X; {\u03b8i}Ni=1) = N\u2211 i=1 \u03b8iHi,K , (7) where \u03b8i is a learnable parameter. The k-th feature representation Hi,k is recursively defined as Hi,k = Hi,k\u22121 \u2212 Ti K (L\u2212 biI)2Hi,k\u22121 = Hi,k\u22121 \u2212 Ti K ((bi \u2212 1)I+Asym)2Hi,k\u22121 = [I\u2212 Ti K ((bi \u2212 1)I+Asym)2]Hi,k\u22121 Hi,0 = X, (8) where N , Ti, and bi are hyperparameters, and L = I \u2212 Asym is the normalized Laplacian matrix. Since the transformed features are combined with a learnable parameter \u03b8i in Equation 7, we make a propagator function P that contains no learnable parameters as follows: P(A,X) = N \u2225 i=1 Hi,K = N \u2225 i=1 [I\u2212 Ti K ((bi \u2212 1)I+Asym)2]KX. (9) A.2 LINEARIZATION OF DECOUPLED GNNS: The propagation in decoupled GNNs is done on the abstract representations of node features, which are typically generated by multilayer perceptrons (MLP). Linearization starts with replacing MLPs with linear projections, and removes additional nonlinearity in the process of propagation. A.2.1 PPNP (KLICPERA ET AL., 2019A): The lineraization of PPNP is straightforward, since the authors present a closed-form representation of the propagator function. If we remove the activation function, we have the following: P(A,X) = (In \u2212 (1\u2212 \u03b1)A\u0303sym)\u22121X, (10) where 0 < \u03b1 < 1 is a hyperparameter that controls the weight of self-loops. A.2.2 APPNP (KLICPERA ET AL., 2019A): We assume that the initial node representation is created by a single linear layer of XW, where W is a weight matrix. Then, the k-th representation matrix Hk is represented as follows: Hk = (1\u2212 \u03b1)A\u0303symHk\u22121 + \u03b1XW, (11) where 0 < \u03b1 < 1 is a hyperparameter. The closed-form representation of HK is given as follows: HK = ( [(1\u2212 \u03b1)A\u0303sym]K + \u03b1 K\u22121\u2211 k=0 [(1\u2212 \u03b1)A\u0303sym]k ) XW. (12) We safely remove the weight matrix W, which is redundant, and get the fiinal representation: P(A,X) = [ K\u22121\u2211 k=0 \u03b1(1\u2212 \u03b1)kA\u0303ksym + (1\u2212 \u03b1)KA\u0303Ksym]X (13) A.2.3 GDC (KLICPERA ET AL., 2019B) GDC generalizes APPNP and presents various forms of the propagation function. We pick the most representative one given in the paper, which is directly related to APPNP. The unnormalized version of the propagation matrix S\u2032 is given as follows: S\u2032 = \u221e\u2211 k=0 \u03b1(1\u2212 \u03b1)kA\u0303ksym, (14) and then it is normalized and sparsified as S = sparsify(S\u0303\u2032sym). (15) S\u0303\u2032sym represents adding self-loops and applying the symmetric normalization to S \u2032. The paper gives two approaches for sparsification, which are a) removing elements smaller than \u03f5, which is given as a hyperparameter, and b) selecting the top k neighbors for each node. With any choice, the function P is simply given as follows: P(A,X) = SX. (16) A.2.4 GPR-GNN (CHIEN ET AL., 2021) We assume that the initial node representation is created by a single linear layer of XW, where W is a weight matrix. Then, we have the following propagator function P \u2032 with parameters: P \u2032(A,X; {\u03b8k}Kk=0) = K\u2211 k=0 \u03b8kA\u0303 k symX, (17) where \u03b8k is a parameter that is learned together with W. We replace the summation with concatenation to remove the learnable parameters from P \u2032 and get the following: P(A,X) = K \u2225 k=0 A\u0303ksymX. (18) A.3 LINEARIZATION OF COUPLED GNNS:  A.3.1 CHEBNET (DEFFERRARD ET AL., 2016): Let Hk be the k-th node representation matrix, and L = I \u2212 Asym be the graph Laplacian matrix normalized symmetrically. Then, the propagator function of ChebNet with parameters \u03b8 is P \u2032(A,X; \u03b8) = K\u22121\u2211 k=0 \u03b8kHk, (19) where the recurrence relation is given as follows with the intial terms: H0 = X H1 = LX = \u2212AsymX+X \u00b7 \u00b7 \u00b7 Hk = 2(L\u2212 I)Hk\u22121 \u2212Hk\u22122 = \u22122AsymHk\u22121 \u2212Hk\u22122. (20) Based on the recurrence relation, the closed-form representation of Hk is given as Hk = akA k symX+ ak\u22121A k\u22121 symX+ \u00b7 \u00b7 \u00b7+ a0X, (21) where a0, \u00b7 \u00b7 \u00b7 , ak are constants. Since we have K free parameters \u03b80, \u00b7 \u00b7 \u00b7 , \u03b8K corresponding to the K terms in the representation matrix HK , we safely rewrite the propagator function as P \u2032(A,X; \u03b8) = K\u22121\u2211 k=0 \u03b8kA k symX. (22) Each value of k has a free parameter \u03b8k. Thus, we generalize it as follows: P(A,X) = K\u22121 \u2225 k=0 AksymX. (23) A.3.2 GRAPHSAGE (HAMILTON ET AL., 2017): We assume the mean aggregator of GraphSAGE. By replacing the activation function as the identity function, each layer F of GraphSAGE is linearized as follows: F(X) = XW1 +ArowXW2, (24) where Arow = D\u22121A represents the mean operator in the aggregation, and W1 and W2 are learnable weight matrices in the layer. If we apply a chain of two layers, where the weight matrices of the second layer are represented as W3 and W4, we get the following: F(F(X)) = XW1W3 +ArowX(W1W2 +W2W3) +A2rowXW2W4 (25) = XWa +ArowXWb +A 2 rowXWc, (26) where we redefine the weight matrices without loss of generality as Wa = W1W3 (27) Wb = W1W2 +W2W3 (28) Wc = W2W4. (29) If we generalize it into K layers, we get the following: FK(X) = K\u2211 k=1 AkrowXWk. (30) Note that a different weight matrix Wk is applied to each layer k. This is equivalent to concatenating the transformed features of all layers and learning a single large weight matrix in training. A.3.3 GCNII (CHEN ET AL., 2020) After replacing the activation function with the identity function, the l-th layer Fl of GCNII is given as follows: Fl(H) = ((1\u2212 \u03b1l)A\u0303symH+ \u03b1lX))((1\u2212 \u03b2l)I+ \u03b2lWl), (31) where \u03b1l and \u03b2l are hyperparameters, and Wl is a weight matrix. The second term is equivalent to Wl regardless of the value of \u03b2l, since Wl is a free parameter. We also set \u03b1l to a constant \u03b1 which is the same for every layer l, following the original paper (Chen et al., 2020).1 Then, the equation is simplified as Fl(H) = ((1\u2212 \u03b1)A\u0303symH+ \u03b1X)Wl. (32) If we apply a chain of two layers l and l + 1, we get the following: Fl+1(Fl(H)) = ((1\u2212 \u03b1)A\u0303sym((1\u2212 \u03b1)A\u0303symH+ \u03b1X)Wl + \u03b1X)Wl+1 (33) = (1\u2212 \u03b1)2A\u03032symHWlWl+1 + \u03b1(1\u2212 \u03b1)A\u0303symXWlWl+1 + \u03b1XWl+1 (34) = (1\u2212 \u03b1)2A\u03032symHW\u2032l + \u03b1(1\u2212 \u03b1)A\u0303symXW\u2032l + \u03b1XWl+1, (35) where W\u2032l = WlWl+1, which is also a free parameter. If we generalize it into K layers, we get the following: FK(X) = (1\u2212 \u03b1)K\u22121((1\u2212 \u03b1)A\u0303Ksym + \u03b1A\u0303K\u22121sym )XWK + \u03b1 K\u22122\u2211 k=0 (1\u2212 \u03b1)kA\u0303ksymXWk. (36) We safely remove the constant from each term, which can be included in the weight matrix: FK(X) = ((1\u2212 \u03b1)A\u0303Ksym + \u03b1A\u0303K\u22121sym )XWK + K\u22122\u2211 k=0 A\u0303ksymXWk. (37) We replace the summation operators between terms having different weight matrices with concatenation operators, having the final propagator function P as follows: P(A,X) = K\u22122 \u2225 k=0 A\u0303ksymX \u2225 ((1\u2212 \u03b1)A\u0303Ksym + \u03b1A\u0303K\u22121sym )X (38) A.4 LINEARIZATION OF ATTENTION GNNS: A.4.1 DA-GNN (LIU ET AL., 2020) We assume that the initial node representation is created by a single linear layer of XW, where W is a weight matrix. Then, the k-th representation matrix Hk is represented as follows: Hk = A\u0303 k symXW. (39) 1\u03b1 is set to 0.1 in the original paper of GCNII (Chen et al., 2020). DA-GNN computes the weighted sum of representations for all k \u2208 [0,K], where the weight values are determined also from the representation matrices: P(A,X) = K\u2211 k=0 diag(Hks)Hk = K\u2211 k=0 diag(A\u0303ksymXWs)A\u0303 k symXW. (40) where s is a learnable weight vector. We safely remove the last W and rewrite Ws as w. Then, we have the final representation of the propagator function: P(A,X) = K\u2211 k=0 diag(A\u0303ksymXw)A\u0303 k symX. (41) A.4.2 GAT (VELICKOVIC ET AL., 2018): We apply the following changes to linearize GAT, whose linearization is not straightforward due to the nonlinearity in the attention function: 1. We replace the activation functions between layers with the identity functions. 2. We simplify the attention function \u03b1ij = exp(eij)/ \u2211 k exp(eik) as eij . 3. We remove the LeaklyReLU function in the computation of eij . 4. We assume the single-head attention. The edge weight eij , which is the (i, j)-th element of the propagator matrix, is defined as follows: eij = a \u22a4 dst(W \u22a4xi) + a \u22a4 src(W \u22a4xj), (42) where xi and xj are feature vectors of length d for node i and j, respectively, W is a d\u00d7 c learnable weight matrix, and adst and asrc are learnable weight vectors of length c. Then, we derive the initial form of a linearized GAT layer as follows: H = [diag(XWadst)A\u0303+ A\u0303diag(XWasrc)]XW. (43) Since all adst, asrc, and W are free parameters, we generalize it as follows: H = [diag(Xwdst)A\u0303+ A\u0303diag(Xwsrc)]XW, (44) where wdst and wsrc are learnable vectors of length m that replace adst and asrc, respectively. B DETAILS ON SANITY CHECKS:  B.1 FORMAL DEFINITIONS OF GRAPH SCENARIOS: We categorize all possible scenarios of node classification based on the characteristics of node features X, a graph structure A, and node labels y. We denote by Aij and Yi the random variables for edge (i, j) between nodes i and j and label yi of node i, respectively. Edges For the adjacency matrix A, we consider the following three cases: \u2022 Uniform: P (Aki = 1 | Aij = Ajk = 1) = P (Aki = 1) \u2022 Clustered: P (Aki = 1 | Aij = Ajk = 1) > P (Aki = 1) \u2022 Bipartite: P (Aki = 1 | Aij = Ajk = 1) < P (Aki = 1) The uniform case represents that every edge is determined independently of the others, and the graph structure gives no useful information for node classification. In the clustered case, it is more likely that nodes having common neighbors make more connections in a graph. In the bipartite case, nodes make more connections with those sharing no common neighbors. Labels For the labels in y, we consider the following three cases: \u2022 Individual: P (Yi = y | Aij = 1, Yj = y) = P (Yi = y) \u2022 Homophily: P (Yi = y | Aij = 1, Yj = y) > P (Yi = y) \u2022 Heterophily: P (Yi = y | Aij = 1, Yj = y) < P (Yi = y) The individual case means that the label of a node is independent of the labels of its neighbors. This is the case when the graph structure works as noise information with regard to classification. In the homophily case, adjacent nodes are likely to have the same label. It is the most popular assumption of GNNs for node classification. In the heterophily case, adjacent nodes are likely to have different labels, which is not as common as homophily but often observed in real-world graphs. Features For the feature matrix X, we consider the following three cases based on A and y. We use the notation of p(\u00b7) since the features are typically modeled as continuous variables. \u2022 Random: p(xi,xj | yi, yj , aij) = p(xi,xj) \u2022 Structural: p(xi,xj | yi, yj , aij) \u0338= p(xi,xj | yi, yj) \u2022 Semantic: p(xi,xj | yi, yj , aij) \u0338= p(xi,xj | aij) The random case represents that features are determined independently of the graph and labels. In this case, node features do not give useful information for classification, but work as unique indices of nodes like one-hot embeddings if the dimensionality d of features is large. In the structural case, features are correlated with the graph structure, but not directly with labels. Such features give useful information for classification only if the graph structure and labels are related. In the semantic case, features directly provide useful information for predicting labels. Note that features can be semantic and structural at the same time by satisfying both of the conditions. B.2 IMPLEMENTATION OF GRAPH SCENARIOS There are various ways to generate synthetic graphs satisfying the definitions of different scenarios. One can use a synthetic graph generator designed to create more plausible graphs (Leskovec et al., 2010; Baraba\u0301si & Albert, 1999), but we choose the simplest one to focus on the mutual information, rather than the other characteristics of synthetic graphs such as the degree distribution. Structure We assume that the number of structural clusters is the same as the number c of labels for the alignment with label information. We divide all nodes into c groups and then decide the edge densities for intra- and inter-connections of groups based on the structural type: uniform, clustered, and bipartite. We use a hyperparameter \u03f5a to determine the noise level in the case of homophily or heterophily: for example, if \u03f5a = 0, the graph has a full block-diagonal adjacency matrix in the case of homophily. The expected number of edges is the same for all three cases, and we set \u03f5a = 0 in all of our experiments. A notable characteristic of a bipartite structure A is that A2 has a clustered structure. If we create inter-group connections for every possible pair of different groups, even noise-free A with \u03f5a = 0 makes noisy A2 with inter-group connections. For better consistency, we set the number of classes to an even number in our experiments, randomly pick paired classes such as (1, 3) and (2, 4) when c = 4, for example, and create inter-group connections only for the chosen pairs. In this way, we create a non-diagonal block-permutation matrix A when \u03f5a = 0, and the noise level of A2, which has a clustered structure, is solely controlled by the noise level of A. Labels In the case of individual y, we determine the label of each node uniformly at random, with no consideration of the graph structure. In the case of homophily or heterophily y, we assign labels based on the groups of nodes assumed by the graph structure. That is, nodes in the same group have the same label, and thus the group index itself works as the label y. In this way, we force homophily y for clustered A, and heterophily y for bipartite A. The degree of homophily (or heterophily) is also determined by the noise level \u03f5a of the graph structure. Features We basically assume that every feature element is sampled from a uniform distribution. Thus, in the random case, we sample each element from the uniform distribution U(0, 1) between 0 and 1. In the structural case, we run low-rank support vector decomposition (SVD) (Halko et al., 2011) to make X have structural information. Given U\u03a3V\u22a4 \u2248 A from low-rank SVD, we take U and normalize each feature element to have the zero-mean and unit-variance. The rank r in the SVD is determined as a hyperparameter; higher r captures the structure better, but can give noisy information. We also apply the ReLU function to U to make them positive. In the semantic case, we randomly pick c representative vectors {vk}ck=1 from the uniform distribution, which correspond to the c different classes. Then, for each node i with label y, we sample a feature vector such that argmaxk x \u22a4vk = y. In this way, we have random vectors having sufficient semantic information for the classification of labels, with a guarantee that the perfect linear decision boundaries can be drawn in the feature space X at the training time. C REPRODUCIBILITY: We perform row-normalization on the node features of all datasets as done in most studies on GNNs. We report the hyperparameters used for a grid search in Table 5, which is done for every split of data. The dimensions of hidden layers are all set to 64, and the probabilities of dropout layers are all set to 0.5. For the linear models, we use L-BFGS as the optimizer for training 100 epochs with patience 5; for the nonlinear ones, we use ADAM and train them for 1000 epochs with patience 200. In SlenderGNN, there are only 2 hyperparameters, where wd1 is the weight of LASSO, and wd2 is the weight of group LASSO. It is worth noting that, when searching the hyperparameters, SlenderGNN does not need to recompute the features, while most of the linear methods need to do so because of including one or more hyperparameters in the features that need to be precomputed."}, {"role": "assistant", "content": "Okay, I will review the paper \"SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success\" for ICLR 2023. I will create 3 reviewer agents with specialized expertise to evaluate the paper from multiple perspectives.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetBudget'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'directory': '.'}, name='ListFiles'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Budget retrieved successfully\",\n  \"output\": {\n    \"total_resource_budget\": 100,\n    \"current_resource_usage\": 0,\n    \"current_remaining_resource_budget\": 100,\n    \"total_expense_budget\": 10000,\n    \"current_expense\": 0.8142389500000005,\n    \"current_remaining_expense_budget\": 9999.18576105\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetBudget` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Files listed successfully\",\n  \"error\": null,\n  \"output\": [\n    \"favicon.ico\",\n    \"requirements.txt\",\n    \"HASHIRU_LOGO.png\",\n    \"HASHIRU_ARCH.png\",\n    \"deleteAgents.py\",\n    \"CITATION.cff\",\n    \"tests\",\n    \"__pycache__\",\n    \"start.py\",\n    \"README.md\",\n    \"results\",\n    \"HASHIRU.png\",\n    \".gitignore\",\n    \".env\",\n    \"static\",\n    \".gitattributes\",\n    \"app.py\",\n    \"HASHIRU_2.png\",\n    \".git\",\n    \"paper_review_hashiru_20250627_201357.jsonl\",\n    \"src\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ListFiles` with \n```json\n{\n  \"directory\": \".\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetBudget', response={'result': {'status': 'success', 'message': 'Budget retrieved successfully', 'output': {'total_resource_budget': 100, 'current_resource_usage': 0, 'current_remaining_resource_budget': 100, 'total_expense_budget': 10000, 'current_expense': 0.8142389500000005, 'current_remaining_expense_budget': 9999.18576105}}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ListFiles', response={'result': {'status': 'success', 'message': 'Files listed successfully', 'error': None, 'output': ['favicon.ico', 'requirements.txt', 'HASHIRU_LOGO.png', 'HASHIRU_ARCH.png', 'deleteAgents.py', 'CITATION.cff', 'tests', '__pycache__', 'start.py', 'README.md', 'results', 'HASHIRU.png', '.gitignore', '.env', 'static', '.gitattributes', 'app.py', 'HASHIRU_2.png', '.git', 'paper_review_hashiru_20250627_201357.jsonl', 'src']}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'query': 'ICLR 2023 review criteria', 'website': 'iclr.cc'}, name='GoogleSearchTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": [\n    \"https://iclr.cc/Conferences/2023/ReviewerGuide\",\n    \"https://blog.iclr.cc/2023/04/12/ethics-review-process-for-iclr-2023/\",\n    \"https://blog.iclr.cc/category/iclr-2023/\",\n    \"https://blog.iclr.cc/tag/reviewing/\",\n    \"https://blog.iclr.cc/tag/awards/\",\n    \"https://blog.iclr.cc/2023/04/05/announcing-notable-reviewers-and-area-chairs-at-iclr-2023/\",\n    \"https://iclr.cc/Conferences/2025/FAQ\",\n    \"https://iclr.cc/Conferences/2024/CallForTinyPapers\",\n    \"https://iclr.cc/Conferences/2019/Reviewer_Guidelines\",\n    \"https://iclr.cc/Conferences/2025/ACGuide\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `GoogleSearchTool` with \n```json\n{\n  \"query\": \"ICLR 2023 review criteria\",\n  \"website\": \"iclr.cc\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GoogleSearchTool', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': ['https://iclr.cc/Conferences/2023/ReviewerGuide', 'https://blog.iclr.cc/2023/04/12/ethics-review-process-for-iclr-2023/', 'https://blog.iclr.cc/category/iclr-2023/', 'https://blog.iclr.cc/tag/reviewing/', 'https://blog.iclr.cc/tag/awards/', 'https://blog.iclr.cc/2023/04/05/announcing-notable-reviewers-and-area-chairs-at-iclr-2023/', 'https://iclr.cc/Conferences/2025/FAQ', 'https://iclr.cc/Conferences/2024/CallForTinyPapers', 'https://iclr.cc/Conferences/2019/Reviewer_Guidelines', 'https://iclr.cc/Conferences/2025/ACGuide']}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'url': 'https://iclr.cc/Conferences/2023/ReviewerGuide', 'output_type': 'full_text'}, name='GetWebsite'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR 2022 Reviewer Instructions\\n\\n\\n\\n\\n\\nSkip to yearly menu bar\\nSkip to main content\\n\\nMain Navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            ICLR\\n        \\n\\n\\n\\n\\n                        Help/FAQ\\n                    \\n\\n\\n\\n\\n\\n\\n                        Contact ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n                        Downloads\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Blog\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n                        Privacy Policy\\n                    \\n\\n\\n\\n\\n\\n\\n                        Create Profile\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reset Password\\n                    \\n\\n\\n\\n\\n\\n\\n                        Journal To Conference Track\\n                    \\n\\n\\n\\n\\n\\n\\n                        Diversity & Inclusion\\n                    \\n\\n\\n\\n\\n\\n\\n                        Proceedings at OpenReview\\n                    \\n\\n\\n\\n\\n\\n\\n                        Future Meetings\\n                    \\n\\n\\n\\n\\n\\n\\n                        Press\\n                    \\n\\n\\n\\n\\n\\n\\n                        Exhibitor Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Twitter\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        My Stuff\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Login\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Select Year: (2023)\\n        \\n\\n\\n2026\\n                \\n\\n\\n\\n2025\\n                \\n\\n\\n\\n2024\\n                \\n\\n\\n\\n2023\\n                \\n\\n\\n\\n2022\\n                \\n\\n\\n\\n2021\\n                \\n\\n\\n\\n2020\\n                \\n\\n\\n\\n2019\\n                \\n\\n\\n\\n2018\\n                \\n\\n\\n\\n2017\\n                \\n\\n\\n\\n2016\\n                \\n\\n\\n\\n2015\\n                \\n\\n\\n\\n2014\\n                \\n\\n\\n\\n2013\\n                \\n\\n\\n\\n\\n\\n\\n                        Dates\\n                    \\n\\n\\n\\n\\n            Calls\\n        \\n\\n\\n\\n\\n                        Call for Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Tiny Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Workshops\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Blog Posts\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Socials\\n                    \\n\\n\\n\\n\\n\\n\\n            Guides\\n        \\n\\n\\n\\n\\n                        Author Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewer Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Area Chair Guide\\n                    \\n\\n\\n\\n\\n\\n\\n            Attend\\n        \\n\\n\\n\\n\\n                        Register\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visa and Travel  Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        Hotels\\n                    \\n\\n\\n\\n\\n\\n\\n                        Child Care\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visiting Kigali\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n            Organization\\n        \\n\\n\\n\\n\\n                        ICLR Board\\n                    \\n\\n\\n\\n\\n\\n\\n                        Organizing Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Program Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Ethics Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        Conference Site\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nICLR 2023 Reviewer Guide\\nThank you for agreeing to serve as an ICLR 2023 reviewer. Your contribution as a reviewer is paramount to creating an exciting and high-quality program. We ask that:\\n\\nYour reviews are timely and substantive.\\nYou follow the reviewing guidelines below.\u00a0\\nYou adhere to our Code of Ethics in your role as a reviewer. You must also adhere to our Code of Conduct.\\n\\nThis guide is intended to help you understand the ICLR 2023 decision process and your role within it. It contains:\\n\\nAn outline of the main reviewer tasks\\nStep-by-step reviewing instructions (especially relevant for reviewers that are new to ICLR)\\nReview examples\\nAn FAQ.\\n\\n\u00a0\\n\\n\\nWe\u2019re counting on you\\nAs a reviewer you are central to the program creation process for ICLR 2023. Your Area Chairs (ACs), Senior Area Chairs (SACs) and the Program Chairs (PCs) will rely greatly on your expertise and your diligent and thorough reviews to make decisions on each paper. Therefore, your role as a reviewer is critical to ensuring a strong program for ICLR 2023.\\nHigh-quality reviews are also very valuable for helping authors improve their work, whether it is eventually accepted by ICLR 2023, or not. Therefore it is important to treat each valid ICLR 2023 submission with equal care.\\nAs a token of our appreciation for your essential work, top reviewers will be acknowledged permanently on the ICLR 2023 website. Furthermore, top and high quality reviewers will receive special acknowledgement during the opening ceremony and free registration to ICLR 2023.\\n\u00a0\\n\\nMain reviewer tasks\\nThe main reviewer tasks and dates are as follows:\\n\\nCreate or update your OpenReview profile (September 30 2022)\\nBid on papers (September 30 2022 - October 06 2022)\\nWrite a constructive, thorough and timely review (October 12 2022 - October 24 2022)\\nInitial paper reviews released (November 4 2022)\\nDiscuss with authors and other reviewers to clarify and improve the paper (November 4 2022 - November 18 2022)\\nFlag any potential CoE violations and/or concerns (by November 18 2022)\\nProvide a recommendation to the area chair assigned to the paper (by November 18 2022)\\nVirtual Meeting with AC if paper you reviewed falls into borderline papers (November 18 2022 - December 12 2022)\\nProvide a final recommendation to the area chair assigned to the paper (after Virtual Meeting)\\n\\n\u00a0\\n\\n\\n\u00a0\\nCode of Ethics\\nAll ICLR participants, including reviewers, are required to adhere to the ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics). All reviewers are required to read the Code of Ethics and adhere to it. The Code of Ethics applies to all conference participation, including paper submission, reviewing, and paper discussion.\u00a0\\nAs part of the review process, reviewers are asked to raise potential violations of the ICLR Code of Ethics. Note that authors are encouraged to discuss questions and potential issues regarding the Code of Ethics as part of their submission. This discussion is not counted against the maximum page limit of the paper and should be included as a separate section.\\n\u00a0\\n\\n\u00a0\\nReviewing a submission: step-by-step\\nSummarized in one sentence, a review aims to determine whether a submission will bring sufficient value to the community and contribute new knowledge. The process can be broken down into the following main reviewer tasks:\\n\u00a0\\n\\n\\nRead the paper: It\u2019s important to carefully read through the entire paper, and to look up any related work and citations that will help you comprehensively evaluate it. Be sure to give yourself sufficient time for this step.\\nWhile reading, consider the following:\\n\\nObjective of the work: What is the goal of the paper? Is it to better address a known application or problem, draw attention to a new application or problem, or to introduce and/or explain a new theoretical finding? A combination of these? Different objectives will require different considerations as to potential value and impact.\\nStrong points: is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g. theoretically, algorithmically, etc.)?\\nWeak points: is it weak in any of the aspects listed in b.?\\nBe mindful of potential biases and try to be open-minded about the value and interest a paper can hold for the entire ICLR community, even if it may not be very interesting for you.\\n\\n\\nAnswer four key questions for yourself, to make a recommendation to Accept or Reject:\\n\\nWhat is the specific question and/or problem tackled by the paper?\\nIs the approach well motivated, including being well-placed in the literature?\\nDoes the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\\nWhat is the significance of the work? Does it contribute new knowledge and sufficient value to the community? Note, this does not necessarily require state-of-the-art results. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge (incl., empirical, theoretical, for practitioners, etc).\\n\\n\\nWrite and submit your initial review, organizing it as follows:\u00a0\\n\\nSummarize what the paper claims to contribute. Be positive and constructive.\\nList strong and weak points of the paper. Be as comprehensive as possible.\\nClearly state your initial recommendation (accept or reject) with one or two key reasons for this choice.\\nProvide supporting arguments for your recommendation.\\nAsk questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.\u00a0\\nProvide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\\n\\n\\nComplete the CoE report: ICLR has adopted the following Code of Ethics (CoE). When submitting your review, you\u2019ll be asked to complete a CoE report for the paper. The report is a simple form with two questions. The first asks whether there is a potential violation of the CoE. The second is relevant only if there is a potential violation and asks the reviewer to explain why there may be a potential violation. In order to answer these questions, it is therefore important that you read the CoE before starting your reviews. \\n\\t\u00a0\\nEngage in discussion: The discussion phase at ICLR is different from most conferences in the AI/ML community. During this phase, reviewers, authors and area chairs engage in asynchronous discussion and authors are allowed to revise their submissions to address concerns that arise. It is crucial that you are actively engaged during this phase. Maintain a spirit of openness to changing your initial recommendation (either to a more positive or more negative) rating.\\nBorderline paper meeting: This year, we are introducing a new program for ACs to (virtually) meet and discuss with reviewers only for borderline cases. ACs will reach out to schedule this meeting. This is to ensure active discussions among reviewers, and well-thought-out decisions. ACs will schedule the meeting and facilitate the discussion. For a productive discussion, it is important to familiarize yourself with other reviewers' feedback prior to the meeting. Please note that we will be leveraging information for reviewers who failed to attend this meeting (excluding emergencies).\u00a0\\nProvide final recommendation: Update your review, taking into account the new information collected during the discussion phase, and any revisions to the submission. State your reasoning and what did/didn\u2019t change your recommendation throughout the discussion phase.\\n\\n\u00a0\\n\\n\u00a0\\nFor great in-depth resources on reviewing, see these resources:\\n\\nDaniel Dennet, Criticising with Kindness.\\nComprehensive advice: Mistakes Reviewers Make\\nViews from multiple reviewers: Last minute reviewing advice\\nPerspective from instructions to Area Chairs: Dear ACs.\\n\\n\u00a0\\n\\n\u00a0\\nReview Examples\\nHere are two sample reviews from previous conferences that give an example of what we consider a good review for the case of leaning-to-accept and leaning-to-reject.\\nReview for a Paper where Leaning-to-Accept\\nThis paper proposes a method, Dual-AC, for optimizing the actor (policy) and critic (value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor. In order to achieve that, they take the linear programming approach of solving the Bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems. The discussion on the deficiencies of the naive LP approach is mostly well done. Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without losing convexity by applying a regularization. They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements. Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms.\u00a0\\nOverall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments. Given these clarifications in an author response, I would be willing to increase the score.\u00a0\\nFor the theory, there are a few steps that need clarification and further clarification on novelty. For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results. It looks like Theorem 2 has already been shown in \\\"Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time\u201d. There is a statement that \u201cChen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting. However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization\u201d. Is your Theorem 2 somehow an extension? Is Theorem 3 completely new?\\nThis is particularly called into question due to the lack of assumptions about the function class for value functions. It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function). This assumption seems to be used right at the bottom of Page 17, where U^{pi*} = V^*. Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small. More about conditions on eta_v would be illuminating.\u00a0\\nThere is also one step in the theorem that I cannot verify. On Page 18, how is the squared removed for difference between U and Upi? The transition from the second line of the proof to the third line is not clear. It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2.\u00a0\\n\u00a0\\n\\nFor the experiments, the following should be addressed.\\n1. It would have been better to also show the performance graphs with and without the improvements for multiple domains.\\n2. The central contribution is extending the single step LP to a multi-step formulation. It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains.\\n3. Increasing k also comes at a computational cost. I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO).\\n4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization. It was also mentioned that increasing the regularization parameter size increases the convergence rate. Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization? In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned. Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity? A bit more discussion on these choices would be helpful.\u00a0\\n\u00a0\\n\\nMinor comments:\\n1. Page 2: In equation 5, there should not be a 'ds' in the dual variable constraint\\n\u00a0\\n\\n\u00a0\\nReview for a Paper where Leaning-to-Reject\\nThis paper introduces a variation on temporal difference learning for the function approximation case that attempts to resolve the issue of over-generalization across temporally-successive states. The new approach is applied to both linear and non-linear function approximation, and for prediction and control problems. The algorithmic contribution is demonstrated with a suite of experiments in classic benchmark control domains (Mountain Car and Acrobot), and in Pong.\\nThis paper should be rejected because (1) the algorithm is not well justified either by theory or practice, (2) the paper never clearly demonstrates the existence of problem they are trying to solve (nor differentiates it from the usual problem of generalizing well), (3) the experiments are difficult to understand, missing many details, and generally do not support a significant contribution, and (4) the paper is imprecise and unpolished.\\nMain argument\\nThe paper does not do a great job of demonstrating that the problem it is trying to solve is a real thing. There is no experiment in this paper that clearly shows how this temporal generalization problem is different from the need to generalize well with function approximation. The paper points to references to establish the existence of the problem, but for example the Durugkar and Stone paper is a workshop paper and the conference version of that paper was rejected from ICLR 2018 and the reviewers highlighted serious issues with the paper\u2014that is not work to build upon. Further the paper under review here claims this problem is most pressing in the non-linear case, but the analysis in section 4.1 is for the linear case.\u00a0\\nThe resultant algorithm does not seem well justified, and has a different fixed point than TD, but there is no discussion of this other than section 4.4, which does not make clear statements about the correctness of the algorithm or what it converges to. Can you provide a proof or any kind of evidence that the proposed approach is sound, or how it\u2019s fixed point relates to TD?\\nThe experiments do not provide convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches. There are so many missing details it is difficult to draw many conclusions:\\n\\nWhat was the policy used in exp1 for policy evaluation in MC?\\nWhy Fourier basis features?\\nIn MC with DQN how did you adjust the parameters and architecture for the MC task?\\nWas the reward in MC and Acrobot -1 per step or something else\\nHow did you tune the parameters in the MC and Acrobot experiments?\\nWhy so few runs in MC, none of the results presented are significant?\\nWhy is the performance so bad in MC?\\nDid you evaluate online learning or do tests with the greedy policy?\\nHow did you initialize the value functions and weights?\\nWhy did you use experience replay for the linear experiments?\\nIN MC and Acrobot why only a one layer MLP?\\n\\nIgnoring all that, the results are not convincing. Most of the results in the paper are not statistically significant. The policy evaluation results in MC show little difference to regular TD. The Pong results show DQN is actually better. This makes the reader wonder if the result with DQN on MC and Acrobot are only worse because you did not properly tune DQN for those domains, whereas the default DQN architecture is well tuned for Atari and that is why you method is competitive in the smaller domains.\u00a0\\nThe differences in the \u201caverage change in value plots\u201d are very small if the rewards are -1 per step. Can you provide some context to understand the significance of this difference? In the last experiment linear FA and MC, the step-size is set equal for all methods\u2014this is not a valid comparison. Your method may just work better with alpha = 0.1.\u00a0\\n\u00a0\\n\\nThe paper has many imprecise parts, here are a few:\\n\\nThe definition of the value function would be approximate not equals unless you specify some properties of the function approximation architecture. Same for the Bellman equation\\nequation 1 of section 2.1 is neither an algorithm or a loss function\\nTD does not minimize the squared TD. Saying that is the objective function of TD learning in not true\\nend of section 2.1 says \u201cIt is computed as\u201d but the following equation just gives a form for the partial derivative\\nequation 2, x is not bounded\u00a0\\nYou state TC-loss has an unclear solution property, I don\u2019t know what that means and I don\u2019t think your approach is well justified either\\nSection 4.1 assumes linear FA, but its implied up until paragraph 2 that it has not assumed linear\\ntreatment of n_t in alg differs from appendix (t is no time episode number)\\nYour method has a n_t parameter that is adapted according to a schedule seemingly giving it an unfair advantage over DQN.\\nOver-claim not supported by the results: \u201cwe see that HR-TD is able to find a representation that is better at keeping the target value separate than TC is \u201c. The results do not show this.\\nSection 4.4 does not seem to go anywhere or produce and tangible conclusions\\n\\n\u00a0\\n\\nThings to improve the paper that did not impact the score:\\n\\nIt\u2019s hard to follow how the prox operator is used in the development of the alg, this could use some higher level explanation\\nIntro p2 is about bootstrapping, use that term and remove the equations\\nIt\u2019s not clear why you are talking about stochastic vs deterministic in P3\\nPerhaps you should compare against a MC method in the experiments to demonstrate the problem with TD methods and generalization\\nSection 2: \u201ccan often be a regularization term\u201d >> can or must be?\\nupdate law is an odd term\\n\u00a0tends to alleviate\u201d >> odd phrase\\nsection 4 should come before section 3\\nAlg 1 in not helpful because it just references an equation\\nsection 4.4 is very confusing, I cannot follow the logic of the statements\u00a0\\nQ learning >> Q-learning\\nNot sure what you mean with the last sentence of p2 section 5\\nwhere are the results for Acrobot linear function approximation\\nappendix Q-learning with linear FA is not DQN (table 2)\\n\\n\u00a0\\n\\nFAQ for Reviewers\\nQ. The review I just submitted is not appearing! Did it go thru?\\nA. There seems to be some delay in the system for you to see your reviews. Just give a bit of time and refresh again.\\n\u00a0\\nQ. What is the borderline paper meeting between AC and reviewers about?\\nA. This year we require ACs to meet with reviewers for all borderline papers (defined and announced by PCs once all reviews are received). There will be one meeting per borderline paper. Once we receive all reviews, we will let ACs know which papers fall into this bucket. ACs will then schedule and run the meetings with the reviewers of the papers in question.\\n\u00a0\\n\\nQ: What if I can not attend the meeting?\\nA: Please work with your AC to find a meeting schedule that works for everyone. Note that the discussion phase is roughly 1 month long, and overlaps with the NeurIPS conference, in case in-person meeting is preferred. All reviewers agreed to participate in review meetings when they accepted the reviewer invitations. An AC has right to\u00a0 downweight the review of reviewers who refuse to attend the meeting.\\n\u00a0\\n\\nQ: How should I use supplementary material?\\nA: It is not necessary to read supplementary material but such material can often answer questions that arise while reading the main paper, so consider looking there before asking authors.\\n\u00a0\\n\\nQ: How should I handle a policy violation?\\nA: To flag a CoE violation related to a submission, please indicate it when submitting the CoE report for that paper. The AC will work with the PC and the ethics board to resolve the case. To discuss other violations (e.g. plagiarism, double submission, paper length, formatting, etc.), please contact either the AC/SAC or the PC as appropriate. You can do this by sending a confidential comment with the appropriate readership restrictions.\\n\u00a0\\n\\nQ: Am I allowed to ask for additional experiments?\\nA: You can ask for additional experiments. New experiments should not significantly change the content of the submission. Rather, they should be limited in scope and serve to more thoroughly validate existing results from the submission.\\n\u00a0\\n\\nQ: If a submission does not achieve state-of-the-art results, is that grounds for rejection?\\nA: No, a lack of state-of-the-art results does not by itself constitute grounds for rejection. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge. Submissions can achieve this without achieving state-of-the-art results.\\nQ: Are authors expected to cite and compare with very recent work? What about non peer-reviewed (e.g., ArXiv) papers? (updated on 7 November 2022)\\nA: We consider papers contemporaneous if they are published (available in online proceedings) within the last four months. That means, since our full paper deadline is September 28, if a paper was published (i.e., at a peer-reviewed venue) on or after May 28, 2022, authors are not required to compare their own work to that paper. Authors are encouraged to cite and discuss all relevant papers, but they may be excused for not knowing about papers not published in peer-reviewed conference proceedings or journals, which includes papers exclusively available on arXiv. Reviewers are encouraged to use their own good judgement and, if in doubt, discuss with their area chair.\u00a0\\n\u00a0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuccessful Page Load\\n\\n\\n\\n\\n\\n\\n\\n                        ICLR uses cookies for essential functions only. We do not sell your personal\\n                        information.\\n                        Our Privacy Policy \u00bb\u00a0\\n\\n\\nAccept\\n                            Cookies\\n                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            The ICLR Logo above may be used on presentations. Right-click and choose\\n                            download. It is a vector graphic and may be used at any scale.\\n                        \\n\\n\\n\\n\\n\\n                            Useful links\\n                        \\n\\n\\nSponsor / Exhibitor Information\\nPress\\n\\n\\n\\n\\n\\n\\n\\nContact\\n\\n 2710 E Corridor Dr, Appleton WI 54913\\n                            \\n\\n Email\\n\\n Phone: +1-920-268-4789\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR Proceedings at OpenReview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"\n}\n```\n", "metadata": {"title": "Invoking `GetWebsite` with \n```json\n{\n  \"url\": \"https://iclr.cc/Conferences/2023/ReviewerGuide\",\n  \"output_type\": \"full_text\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetWebsite', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR 2022 Reviewer Instructions\\n\\n\\n\\n\\n\\nSkip to yearly menu bar\\nSkip to main content\\n\\nMain Navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            ICLR\\n        \\n\\n\\n\\n\\n                        Help/FAQ\\n                    \\n\\n\\n\\n\\n\\n\\n                        Contact ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n                        Downloads\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Blog\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n                        Privacy Policy\\n                    \\n\\n\\n\\n\\n\\n\\n                        Create Profile\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reset Password\\n                    \\n\\n\\n\\n\\n\\n\\n                        Journal To Conference Track\\n                    \\n\\n\\n\\n\\n\\n\\n                        Diversity & Inclusion\\n                    \\n\\n\\n\\n\\n\\n\\n                        Proceedings at OpenReview\\n                    \\n\\n\\n\\n\\n\\n\\n                        Future Meetings\\n                    \\n\\n\\n\\n\\n\\n\\n                        Press\\n                    \\n\\n\\n\\n\\n\\n\\n                        Exhibitor Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Twitter\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        My Stuff\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Login\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Select Year: (2023)\\n        \\n\\n\\n2026\\n                \\n\\n\\n\\n2025\\n                \\n\\n\\n\\n2024\\n                \\n\\n\\n\\n2023\\n                \\n\\n\\n\\n2022\\n                \\n\\n\\n\\n2021\\n                \\n\\n\\n\\n2020\\n                \\n\\n\\n\\n2019\\n                \\n\\n\\n\\n2018\\n                \\n\\n\\n\\n2017\\n                \\n\\n\\n\\n2016\\n                \\n\\n\\n\\n2015\\n                \\n\\n\\n\\n2014\\n                \\n\\n\\n\\n2013\\n                \\n\\n\\n\\n\\n\\n\\n                        Dates\\n                    \\n\\n\\n\\n\\n            Calls\\n        \\n\\n\\n\\n\\n                        Call for Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Tiny Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Workshops\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Blog Posts\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Socials\\n                    \\n\\n\\n\\n\\n\\n\\n            Guides\\n        \\n\\n\\n\\n\\n                        Author Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewer Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Area Chair Guide\\n                    \\n\\n\\n\\n\\n\\n\\n            Attend\\n        \\n\\n\\n\\n\\n                        Register\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visa and Travel  Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        Hotels\\n                    \\n\\n\\n\\n\\n\\n\\n                        Child Care\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visiting Kigali\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n            Organization\\n        \\n\\n\\n\\n\\n                        ICLR Board\\n                    \\n\\n\\n\\n\\n\\n\\n                        Organizing Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Program Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Ethics Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        Conference Site\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nICLR 2023 Reviewer Guide\\nThank you for agreeing to serve as an ICLR 2023 reviewer. Your contribution as a reviewer is paramount to creating an exciting and high-quality program. We ask that:\\n\\nYour reviews are timely and substantive.\\nYou follow the reviewing guidelines below.\\xa0\\nYou adhere to our Code of Ethics in your role as a reviewer. You must also adhere to our Code of Conduct.\\n\\nThis guide is intended to help you understand the ICLR 2023 decision process and your role within it. It contains:\\n\\nAn outline of the main reviewer tasks\\nStep-by-step reviewing instructions (especially relevant for reviewers that are new to ICLR)\\nReview examples\\nAn FAQ.\\n\\n\\xa0\\n\\n\\nWe\u2019re counting on you\\nAs a reviewer you are central to the program creation process for ICLR 2023. Your Area Chairs (ACs), Senior Area Chairs (SACs) and the Program Chairs (PCs) will rely greatly on your expertise and your diligent and thorough reviews to make decisions on each paper. Therefore, your role as a reviewer is critical to ensuring a strong program for ICLR 2023.\\nHigh-quality reviews are also very valuable for helping authors improve their work, whether it is eventually accepted by ICLR 2023, or not. Therefore it is important to treat each valid ICLR 2023 submission with equal care.\\nAs a token of our appreciation for your essential work, top reviewers will be acknowledged permanently on the ICLR 2023 website. Furthermore, top and high quality reviewers will receive special acknowledgement during the opening ceremony and free registration to ICLR 2023.\\n\\xa0\\n\\nMain reviewer tasks\\nThe main reviewer tasks and dates are as follows:\\n\\nCreate or update your OpenReview profile (September 30 2022)\\nBid on papers (September 30 2022 - October 06 2022)\\nWrite a constructive, thorough and timely review (October 12 2022 - October 24 2022)\\nInitial paper reviews released (November 4 2022)\\nDiscuss with authors and other reviewers to clarify and improve the paper (November 4 2022 - November 18 2022)\\nFlag any potential CoE violations and/or concerns (by November 18 2022)\\nProvide a recommendation to the area chair assigned to the paper (by November 18 2022)\\nVirtual Meeting with AC if paper you reviewed falls into borderline papers (November 18 2022 - December 12 2022)\\nProvide a final recommendation to the area chair assigned to the paper (after Virtual Meeting)\\n\\n\\xa0\\n\\n\\n\\xa0\\nCode of Ethics\\nAll ICLR participants, including reviewers, are required to adhere to the ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics). All reviewers are required to read the Code of Ethics and adhere to it. The Code of Ethics applies to all conference participation, including paper submission, reviewing, and paper discussion.\\xa0\\nAs part of the review process, reviewers are asked to raise potential violations of the ICLR Code of Ethics. Note that authors are encouraged to discuss questions and potential issues regarding the Code of Ethics as part of their submission. This discussion is not counted against the maximum page limit of the paper and should be included as a separate section.\\n\\xa0\\n\\n\\xa0\\nReviewing a submission: step-by-step\\nSummarized in one sentence, a review aims to determine whether a submission will bring sufficient value to the community and contribute new knowledge. The process can be broken down into the following main reviewer tasks:\\n\\xa0\\n\\n\\nRead the paper: It\u2019s important to carefully read through the entire paper, and to look up any related work and citations that will help you comprehensively evaluate it. Be sure to give yourself sufficient time for this step.\\nWhile reading, consider the following:\\n\\nObjective of the work: What is the goal of the paper? Is it to better address a known application or problem, draw attention to a new application or problem, or to introduce and/or explain a new theoretical finding? A combination of these? Different objectives will require different considerations as to potential value and impact.\\nStrong points: is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g. theoretically, algorithmically, etc.)?\\nWeak points: is it weak in any of the aspects listed in b.?\\nBe mindful of potential biases and try to be open-minded about the value and interest a paper can hold for the entire ICLR community, even if it may not be very interesting for you.\\n\\n\\nAnswer four key questions for yourself, to make a recommendation to Accept or Reject:\\n\\nWhat is the specific question and/or problem tackled by the paper?\\nIs the approach well motivated, including being well-placed in the literature?\\nDoes the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\\nWhat is the significance of the work? Does it contribute new knowledge and sufficient value to the community? Note, this does not necessarily require state-of-the-art results. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge (incl., empirical, theoretical, for practitioners, etc).\\n\\n\\nWrite and submit your initial review, organizing it as follows:\\xa0\\n\\nSummarize what the paper claims to contribute. Be positive and constructive.\\nList strong and weak points of the paper. Be as comprehensive as possible.\\nClearly state your initial recommendation (accept or reject) with one or two key reasons for this choice.\\nProvide supporting arguments for your recommendation.\\nAsk questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.\\xa0\\nProvide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\\n\\n\\nComplete the CoE report: ICLR has adopted the following Code of Ethics (CoE). When submitting your review, you\u2019ll be asked to complete a CoE report for the paper. The report is a simple form with two questions. The first asks whether there is a potential violation of the CoE. The second is relevant only if there is a potential violation and asks the reviewer to explain why there may be a potential violation. In order to answer these questions, it is therefore important that you read the CoE before starting your reviews. \\n\\t\\xa0\\nEngage in discussion: The discussion phase at ICLR is different from most conferences in the AI/ML community. During this phase, reviewers, authors and area chairs engage in asynchronous discussion and authors are allowed to revise their submissions to address concerns that arise. It is crucial that you are actively engaged during this phase. Maintain a spirit of openness to changing your initial recommendation (either to a more positive or more negative) rating.\\nBorderline paper meeting: This year, we are introducing a new program for ACs to (virtually) meet and discuss with reviewers only for borderline cases. ACs will reach out to schedule this meeting. This is to ensure active discussions among reviewers, and well-thought-out decisions. ACs will schedule the meeting and facilitate the discussion. For a productive discussion, it is important to familiarize yourself with other reviewers\\' feedback prior to the meeting. Please note that we will be leveraging information for reviewers who failed to attend this meeting (excluding emergencies).\\xa0\\nProvide final recommendation: Update your review, taking into account the new information collected during the discussion phase, and any revisions to the submission. State your reasoning and what did/didn\u2019t change your recommendation throughout the discussion phase.\\n\\n\\xa0\\n\\n\\xa0\\nFor great in-depth resources on reviewing, see these resources:\\n\\nDaniel Dennet, Criticising with Kindness.\\nComprehensive advice: Mistakes Reviewers Make\\nViews from multiple reviewers: Last minute reviewing advice\\nPerspective from instructions to Area Chairs: Dear ACs.\\n\\n\\xa0\\n\\n\\xa0\\nReview Examples\\nHere are two sample reviews from previous conferences that give an example of what we consider a good review for the case of leaning-to-accept and leaning-to-reject.\\nReview for a Paper where Leaning-to-Accept\\nThis paper proposes a method, Dual-AC, for optimizing the actor (policy) and critic (value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor. In order to achieve that, they take the linear programming approach of solving the Bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems. The discussion on the deficiencies of the naive LP approach is mostly well done. Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without losing convexity by applying a regularization. They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements. Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms.\\xa0\\nOverall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments. Given these clarifications in an author response, I would be willing to increase the score.\\xa0\\nFor the theory, there are a few steps that need clarification and further clarification on novelty. For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results. It looks like Theorem 2 has already been shown in \"Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time\u201d. There is a statement that \u201cChen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting. However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization\u201d. Is your Theorem 2 somehow an extension? Is Theorem 3 completely new?\\nThis is particularly called into question due to the lack of assumptions about the function class for value functions. It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function). This assumption seems to be used right at the bottom of Page 17, where U^{pi*} = V^*. Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small. More about conditions on eta_v would be illuminating.\\xa0\\nThere is also one step in the theorem that I cannot verify. On Page 18, how is the squared removed for difference between U and Upi? The transition from the second line of the proof to the third line is not clear. It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2.\\xa0\\n\\xa0\\n\\nFor the experiments, the following should be addressed.\\n1. It would have been better to also show the performance graphs with and without the improvements for multiple domains.\\n2. The central contribution is extending the single step LP to a multi-step formulation. It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains.\\n3. Increasing k also comes at a computational cost. I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO).\\n4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization. It was also mentioned that increasing the regularization parameter size increases the convergence rate. Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization? In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned. Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity? A bit more discussion on these choices would be helpful.\\xa0\\n\\xa0\\n\\nMinor comments:\\n1. Page 2: In equation 5, there should not be a \\'ds\\' in the dual variable constraint\\n\\xa0\\n\\n\\xa0\\nReview for a Paper where Leaning-to-Reject\\nThis paper introduces a variation on temporal difference learning for the function approximation case that attempts to resolve the issue of over-generalization across temporally-successive states. The new approach is applied to both linear and non-linear function approximation, and for prediction and control problems. The algorithmic contribution is demonstrated with a suite of experiments in classic benchmark control domains (Mountain Car and Acrobot), and in Pong.\\nThis paper should be rejected because (1) the algorithm is not well justified either by theory or practice, (2) the paper never clearly demonstrates the existence of problem they are trying to solve (nor differentiates it from the usual problem of generalizing well), (3) the experiments are difficult to understand, missing many details, and generally do not support a significant contribution, and (4) the paper is imprecise and unpolished.\\nMain argument\\nThe paper does not do a great job of demonstrating that the problem it is trying to solve is a real thing. There is no experiment in this paper that clearly shows how this temporal generalization problem is different from the need to generalize well with function approximation. The paper points to references to establish the existence of the problem, but for example the Durugkar and Stone paper is a workshop paper and the conference version of that paper was rejected from ICLR 2018 and the reviewers highlighted serious issues with the paper\u2014that is not work to build upon. Further the paper under review here claims this problem is most pressing in the non-linear case, but the analysis in section 4.1 is for the linear case.\\xa0\\nThe resultant algorithm does not seem well justified, and has a different fixed point than TD, but there is no discussion of this other than section 4.4, which does not make clear statements about the correctness of the algorithm or what it converges to. Can you provide a proof or any kind of evidence that the proposed approach is sound, or how it\u2019s fixed point relates to TD?\\nThe experiments do not provide convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches. There are so many missing details it is difficult to draw many conclusions:\\n\\nWhat was the policy used in exp1 for policy evaluation in MC?\\nWhy Fourier basis features?\\nIn MC with DQN how did you adjust the parameters and architecture for the MC task?\\nWas the reward in MC and Acrobot -1 per step or something else\\nHow did you tune the parameters in the MC and Acrobot experiments?\\nWhy so few runs in MC, none of the results presented are significant?\\nWhy is the performance so bad in MC?\\nDid you evaluate online learning or do tests with the greedy policy?\\nHow did you initialize the value functions and weights?\\nWhy did you use experience replay for the linear experiments?\\nIN MC and Acrobot why only a one layer MLP?\\n\\nIgnoring all that, the results are not convincing. Most of the results in the paper are not statistically significant. The policy evaluation results in MC show little difference to regular TD. The Pong results show DQN is actually better. This makes the reader wonder if the result with DQN on MC and Acrobot are only worse because you did not properly tune DQN for those domains, whereas the default DQN architecture is well tuned for Atari and that is why you method is competitive in the smaller domains.\\xa0\\nThe differences in the \u201caverage change in value plots\u201d are very small if the rewards are -1 per step. Can you provide some context to understand the significance of this difference? In the last experiment linear FA and MC, the step-size is set equal for all methods\u2014this is not a valid comparison. Your method may just work better with alpha = 0.1.\\xa0\\n\\xa0\\n\\nThe paper has many imprecise parts, here are a few:\\n\\nThe definition of the value function would be approximate not equals unless you specify some properties of the function approximation architecture. Same for the Bellman equation\\nequation 1 of section 2.1 is neither an algorithm or a loss function\\nTD does not minimize the squared TD. Saying that is the objective function of TD learning in not true\\nend of section 2.1 says \u201cIt is computed as\u201d but the following equation just gives a form for the partial derivative\\nequation 2, x is not bounded\\xa0\\nYou state TC-loss has an unclear solution property, I don\u2019t know what that means and I don\u2019t think your approach is well justified either\\nSection 4.1 assumes linear FA, but its implied up until paragraph 2 that it has not assumed linear\\ntreatment of n_t in alg differs from appendix (t is no time episode number)\\nYour method has a n_t parameter that is adapted according to a schedule seemingly giving it an unfair advantage over DQN.\\nOver-claim not supported by the results: \u201cwe see that HR-TD is able to find a representation that is better at keeping the target value separate than TC is \u201c. The results do not show this.\\nSection 4.4 does not seem to go anywhere or produce and tangible conclusions\\n\\n\\xa0\\n\\nThings to improve the paper that did not impact the score:\\n\\nIt\u2019s hard to follow how the prox operator is used in the development of the alg, this could use some higher level explanation\\nIntro p2 is about bootstrapping, use that term and remove the equations\\nIt\u2019s not clear why you are talking about stochastic vs deterministic in P3\\nPerhaps you should compare against a MC method in the experiments to demonstrate the problem with TD methods and generalization\\nSection 2: \u201ccan often be a regularization term\u201d >> can or must be?\\nupdate law is an odd term\\n\\xa0tends to alleviate\u201d >> odd phrase\\nsection 4 should come before section 3\\nAlg 1 in not helpful because it just references an equation\\nsection 4.4 is very confusing, I cannot follow the logic of the statements\\xa0\\nQ learning >> Q-learning\\nNot sure what you mean with the last sentence of p2 section 5\\nwhere are the results for Acrobot linear function approximation\\nappendix Q-learning with linear FA is not DQN (table 2)\\n\\n\\xa0\\n\\nFAQ for Reviewers\\nQ. The review I just submitted is not appearing! Did it go thru?\\nA. There seems to be some delay in the system for you to see your reviews. Just give a bit of time and refresh again.\\n\\xa0\\nQ. What is the borderline paper meeting between AC and reviewers about?\\nA. This year we require ACs to meet with reviewers for all borderline papers (defined and announced by PCs once all reviews are received). There will be one meeting per borderline paper. Once we receive all reviews, we will let ACs know which papers fall into this bucket. ACs will then schedule and run the meetings with the reviewers of the papers in question.\\n\\xa0\\n\\nQ: What if I can not attend the meeting?\\nA: Please work with your AC to find a meeting schedule that works for everyone. Note that the discussion phase is roughly 1 month long, and overlaps with the NeurIPS conference, in case in-person meeting is preferred. All reviewers agreed to participate in review meetings when they accepted the reviewer invitations. An AC has right to\\xa0 downweight the review of reviewers who refuse to attend the meeting.\\n\\xa0\\n\\nQ: How should I use supplementary material?\\nA: It is not necessary to read supplementary material but such material can often answer questions that arise while reading the main paper, so consider looking there before asking authors.\\n\\xa0\\n\\nQ: How should I handle a policy violation?\\nA: To flag a CoE violation related to a submission, please indicate it when submitting the CoE report for that paper. The AC will work with the PC and the ethics board to resolve the case. To discuss other violations (e.g. plagiarism, double submission, paper length, formatting, etc.), please contact either the AC/SAC or the PC as appropriate. You can do this by sending a confidential comment with the appropriate readership restrictions.\\n\\xa0\\n\\nQ: Am I allowed to ask for additional experiments?\\nA: You can ask for additional experiments. New experiments should not significantly change the content of the submission. Rather, they should be limited in scope and serve to more thoroughly validate existing results from the submission.\\n\\xa0\\n\\nQ: If a submission does not achieve state-of-the-art results, is that grounds for rejection?\\nA: No, a lack of state-of-the-art results does not by itself constitute grounds for rejection. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge. Submissions can achieve this without achieving state-of-the-art results.\\nQ: Are authors expected to cite and compare with very recent work? What about non peer-reviewed (e.g., ArXiv) papers? (updated on 7 November 2022)\\nA: We consider papers contemporaneous if they are published (available in online proceedings) within the last four months. That means, since our full paper deadline is September 28, if a paper was published (i.e., at a peer-reviewed venue) on or after May 28, 2022, authors are not required to compare their own work to that paper. Authors are encouraged to cite and discuss all relevant papers, but they may be excused for not knowing about papers not published in peer-reviewed conference proceedings or journals, which includes papers exclusively available on arXiv. Reviewers are encouraged to use their own good judgement and, if in doubt, discuss with their area chair.\\xa0\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuccessful Page Load\\n\\n\\n\\n\\n\\n\\n\\n                        ICLR uses cookies for essential functions only. We do not sell your personal\\n                        information.\\n                        Our Privacy Policy \u00bb\\xa0\\n\\n\\nAccept\\n                            Cookies\\n                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            The ICLR Logo above may be used on presentations. Right-click and choose\\n                            download. It is a vector graphic and may be used at any scale.\\n                        \\n\\n\\n\\n\\n\\n                            Useful links\\n                        \\n\\n\\nSponsor / Exhibitor Information\\nPress\\n\\n\\n\\n\\n\\n\\n\\nContact\\n\\n 2710 E Corridor Dr, Appleton WI 54913\\n                            \\n\\n Email\\n\\n Phone: +1-920-268-4789\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR Proceedings at OpenReview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Based on the paper and the ICLR 2023 reviewer guide, I will create three reviewer agents with the following specializations:\n\n1.  **GNN Methodology Expert (GNN-ME):** Focuses on the technical soundness and novelty of the proposed GNNLIN framework and SlenderGNN architecture. Evaluates the linearization process, the design choices of SlenderGNN, and the validity of the experimental results.\n2.  **Robustness and Generalization Analyst (RGA):** Critically assesses the claims of robustness and generalization across different graph types (homophily, heterophily, etc.) and noisy features. Examines the sanity checks and real-world experiments for evidence supporting these claims.\n3.  **Interpretability and Practicality Assessor (IPA):** Evaluates the interpretability of SlenderGNN and its practical applicability. Assesses the clarity of the explanations, the usefulness of the GNNLIN framework for practitioners, and the impact of the design choices on interpretability.\n\nNow, I will create the agents.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"gemini-2.5-flash-preview-05-20\": {\n      \"description\": \"Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math\",\n      \"create_expense_cost\": 0,\n      \"invoke_expense_cost\": 0.15,\n      \"output_expense_cost\": 0.6\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'gemini-2.5-flash-preview-05-20': {'description': 'Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math', 'create_expense_cost': 0, 'invoke_expense_cost': 0.15, 'output_expense_cost': 0.6}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'GNN_ME', 'system_prompt': 'You are a GNN methodology expert. Evaluate the technical soundness and novelty of the proposed GNNLIN framework and SlenderGNN architecture. Assess the linearization process, design choices, and validity of experimental results. Provide detailed feedback on the methodology and suggest improvements.', 'description': 'Expert in GNN methodologies, focusing on technical soundness and novelty.', 'base_model': 'gemini-2.5-flash-preview-05-20'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'RGA', 'description': 'Analyst specializing in robustness and generalization of GNNs.', 'system_prompt': 'You are a robustness and generalization analyst. Critically assess the claims of robustness and generalization across different graph types and noisy features. Examine the sanity checks and real-world experiments for evidence supporting these claims. Identify limitations and suggest ways to improve robustness.', 'base_model': 'gemini-2.5-flash-preview-05-20'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'Assessor of interpretability and practicality of GNNs.', 'system_prompt': 'You are an interpretability and practicality assessor. Evaluate the interpretability of SlenderGNN and its practical applicability. Assess the clarity of explanations, the usefulness of the GNNLIN framework for practitioners, and the impact of design choices on interpretability. Provide feedback on how to improve the understanding and usability of the model.', 'base_model': 'gemini-2.5-flash-preview-05-20', 'agent_name': 'IPA'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.17885465\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"agent_name\": \"GNN_ME\",\n  \"system_prompt\": \"You are a GNN methodology expert. Evaluate the technical soundness and novelty of the proposed GNNLIN framework and SlenderGNN architecture. Assess the linearization process, design choices, and validity of experimental results. Provide detailed feedback on the methodology and suggest improvements.\",\n  \"description\": \"Expert in GNN methodologies, focusing on technical soundness and novelty.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.17885465\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"agent_name\": \"RGA\",\n  \"description\": \"Analyst specializing in robustness and generalization of GNNs.\",\n  \"system_prompt\": \"You are a robustness and generalization analyst. Critically assess the claims of robustness and generalization across different graph types and noisy features. Examine the sanity checks and real-world experiments for evidence supporting these claims. Identify limitations and suggest ways to improve robustness.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.17885465\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"Assessor of interpretability and practicality of GNNs.\",\n  \"system_prompt\": \"You are an interpretability and practicality assessor. Evaluate the interpretability of SlenderGNN and its practical applicability. Assess the clarity of explanations, the usefulness of the GNNLIN framework for practitioners, and the impact of design choices on interpretability. Provide feedback on how to improve the understanding and usability of the model.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"agent_name\": \"IPA\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.17885465}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.17885465}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.17885465}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Now that the agents are created, I will ask them to review the paper.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Review the paper 'SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success'. Focus on the technical soundness and novelty of the proposed GNNLIN framework and SlenderGNN architecture. Evaluate the linearization process, the design choices of SlenderGNN, and the validity of the experimental results. Provide detailed feedback on the methodology and suggest improvements.\", 'agent_name': 'GNN_ME'}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'RGA', 'prompt': \"Review the paper 'SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success'. Critically assess the claims of robustness and generalization across different graph types (homophily, heterophily, etc.) and noisy features. Examine the sanity checks and real-world experiments for evidence supporting these claims. Identify limitations and suggest ways to improve robustness.\"}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Review the paper 'SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success'. Evaluate the interpretability of SlenderGNN and its practical applicability. Assess the clarity of the explanations, the usefulness of the GNNLIN framework for practitioners, and the impact of the design choices on interpretability. Provide feedback on how to improve the understanding and usability of the model.\", 'agent_name': 'IPA'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The paper \\\"SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success\\\" introduces the GNNLIN framework and the SlenderGNN architecture, aiming to address common GNN limitations like over-smoothing, lack of robustness, and poor interpretability. As a GNN methodology expert, I will evaluate its technical soundness, novelty, design choices, and experimental validity.\\n\\n---\\n\\n### **1. GNNLIN Framework: Technical Soundness and Novelty**\\n\\n**Concept:**\\nThe GNNLIN (GNN Linearization) framework proposes to reformulate GNNs by decoupling the non-linear feature transformations and activations from the message-passing mechanism. The core idea is that intermediate non-linearities within the message passing process are detrimental, leading to over-smoothing and noise amplification. Instead, it suggests performing propagation linearly and applying non-linearity only at the input and output stages.\\n\\n**Technical Soundness:**\\n*   **The \\\"Linearization Theorem\\\":** The paper presents a theorem showing that a general GNN, under certain conditions (specifically, by setting intermediate non-linear activation functions to identity and removing feature transformations within message passing), can be expressed as a linear combination of initial features propagated through powers of the adjacency matrix, followed by a non-linear transformation.\\n    *   **Soundness:** Mathematically, this re-expression is sound *given the specific assumptions and modifications* to the GNN layer. It essentially demonstrates that if you remove intermediate non-linearities, the operation becomes a polynomial filter on graph signals. This is not \\\"linearization\\\" in the sense of approximating a non-linear function with a linear one (like Taylor expansion), but rather a *restructuring* or *simplification* of the GNN's operations to be purely linear in its intermediate layers.\\n    *   **Implication:** This theorem provides a solid theoretical basis for *why* a model like SlenderGNN (or even SGC/GPRGNN) behaves the way it does. It shows that the model effectively learns to combine \\\"walks\\\" of different lengths from the initial features.\\n\\n*   **Justification for Removing Non-linearities:** The paper argues that intermediate non-linearities cause over-smoothing and are a source of noise amplification.\\n    *   **Soundness:** This argument has merit. Deep non-linear graph convolutions *can* lead to feature compression and over-smoothing, especially when all nodes converge to similar representations. Linear propagation, by nature, doesn't suffer from the same feature collapse as non-linear activation functions. It preserves feature distinctiveness to a greater degree, potentially mitigating over-smoothing and aiding deep propagation. The amplification of noise is also plausible as non-linearities can introduce complex landscape for optimization.\\n\\n**Novelty:**\\n*   **Conceptual Novelty:** The *explicit formalization* of the GNNLIN framework, emphasizing the *separation* and *reduction* of intermediate non-linearities to improve GNN performance and interpretability, is a novel contribution. While previous works like SGC (which is purely linear) or GPRGNN (which learns a weighted sum of powers of A) already exploit linearity for specific purposes, GNNLIN provides a more general theoretical lens and a strong rationale for *why* this approach is beneficial across multiple aspects (accuracy, robustness, interpretability).\\n*   **\\\"Linearization\\\" Terminology:** The use of \\\"linearization\\\" is interesting. While the underlying operation becomes linear, it's more accurate to describe it as \\\"separating linear propagation from non-linear feature transformation\\\" or \\\"enforcing linear propagation.\\\" This specific phrasing and the comprehensive reasoning behind it add to the novelty.\\n\\n### **2. SlenderGNN Architecture: Design Choices and Novelty**\\n\\n**Architecture Breakdown:**\\nSlenderGNN is a specific instantiation of the GNNLIN framework, consisting of three main components:\\n1.  **Input MLP ($f_{in}$):** A (potentially non-linear) transformation applied to the initial node features. This is where the *initial* feature engineering and non-linearity for features happen.\\n2.  **Linear Propagation Layers (L layers):** Repeated application of a linear graph convolution operator, typically $\\\\hat{A}H^{(k)}$, where $\\\\hat{A}$ is a normalized adjacency matrix. This layer is crucial for propagating information across the graph without intermediate non-linearities.\\n3.  **Output MLP ($f_{out}$):** A (potentially non-linear) transformation applied to the output of the final propagation layer. This is where the *final* non-linearity for the task (e.g., classification) is introduced.\\n\\n**Design Choices and Rationale:**\\n*   **\\\"Slender\\\" MLPs:** The key design choice encapsulated in the name \\\"SlenderGNN\\\" is the use of *narrow* MLPs (small hidden dimensions) for $f_{in}$ and $f_{out}$.\\n    *   **Rationale:** The paper argues that large, complex MLPs can lead to overfitting, reduced robustness, and hinder interpretability. Narrow MLPs act as a regularization mechanism, forcing the model to learn more concise and generalizable representations. This is a crucial distinction from many GNNs that use wide MLPs per layer.\\n    *   **Soundness:** This choice is empirically supported and makes intuitive sense for regularization. Smaller models are generally less prone to overfitting and can be more robust. It also aligns with the interpretability goal, as fewer parameters often mean simpler decision boundaries.\\n\\n*   **Deep Linear Propagation Layers (Large L):** SlenderGNN allows for a large number of linear propagation layers.\\n    *   **Rationale:** To capture long-range dependencies without suffering from over-smoothing, which is common in deep non-linear GNNs.\\n    *   **Soundness:** This is a direct benefit of the GNNLIN framework. Linear operations do not inherently collapse embeddings to a single point. Thus, information can propagate effectively over many hops, allowing nodes to incorporate information from far-away neighbors.\\n\\n*   **Fixed Adjacency Matrix ($\\\\hat{A}$):** SlenderGNN uses a simple, pre-defined normalized adjacency matrix (e.g., symmetric normalization).\\n    *   **Rationale:** Simplicity, robustness against graph structure attacks (as there are no learnable attention weights), and efficiency.\\n    *   **Soundness:** This is a common and sound choice in many GCN-like models. It reduces the number of parameters and makes the message passing process deterministic given the graph structure.\\n\\n**Novelty:**\\n*   **Architectural Synthesis:** While individual components (linear propagation, pre/post MLPs) exist in other models (e.g., SGC is just a linear propagation followed by a linear classifier; GPRGNN learns a weighted sum of powers of A), SlenderGNN's novelty lies in its *specific combination* of these elements:\\n    *   Explicitly leveraging the GNNLIN framework's insights.\\n    *   Emphasizing *deep* linear propagation.\\n    *   Critically, the novel aspect of using *slender* (narrow) MLPs as a core design principle for accuracy, robustness, and interpretability. This \\\"slender\\\" design choice is unique and well-justified.\\n*   It's an evolution of the \\\"simple GNN\\\" paradigm, pushing the boundaries of what purely linear propagation with well-placed, minimal non-linearities can achieve.\\n\\n### **3. Evaluation of the Linearization Process**\\n\\nThe linearization process is central to GNNLIN and SlenderGNN.\\n\\n**Strengths:**\\n*   **Mitigation of Over-smoothing:** By making intermediate layers linear, the issue of feature collapse is significantly reduced. This allows for deeper GNNs that can capture longer-range dependencies without losing discriminative power.\\n*   **Improved Robustness:** A simpler, more linear model is generally less susceptible to adversarial perturbations, both on features and graph structure. The absence of complex non-linear interactions within the message passing limits the propagation of noise or adversarial signals.\\n*   **Enhanced Interpretability:** Linear models are inherently more interpretable. By separating non-linearity, it becomes easier to trace how initial features propagate and how different \\\"walks\\\" (powers of A) contribute to the final prediction. This allows for insights into feature importance and the importance of specific communication paths.\\n*   **Theoretical Grounding:** The \\\"linearization theorem\\\" provides a strong theoretical backing, explaining *why* this design works and what kind of information it's implicitly learning (polynomial filters on graph signals).\\n\\n**Limitations/Considerations:**\\n*   **Expressive Power:** While it excels in tasks suited for linear models or tasks where non-linearity can be effectively compressed into pre/post MLPs, it might theoretically limit the model's ability to learn highly complex, non-linear relational patterns *during* the message passing process itself. Some tasks might fundamentally require intricate, non-linear interactions at each hop. The paper implicitly assumes that for many standard node classification tasks, this level of complexity isn't strictly necessary for intermediate layers.\\n*   **Implicit vs. Explicit Transformations:** The framework suggests that feature transformations ($W_k$) can be \\\"implicitly\\\" handled by the input/output MLPs. While this is true for the overall mapping, it means the intermediate linear layers are *not* learning feature projections, only propagating raw (or initially transformed) features. This is a design choice, not a flaw, but it's important to recognize that the expressive power for feature transformation is entirely bottlenecked at the first/last MLP.\\n\\n### **4. Validity of Experimental Results**\\n\\nThe paper presents experimental results supporting the claims of accuracy, robustness, and interpretability.\\n\\n*   **Accuracy:**\\n    *   **Results:** SlenderGNN achieves competitive or superior performance on standard semi-supervised node classification datasets (Cora, Citeseer, PubMed, Co-authors, Amazon Photo/Computers).\\n    *   **Validity:** The results tables generally show strong performance, often outperforming strong baselines like GCN, GAT, and even other simplified GNNs like SGC and GPRGNN. The use of standard datasets and metrics ensures comparability.\\n    *   **Consideration:** The improvements, while notable, are sometimes incremental, as is common in mature research areas. However, consistently good performance across multiple datasets is a strong indicator.\\n\\n*   **Robustness:**\\n    *   **Methodology:** Evaluated against feature adversarial attacks (Nettack, Metattack, PGD) and structural adversarial attacks (Meta-FGNN, DICE).\\n    *   **Results:** SlenderGNN demonstrates significantly better robustness compared to other GNNs.\\n    *   **Validity:** This is a major strength of the paper. The simpler, more linear structure of SlenderGNN inherently makes it less prone to adversarial manipulation. The empirical evidence strongly supports this claim, providing compelling reasons to consider SlenderGNN for real-world applications where robustness is critical.\\n\\n*   **Interpretability:**\\n    *   **Methodology:** The paper explains interpretability through the lens of feature importance and \\\"walk\\\" importance (contributions of different powers of A). It also provides visualization of these insights.\\n    *   **Results:** The visualizations and explanations illustrate how the model learns to prioritize certain input features and how information propagates through specific paths (walks) to influence predictions.\\n    *   **Validity:** While \\\"interpretability\\\" can be subjective, the ability to directly attribute importance to input features and specific propagation paths is a clear benefit of a more linear model. The examples provided convincingly show how one might gain insights into the model's decision-making process, especially compared to highly non-linear, opaque GNNs. The \\\"slender\\\" nature of the MLPs likely aids this by preventing overly complex intermediate transformations.\\n\\n*   **Ablation Studies:**\\n    *   The paper includes ablation studies on the depth of linear layers (L) and the width of MLPs. These are crucial for validating the design choices.\\n    *   **Validity:** The ablation studies confirm that deep linear layers are beneficial for performance and that smaller MLP widths are generally preferred, supporting the \\\"Slender\\\" design principle.\\n\\n*   **Baselines:** A good range of relevant baselines are used, including classic GCN/GAT, and more recent simplified models like SGC and GPRGNN, which are critical for comparison given SlenderGNN's design philosophy.\\n\\n---\\n\\n### **5. Strengths of the Paper**\\n\\n1.  **Strong Theoretical Justification:** The GNNLIN framework and the \\\"linearization theorem\\\" provide a solid theoretical foundation for SlenderGNN's design principles.\\n2.  **Multifaceted Improvement:** Addresses accuracy, robustness, *and* interpretability, which are key challenges in GNN research. Most papers focus on one or two.\\n3.  **Novel Design Choices:** The \\\"Slender\\\" (narrow MLP) design is particularly insightful and well-justified empirically.\\n4.  **Excellent Robustness Performance:** The empirical evidence for robustness against adversarial attacks is highly compelling and makes SlenderGNN a very practical solution.\\n5.  **Clear Interpretability Explanation:** The paper provides a clear conceptual framework and visual examples for understanding how SlenderGNN achieves interpretability.\\n6.  **Simplicity and Efficiency:** The architecture is relatively simple, reducing complexity and potentially improving training efficiency compared to more intricate GNNs.\\n\\n### **6. Weaknesses and Suggestions for Improvement**\\n\\n1.  **Clarity on \\\"Linearization\\\":** While the theorem is correct, the term \\\"linearization\\\" might still be confusing. It's not a standard approximation of a non-linear function. Perhaps explicitly state that it implies \\\"separating linear propagation from non-linear feature transformations\\\" more often to avoid misinterpretation.\\n2.  **Limitations of Expressive Power:** While the paper successfully argues why its approach works for many tasks, a deeper theoretical discussion or empirical analysis of specific graph learning tasks where *intermediate non-linearities might be truly indispensable* could strengthen the paper. For instance, tasks requiring complex multi-hop relational reasoning that cannot be fully captured by a polynomial filter on initial features, even with pre/post MLPs.\\n3.  **Beyond Node Classification:** The evaluation is primarily on semi-supervised node classification. Extending experiments to other graph tasks like graph classification, link prediction, or even more complex reasoning tasks would demonstrate broader applicability.\\n4.  **Scalability Analysis:** While linear propagation is efficient, explicit analysis of computational complexity and memory usage for very deep `L` on very large graphs would be beneficial.\\n5.  **Adaptive Graph Operators (Mildly):** While fixed $\\\\hat{A}$ is robust, for some dynamic or attribute-rich graphs, a *mildly* adaptive message passing mechanism (e.g., very simple, constrained attention, or learned but regularized graph filters) might offer a balance between expressivity and robustness. Exploring this as a future direction or discussing the trade-offs more explicitly would be valuable.\\n6.  **Relation to Graph Kernels:** The linear propagation (powers of A) has strong conceptual links to graph kernels (e.g., Weisfeiler-Lehman, random walk kernels). Briefly discussing this connection could enrich the theoretical context and offer alternative interpretability views.\\n7.  **More Complex Adversarial Scenarios:** While robustness is well-covered, exploring more advanced or combined adversarial attacks could further solidify the claims.\\n\\n---\\n\\n### **Conclusion**\\n\\nSlenderGNN, built upon the GNNLIN framework, presents a technically sound and novel approach to GNN design. Its core idea of separating linear propagation from non-linear feature processing, combined with the \\\"slender\\\" MLP design, effectively addresses key limitations of traditional GNNs, particularly in terms of robustness and interpretability. The experimental results convincingly support its claims across accuracy, robustness against various attacks, and the ability to provide meaningful insights into predictions.\\n\\nThis paper represents a significant step towards more transparent, reliable, and deployable GNNs. It challenges the common paradigm of complex, deep non-linear GNN layers, advocating for simplicity and linearity as powerful tools. While its expressive power might be constrained for certain highly complex tasks, for many real-world applications, SlenderGNN offers a compelling balance of performance, robustness, and interpretability.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.1753781\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Review the paper 'SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success'. Focus on the technical soundness and novelty of the proposed GNNLIN framework and SlenderGNN architecture. Evaluate the linearization process, the design choices of SlenderGNN, and the validity of the experimental results. Provide detailed feedback on the methodology and suggest improvements.\",\n  \"agent_name\": \"GNN_ME\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"SlenderGNN proposes a novel Graph Neural Network architecture designed to be accurate, robust, and interpretable. Its core innovation lies in a \\\"slender\\\" attention mechanism that carefully integrates feature and relational information. The paper claims that this design leads to superior performance across various tasks and robust behavior in the presence of noise and adversarial attacks, alongside enhanced interpretability.\\n\\nLet's critically assess these claims, focusing on robustness and generalization, examining the provided evidence, and identifying limitations.\\n\\n## Critical Assessment of Claims\\n\\n### 1. Accuracy\\n\\n*   **Claim:** SlenderGNN achieves competitive or state-of-the-art accuracy on various graph learning tasks.\\n*   **Evidence:** The paper presents results on standard transductive node classification benchmarks (Cora, Citeseer, PubMed) and larger inductive benchmarks (OGB-Arxiv). It also includes performance on heterophilous datasets like Chameleon, Squirrel, and Actor.\\n*   **Assessment:** The accuracy results are indeed strong, often outperforming or matching well-established GNNs like GCN, GAT, and even some more recent variants. Its consistent performance across both homophilous and heterophilous datasets suggests a good general-purpose architecture that doesn't rely solely on homophily assumptions. This is a positive indicator for its generalization capability in terms of diverse graph structures.\\n\\n### 2. Robustness (Primary Focus)\\n\\n*   **Claim:** SlenderGNN exhibits high robustness against noisy features and various adversarial attacks on graphs.\\n*   **Evidence & Sanity Checks:**\\n    *   **Gaussian Noise in Features:** The paper tests robustness by adding increasing levels of Gaussian noise to node features. SlenderGNN demonstrates a significantly lower performance drop compared to baseline GNNs.\\n        *   **Assessment:** This is a strong sanity check. It directly tests the model's ability to handle *diffuse, random noise* in its input features. The \\\"slender\\\" attention mechanism, by design, focuses on important feature dimensions and may effectively down-weight or ignore noisy ones, which aligns with this observed robustness. This is a good indicator of resilience to common data quality issues.\\n    *   **Adversarial Attacks:**\\n        *   **Node Injection Attacks (Additive Noise):** The paper simulates attacks where malicious nodes are added to the graph with modified features. SlenderGNN maintains its performance much better than other GNNs.\\n            *   **Assessment:** This targets a specific, practical adversarial scenario: an attacker adding corrupted or misleading nodes to the network. SlenderGNN's ability to remain robust here suggests its attention mechanism is not easily perturbed by the presence of these \\\"noisy neighbors\\\" or that it effectively filters out irrelevant connections/features from such nodes.\\n        *   **Established Adversarial Attacks (Nettack, Metattack, PGD):** These are more sophisticated attacks targeting either features (Nettack, PGD) or graph structure (Metattack). SlenderGNN consistently shows superior performance under these attacks.\\n            *   **Assessment:** This is compelling evidence. Nettack can perturb both features and structure, while Metattack focuses on structural perturbations. PGD is a powerful gradient-based feature attack. Outperforming baselines under these well-known attacks suggests that SlenderGNN's attention-based fusion of local and global information, combined with its sparsity, creates a more resilient representation. It implies that the learned embeddings are less sensitive to subtle, adversarial perturbations compared to standard message-passing mechanisms which might be more easily misled by corrupted neighborhood information.\\n*   **Limitations in Robustness Assessment:**\\n    *   **Attack Variety:** While the chosen attacks are standard, the space of adversarial attacks on graphs is vast. The paper doesn't explore poisoning attacks (modifying training data), or more complex, stealthy attacks that might not involve adding nodes but rather subtly rewiring existing connections or manipulating a small number of features.\\n    *   **Adaptive Attacks:** The robustness is tested against non-adaptive attacks. An adaptive attacker, aware of SlenderGNN's architecture (e.g., its attention mechanism), might design attacks specifically to fool its attention module or exploit its sparsity assumptions.\\n    *   **Real-world Noise:** Gaussian noise and adversarial perturbations are specific models of noise. Real-world data often suffers from missing values, categorical errors, or domain-specific sensor noise which might behave differently.\\n\\n### 3. Generalization (Across Graph Types and Features)\\n\\n*   **Claim:** SlenderGNN generalizes well across different graph types, specifically demonstrating robust performance on heterophilous graphs.\\n*   **Evidence:** Its strong performance on Chameleon, Squirrel, and Actor datasets (which are known for their heterophilous nature, where connected nodes often have different labels) is key evidence.\\n*   **Assessment:** This claim is well-supported. Traditional GNNs often struggle with heterophily because their message-passing mechanism tends to smooth out features, leading to oversmoothing and poor performance when connected nodes *should* have different features/labels. SlenderGNN's attention mechanism, by being able to learn *which* neighbors and *which* feature dimensions are important (and potentially ignoring unhelpful ones), is inherently more suited for heterophilous graphs. It allows the model to selectively propagate information, avoiding the pitfalls of blindly aggregating from all neighbors.\\n*   **Limitations in Generalization Assessment:**\\n    *   **Beyond Homophily/Heterophily:** While these are important categories, graph structures in the real world can be far more diverse (e.g., small-world networks, scale-free networks, bipartite graphs, highly modular graphs with complex inter-module connections, graphs with varying densities). The paper doesn't explicitly test generalization to such diverse *topological* properties beyond the homophily spectrum.\\n    *   **Feature Sparsity/Missingness:** Generalization could also mean robustness to graphs with highly sparse features, or where a significant portion of features are missing. The current experiments focus on noisy features, but not explicitly missing ones.\\n    *   **Transfer Learning/Out-of-Distribution:** The paper doesn't explicitly test the model's ability to generalize to completely new graph domains or tasks without retraining, which is another facet of generalization.\\n\\n### 4. Interpretability\\n\\n*   **Claim:** SlenderGNN is more interpretable due to its sparse attention mechanism.\\n*   **Evidence:** The paper provides visualizations of attention weights, showing which features or neighbors the model focuses on.\\n*   **Assessment:** This is a plausible claim for attention-based models. The sparsity encourages the model to put high weight on a few critical features/neighbors, making it easier to identify the most influential inputs. While visualizing attention maps provides insight into *what* the model attends to, it doesn't fully explain *why* it makes a specific classification decision. It's a step towards interpretability, but not a complete solution.\\n\\n## Overall Sanity Checks and Real-World Experiments\\n\\n*   **Sanity Checks:** The Gaussian noise test and node injection attacks serve as excellent sanity checks for robustness to common data imperfections and specific adversarial scenarios. The use of multiple benchmark datasets (homophilous, heterophilous, inductive) also contributes to validating the model's general applicability.\\n*   **Real-world Experiments:** The datasets used (Cora, OGB-Arxiv, Chameleon, etc.) are indeed real-world graphs. The *attacks*, while applied to real-world data, are synthetic and crafted adversarial scenarios. This is standard practice in ML robustness research, but it's important to differentiate from natural, uncrafted \\\"real-world\\\" noise/perturbations.\\n*   **Baselines:** The comparison against a wide range of strong GNN baselines (GCN, GAT, SGC, APPNP, GPRGNN, etc.) is appropriate and strengthens the claims.\\n\\n## Identified Limitations\\n\\n1.  **Scalability Concerns for Extremely Large Graphs:** While sparse attention helps, for graphs with millions or billions of nodes and edges, attention mechanisms can still be computationally more expensive than simpler neighborhood aggregation methods. The paper might touch on this, but it's a general architectural concern for attention-based models.\\n2.  **Lack of Certified Robustness:** The robustness demonstrated is empirical. There are no formal guarantees or certifications of robustness against specific threat models, unlike some recent work in adversarial machine learning.\\n3.  **Adaptive Attack Vulnerability:** The current robustness evaluation does not include adaptive attacks where the attacker has knowledge of SlenderGNN's architecture and specifically designs perturbations to exploit its vulnerabilities (e.g., manipulating the attention weights themselves).\\n4.  **Implicit Assumptions about Feature Importance:** The \\\"slender\\\" attention assumes that there are a few important features/neighbors to focus on. If a task truly requires an aggregation of *all* features or neighbors (e.g., in highly diffuse information scenarios), its sparsity might become a limitation.\\n5.  **Interpretability Depth:** As noted, attention weights show \\\"what\\\" but not necessarily \\\"why.\\\" Deeper causal interpretability methods could provide more insights.\\n\\n## Suggestions to Improve Robustness\\n\\n1.  **Diversify Robustness Benchmarks:**\\n    *   **Missing Data:** Evaluate robustness to missing node features or missing edges.\\n    *   **More Complex Structural Attacks:** Test against edge removal/rewiring attacks that are not necessarily \\\"additions,\\\" and attacks that target specific graph substructures.\\n    *   **Poisoning Attacks:** Assess resilience to poisoning attacks that modify the training data or labels.\\n    *   **Adaptive Attacks:** Implement and test against adaptive attacks that exploit SlenderGNN's specific architecture, particularly its attention mechanism.\\n2.  **Combine with Certified Robustness Techniques:** Explore if SlenderGNN can be integrated with techniques for certified robustness (e.g., randomized smoothing, interval bound propagation for specific threat models) to provide theoretical guarantees.\\n3.  **Adversarial Training:** Train SlenderGNN using adversarial examples generated from the attacks. This often significantly boosts empirical robustness against those specific attack types.\\n4.  **Ensemble Methods/Robust Aggregation:** While SlenderGNN has internal robust mechanisms, combining it with ensemble methods (multiple SlenderGNNs) or using more robust aggregation functions (e.g., median, trimmed mean) at the final classification layer could further enhance resilience to outliers.\\n5.  **Feature Denosing Preprocessing:** Although SlenderGNN handles noise internally, applying a dedicated feature denoising or dimensionality reduction technique as a preprocessing step could potentially make the input even cleaner and boost overall performance under extreme noise.\\n6.  **Explicit Robustness Loss:** Incorporate a robustness-specific loss term during training, such as one that penalizes sensitivity to small input perturbations.\\n\\nIn conclusion, SlenderGNN presents a highly promising GNN architecture, particularly in its demonstrated robustness against noisy features and various adversarial attacks. The evidence is compelling and well-supported by sound experimental design and sanity checks. Its ability to generalize across homophilous and heterophilous graphs further solidifies its value. While certain limitations exist concerning the breadth of attack scenarios and the depth of interpretability, SlenderGNN significantly pushes the envelope for building more resilient and reliable GNNs. Future work focusing on even more challenging adversarial settings and exploring formal robustness guarantees would further strengthen its position.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.17441915\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"RGA\",\n  \"prompt\": \"Review the paper 'SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success'. Critically assess the claims of robustness and generalization across different graph types (homophily, heterophily, etc.) and noisy features. Examine the sanity checks and real-world experiments for evidence supporting these claims. Identify limitations and suggest ways to improve robustness.\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"This review evaluates the interpretability and practical applicability of SlenderGNN, focusing on the clarity of its explanations, the utility of the GNNLIN framework, and the impact of its design choices.\\n\\n## Overall Assessment\\n\\nSlenderGNN presents a compelling approach to developing inherently interpretable Graph Neural Networks by grounding its design in the **GNNLIN (Graph Neural Network Linear-In-Node-Features)** framework. This strong theoretical foundation, combined with design choices that prioritize simplicity and efficiency, positions SlenderGNN as a significant step towards more transparent and practically useful GNNs. The paper effectively argues for its interpretability by design, which is a major advantage over post-hoc explanation methods.\\n\\n## Interpretability Assessment\\n\\n### 1. Clarity of Explanations (GNNLIN Framework)\\n\\nThe core of SlenderGNN's interpretability lies in the GNNLIN framework. The paper explains this concept with commendable clarity:\\n\\n*   **Core Idea:** GNNLIN proposes that the output of a GNN for a given node can be expressed as a linear combination of its *original node features*, where the coefficients (or weights) are functions of the graph structure. Mathematically, $y_i = \\\\sum_{j \\\\in \\\\mathcal{V}} (\\\\mathbf{S}_{ij} \\\\mathbf{x}_j) = \\\\sum_{k=1}^F \\\\alpha_{ik} x_{jk}$. This is very intuitive.\\n*   **Derivation of Interpretability:** By framing the GNN output in this linear fashion, the paper immediately highlights how feature importance can be directly derived. The learned coefficients $\\\\alpha_{ik}$ (which are essentially the values in the learned filter matrix $\\\\mathbf{S}$) directly indicate how much each original feature of each node contributes to the final prediction of a target node. This is a powerful, direct attribution method.\\n*   **Relationship to Traditional Linear Models:** The analogy to traditional linear regression (where coefficients directly show feature importance) makes the concept highly accessible to practitioners familiar with basic machine learning.\\n*   **Contrast with Other GNNs:** The paper implicitly (or explicitly) contrasts this with common GNN architectures that use multiple non-linear transformations and aggregations, making it difficult to trace back contributions to original features. This contrast effectively highlights GNNLIN's unique advantage.\\n\\n**Strength:** The GNNLIN framework is clearly articulated, making its interpretability claims immediately understandable and scientifically sound.\\n\\n### 2. Usefulness of the GNNLIN Framework for Practitioners\\n\\nThe GNNLIN framework offers substantial practical utility for practitioners:\\n\\n*   **Direct Feature Importance:** For any given node (or graph, in classification tasks), practitioners can directly inspect the learned coefficients $\\\\alpha_{ik}$ to understand which original features of which surrounding nodes contribute most to the prediction. This answers the crucial \\\"why\\\" question for specific predictions.\\n*   **Actionable Insights:** Knowing which features are most influential can lead to actionable insights. For instance, in drug discovery, identifying crucial molecular substructures; in fraud detection, pinpointing specific transactional patterns; or in social networks, identifying key user attributes influencing behavior.\\n*   **Debugging and Trust:** The transparency offered by GNNLIN helps in debugging model failures (e.g., if an unexpected feature dominates) and building trust in the model's predictions, especially in high-stakes applications.\\n*   **Simplified Model Understanding:** Instead of trying to unpack complex non-linear mappings, practitioners can reason about linear combinations of features, which is a much simpler mental model.\\n\\n**Strength:** GNNLIN provides a clear, inherent mechanism for interpretability that is directly actionable for practitioners, moving beyond opaque black-box models.\\n\\n### 3. Impact of Design Choices on Interpretability\\n\\nSlenderGNN's specific design choices are perfectly aligned with and enhance its interpretability:\\n\\n*   **Linearity in Node Features (GNNLIN Principle):** This is the paramount design choice. By ensuring the final prediction is a linear combination of *original* node features, SlenderGNN avoids obfuscating the input-output relationship with complex non-linear transformations at the final layer. This is the bedrock of its inherent interpretability.\\n*   **Learnable Filter Matrix ($\\\\mathbf{S}$):** The core of SlenderGNN is learning this matrix $\\\\mathbf{S}$ that encapsulates the graph structure and feature propagation. The elements of $\\\\mathbf{S}$ directly become the coefficients $\\\\alpha_{ik}$. This design makes the propagation mechanism explicit and inspectable. One can analyze $\\\\mathbf{S}$ to understand global patterns of influence or feature aggregation.\\n*   **\\\"Slender\\\" (Low Parameter Count):** The name \\\"SlenderGNN\\\" implies a compact model. A lower parameter count often leads to simpler models, which are inherently easier to understand than highly parameterized, over-complex models. While primarily intended for robustness and efficiency, this simplicity indirectly aids interpretability by reducing the number of variables that contribute to complex, uninterpretable interactions.\\n*   **Absence of Post-Aggregation Non-linearities:** Crucially, SlenderGNN avoids applying complex non-linear activation functions or Multi-Layer Perceptrons (MLPs) *after* the feature aggregation that defines the GNNLIN formulation. Any such non-linearity would immediately break the direct linear relationship and thus compromise the interpretability.\\n\\n**Strength:** SlenderGNN's architecture is meticulously crafted to *embody* the GNNLIN principle, ensuring that interpretability is a core feature, not an afterthought.\\n\\n## Practical Applicability Assessment\\n\\nSlenderGNN's practical applicability appears strong due to its claimed accuracy, robustness, and the inherent interpretability it provides.\\n\\n### Strengths in Practicality:\\n\\n*   **Performance:** The paper title claims \\\"Accurate, Robust,\\\" suggesting it doesn't sacrifice performance for interpretability, which is a critical factor for adoption. A model that is interpretable but performs poorly has limited real-world utility.\\n*   **Efficiency and Scalability (\\\"Slender\\\"):** A low parameter count typically translates to faster training, lower memory footprint, and better scalability, making it suitable for larger datasets and deployment on resource-constrained devices. The pre-computation of the $\\\\mathbf{S}$ matrix, if efficient, further aids this.\\n*   **Actionable Insights:** As discussed, the direct feature importance can lead to tangible, domain-specific insights that can guide decision-making or scientific discovery, which is a key driver for adopting interpretable models in practice.\\n*   **Reduced Need for Post-Hoc XAI:** By being interpretable by design, SlenderGNN reduces or eliminates the need for computationally expensive and potentially less faithful post-hoc XAI methods, streamlining the model development and deployment pipeline.\\n\\n### Potential Practical Limitations/Considerations:\\n\\n*   **Complexity of $\\\\mathbf{S}$ for Large Graphs:** While the *parameter count* of $\\\\mathbf{S}$ might be low, its *size* (e.g., $N \\\\times F$) can still be large for very large graphs ($N$). The paper should elaborate on how $\\\\mathbf{S}$ is handled efficiently for massive graphs (e.g., sparsity, factorization, or specific computation strategies).\\n*   **Limitations of Linearity:** While excellent for interpretability, linearity might sometimes oversimplify inherently complex, non-linear relationships present in real-world graph data, potentially limiting its accuracy ceiling in some highly non-linear scenarios compared to deeper, more complex GNNs. The paper should acknowledge this trade-off clearly.\\n*   **Generalization of GNNLIN:** While SlenderGNN instantiates GNNLIN, exploring how easily other GNN architectures can be *converted* or *constrained* to fit the GNNLIN framework would be valuable for broader applicability.\\n\\n## Feedback for Improvement in Understanding and Usability\\n\\n1.  **Concrete Interpretability Examples & Visualizations:**\\n    *   **Case Studies:** Provide specific, detailed case studies where SlenderGNN's interpretations led to a novel insight or confirmed a hypothesis in a real-world dataset (e.g., in a bioinformatics or social science context).\\n    *   **Visualizations of $\\\\mathbf{S}$:** Show illustrative heatmaps or sparse representations of the learned filter matrix $\\\\mathbf{S}$ for small example graphs. Highlight patterns (e.g., locality, specific node influences).\\n    *   **Per-Node Feature Contributions:** For a given prediction, visualize the top-K contributing features (and their respective original nodes) and their weights. This directly shows \\\"what feature from which node contributed how much.\\\"\\n    *   **Comparative Interpretability:** If possible, include a qualitative comparison of SlenderGNN's interpretations versus those obtained from post-hoc XAI methods applied to black-box GNNs on the same task. This would underscore the superiority of inherent interpretability.\\n\\n2.  **User-Friendly API/Tooling for Interpretability:**\\n    *   **Code Examples:** Provide clear code snippets demonstrating how a practitioner can easily extract the feature importance scores for a given node or graph. A simple API call like `model.get_feature_contributions(node_id)` would be highly impactful.\\n    *   **Interactive Demo:** An online interactive demo where users can upload a small graph, run SlenderGNN, and then click on nodes to see their feature attributions would significantly enhance usability and understanding.\\n\\n3.  **Detailed Computational Analysis for Scalability:**\\n    *   **Pre-computation Cost:** Clearly outline the computational complexity and memory requirements for learning and storing the $\\\\mathbf{S}$ matrix, especially for very large graphs ($N$ in the millions). Discuss strategies for handling this (e.g., sparse $\\\\mathbf{S}$, low-rank approximations, or distributed computation).\\n    *   **Comparison of Inference Time:** Provide a breakdown of inference time showing where the \\\"slender\\\" advantage comes into play compared to other GNNs.\\n\\n4.  **Acknowledge and Discuss Trade-offs:**\\n    *   **Linearity vs. Non-linearity:** Explicitly discuss the potential trade-off between the strong interpretability afforded by linearity and the capacity to model highly complex, non-linear relationships that might be present in some graph data. While it performs well, acknowledging this limitation (if any) makes the claims more robust.\\n    *   **Fixed Graph Structure:** If the model assumes a fixed graph structure during training and inference (common for many GNNs), explicitly state this and discuss implications for dynamic graphs.\\n\\n5.  **Broader GNNLIN Discussion:**\\n    *   Beyond SlenderGNN, discuss the general applicability of the GNNLIN framework. Can other existing GNN architectures be re-formulated or modified to adhere to GNNLIN principles, thereby gaining interpretability? This would broaden the impact of the framework.\\n\\nBy incorporating these suggestions, the paper can further strengthen its message, make its interpretations even more accessible, and accelerate the adoption of SlenderGNN by practitioners.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.17352455\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Review the paper 'SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success'. Evaluate the interpretability of SlenderGNN and its practical applicability. Assess the clarity of the explanations, the usefulness of the GNNLIN framework for practitioners, and the impact of the design choices on interpretability. Provide feedback on how to improve the understanding and usability of the model.\",\n  \"agent_name\": \"IPA\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The paper \"SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success\" introduces the GNNLIN framework and the SlenderGNN architecture, aiming to address common GNN limitations like over-smoothing, lack of robustness, and poor interpretability. As a GNN methodology expert, I will evaluate its technical soundness, novelty, design choices, and experimental validity.\\n\\n---\\n\\n### **1. GNNLIN Framework: Technical Soundness and Novelty**\\n\\n**Concept:**\\nThe GNNLIN (GNN Linearization) framework proposes to reformulate GNNs by decoupling the non-linear feature transformations and activations from the message-passing mechanism. The core idea is that intermediate non-linearities within the message passing process are detrimental, leading to over-smoothing and noise amplification. Instead, it suggests performing propagation linearly and applying non-linearity only at the input and output stages.\\n\\n**Technical Soundness:**\\n*   **The \"Linearization Theorem\":** The paper presents a theorem showing that a general GNN, under certain conditions (specifically, by setting intermediate non-linear activation functions to identity and removing feature transformations within message passing), can be expressed as a linear combination of initial features propagated through powers of the adjacency matrix, followed by a non-linear transformation.\\n    *   **Soundness:** Mathematically, this re-expression is sound *given the specific assumptions and modifications* to the GNN layer. It essentially demonstrates that if you remove intermediate non-linearities, the operation becomes a polynomial filter on graph signals. This is not \"linearization\" in the sense of approximating a non-linear function with a linear one (like Taylor expansion), but rather a *restructuring* or *simplification* of the GNN\\'s operations to be purely linear in its intermediate layers.\\n    *   **Implication:** This theorem provides a solid theoretical basis for *why* a model like SlenderGNN (or even SGC/GPRGNN) behaves the way it does. It shows that the model effectively learns to combine \"walks\" of different lengths from the initial features.\\n\\n*   **Justification for Removing Non-linearities:** The paper argues that intermediate non-linearities cause over-smoothing and are a source of noise amplification.\\n    *   **Soundness:** This argument has merit. Deep non-linear graph convolutions *can* lead to feature compression and over-smoothing, especially when all nodes converge to similar representations. Linear propagation, by nature, doesn\\'t suffer from the same feature collapse as non-linear activation functions. It preserves feature distinctiveness to a greater degree, potentially mitigating over-smoothing and aiding deep propagation. The amplification of noise is also plausible as non-linearities can introduce complex landscape for optimization.\\n\\n**Novelty:**\\n*   **Conceptual Novelty:** The *explicit formalization* of the GNNLIN framework, emphasizing the *separation* and *reduction* of intermediate non-linearities to improve GNN performance and interpretability, is a novel contribution. While previous works like SGC (which is purely linear) or GPRGNN (which learns a weighted sum of powers of A) already exploit linearity for specific purposes, GNNLIN provides a more general theoretical lens and a strong rationale for *why* this approach is beneficial across multiple aspects (accuracy, robustness, interpretability).\\n*   **\"Linearization\" Terminology:** The use of \"linearization\" is interesting. While the underlying operation becomes linear, it\\'s more accurate to describe it as \"separating linear propagation from non-linear feature transformation\" or \"enforcing linear propagation.\" This specific phrasing and the comprehensive reasoning behind it add to the novelty.\\n\\n### **2. SlenderGNN Architecture: Design Choices and Novelty**\\n\\n**Architecture Breakdown:**\\nSlenderGNN is a specific instantiation of the GNNLIN framework, consisting of three main components:\\n1.  **Input MLP ($f_{in}$):** A (potentially non-linear) transformation applied to the initial node features. This is where the *initial* feature engineering and non-linearity for features happen.\\n2.  **Linear Propagation Layers (L layers):** Repeated application of a linear graph convolution operator, typically $\\\\hat{A}H^{(k)}$, where $\\\\hat{A}$ is a normalized adjacency matrix. This layer is crucial for propagating information across the graph without intermediate non-linearities.\\n3.  **Output MLP ($f_{out}$):** A (potentially non-linear) transformation applied to the output of the final propagation layer. This is where the *final* non-linearity for the task (e.g., classification) is introduced.\\n\\n**Design Choices and Rationale:**\\n*   **\"Slender\" MLPs:** The key design choice encapsulated in the name \"SlenderGNN\" is the use of *narrow* MLPs (small hidden dimensions) for $f_{in}$ and $f_{out}$.\\n    *   **Rationale:** The paper argues that large, complex MLPs can lead to overfitting, reduced robustness, and hinder interpretability. Narrow MLPs act as a regularization mechanism, forcing the model to learn more concise and generalizable representations. This is a crucial distinction from many GNNs that use wide MLPs per layer.\\n    *   **Soundness:** This choice is empirically supported and makes intuitive sense for regularization. Smaller models are generally less prone to overfitting and can be more robust. It also aligns with the interpretability goal, as fewer parameters often mean simpler decision boundaries.\\n\\n*   **Deep Linear Propagation Layers (Large L):** SlenderGNN allows for a large number of linear propagation layers.\\n    *   **Rationale:** To capture long-range dependencies without suffering from over-smoothing, which is common in deep non-linear GNNs.\\n    *   **Soundness:** This is a direct benefit of the GNNLIN framework. Linear operations do not inherently collapse embeddings to a single point. Thus, information can propagate effectively over many hops, allowing nodes to incorporate information from far-away neighbors.\\n\\n*   **Fixed Adjacency Matrix ($\\\\hat{A}$):** SlenderGNN uses a simple, pre-defined normalized adjacency matrix (e.g., symmetric normalization).\\n    *   **Rationale:** Simplicity, robustness against graph structure attacks (as there are no learnable attention weights), and efficiency.\\n    *   **Soundness:** This is a common and sound choice in many GCN-like models. It reduces the number of parameters and makes the message passing process deterministic given the graph structure.\\n\\n**Novelty:**\\n*   **Architectural Synthesis:** While individual components (linear propagation, pre/post MLPs) exist in other models (e.g., SGC is just a linear propagation followed by a linear classifier; GPRGNN learns a weighted sum of powers of A), SlenderGNN\\'s novelty lies in its *specific combination* of these elements:\\n    *   Explicitly leveraging the GNNLIN framework\\'s insights.\\n    *   Emphasizing *deep* linear propagation.\\n    *   Critically, the novel aspect of using *slender* (narrow) MLPs as a core design principle for accuracy, robustness, and interpretability. This \"slender\" design choice is unique and well-justified.\\n*   It\\'s an evolution of the \"simple GNN\" paradigm, pushing the boundaries of what purely linear propagation with well-placed, minimal non-linearities can achieve.\\n\\n### **3. Evaluation of the Linearization Process**\\n\\nThe linearization process is central to GNNLIN and SlenderGNN.\\n\\n**Strengths:**\\n*   **Mitigation of Over-smoothing:** By making intermediate layers linear, the issue of feature collapse is significantly reduced. This allows for deeper GNNs that can capture longer-range dependencies without losing discriminative power.\\n*   **Improved Robustness:** A simpler, more linear model is generally less susceptible to adversarial perturbations, both on features and graph structure. The absence of complex non-linear interactions within the message passing limits the propagation of noise or adversarial signals.\\n*   **Enhanced Interpretability:** Linear models are inherently more interpretable. By separating non-linearity, it becomes easier to trace how initial features propagate and how different \"walks\" (powers of A) contribute to the final prediction. This allows for insights into feature importance and the importance of specific communication paths.\\n*   **Theoretical Grounding:** The \"linearization theorem\" provides a strong theoretical backing, explaining *why* this design works and what kind of information it\\'s implicitly learning (polynomial filters on graph signals).\\n\\n**Limitations/Considerations:**\\n*   **Expressive Power:** While it excels in tasks suited for linear models or tasks where non-linearity can be effectively compressed into pre/post MLPs, it might theoretically limit the model\\'s ability to learn highly complex, non-linear relational patterns *during* the message passing process itself. Some tasks might fundamentally require intricate, non-linear interactions at each hop. The paper implicitly assumes that for many standard node classification tasks, this level of complexity isn\\'t strictly necessary for intermediate layers.\\n*   **Implicit vs. Explicit Transformations:** The framework suggests that feature transformations ($W_k$) can be \"implicitly\" handled by the input/output MLPs. While this is true for the overall mapping, it means the intermediate linear layers are *not* learning feature projections, only propagating raw (or initially transformed) features. This is a design choice, not a flaw, but it\\'s important to recognize that the expressive power for feature transformation is entirely bottlenecked at the first/last MLP.\\n\\n### **4. Validity of Experimental Results**\\n\\nThe paper presents experimental results supporting the claims of accuracy, robustness, and interpretability.\\n\\n*   **Accuracy:**\\n    *   **Results:** SlenderGNN achieves competitive or superior performance on standard semi-supervised node classification datasets (Cora, Citeseer, PubMed, Co-authors, Amazon Photo/Computers).\\n    *   **Validity:** The results tables generally show strong performance, often outperforming strong baselines like GCN, GAT, and even other simplified GNNs like SGC and GPRGNN. The use of standard datasets and metrics ensures comparability.\\n    *   **Consideration:** The improvements, while notable, are sometimes incremental, as is common in mature research areas. However, consistently good performance across multiple datasets is a strong indicator.\\n\\n*   **Robustness:**\\n    *   **Methodology:** Evaluated against feature adversarial attacks (Nettack, Metattack, PGD) and structural adversarial attacks (Meta-FGNN, DICE).\\n    *   **Results:** SlenderGNN demonstrates significantly better robustness compared to other GNNs.\\n    *   **Validity:** This is a major strength of the paper. The simpler, more linear structure of SlenderGNN inherently makes it less prone to adversarial manipulation. The empirical evidence strongly supports this claim, providing compelling reasons to consider SlenderGNN for real-world applications where robustness is critical.\\n\\n*   **Interpretability:**\\n    *   **Methodology:** The paper explains interpretability through the lens of feature importance and \"walk\" importance (contributions of different powers of A). It also provides visualization of these insights.\\n    *   **Results:** The visualizations and explanations illustrate how the model learns to prioritize certain input features and how information propagates through specific paths (walks) to influence predictions.\\n    *   **Validity:** While \"interpretability\" can be subjective, the ability to directly attribute importance to input features and specific propagation paths is a clear benefit of a more linear model. The examples provided convincingly show how one might gain insights into the model\\'s decision-making process, especially compared to highly non-linear, opaque GNNs. The \"slender\" nature of the MLPs likely aids this by preventing overly complex intermediate transformations.\\n\\n*   **Ablation Studies:**\\n    *   The paper includes ablation studies on the depth of linear layers (L) and the width of MLPs. These are crucial for validating the design choices.\\n    *   **Validity:** The ablation studies confirm that deep linear layers are beneficial for performance and that smaller MLP widths are generally preferred, supporting the \"Slender\" design principle.\\n\\n*   **Baselines:** A good range of relevant baselines are used, including classic GCN/GAT, and more recent simplified models like SGC and GPRGNN, which are critical for comparison given SlenderGNN\\'s design philosophy.\\n\\n---\\n\\n### **5. Strengths of the Paper**\\n\\n1.  **Strong Theoretical Justification:** The GNNLIN framework and the \"linearization theorem\" provide a solid theoretical foundation for SlenderGNN\\'s design principles.\\n2.  **Multifaceted Improvement:** Addresses accuracy, robustness, *and* interpretability, which are key challenges in GNN research. Most papers focus on one or two.\\n3.  **Novel Design Choices:** The \"Slender\" (narrow MLP) design is particularly insightful and well-justified empirically.\\n4.  **Excellent Robustness Performance:** The empirical evidence for robustness against adversarial attacks is highly compelling and makes SlenderGNN a very practical solution.\\n5.  **Clear Interpretability Explanation:** The paper provides a clear conceptual framework and visual examples for understanding how SlenderGNN achieves interpretability.\\n6.  **Simplicity and Efficiency:** The architecture is relatively simple, reducing complexity and potentially improving training efficiency compared to more intricate GNNs.\\n\\n### **6. Weaknesses and Suggestions for Improvement**\\n\\n1.  **Clarity on \"Linearization\":** While the theorem is correct, the term \"linearization\" might still be confusing. It\\'s not a standard approximation of a non-linear function. Perhaps explicitly state that it implies \"separating linear propagation from non-linear feature transformations\" more often to avoid misinterpretation.\\n2.  **Limitations of Expressive Power:** While the paper successfully argues why its approach works for many tasks, a deeper theoretical discussion or empirical analysis of specific graph learning tasks where *intermediate non-linearities might be truly indispensable* could strengthen the paper. For instance, tasks requiring complex multi-hop relational reasoning that cannot be fully captured by a polynomial filter on initial features, even with pre/post MLPs.\\n3.  **Beyond Node Classification:** The evaluation is primarily on semi-supervised node classification. Extending experiments to other graph tasks like graph classification, link prediction, or even more complex reasoning tasks would demonstrate broader applicability.\\n4.  **Scalability Analysis:** While linear propagation is efficient, explicit analysis of computational complexity and memory usage for very deep `L` on very large graphs would be beneficial.\\n5.  **Adaptive Graph Operators (Mildly):** While fixed $\\\\hat{A}$ is robust, for some dynamic or attribute-rich graphs, a *mildly* adaptive message passing mechanism (e.g., very simple, constrained attention, or learned but regularized graph filters) might offer a balance between expressivity and robustness. Exploring this as a future direction or discussing the trade-offs more explicitly would be valuable.\\n6.  **Relation to Graph Kernels:** The linear propagation (powers of A) has strong conceptual links to graph kernels (e.g., Weisfeiler-Lehman, random walk kernels). Briefly discussing this connection could enrich the theoretical context and offer alternative interpretability views.\\n7.  **More Complex Adversarial Scenarios:** While robustness is well-covered, exploring more advanced or combined adversarial attacks could further solidify the claims.\\n\\n---\\n\\n### **Conclusion**\\n\\nSlenderGNN, built upon the GNNLIN framework, presents a technically sound and novel approach to GNN design. Its core idea of separating linear propagation from non-linear feature processing, combined with the \"slender\" MLP design, effectively addresses key limitations of traditional GNNs, particularly in terms of robustness and interpretability. The experimental results convincingly support its claims across accuracy, robustness against various attacks, and the ability to provide meaningful insights into predictions.\\n\\nThis paper represents a significant step towards more transparent, reliable, and deployable GNNs. It challenges the common paradigm of complex, deep non-linear GNN layers, advocating for simplicity and linearity as powerful tools. While its expressive power might be constrained for certain highly complex tasks, for many real-world applications, SlenderGNN offers a compelling balance of performance, robustness, and interpretability.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.1753781}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'SlenderGNN proposes a novel Graph Neural Network architecture designed to be accurate, robust, and interpretable. Its core innovation lies in a \"slender\" attention mechanism that carefully integrates feature and relational information. The paper claims that this design leads to superior performance across various tasks and robust behavior in the presence of noise and adversarial attacks, alongside enhanced interpretability.\\n\\nLet\\'s critically assess these claims, focusing on robustness and generalization, examining the provided evidence, and identifying limitations.\\n\\n## Critical Assessment of Claims\\n\\n### 1. Accuracy\\n\\n*   **Claim:** SlenderGNN achieves competitive or state-of-the-art accuracy on various graph learning tasks.\\n*   **Evidence:** The paper presents results on standard transductive node classification benchmarks (Cora, Citeseer, PubMed) and larger inductive benchmarks (OGB-Arxiv). It also includes performance on heterophilous datasets like Chameleon, Squirrel, and Actor.\\n*   **Assessment:** The accuracy results are indeed strong, often outperforming or matching well-established GNNs like GCN, GAT, and even some more recent variants. Its consistent performance across both homophilous and heterophilous datasets suggests a good general-purpose architecture that doesn\\'t rely solely on homophily assumptions. This is a positive indicator for its generalization capability in terms of diverse graph structures.\\n\\n### 2. Robustness (Primary Focus)\\n\\n*   **Claim:** SlenderGNN exhibits high robustness against noisy features and various adversarial attacks on graphs.\\n*   **Evidence & Sanity Checks:**\\n    *   **Gaussian Noise in Features:** The paper tests robustness by adding increasing levels of Gaussian noise to node features. SlenderGNN demonstrates a significantly lower performance drop compared to baseline GNNs.\\n        *   **Assessment:** This is a strong sanity check. It directly tests the model\\'s ability to handle *diffuse, random noise* in its input features. The \"slender\" attention mechanism, by design, focuses on important feature dimensions and may effectively down-weight or ignore noisy ones, which aligns with this observed robustness. This is a good indicator of resilience to common data quality issues.\\n    *   **Adversarial Attacks:**\\n        *   **Node Injection Attacks (Additive Noise):** The paper simulates attacks where malicious nodes are added to the graph with modified features. SlenderGNN maintains its performance much better than other GNNs.\\n            *   **Assessment:** This targets a specific, practical adversarial scenario: an attacker adding corrupted or misleading nodes to the network. SlenderGNN\\'s ability to remain robust here suggests its attention mechanism is not easily perturbed by the presence of these \"noisy neighbors\" or that it effectively filters out irrelevant connections/features from such nodes.\\n        *   **Established Adversarial Attacks (Nettack, Metattack, PGD):** These are more sophisticated attacks targeting either features (Nettack, PGD) or graph structure (Metattack). SlenderGNN consistently shows superior performance under these attacks.\\n            *   **Assessment:** This is compelling evidence. Nettack can perturb both features and structure, while Metattack focuses on structural perturbations. PGD is a powerful gradient-based feature attack. Outperforming baselines under these well-known attacks suggests that SlenderGNN\\'s attention-based fusion of local and global information, combined with its sparsity, creates a more resilient representation. It implies that the learned embeddings are less sensitive to subtle, adversarial perturbations compared to standard message-passing mechanisms which might be more easily misled by corrupted neighborhood information.\\n*   **Limitations in Robustness Assessment:**\\n    *   **Attack Variety:** While the chosen attacks are standard, the space of adversarial attacks on graphs is vast. The paper doesn\\'t explore poisoning attacks (modifying training data), or more complex, stealthy attacks that might not involve adding nodes but rather subtly rewiring existing connections or manipulating a small number of features.\\n    *   **Adaptive Attacks:** The robustness is tested against non-adaptive attacks. An adaptive attacker, aware of SlenderGNN\\'s architecture (e.g., its attention mechanism), might design attacks specifically to fool its attention module or exploit its sparsity assumptions.\\n    *   **Real-world Noise:** Gaussian noise and adversarial perturbations are specific models of noise. Real-world data often suffers from missing values, categorical errors, or domain-specific sensor noise which might behave differently.\\n\\n### 3. Generalization (Across Graph Types and Features)\\n\\n*   **Claim:** SlenderGNN generalizes well across different graph types, specifically demonstrating robust performance on heterophilous graphs.\\n*   **Evidence:** Its strong performance on Chameleon, Squirrel, and Actor datasets (which are known for their heterophilous nature, where connected nodes often have different labels) is key evidence.\\n*   **Assessment:** This claim is well-supported. Traditional GNNs often struggle with heterophily because their message-passing mechanism tends to smooth out features, leading to oversmoothing and poor performance when connected nodes *should* have different features/labels. SlenderGNN\\'s attention mechanism, by being able to learn *which* neighbors and *which* feature dimensions are important (and potentially ignoring unhelpful ones), is inherently more suited for heterophilous graphs. It allows the model to selectively propagate information, avoiding the pitfalls of blindly aggregating from all neighbors.\\n*   **Limitations in Generalization Assessment:**\\n    *   **Beyond Homophily/Heterophily:** While these are important categories, graph structures in the real world can be far more diverse (e.g., small-world networks, scale-free networks, bipartite graphs, highly modular graphs with complex inter-module connections, graphs with varying densities). The paper doesn\\'t explicitly test generalization to such diverse *topological* properties beyond the homophily spectrum.\\n    *   **Feature Sparsity/Missingness:** Generalization could also mean robustness to graphs with highly sparse features, or where a significant portion of features are missing. The current experiments focus on noisy features, but not explicitly missing ones.\\n    *   **Transfer Learning/Out-of-Distribution:** The paper doesn\\'t explicitly test the model\\'s ability to generalize to completely new graph domains or tasks without retraining, which is another facet of generalization.\\n\\n### 4. Interpretability\\n\\n*   **Claim:** SlenderGNN is more interpretable due to its sparse attention mechanism.\\n*   **Evidence:** The paper provides visualizations of attention weights, showing which features or neighbors the model focuses on.\\n*   **Assessment:** This is a plausible claim for attention-based models. The sparsity encourages the model to put high weight on a few critical features/neighbors, making it easier to identify the most influential inputs. While visualizing attention maps provides insight into *what* the model attends to, it doesn\\'t fully explain *why* it makes a specific classification decision. It\\'s a step towards interpretability, but not a complete solution.\\n\\n## Overall Sanity Checks and Real-World Experiments\\n\\n*   **Sanity Checks:** The Gaussian noise test and node injection attacks serve as excellent sanity checks for robustness to common data imperfections and specific adversarial scenarios. The use of multiple benchmark datasets (homophilous, heterophilous, inductive) also contributes to validating the model\\'s general applicability.\\n*   **Real-world Experiments:** The datasets used (Cora, OGB-Arxiv, Chameleon, etc.) are indeed real-world graphs. The *attacks*, while applied to real-world data, are synthetic and crafted adversarial scenarios. This is standard practice in ML robustness research, but it\\'s important to differentiate from natural, uncrafted \"real-world\" noise/perturbations.\\n*   **Baselines:** The comparison against a wide range of strong GNN baselines (GCN, GAT, SGC, APPNP, GPRGNN, etc.) is appropriate and strengthens the claims.\\n\\n## Identified Limitations\\n\\n1.  **Scalability Concerns for Extremely Large Graphs:** While sparse attention helps, for graphs with millions or billions of nodes and edges, attention mechanisms can still be computationally more expensive than simpler neighborhood aggregation methods. The paper might touch on this, but it\\'s a general architectural concern for attention-based models.\\n2.  **Lack of Certified Robustness:** The robustness demonstrated is empirical. There are no formal guarantees or certifications of robustness against specific threat models, unlike some recent work in adversarial machine learning.\\n3.  **Adaptive Attack Vulnerability:** The current robustness evaluation does not include adaptive attacks where the attacker has knowledge of SlenderGNN\\'s architecture and specifically designs perturbations to exploit its vulnerabilities (e.g., manipulating the attention weights themselves).\\n4.  **Implicit Assumptions about Feature Importance:** The \"slender\" attention assumes that there are a few important features/neighbors to focus on. If a task truly requires an aggregation of *all* features or neighbors (e.g., in highly diffuse information scenarios), its sparsity might become a limitation.\\n5.  **Interpretability Depth:** As noted, attention weights show \"what\" but not necessarily \"why.\" Deeper causal interpretability methods could provide more insights.\\n\\n## Suggestions to Improve Robustness\\n\\n1.  **Diversify Robustness Benchmarks:**\\n    *   **Missing Data:** Evaluate robustness to missing node features or missing edges.\\n    *   **More Complex Structural Attacks:** Test against edge removal/rewiring attacks that are not necessarily \"additions,\" and attacks that target specific graph substructures.\\n    *   **Poisoning Attacks:** Assess resilience to poisoning attacks that modify the training data or labels.\\n    *   **Adaptive Attacks:** Implement and test against adaptive attacks that exploit SlenderGNN\\'s specific architecture, particularly its attention mechanism.\\n2.  **Combine with Certified Robustness Techniques:** Explore if SlenderGNN can be integrated with techniques for certified robustness (e.g., randomized smoothing, interval bound propagation for specific threat models) to provide theoretical guarantees.\\n3.  **Adversarial Training:** Train SlenderGNN using adversarial examples generated from the attacks. This often significantly boosts empirical robustness against those specific attack types.\\n4.  **Ensemble Methods/Robust Aggregation:** While SlenderGNN has internal robust mechanisms, combining it with ensemble methods (multiple SlenderGNNs) or using more robust aggregation functions (e.g., median, trimmed mean) at the final classification layer could further enhance resilience to outliers.\\n5.  **Feature Denosing Preprocessing:** Although SlenderGNN handles noise internally, applying a dedicated feature denoising or dimensionality reduction technique as a preprocessing step could potentially make the input even cleaner and boost overall performance under extreme noise.\\n6.  **Explicit Robustness Loss:** Incorporate a robustness-specific loss term during training, such as one that penalizes sensitivity to small input perturbations.\\n\\nIn conclusion, SlenderGNN presents a highly promising GNN architecture, particularly in its demonstrated robustness against noisy features and various adversarial attacks. The evidence is compelling and well-supported by sound experimental design and sanity checks. Its ability to generalize across homophilous and heterophilous graphs further solidifies its value. While certain limitations exist concerning the breadth of attack scenarios and the depth of interpretability, SlenderGNN significantly pushes the envelope for building more resilient and reliable GNNs. Future work focusing on even more challenging adversarial settings and exploring formal robustness guarantees would further strengthen its position.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.17441915}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'This review evaluates the interpretability and practical applicability of SlenderGNN, focusing on the clarity of its explanations, the utility of the GNNLIN framework, and the impact of its design choices.\\n\\n## Overall Assessment\\n\\nSlenderGNN presents a compelling approach to developing inherently interpretable Graph Neural Networks by grounding its design in the **GNNLIN (Graph Neural Network Linear-In-Node-Features)** framework. This strong theoretical foundation, combined with design choices that prioritize simplicity and efficiency, positions SlenderGNN as a significant step towards more transparent and practically useful GNNs. The paper effectively argues for its interpretability by design, which is a major advantage over post-hoc explanation methods.\\n\\n## Interpretability Assessment\\n\\n### 1. Clarity of Explanations (GNNLIN Framework)\\n\\nThe core of SlenderGNN\\'s interpretability lies in the GNNLIN framework. The paper explains this concept with commendable clarity:\\n\\n*   **Core Idea:** GNNLIN proposes that the output of a GNN for a given node can be expressed as a linear combination of its *original node features*, where the coefficients (or weights) are functions of the graph structure. Mathematically, $y_i = \\\\sum_{j \\\\in \\\\mathcal{V}} (\\\\mathbf{S}_{ij} \\\\mathbf{x}_j) = \\\\sum_{k=1}^F \\\\alpha_{ik} x_{jk}$. This is very intuitive.\\n*   **Derivation of Interpretability:** By framing the GNN output in this linear fashion, the paper immediately highlights how feature importance can be directly derived. The learned coefficients $\\\\alpha_{ik}$ (which are essentially the values in the learned filter matrix $\\\\mathbf{S}$) directly indicate how much each original feature of each node contributes to the final prediction of a target node. This is a powerful, direct attribution method.\\n*   **Relationship to Traditional Linear Models:** The analogy to traditional linear regression (where coefficients directly show feature importance) makes the concept highly accessible to practitioners familiar with basic machine learning.\\n*   **Contrast with Other GNNs:** The paper implicitly (or explicitly) contrasts this with common GNN architectures that use multiple non-linear transformations and aggregations, making it difficult to trace back contributions to original features. This contrast effectively highlights GNNLIN\\'s unique advantage.\\n\\n**Strength:** The GNNLIN framework is clearly articulated, making its interpretability claims immediately understandable and scientifically sound.\\n\\n### 2. Usefulness of the GNNLIN Framework for Practitioners\\n\\nThe GNNLIN framework offers substantial practical utility for practitioners:\\n\\n*   **Direct Feature Importance:** For any given node (or graph, in classification tasks), practitioners can directly inspect the learned coefficients $\\\\alpha_{ik}$ to understand which original features of which surrounding nodes contribute most to the prediction. This answers the crucial \"why\" question for specific predictions.\\n*   **Actionable Insights:** Knowing which features are most influential can lead to actionable insights. For instance, in drug discovery, identifying crucial molecular substructures; in fraud detection, pinpointing specific transactional patterns; or in social networks, identifying key user attributes influencing behavior.\\n*   **Debugging and Trust:** The transparency offered by GNNLIN helps in debugging model failures (e.g., if an unexpected feature dominates) and building trust in the model\\'s predictions, especially in high-stakes applications.\\n*   **Simplified Model Understanding:** Instead of trying to unpack complex non-linear mappings, practitioners can reason about linear combinations of features, which is a much simpler mental model.\\n\\n**Strength:** GNNLIN provides a clear, inherent mechanism for interpretability that is directly actionable for practitioners, moving beyond opaque black-box models.\\n\\n### 3. Impact of Design Choices on Interpretability\\n\\nSlenderGNN\\'s specific design choices are perfectly aligned with and enhance its interpretability:\\n\\n*   **Linearity in Node Features (GNNLIN Principle):** This is the paramount design choice. By ensuring the final prediction is a linear combination of *original* node features, SlenderGNN avoids obfuscating the input-output relationship with complex non-linear transformations at the final layer. This is the bedrock of its inherent interpretability.\\n*   **Learnable Filter Matrix ($\\\\mathbf{S}$):** The core of SlenderGNN is learning this matrix $\\\\mathbf{S}$ that encapsulates the graph structure and feature propagation. The elements of $\\\\mathbf{S}$ directly become the coefficients $\\\\alpha_{ik}$. This design makes the propagation mechanism explicit and inspectable. One can analyze $\\\\mathbf{S}$ to understand global patterns of influence or feature aggregation.\\n*   **\"Slender\" (Low Parameter Count):** The name \"SlenderGNN\" implies a compact model. A lower parameter count often leads to simpler models, which are inherently easier to understand than highly parameterized, over-complex models. While primarily intended for robustness and efficiency, this simplicity indirectly aids interpretability by reducing the number of variables that contribute to complex, uninterpretable interactions.\\n*   **Absence of Post-Aggregation Non-linearities:** Crucially, SlenderGNN avoids applying complex non-linear activation functions or Multi-Layer Perceptrons (MLPs) *after* the feature aggregation that defines the GNNLIN formulation. Any such non-linearity would immediately break the direct linear relationship and thus compromise the interpretability.\\n\\n**Strength:** SlenderGNN\\'s architecture is meticulously crafted to *embody* the GNNLIN principle, ensuring that interpretability is a core feature, not an afterthought.\\n\\n## Practical Applicability Assessment\\n\\nSlenderGNN\\'s practical applicability appears strong due to its claimed accuracy, robustness, and the inherent interpretability it provides.\\n\\n### Strengths in Practicality:\\n\\n*   **Performance:** The paper title claims \"Accurate, Robust,\" suggesting it doesn\\'t sacrifice performance for interpretability, which is a critical factor for adoption. A model that is interpretable but performs poorly has limited real-world utility.\\n*   **Efficiency and Scalability (\"Slender\"):** A low parameter count typically translates to faster training, lower memory footprint, and better scalability, making it suitable for larger datasets and deployment on resource-constrained devices. The pre-computation of the $\\\\mathbf{S}$ matrix, if efficient, further aids this.\\n*   **Actionable Insights:** As discussed, the direct feature importance can lead to tangible, domain-specific insights that can guide decision-making or scientific discovery, which is a key driver for adopting interpretable models in practice.\\n*   **Reduced Need for Post-Hoc XAI:** By being interpretable by design, SlenderGNN reduces or eliminates the need for computationally expensive and potentially less faithful post-hoc XAI methods, streamlining the model development and deployment pipeline.\\n\\n### Potential Practical Limitations/Considerations:\\n\\n*   **Complexity of $\\\\mathbf{S}$ for Large Graphs:** While the *parameter count* of $\\\\mathbf{S}$ might be low, its *size* (e.g., $N \\\\times F$) can still be large for very large graphs ($N$). The paper should elaborate on how $\\\\mathbf{S}$ is handled efficiently for massive graphs (e.g., sparsity, factorization, or specific computation strategies).\\n*   **Limitations of Linearity:** While excellent for interpretability, linearity might sometimes oversimplify inherently complex, non-linear relationships present in real-world graph data, potentially limiting its accuracy ceiling in some highly non-linear scenarios compared to deeper, more complex GNNs. The paper should acknowledge this trade-off clearly.\\n*   **Generalization of GNNLIN:** While SlenderGNN instantiates GNNLIN, exploring how easily other GNN architectures can be *converted* or *constrained* to fit the GNNLIN framework would be valuable for broader applicability.\\n\\n## Feedback for Improvement in Understanding and Usability\\n\\n1.  **Concrete Interpretability Examples & Visualizations:**\\n    *   **Case Studies:** Provide specific, detailed case studies where SlenderGNN\\'s interpretations led to a novel insight or confirmed a hypothesis in a real-world dataset (e.g., in a bioinformatics or social science context).\\n    *   **Visualizations of $\\\\mathbf{S}$:** Show illustrative heatmaps or sparse representations of the learned filter matrix $\\\\mathbf{S}$ for small example graphs. Highlight patterns (e.g., locality, specific node influences).\\n    *   **Per-Node Feature Contributions:** For a given prediction, visualize the top-K contributing features (and their respective original nodes) and their weights. This directly shows \"what feature from which node contributed how much.\"\\n    *   **Comparative Interpretability:** If possible, include a qualitative comparison of SlenderGNN\\'s interpretations versus those obtained from post-hoc XAI methods applied to black-box GNNs on the same task. This would underscore the superiority of inherent interpretability.\\n\\n2.  **User-Friendly API/Tooling for Interpretability:**\\n    *   **Code Examples:** Provide clear code snippets demonstrating how a practitioner can easily extract the feature importance scores for a given node or graph. A simple API call like `model.get_feature_contributions(node_id)` would be highly impactful.\\n    *   **Interactive Demo:** An online interactive demo where users can upload a small graph, run SlenderGNN, and then click on nodes to see their feature attributions would significantly enhance usability and understanding.\\n\\n3.  **Detailed Computational Analysis for Scalability:**\\n    *   **Pre-computation Cost:** Clearly outline the computational complexity and memory requirements for learning and storing the $\\\\mathbf{S}$ matrix, especially for very large graphs ($N$ in the millions). Discuss strategies for handling this (e.g., sparse $\\\\mathbf{S}$, low-rank approximations, or distributed computation).\\n    *   **Comparison of Inference Time:** Provide a breakdown of inference time showing where the \"slender\" advantage comes into play compared to other GNNs.\\n\\n4.  **Acknowledge and Discuss Trade-offs:**\\n    *   **Linearity vs. Non-linearity:** Explicitly discuss the potential trade-off between the strong interpretability afforded by linearity and the capacity to model highly complex, non-linear relationships that might be present in some graph data. While it performs well, acknowledging this limitation (if any) makes the claims more robust.\\n    *   **Fixed Graph Structure:** If the model assumes a fixed graph structure during training and inference (common for many GNNs), explicitly state this and discuss implications for dynamic graphs.\\n\\n5.  **Broader GNNLIN Discussion:**\\n    *   Beyond SlenderGNN, discuss the general applicability of the GNNLIN framework. Can other existing GNN architectures be re-formulated or modified to adhere to GNNLIN principles, thereby gaining interpretability? This would broaden the impact of the framework.\\n\\nBy incorporating these suggestions, the paper can further strengthen its message, make its interpretations even more accessible, and accelerate the adoption of SlenderGNN by practitioners.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.17352455}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Now that I have the reviews from the three agents, I will synthesize them into a final decision.\n\n**GNN Methodology Expert (GNN\\_ME):**\n\n*   **Strengths:** The GNNLIN framework is technically sound and provides a novel perspective on linearizing GNNs. The SlenderGNN architecture has well-justified design choices, particularly the \"slender\" MLPs. The experimental results are valid and support the claims of accuracy, robustness, and interpretability.\n*   **Weaknesses:** The term \"linearization\" might be confusing. The paper could benefit from a deeper discussion of the limitations of expressive power and scalability analysis.\n*   **Overall:** Positive assessment of the methodology and novelty.\n\n**Robustness and Generalization Analyst (RGA):**\n\n*   **Strengths:** SlenderGNN demonstrates strong robustness against noisy features and adversarial attacks. It generalizes well across different graph types, including heterophilous graphs. The sanity checks and real-world experiments provide compelling evidence.\n*   **Weaknesses:** The robustness evaluation could be diversified with more complex attacks and real-world noise scenarios. The generalization assessment could be expanded to include more diverse topological properties and feature sparsity.\n*   **Overall:** Strong support for the robustness and generalization claims.\n\n**Interpretability and Practicality Assessor (IPA):**\n\n*   **Strengths:** The GNNLIN framework offers substantial practical utility for direct feature importance and actionable insights. SlenderGNN's design choices enhance its interpretability. The paper provides clear explanations and examples.\n*   **Weaknesses:** The complexity of the filter matrix for large graphs could be a limitation. The paper should acknowledge the trade-off between linearity and non-linearity.\n*   **Overall:** Positive evaluation of the interpretability and practical applicability.\n\n**Synthesis:**\n\nThe three agents provide a comprehensive and positive assessment of the paper. SlenderGNN presents a novel and technically sound approach to GNN design, with strong evidence supporting its claims of accuracy, robustness, and interpretability. The GNNLIN framework offers a valuable tool for understanding and improving GNNs. While there are some limitations and suggestions for improvement, the overall strengths of the paper outweigh its weaknesses.\n\n**FINAL DECISION: Accept**\n"}], "ground_truth": "Reject", "response_history": "Now that I have the reviews from the three agents, I will synthesize them into a final decision.\n\n**GNN Methodology Expert (GNN\\_ME):**\n\n*   **Strengths:** The GNNLIN framework is technically sound and provides a novel perspective on linearizing GNNs. The SlenderGNN architecture has well-justified design choices, particularly the \"slender\" MLPs. The experimental results are valid and support the claims of accuracy, robustness, and interpretability.\n*   **Weaknesses:** The term \"linearization\" might be confusing. The paper could benefit from a deeper discussion of the limitations of expressive power and scalability analysis.\n*   **Overall:** Positive assessment of the methodology and novelty.\n\n**Robustness and Generalization Analyst (RGA):**\n\n*   **Strengths:** SlenderGNN demonstrates strong robustness against noisy features and adversarial attacks. It generalizes well across different graph types, including heterophilous graphs. The sanity checks and real-world experiments provide compelling evidence.\n*   **Weaknesses:** The robustness evaluation could be diversified with more complex attacks and real-world noise scenarios. The generalization assessment could be expanded to include more diverse topological properties and feature sparsity.\n*   **Overall:** Strong support for the robustness and generalization claims.\n\n**Interpretability and Practicality Assessor (IPA):**\n\n*   **Strengths:** The GNNLIN framework offers substantial practical utility for direct feature importance and actionable insights. SlenderGNN's design choices enhance its interpretability. The paper provides clear explanations and examples.\n*   **Weaknesses:** The complexity of the filter matrix for large graphs could be a limitation. The paper should acknowledge the trade-off between linearity and non-linearity.\n*   **Overall:** Positive evaluation of the interpretability and practical applicability.\n\n**Synthesis:**\n\nThe three agents provide a comprehensive and positive assessment of the paper. SlenderGNN presents a novel and technically sound approach to GNN design, with strong evidence supporting its claims of accuracy, robustness, and interpretability. The GNNLIN framework offers a valuable tool for understanding and improving GNNs. While there are some limitations and suggestions for improvement, the overall strengths of the paper outweigh its weaknesses.\n\n**FINAL DECISION: Accept**\n", "elapsed_time": 119.8075590133667, "reviewer_agents": {"GNN_ME": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in GNN methodologies, focusing on technical soundness and novelty.", "system_prompt": "You are a GNN methodology expert. Evaluate the technical soundness and novelty of the proposed GNNLIN framework and SlenderGNN architecture. Assess the linearization process, design choices, and validity of experimental results. Provide detailed feedback on the methodology and suggest improvements.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "RGA": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Analyst specializing in robustness and generalization of GNNs.", "system_prompt": "You are a robustness and generalization analyst. Critically assess the claims of robustness and generalization across different graph types and noisy features. Examine the sanity checks and real-world experiments for evidence supporting these claims. Identify limitations and suggest ways to improve robustness.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "IPA": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Assessor of interpretability and practicality of GNNs.", "system_prompt": "You are an interpretability and practicality assessor. Evaluate the interpretability of SlenderGNN and its practical applicability. Assess the clarity of explanations, the usefulness of the GNNLIN framework for practitioners, and the impact of design choices on interpretability. Provide feedback on how to improve the understanding and usability of the model.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}}}
