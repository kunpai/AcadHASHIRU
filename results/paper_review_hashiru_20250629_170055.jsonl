{"paper_id": "xfqDe72zh41", "prompt": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Actionable Neural Representations: Grid Cells from Minimal Constraints\n\n1 INTRODUCTION: Animals and machines should build representations that afford flexible behaviours. However, different representation make some tasks easy and others hard; representing red versus white is good for understanding wines but less good for opening screw-top versus corked bottles. A central mystery in neuroscience is the relationship between tasks and their optimal representations. Resolving this requires understanding the representational principles that permit flexible behaviours such as zero-shot inference and short-cut planning. Here, we introduce actionable representations, a representation that permits flexible behaviours. Being actionable means encoding not only variables of interest, but also how the variable transforms. Actions cause many variables to transform in predictable ways. For example, actions in 2D space obey rules; north, east, south, and west, have a universal meaning, and combine in the same way everywhere. Embedding these rules into a representation of self-position permits deep inferences: having stepped north, then east, then south, an agent can infer that stepping west will lead home, having never taken that path - a zero-shot inference (Figure 1A). Indeed biology represents 2D space in a structured manner. Grid cells in medial entorhinal cortex represent an abstracted \u2018cognitive map\u2019 of 2D space(Tolman, 1948). These cells fire in a hexagonal lattice of positions (Hafting et al., 2005), (Figure 1B), and are organised in modules; cells within one module have receptive fields that are translated versions of one another, and different modules have firing lattices of different scales and orientations (Figure 1B), (Stensola et al., 2012). Biological representations must be more than just actionable - they must be functional, encoding the world efficiently, and obey biological constraints. We formalise these three ideas - actionable, functional, and biological - and analyse the resulting optimal representations. We define actionability using group and representation theory, as the requirement that each action has a corresponding matrix that linearly updates the representation; for example, the \u2018step north\u2019 matrix updates the representation to its value one step north. Functionally, we want different points in space to be represented maximally differently, allowing inputs to be distinguished from one another. Biologically, we ensure all neurons have non-negative and bounded activity. From this constrained optimisation problem we derive optimal representations that resemble multiple modules of grid cells. Our problem formulation allows analytic explanations for grid cell phenomena, matches experimental findings, such as the alignment of grids cells to room geometry (Stensola et al., 2015), and predicts some underappreciated aspects, such as the relative angle between modules. In sum, we 1) propose actionable neural representations to support flexible behaviours; 2) formalise the actionable constraint with group and representation theory; 3) mix actionability with biological and functional constraints to create a constrained optimisation problem; 4) analyse this problem and show that in 2D the optimal representation is a good model of grid cells, thus offering a mathematical understanding of why grid cells look the way they do; 5) provide several neural predictions; 6) highlight the generality of this normative method beyond 2D space. 1.1 RELATED WORK: Neuroscientists have long explained representations with normative principles like information maximisation (Attneave, 1954; Barlow et al., 1961), sparse (Olshausen & Field, 1996) or independent (Hyva\u0308rinen, 2010) latent encodings, often mixed with biological constraints such as non-negativity (Sengupta et al., 2018), energy budgets (Niven et al., 2007), or wiring minimisation (Hyva\u0308rinen et al., 2001). On the other hand, deep learning learns task optimised representations. A host of representation-learning principles have been considered (Bengio et al., 2013); but our work is most related to geometric deep learning (Bronstein et al., 2021) which emphasises input transformations, and building neural networks which respect (equivariant) or ignore (invariant) them. This is similar in spirit but not in detail to our approach, since equivariant networks do not build representations in which all transformations of the input are implementable through matrices. There is much previous theory on grid cells, which can be categorised as relating to our actionable, functional, and biological constraints. Functional: Many works argue that grid cells provide an efficient representation of position, that hexagons are optimal (Mathis et al., 2012a;b; Sreenivasan & Fiete, 2011; Wei et al., 2015) and make predictions for relative module lengthscales (Wei et al., 2015). Since we use similar functional principles, we suspect that some of our novel results, such as grid-to-room alignment, could have been derived by these authors. However, in contrast to our work, these authors assume a grid-like tuning curve. Instead we give a normative explanation of why be grid-like at all, explaining features like the alignment of grid axes within a module, which are detrimental from a pure decoding view (Stemmler et al., 2015). Actionability: Grid cells are classically understood as affording path-integration (integrating velocity to predict position) with units from both hand-tuned Burak & Fiete (2009) and trained recurrent neural network resembling grid cells (Cueva & Wei, 2018; Banino et al., 2018; Sorscher et al., 2019). Recently, these approaches have been questioned for their parameter dependence (Schaeffer et al., 2022), or relying on decoding place cells with bespoke shapes that are not observed experimentally (Sorscher et al., 2019; Dordek et al., 2016). Our mathematical formalisation of path-integration, combined with biological and functional constraints, provides clarity on this issue. Our approach is linear, in that actions update the representation linearly, which has previously been explored theoretically (Issa & Zhang, 2012), and numerically, in two works that learnt grid cells (Whittington et al., 2020; Gao et al., 2021). Our work could be seen as extracting and simplifying the key ideas from these papers that make hexagonal grids optimal (see Appendix H), and extending them to multiple modules, something both papers had to hard code. Biological: Lastly, both theoretically (Sorscher et al., 2019) and computationally (Dordek et al., 2016; Whittington et al., 2021), non-negativity has played a key role in normative derivations of hexagonal grid cells, as it will here. 2 ACTIONABLE NEURAL REPRESENTATIONS: AN OBJECTIVE: We seek a representation g(x) \u2208 RN of position x \u2208 R2, where N is the number of neurons. Our representation is built using three ideas: functional, biological, and actionable; whose combination will lead to multiple modules of grid cells, and which we\u2019ll now formalise. Functional: To be useful, the representation must encode different positions differently. However, it is more important to distinguish positions 1km apart than 1mm, and frequently visited positions should be separated the most. To account for these, we ask our representation to minimise L = \u222b\u222b e\u2212 \u2225g(x)\u2212g(x\u2032)\u22252 2\u03c32 \u03c7(x,x\u2032)p(x)p(x\u2032)dxdx\u2032 (1) The red term measures the representational similarity of x and x\u2032; it is large if their representations are nearer than some distance \u03c3 in neural space and small otherwise. By integrating over all pairs x and x\u2032, L measures the total representational similarity, which we seek to minimise. The green term is the agent\u2019s position occupancy distribution, which ensures only visited points contribute to the loss, for now simply a Gaussian of lengthscale L. Finally, the blue term weights the importance of separating each pair, encouraging separation of points more distant than a lengthscale, l. \u03c7(x,x\u2032) = 1\u2212 e\u2212 \u2225x\u2212x\u2032\u22252 2l2 p(x) =e\u2212 \u2225x\u22252 2L2 (2) Biological: Neurons have non-negative firing rates, so we constrain gi(x) \u2265 0. Further, neurons can\u2019t fire arbitrarily fast, and firing is energetically costly, so we constrain each neuron\u2019s response gn(x) via \u222b g2n(x)p(x)dx = 1 Actionable: Our final constraint requires that the representation is actionable. This means each transformations of the input must have its own transformation in neural space, independent of position. For mathematical convenience we enact the neural transformation using a matrix. Labelling this matrix T (\u2206x) \u2208 RN\u00d7N , for transformation \u2206x, this means that for all positions x, g(x+\u2206x) = T (\u2206x)g(x) (3) For intuition into how this constrains the neural code g(x), we consider a simple example of two neurons representing an angle \u03b8 \u2208 [0, 2\u03c0). Replacing x with \u03b8 in equation 3 we get the equivalent constraint: g(\u03b8 +\u2206\u03b8) = T (\u2206\u03b8)g(\u03b8). Here the matrix T performs a rotation, and the solution (up to a linear transform) is for T to be the standard 2\u00d7 2 rotation matrix, with frequency n. g(\u03b8 +\u2206\u03b8) = ( cos(n[\u03b8 +\u2206\u03b8]) sin(n[\u03b8 +\u2206\u03b8]) ) = ( cos(n\u2206\u03b8) \u2212 sin(n\u2206\u03b8) sin(n\u2206\u03b8) cos(n\u2206\u03b8) )( cos(n\u03b8) sin(n\u03b8) ) = T (\u2206\u03b8)g(\u03b8) (4) Thus as \u03b8 rotates by 2\u03c0 the neural representation traces out a circle an integer number, n, times. Thanks to the problem\u2019s linearity, extending to more neurons is easy. Adding two more neurons lets the population contain another sine and cosine at some frequency, just like the two neurons in equation 4. Extrapolating this we get our actionability constraint: the neural response must be constructed from some invertible linear mixing of the sines and cosines of D < N2 frequencies, g(\u03b8) = a0 + D\u2211 d=1 ad sin(nd\u03b8) + bd cos(nd\u03b8) for integer nd (5) The vectors {ad, bd}Dd=1 \u2208 RN are coefficient vectors that mix together the sines and cosines, of which there are D. a0 is the coefficient vector for a frequency that cycles 0 times. This argument comes from an area of maths called Representation Theory (a different meaning of representation!) that places constraints on the matrices T for variables whose transformations form a mathematical object called a group. This includes many of interest, such as position on a plane, torus, or sphere. These constraints on matrices can be translated into constraints on an actionable neural code just like we did for g(\u03b8) (see Appendix A for 2D space and more). 2D space, however, differs from the above example. First the variable is two-dimensional, so the frequencies, denoted kd, become two dimensional. Second, the space is infinite. Previously, frequencies were integers in order that activity returned home after a loop: g(2\u03c0) = g(0). But increasing the ring radius allows frequencies closer to each other, and as the radius goes to infinity, turning the circle into a 1D line, all frequencies are allowed. This applies equally in 2D, allowing us to drop the integer constraint, g(x) = a0 + D\u2211 d=1 ad sin(kd \u00b7 x) + bd cos(kd \u00b7 x) (6) Our constrained optimisation problem is complete. Equation 6 specifies the set of actionable representations. Without additional constraints these codes are meaningless: random combinations of sines and cosines produce random neural responses (Figure 2A). We will choose from amongst the set of actionable codes by optimising the parameters a0, {ad, bd,kd}Dd=1 to minimise L, subject to biological (non-negative and bounded firing rates) constraints. 3 OPTIMAL REPRESENTATIONS: Optimising over the set of actionable codes to minimise L with biological constraints gives multiple modules of grid cells (Figure 2B). This section will, using intuition and analytics, explain why. 3.1 NON-NEGATIVITY LEADS TO A MODULE OF LATTICE CELLS: To understand how non-negativity produces modules of lattice responses we will study the following simplified loss, which maximises the Euclidean distance between representations of angle, g(\u03b8), L0 = \u2212 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252d\u03b8d\u03b8\u2032 (7) This is equivalent to the full loss (equation 1) for uniform p(\u03b8), \u03c7(\u03b8, \u03b8\u2032) = 1, and \u03c3 very large. Make no mistake, this is a bad loss. For contrast, the full loss encouraged the representations of different positions to be separated by more than \u03c3, enabling discrimination1. Therefore, sensibly, the representation is most rewarded for separating points that would otherwise be closer than \u03c3. L0 does the opposite! It grows quadratically with separation, so g(\u03b8) is most rewarded for pushing apart already well-separated points, a terrible representational principle! Nonetheless, L0 will give us key insights. 1\u03c3 could be interpreted as a noise level, or a minimum discriminable distance, then points should be far enough away for a downstream decoder to distinguish them. Since actionability gives us a parameterised form of the representations (equation 5), we can compute the integrals to obtain the following constrained optimisation problem (details: Appendix C) min a0, {ad,bd,nd}Dd=1 L0 = \u2212 D\u2211 d=1 \u2225ad\u22252+\u2225bd\u22252 with Non-negativity\ufe37 \ufe38\ufe38 \ufe37 g(\u03b8) > 0, Bounded firing rates\ufe37 \ufe38\ufe38 \ufe37 \u2225a0\u22252 + 1 2 D\u2211 d=1 \u2225ad\u22252 + \u2225bd\u22252 = N (8) Where N is the number of neurons. This is now something we can understand. First, reminding ourselves that the neural code, g(\u03b8), is made from a constant vector, a0, and \u03b8 dependent parts (equation 6; Figure 3A), we can see that L0 separates representations by encouraging the size of each varying part, \u2225ad\u22252 + \u2225bd\u22252, to be maximised. This effect is limited by the firing rate bound, \u2225a0\u22252 \u2212 2L0 = N . Thus, to minimise L0 we must minimise the constant vector, a0. This would be easy without non-negativity (when any code with \u2225a0\u2225 = 0 is optimal), but no sum of sines and cosines can be non-negative for all \u03b8 without an offset. Thus the game is simple; choose frequencies and coefficients so the firing rates are non-negative, but using the smallest possible constant vector. One lattice cell. We now heuristically argue, and confirm in simulation, that the optimal solution for a single neuron is a lattice tuning curve (see Appendix G for why this problem is non-convex). Starting with a single frequency component, e.g. sin(\u03b8), achieving non-negativity requires adding a constant offset, sin(\u03b8) + 1 (Figure 3B). However, we could also have just added another frequency. In particular adding harmonics of the base frequency (with appropriate phase shifts) pushes up the minima (Figure 3B). Extending this argument, we suggest non-negativity, for a single cell, can be achieved by including a grid of frequencies. This gives a lattice tuning curve (Figure 3B right). Module of lattice cells. Achieving non-negativity for this cell used up many frequencies. But as discussed (Section 2), actionability only allows a limited number frequencies (< N2 since each fre- quency uses 2 neurons (sine and cosine)), thus how can we make lots of neurons non-negative with limited frequencies? Fortunately, we can do so by making all neuron\u2019s tuning curves translated versions of each other, as translated curves contain the same frequencies but with different phases. This is a module of lattice cells. We validate our arguments by numerically optimising the coefficients a0, {ad, bd}Dd=1 and frequencies {nd}Dd=1 to minimise L0 subject to constraints, producing a module of lattices (Figure 3C; details in Appendix B). These arguments equally apply to representations of a periodic 2D space (a torus; Figure 3D). Studying L0 has told us why lattice response curves are good. But surprisingly, all lattices are equally good, even at infinitely high frequency. Returning to the full loss will break this degeneracy. 3.2 PRIORITISING IMPORTANT PAIRS OF POSITIONS PRODUCES HEXAGONAL GRID CELLS: Now we return to the full loss and understand its impact in two steps, beginning with the reintroduction of \u03c7 and p, which break the lattice degeneracy, forming hexagonal grid cells. L = \u222b\u222b \u221e \u2212\u221e e\u2212 \u2225g(x)\u2212g(x\u2032)\u22252 2\u03c32 \u03c7(x,x\u2032)p(x)p(x\u2032)dxdx\u2032 (9) \u03c7 prefers low frequencies: recall that \u03c7 = 1\u2212 e\u2212 \u2225x\u2212x\u2032\u22252 2l2 ensures very distant inputs have different representations, while allowing similar inputs to have similar representations, up to a resolution, l. This encourages low frequencies (\u2225kd\u2225 < 1l ), which separate distant points but produce similar representations for pairs closer than l (Analytics: Appendix D.1). At this stage, for periodic 2D space, the lowest frequency lattices, place cells, are optimal (see Appendix F; Sengupta et al. (2018)). p(x) prefers high frequencies: However, the occupancy distribution of the animal, p(x), counters \u03c7. On an infinite 2D plane animals must focus on representing a limited area, of lengthscale L. This encourages high frequencies (\u2225kd\u2225 > 1L ), whose response varies among the visited points (Analytics: Appendix D.2). More complex p(x) induce more complex frequency biases, but, to first order, the effect is always a high frequency bias (Figure 5F-G, Appendix L). Combination \u2192 Hexagons: Satisfying non-negativity and functionality required a lattice of many frequencies, but now p and \u03c7 bias our frequency choice, preferring those beyond 1L (to separate points the animal visits) but smaller than 1l (to separate distant visited points). Thus to get as many of these preferred frequencies as possible, we want the lattice with the densest packing within a Goldilocks annulus in frequency space (Figure 4A). This is a hexagonal lattice in frequency space which leads to a hexagonal grid cell. Simulations with few neurons agree, giving a module of hexagonal grid cells (Figure 4B). 3.3 A HARMONIC TUSSLE PRODUCES MULTIPLE MODULES: Finally, we will study the neural lengthscale \u03c3, and understand how it produces multiple modules. L = \u222b\u222b \u221e \u2212\u221e e\u2212 \u2225g(x)\u2212g(x\u2032)\u22252 2\u03c32 \u03c7(x,x\u2032)p(x)p(x\u2032)dxdx\u2032 (10) As discussed, L prioritises the separation of poorly distinguished points, those whose representations are closer than \u03c3. This causes certain frequencies to be desired in the overall population, in particular those unrelated to existing frequencies by simple harmonic ratios, i.e. not \u03c91 = 32\u03c92 (Figure 4C; see Appendix E for a perturbative derivation of this effect). This is because pairs of harmonically related frequencies represent more positions identically than a non-harmonically related pair, so are worse for separation (similar to arguments made in Wei et al. (2015)). This, however, sets up a \u2018harmonic tussle\u2019 between what the population wants - non-harmonically related frequencies for L - and what single neurons want - harmonically related frequency lattices for non-negativity (Section 3.1). Modules of grid cells resolve this tension: harmonic frequencies exist within modules to give non-negativity, and non-harmonically related modules allow for separation, explaining the earlier simulation results (Figure 2B; further details in Appendix E.3). This concludes our main result. We have shown three constraints on neural populations - actionable, functional, and biological - lead to multiple modules of hexagonal grid cells, and we have understood why. We posit this is the minimal set of requirements for grid cells (see Appendix I for ablation simulations and discussion). 4 PREDICTIONS: Our theory makes testable predictions about the structure of optimal actionable codes for 2D space. We describe three here: tuning curve sharpness scales with the number of neurons in a module; the optimal angle between modules; and the optimal grid alignment to room geometry. 4.1 LATTICE SIZE:FIELD WIDTH RATIO SCALES WITH NUMBER OF NEURONS IN MODULE: In our framework the number of neurons controls the number of frequencies in the representation (equation 6). A neuron within a module only contains frequencies from that module\u2019s frequency lattice, since other modules have non-harmonically related frequencies. More neurons in a module, means more and higher frequencies in the lattice, which sharpen grid peaks (Figure 5A). We formalise this (Appendix J) and predict that the number of neurons within a module scales with the square of the lattice lengthscale, \u03bd, to field width, \u00b5, ratio, N \u221d ( \u03bd \u00b5 )2 . This matches the intuition that the sharper a module\u2019s peak, the more neurons you need to tile the entire space. In a rudimentary analysis, our predictions compare favourably to data from Stensola et al. (2012) assuming uniform sampling of grid cells across modules (Figure 5B). We are eager to test these claims quantitatively. 4.2 MODULES ARE OPTIMALLY ORIENTED AT SMALL OFFSETS (\u223c 4\u25e6): In section 3.3 we saw how frequencies of different modules are maximally non-harmonically related in order to separate the representation of as many points as possible. To maximise non-harmonicity between two modules, the second module\u2019s frequency lattice can be both stretched and rotated relative to the first. 0 or 30\u25e6 relative orientations are particularly bad coding choices as they align the high density axes of the two lattices (Figure 5C). The optimal angular offset of two modules, calculated via a frequency overlap metric (Appendix K), is small (Figure 5D); the value depends on the grid peak and lattice lengthscales, \u00b5 and \u03bd, but varies between 3\u25e6 and 8\u25e6 degrees. Multiple modules should orient at a sequence of small angles (Appendix K). In a rudimentary analysis, our predictions compare favourably to the observations of Stensola et al. (2012) (Figure 5E). 4.3 OPTIMAL GRIDS MORPH TO ROOM GEOMETRY: In Section 3.2 (and Appendix D) we showed that p(x), the animal\u2019s occupancy distribution, introduced a high frequency bias - grid cells must peak often enough to encode visited points. However, changing p(x) changes the shape of this high frequency bias (Appendix L). In particular, we examine an animal\u2019s encoding of square, circular, or rectangular environments, Appendix L, with the assumption that p(x) is uniform over that space. In each case the bias is coarsely towards high frequencies, but has additional intricacies: in square and rectangular rooms optimal frequencies lie on a lattice, with peaks at integer multiples of 2\u03c0L along one of the cardinal axes, for room width/height L (Figure 5F); whereas in circular rooms optima are at the zeros of a Bessel function (Figure 5G). These ideas make several predictions. For example, grid modules in circular rooms should have lengthscales set by the optimal radii in Figure 5G, but they should still remain hexagonal since the Bessel function is circularly symmetric. However, the optimal representation in square rooms should not be perfectly hexagonal since p(x) induces a bias inconsistent with a hexagonal lattice (this effect is negligible for high frequency grids). Intriguingly, shearing towards squarer lattices is observed in square rooms(Stensola et al., 2015), and it would be interesting to its grid-size dependence. Lastly, these effects make predictions about how grid cells morph when the environment geometry changes. A grid cell that is optimal in both geometries can remain the same, however sub-optimal grid cells should change. For example turning a square into a squashed square (i.e. a rectangle), stretches the optimal frequencies along the squashed dimension. Thus, some cells are optimal in both rooms and should stay stable, will others will change, presumably to nearby optimal frequencies (Figure 5H). Indeed Stensola et al. (2012) recorded the same grid cells in a square and rectangular environment (Figure 5I), and observed exactly these phenomena. 5 DISCUSSION & CONCLUSIONS: We have proposed actionability as a fundamental representational principle to afford flexible behaviours. We have shown in simulation and with analytic justification that the optimal actionable representations of 2D space are, when constrained to be both biological and functional, multiple modules of hexagonal grid cells, thus offering a mathematical understanding of grid cells. We then used this theory to make three novel grid cell predictions that match data on early inspection. While this is promising for our theory, there remain some grid cell phenomena that, as it stands, it will never predict. For example, grid cell peaks vary in intensity (Dunn et al., 2017), and grid lattices bend in trapezoidal environments (Krupic et al., 2015). These effects may be due to incorporation of sensory information - something we have not included in our theory - to better infer position. Including such effects may recapitulate these findings, as in Ocko et al. (2018). Our theory is normative and abstracted from implementation. However, both modelling (Burak & Fiete, 2009) and experimental (Gardner et al., 2022; Kim et al., 2017) work suggests that continuous attractor networks (CANs) implement path integrating circuits. Actionability and CANs imply seemingly different representation update equations, which future work should compare or contrast. While we focused on understanding the optimal representations of 2D space and their relationship to grid cells, our theory is more general and can be applied to behaviour in other, non 2D, spaces. In fact any variable whose transformations form a group is potentially actionable. The brain represents many such variables, e.g. heading directions, (Finkelstein et al., 2015), object orientations, (Logothetis et al., 1995), the\u20184-loop task\u2019 of Sun et al. (2020) or 3-dimensional space (Grieves et al., 2021; Ginosar et al., 2021). Interestingly, our theory predicts 3D representations with regular order (Figure 14E in Appendix M), unlike those found in the brain (Grieves et al., 2021; Ginosar et al., 2021) suggesting the animal\u2019s 3D navigation is sub-optimal. But, most expansively, these principles express a view that representations must be more than just passive encodings of the world; they must embed the consequences of predictable actions, allowing planning and inferences in never-before-seen situations. We codified these ideas using Group and Representation theory, and demonstrated their utility in understanding grid cells. However, the underlying principle is broader than the crystalline nature of group structures: the world and your actions within it have endlessly repeating structures whose understanding permits creative analogising and flexible behaviours. A well-designed representation should reflect this. A CONSTRAINING REPRESENTATIONS WITH REPRESENTATION THEORY: Having an actionable code means the representational effect of every transformation of the variable, \u2206x, can be implemented by a matrix: g(x+\u2206x) = T (\u2206x)g(x) (11) In Section 2 we outlined a rough argument for how this equation leads to the following constraint on actionable representations of 2D position, that was vital for our work: g(x) = a0 + D\u2211 d=1 ad sin(kd \u00b7 x) + bd cos(kd \u00b7 x) (12) In this section, we\u2019ll repeat this argument more robustly using Group and Representation Theory. In doing so, it\u2019ll become clear how broadly our version of actionability can be used to derive clean analytic representational constraints; namely, the arguments presented here can be applied to the representation of any variable whose transformations form a group. Sections A.1 and A.2 contain a review of the Group and Representation theory used. Section A.3 applies it to our problem. A.1 GROUP THEORY: A mathematical group is a collection of things (like the set of integers), and a way to combine two members of the group that makes a third (like addition of two integers, that always creates another integer), in a way that satisfies a few rules: 1. There is an identity element, which is a member of the group that when combined with any other element doesn\u2019t change it. For adding integers the identity element is 0, since a+ 0 = a for all a. 2. Every member of the group has an inverse, defined by its property that combining an element with its inverse produces the identity element. In our integer-addition example the inverse of any integer a is \u2212a, since \u2212a+ a = 0, and 0 is the identity element. 3. Associativity applies, which just means the order in which you perform operations doesn\u2019t matter: (a+ b) + c = a+ (b+ c) Groups are ubiquitous. We mention them here because they will be our route to understanding actionable representations - representations in which transformations are also encoded consistently. The set of transformations of many variables of interest are groups. For example, the set of transformations of 2D position, i.e. 2D translations - \u2206x, is a group if you define the combination of two translations via simple vector addition, \u2206x1+2 = \u2206x1 +\u2206x2. We can easily check that they satisfy all three of the group rules: 1. There is an identity translation: simply add 0. 2. Each 2D translation has its inverse: \u2212\u2206x+\u2206x = 0 3. Associativity: (\u2206a+\u2206b) + \u2206c = \u2206a+ (\u2206b+\u2206c) The same applies for the transformations of an angle, \u03b8, or of positions on a torus or sphere, and much else besides. This is a nice observation, but in order to put it to work we need a second area of mathematics, Representation Theory. A.2 REPRESENTATION THEORY: Groups are abstract, in the sense that the behaviour of many different mathematical objects can embody the same group. For example, the integers modulo P with an addition combination rule form a group, called CP . But equally, the P roots of unity ({e 2\u03c0n P }P\u22121n=0 ) with a multiplication combination rule obey all the same rules: you can create a 1-1 correspondence between the integers 0 through P \u2212 1 and the P roots of unity by labelling all the roots with the integer n that appears in the exponent e 2\u03c0n P , and, under their respective combination rules, all additions of integers will exactly match onto multiplication of complex roots (i.e. e 2\u03c0n1 P \u2217 e 2\u03c0n2 P = e 2\u03c0 mod P (n1+n2) P ) In this work we\u2019ll be interested in a particular instantiation of our groups, defined by sets of matrices, T (\u2206x), combined using matrix multiplication. For example, a matrix version of the group CP would be the set of 2-by-2 rotation matrices that rotate by 2\u03c0P increments: T (n) = ( cos( 2\u03c0nP ) \u2212 sin( 2\u03c0n P ) sin( 2\u03c0nP ) cos( 2\u03c0n P ) ) such that T (n1)T (n2) = T (modP (n1 + n2)) (13) Combining these matrices using matrix multiplication follows all the same patterns that adding the integers modulo P, or multiplying the P roots of unity, followed; they all embody the same group. Representation theory is the branch of mathematics that specifies the structure of sets of matrices that, when combined using matrix multiplication, follow the rules of a particular group. Such sets of matrices are called a representation of the group; to avoid confusion arising from this unfortunate though understandable convergent terminology, we distinguish group representations from neural representations by henceforth denoting representations of a group in italics. We will make use of one big result from Representation Theory, hinted at in Section 2: for all the groups we will be considering, any matrix that is a representation of a group can be composed from the direct product of a set of blocks, called irreducible representations, or irreps for short, up to a linear transformation. i.e. if T (\u2206a) is a representation of the group of transformations of some variable a: T (\u2206a) = S  I1(\u2206a) 0 0 . . . 0 0 I2(\u2206a) 0 . . . 0 0 0 I3(\u2206a) . . . 0 ... ... ... . . . ... 0 0 0 . . . ID(\u2206a) S\u22121 (14) where Id(\u2206a) are the irreps of the group in question, which are square matrices not necessarily of the same size as d varies, and S is some invertible square matrix. To motivate our current rabbit hole further, this is exactly the result we hinted towards in Section 2. We discussed T (\u2206\u03b8), and, in 2-dimensions, argued it was, up to a linear transform, the 2-dimensional rotation matrix. Further, we discussed how every extra two neurons allowed you to add another frequency, i.e. another rotation matrix. These rotation matrices are the irreps of the rotation group, and adding two neurons allows you to create a larger T (\u2206\u03b8) by stacking rotation matrices on top of one another. Now including the invertible linear transform, S, we can state the 4-dimensional version of equation 4: T (\u2206\u03b8) = S cos(n1\u2206\u03b8) \u2212 sin(n1\u2206\u03b8) 0 0sin(n1\u2206\u03b8) cos(n1\u2206\u03b8) 0 00 0 cos(n2\u2206\u03b8) \u2212 sin(n2\u2206\u03b8) 0 0 sin(n2\u2206\u03b8) cos(n2\u2206\u03b8) S\u22121 (15) In performing this decomposition the role of the rotation matrices as irreps is clear. There are infinitely many types, each specified by an integer frequency, nd, and their direct product produces any representation of the rotation group. Returning to actionable neural representations of 2D space, this result is a very useful specification. We sought codes, g(x), that could be manipulated via matrices: g(x+\u2206x) = T (\u2206x)g(x) (16) Now we recognise that we\u2019re asking T (\u2206x) to be a representation of the transformation group of x, and we know the shape T (\u2206x) must take to fit this criteria. To specify T (\u2206x) from this constrained class we must simply choose a set of irreps that fill up the dimensionality of the neural space, stack them one on top of each other, and rotate and scale them using the matrix S. A final detail is that there is a trivial way to make a representation of any group: choose T (\u03b8) = IN , the N-by-N identity matrix. This also fits our previous discussion, but corresponds to a representation made from the direct product of N copies of something called the trivial irrep, a 1-dimensional irrep, Itrivial(\u2206a) = 1. Armed with this knowledge, the following subsection will show how this theory can be used to constrain the neural code, g(x). To finish this review subsection, we list the non-trivial irreps of the groups used in this work. To be specific, in all our work we use only the real-valued irreps, i.e all elements of T are real numbers, since firing rates are real numbers. These are less common than complex irreps, which is often what is implied by the simple name irreps. Transformation group of which variable Non-trivial Real Irreps An angle on a unit circle, \u03b8 ( cos(n\u2206\u03b8) \u2212 sin(n\u2206\u03b8) sin(n\u2206\u03b8) cos(n\u2206\u03b8) ) for n \u2208 Z Position on a line, x ( cos(\u03c9\u2206\u03b8) \u2212 sin(\u03c9\u2206\u03b8) sin(\u03c9\u2206\u03b8) cos(\u03c9\u2206\u03b8) ) for \u03c9 \u2208 R Angles on a unit torus, \u03b8 ( cos(a \u00b7\u2206\u03b8) \u2212 sin(a \u00b7\u2206\u03b8) sin(a \u00b7\u2206\u03b8) cos(a \u00b7\u2206\u03b8) ) for a \u2208 Z2 Position on a plane, x ( cos(k \u00b7\u2206x) \u2212 sin(k \u00b7\u2206x) sin(k \u00b7\u2206x) cos(k \u00b7\u2206x) ) for k \u2208 R2 Position on a unit sphere, \u03d5 Real Wigner-D Matrices A.3 REPRESENTATIONAL CONSTRAINTS: Finally, we will translate these constraints on transformation matrices into constraints on the neural code. This is, thankfully, not too difficult. Consider the representation of an angle for simplicity, and take some arbitrary origin in the input space, \u03b80 = 0. The representation of all other angles can be derived via: g(\u03b8) = T (\u03b8)g(0) = S  I1(\u03b8) 0 0 . . . 0 0 I2(\u03b8) 0 . . . 0 0 0 I3(\u03b8) . . . 0 ... ... ... . . . ... 0 0 0 . . . ID(\u03b8) S\u22121g(0) (17) In the case of an angle, each irrep, Id(\u03b8), is just a 2-by-2 rotation matrix at frequency d, table A.2, and for an N -dimensional T we can fit maximally N2 (for N even) different frequencies, where N is the number of neurons. Hence the representation, g(\u03b8), is just a linear combination of the sine and cosine of these different frequencies, exactly as quoted previously: g(\u03b8) = a0 + D\u2211 d=1 ad sin(nd\u03b8) + bd cos(nd\u03b8) for integer nd, (18) a0 corresponds to the trivial irrep, and we include it since we know it must be in the code to make the firing rates non-negative for all \u03b8. It also cleans up the constraint on the number of allowable frequencies, for even or odd N we require D < N2 . This is because if N is odd one dimension is used for the trivial irrep, the rest can be shared up amongst N\u221212 frequencies, so D must be an integer smaller than N2 . If N is even, one dimension must still be used by the trivial irrep, so D can still maximally be only the largest integer smaller than N2 . Extending this to other groups is relatively simple, the representation is just a linear combination of the functions that appear in the irrep of the group in question, see table A.2. For position on a line, circle, torus, or plane, they all take the relatively simple form as in equation 18, but requiring appropriate changes from 1 to 2 dimensions, or integer to real frequencies. Representations of position on a sphere are slightly more complex, instead being constructed from linear combinations of groups of spherical harmonics. B NUMERICAL OPTIMISATION DETAILS: In order to test our claims, we numerically optimise the parameters that define the representation to minimise the loss, subject to constraints. Despite sharing many commonalities, our optimisation procedures are different depending on whether the variable being represented lives in an infinite space (line, plane, volume) or finite (circle, torus, sphere). We describe each of these schemes in turn, beginning with infinite spaces. All code will be made available on github. B.1 NUMERICAL OPTIMISATION FOR INFINITE SPACES: We will use the representation of a point on a line for explanation, planes or volumes are a simple extension. g(x) is parameterised as follows: g(x) = a0 + D\u2211 d=1 ad cos(\u03c9dx) + bd sin(\u03c9dx) a0, {ad, bd}Dd=1 \u2208 RN , \u03c9d \u2208 R (19) Our loss is made from three terms: the first is the functional objective, the last two enforce the non-negativity and boundedness constraint respectively: L = Lfunctional + \u03bbpLnon-negativity + \u03bbbLbounded (20) To evaluate the functional component of our loss we sample a set of points {xi}Mi=1 from p(x) and calculate their representations {g(xi)}Mi=1. To make the bounded constraint approximately satisfied (there\u2019s still some work to be done, taken care of by Lbounded) we calculate the following neuron-wise norm and use it to normalise each neuron\u2019s firing: \u2225gn\u22252 = M\u2211 m=1 gn(xm) 2 g\u0303n(\u03b8) = gn(\u03b8) \u2225gn\u2225 (21) The functional form of Lfunctional varies, as discussed in the main paper. For example, for the full loss we compute the following using the normalised firing, a discrete approximation to equation 1: Lfunctional = 1 M2 M\u2211 m=1 M\u2211 m\u2032=1 e\u2212 \u2225g\u0303(xm)\u2212g\u0303(xm\u2032 )\u2225 2 2\u03c32 \u03c7(xm, xm\u2032) (22) Now we come to our two constraints, which enforce that the representation is non-negative and bounded. We would like our representation to be reasonable (i.e. non-negative and bounded) for all values of x. If we do not enforce this then the optimiser finds various trick solutions in which the representation is non-negative and bounded only in small region, but negative and growing explosively outside of this, which is completely unbiological, and uninteresting. Of course, x is infinite, so we cannot numerically ensure these constraints are satisfied for all x. However, ensuring they are true in a region local to the animal\u2019s explorations (which are represented by p(x)) suffices to remove most of these trick solutions. As such, we sample a second set of MS \u2018shift positions\u2019, {xm}MSm=1, from a scaled up version of p(x), using a scale factor S. We then create a much larger set of positions, by shifting the original set by each of the \u2018shift positions\u2019, creating a dataset of size M \u2217MS , and use these to calculate our two constraint losses. Our non-negativity loss penalises negative firing rates: Lnon-negativity = 1 MMSN MS\u2211 ms=1 M\u2211 m=1 N\u2211 n=1 |g\u0303n(xms + xm)|I(g\u0303n(xms + xm) < 0) (23) Where I is the indicator function, 1 if the firing rate is negative, 0 else, i.e. we just average the magnitude of the negative portion of the firing rates. The bounded loss penalises the deviation of each neuron\u2019s norm from 1, in each of the shifted rooms: Lbounded = 1 NMS N\u2211 n=1 MS\u2211 ms=1 ( M\u2211 m=1 g\u03032n(xms + xm)\u2212 1)2 (24) That completes our specification of the losses. We minimise the full loss over the parameters (a0, {ad, bd, \u03c9d}Dd=1) using a gradient-based algorithm, ADAM (Kingma & Ba, 2014). We initialise these parameters by sampling from independent zero-mean gaussians, with variances as in table B.3. The final detail of our approach is the setting of \u03bbp and \u03bbb, which control the relative weight of the constraints vs. the functional objective. We don\u2019t want the optimiser to strike a tradeoff between the objective and constraints, since the constraints must actually be satisfied. But we also don\u2019t want to make \u03bbp so large that the objective is badly minimised in the pursuit of only constraint satisfaction. To balance these demands we use GECO (Rezende & Viola, 2018), which specifies a set of stepwise updates for \u03bbp and \u03bbb. These ensure that if the constraint is not satisfied their coefficients increase, else they decrease allowing the optimiser to focus on the loss. The dynamics that implement this are as follows, for a timestep, t. GECO defines a log-space measure of how well the constraint is being satisfied: Lt = log(Lt)\u2212 k (25) (or small variations on this), where k is a log-threshold that sets the target log-size of the constraint loss in question, and Lt is the value of the loss at timestep t. Lt is then smoothed: L\u0302t = \u03b1L\u0302t + (1\u2212 \u03b1)Lt (26) And this smoothed measure of how well the constraint is being satisfied controls the behaviour of the coefficient: \u03bbt+1 = \u03bbte \u03b3L\u0302t (27) This specifies the full optimisation, parameters are listed in table B.3. B.2 NUMERICAL OPTIMISATION FOR FINITE SPACES: Optimising the representation of finite variables has one added difficulty, and one simplification. Again, we have a parameterised form, e.g. for an angle \u03b8: g(\u03b8) = a0 + D\u2211 d=1 ad cos(nd\u03b8) + bd sin(nd\u03b8) a0, {ad, bd}Dd=1 \u2208 RN , nd \u2208 Z (28) The key difficulty is that each frequency, nd, has to be an integer, so we cannot optimise it by gradient descent, and similar problems arise for positions on a torus, or on a sphere. We shall spend the rest of this section outlining how we circumvent this problem. However, we do also have one major simplification. Because the variable is finite, we can easily ensure the representation is nonnegative and bounded across the whole space, avoiding the need for an additional normalisation constraint. Further, for the uniform occupancy distributions we consider in finite spaces, we can analytically calculate the neuron norm, and scale the parameters appropriately to ensure it is always 1: \u2225gn(\u03b8)\u22252 = 1 2\u03c0 \u222b \u03c0 \u2212\u03c0 g2n(\u03b8)d\u03b8 = \u2225a0\u22252 + 1 2 D\u2211 d=1 \u2225ad\u22252 + \u2225bd\u22252 (29) g\u0303n(\u03b8) = gn(\u03b8) \u2225gn(\u03b8)\u2225 (30) As such, we simply sample a set of angles {\u03b8m}Mm=1, and their normalised representations {g\u0303(\u03b8m)}Mm=1, and compute the appropriate functional and non-negativity loss, as in equations 22 & 23. The final thing we must clear up is how to learn which frequencies, nd, to include in the code. To do this, we make a code that contains many many frequencies, up to some cutoff Dmax, where Dmax is bigger than N : g(\u03b8) = a0 + Dmax\u2211 d=1 ad cos(d\u03b8) + bd sin(d\u03b8) a0, {ad, bd}Dmaxd=1 \u2208 R N (31) We then add a term to the loss that forces the code to choose only D of these Dmax frequencies, setting all other coefficient vectors to zero, and hence making the code actionable again but allowing the optimiser to do so in a way that minimises the functional loss. The term that we add to achieve this is inspired by the representation theory we used to write these constraints, Section A.2. We create first a 2Dmax + 1-dimesional vector, I(\u03b8), that contains all the irreps i.e. each of the Dmax sines and cosines and a constant. We then create a (2Dmax + 1) \u00d7 (2Dmax + 1)-dimensional transition matrix, GI(\u2206\u03b8) that is a representation of the rotation group in this space: GI(\u2206\u03b8)I(\u03b8) = I(\u03b8 + \u2206\u03b8). GI can be simply made by stacking 2-by-2 rotation matrices. Then we create the neural code by projecting the frequency basis through a rectangular weight matrix, W : g(\u03b8) =W I(\u03b8). Finally, we create the best possible representation of the group in the neural space: G(\u2206\u03b8) =WGI(\u2206\u03b8)W \u2217 (32) Where W \u2217 denotes the Moore-Penrose pseudoinverse. We then learn W , which is equivalent to learning a0 and {ad, bd}Dmaxd=1 . Representation theory tells us this will not do a perfect job at rotating the neural representation, unless the optimiser choosesW to cut out all but D of the frequencies. As such, we sample a set of shifts {\u03b8ms}Mms=1, and measure the following discrepancy: LTransformation = 1 MSM MS\u2211 mS=1 M\u2211 m=1 \u2225G(\u03b8ms)g(\u03b8m)\u2212 g(\u03b8m + \u03b8ms)\u22252 (33) Minimising it as we minimised the other constraint terms will force the code to chooseD frequencies and hence make the code actionable. Since calculating the pseudoinverse is expensive, we replaceW \u2217 with a matrixB that we also learn by gradient descent. B quickly learns to approximate the pseudoinverse, speeding our computations. That wraps up our numerical description. We are again left with three loss terms, two of which enforce constraints. This can be applied to any group, even if you can\u2019t do gradient descent through their irreps, at the cost of limiting the optimisation to a subset of irreps below a certain frequency, Dmax. L = Lfunctional + \u03bbpLnon-negativity + \u03bbTLTransformation (34) B.3 PARAMETER VALUES: We list the parameter values used to generate the grid cells in Figure 2B. B.4 FULL OPTIMAL REPRESENTATION:  C ANALYSIS OF OUR SIMPLEST LOSS THAT LEADS TO ONE LATTICE MODULE: In this section we analytically study the simplified loss suggested in section 3.1, and we show how to derive the equations 8. The actionability constraint tells us that: g(\u03b8) = a0 + D\u2211 d=1 ad sin(nd\u03b8) + bd cos(nd\u03b8) nd \u2208 Z (35) Where D is smaller than half the number of neurons (Appendix A). We ensure nd \u0338= nd\u2032 if d \u0338= d\u2032 by combining like terms. Our loss is: L0 = \u2212 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252d\u03b8d\u03b8\u2032 (36) Developing this and using trig identities for the sum of sines and cosines: L0 = \u2212 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252d\u03b8d\u03b8\u2032 = \u2212 1 4\u03c02 \u2211 n \u222b\u222b \u03c0 \u2212\u03c0 (gn(\u03b8)\u2212 gn(\u03b8\u2032))2d\u03b8d\u03b8\u2032 (37) = \u2212 \u2211 n \u222b\u222b \u03c0 \u2212\u03c0 ( \u2211 d an,d cos [nd(\u03b8 + \u03b8\u2032) 2 ] sin [nd(\u03b8 \u2212 \u03b8\u2032) 2 ] \u2212bn,d sin [nd(\u03b8 \u2212 \u03b8\u2032) 2 ] sin [nd(\u03b8 \u2212 \u03b8\u2032) 2 ] )2 d\u03b8d\u03b8\u2032 \u03c02 (38) We now change variables to \u03b1 = \u03b8\u2212\u03b8 \u2032 2 and \u03b2 = \u03b8+\u03b8\u2032 2 . This introduces a factor of two from the Jacobian (12 ), which we cancel by doubling the range of the integral while keeping the same limits, despite the change of variable, Figure 7. This gives us: = \u2212 1 \u03c02 \u2211 n \u222b\u222b \u03c0 \u2212\u03c0 ( \u2211 d an,d cosnd\u03b2 sinnd\u03b1\u2212 bn,d sinnd\u03b2 sinnd\u03b1)2d\u03b1d\u03b2 (39) = \u2212 1 \u03c02 \u2211 n,d,d\u2032 \u222b\u222b \u03c0 \u2212\u03c0 [ an,dan,d\u2032 sin(nd\u03b1) sin(nd\u2032\u03b1) cos(nd\u03b2) cos(nd\u2032\u03b2) \u2212 an,dbn,d\u2032 sin(nd\u03b1) sin(nd\u2032\u03b1) cos(nd\u03b2) sin(nd\u2032\u03b2) \u2212 bn,dan,d\u2032 sin(nd\u03b1) sin(nd\u2032\u03b1) sin(nd\u03b2) cos(nd\u2032\u03b2) + bn,dbn,d\u2032 sin(nd\u03b1) sin(nd\u2032\u03b1) sin(nd\u03b2) sin(nd\u2032\u03b2) ] d\u03b1d\u03b2 (40) Performing these integrals is easy using the fourier orthogonality relations; the two cross terms with mixed sin and cos of \u03b2 are instantly zero, the other two terms are non-zero only if nd = nd\u2032 , i.e. d = d\u2032. When d = d\u2032 these integrals evaluate to \u03c02, giving: L0 = \u2212 \u2211 n,d (a2n,d + b 2 n,d\u2032) (41) Exactly as quoted in equation 7. We can do the same development for the firing rate constraint: 1 2\u03c0 \u222b \u03c0 \u2212\u03c0 g2i (\u03b8)d\u03b8 = a 2 0,i + 1 2 D\u2211 d=1 a2d,i + b 2 d,i = 1, \u2200i = 1, ..., N (42) Where we have again simply used the orthogonality relation of sines and cosines. These are the constraints that must be enforced. For illustration it is useful to see one slice of the constraints made by summing over the neuron index: N = \u2225a0\u22252 + 1 2 D\u2211 d=1 \u2225ad\u22252 + \u2225bd\u22252 (43) This is the constraint shown in equation 8, and is strongly intuition guiding. But remember it is really a stand-in for the N constraints, one per neuron, that must each be satisfied! This analysis can be generalised to derive exactly the same result in 2D by integrating over pairs of points on the torus. It yields no extra insight, and is no different, so we just quote the result: L0 = \u2212 1 16\u03c04 \u222b \u222b\u222b \u222b \u03c0 \u2212\u03c0 \u2225g(x)\u2212 g(x\u2032)\u22252dxdx\u2032 = \u2212 \u2211 d \u2225ad\u22252 + \u2225bd\u22252 (44) D ANALYSIS OF PARTIALLY SIMPLIFIED LOSS THAT LEADS TO A MODULE: OF HEXAGONAL GRIDS In this section we will study an intermediate loss: it will compute the euclidean distance, like in Appendix C, but we\u2019ll include \u03c7 and p. We\u2019ll show that they induce a frequency bias, \u03c7 for low frequencies, and p for high, which together lead to hexagonal lattices. We\u2019ll study \u03c7 first for representations of a circular variable, then we\u2019ll study p for representations of a line, then we\u2019ll combine them. At each stage generalising to 2-dimensions is a conceptually trivial but algebraically nasty operation with no utility, so we do not detail here. In Appendix L we will return to the differences between 1D and 2D, and show instances where they become important. D.1 LOW FREQUENCY BIAS: PLACE CELLS ON THE CIRCLE: We begin with the representation of an angle on a circle, introduce \u03c7, and show the analytic form of the low frequency bias it produces. Since we\u2019re on a circle \u03c7 must depend on distance on the circle, rather than on a line (i.e. not \u03c7 = 1 \u2212 e\u2212 \u2225x\u2212x\u2032\u22252 2l2 ). We define the obvious generalisation of this to periodic spaces, Figure 8A, which is normalised to integrate to one under a uniform p(\u03b8): \u03c7(\u03b8, \u03b8\u2032) = ek \u2212 ek cos(\u03b8\u2212\u03b8\u2032) ek \u2212 I0(k) (45) Where I0 is the zeroth-order modified Bessel function. Hence the loss is: L1 = \u2212 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252 e k \u2212 ek cos(\u03b8\u2212\u03b8\u2032) ek \u2212 I0(k) d\u03b8d\u03b8\u2032 (46) In this expression k is a parameter that controls the generalisation width, playing the inverse role of l in the main paper. Taking the same steps as in Appendix C we arrive at: = \u2212 1 \u03c02 \u2211 n \u222b\u222b \u03c0 \u2212\u03c0 ( \u2211 d an,d cosnd\u03b2 sinnd\u03b1\u2212 bn,d sinnd\u03b2 sinnd\u03b1)2 ek \u2212 ek cos(2\u03b1) ek \u2212 I0(k) d\u03b1d\u03b2 (47) The \u03b2 integral again kills the cross terms: = \u2212 1 \u03c0 \u2211 n,d (a2n,d + b 2 n,d) \u222b \u03c0 \u2212\u03c0 sin2(nd\u03b1) ek \u2212 ek cos(2\u03b1) ek \u2212 I0(k) d\u03b1 (48) These integrals can be computed by relating them to modified Bessel functions of the first kind, which can be defined for integer n as: In(k) = 1 2\u03c0 \u222b \u03c0 \u2212\u03c0 ek cos(\u03b8) cos(n\u03b8)d\u03b8 (49) Hence the loss is: L1 = \u2212 \u2211 n,d (a2n,d + b 2 n,d) ek + Ind(k) ek \u2212 I0(k) (50) This is basically the same result as before, equation 41, all the frequencies decouple and decrease the loss in proportion to their magnitude. However, we have added a weighting factor, that decreases with frequency, Figure 8B, i.e. a simple low frequency bias! This analysis agrees with that of Sengupta et al. (2018), who argued that the best representation of angles on a ring for a loss related to the one studied here should be place cells. We disentangle the relationship between our two works in Appendix F. Regardless, our optimisation problem now asks us to build a module of lattice neurons with as many low frequencies as possible. The optimal solution to this is the lowest frequency lattice, i.e. place cells! (we validate this numerically in Appendix M). Therefore, it is only additional developments of the loss, namely the introduction of an infinite space for which you prioritise the coding of visited regions, and the introduction of the neural lengthscale, \u03c3, that lead us to conclude that modules of grid cells are better than place cells. D.2 HIGH FREQUENCY BIAS: OCCUPANCY DISTRIBUTION: Now we\u2019ll examine how a Gaussian p(x) affects the result, using a representation of x, a 1- dimensional position: L1 = \u2212 \u222b\u222b \u221e \u2212\u221e \u2225g(x)\u2212g(x\u2032)\u22252p(x)p(x\u2032)dxdx\u2032 = \u2212 1 2\u03c0L2 \u222b\u222b \u221e \u2212\u221e \u2225g(x)\u2212g(x\u2032)\u22252e\u2212 x2+x\u20322 2L2 dxdx\u2032 (51) Developing the argument as before: = \u2212 4 \u03c0L2 \u2211 n \u222b\u222b \u221e \u2212\u221e ( \u2211 d an,d cos\u03c9d\u03b2 sin\u03c9d\u03b1\u2212 bn,d sin\u03c9d\u03b2 sin\u03c9d\u03b1)2e\u2212 \u03b12+\u03b22 L2 d\u03b1d\u03b2 (52) The cross terms again are zero, due to the symmetry of the \u03b2 integral around 0. = \u2212 4 \u03c0L2 \u2211 n,d,d\u2032 an,dan,d\u2032C(\u03c9d, \u03c9d\u2032\u2225L)S(\u03c9d, \u03c9d\u2032\u2225L) + bn,dbn,d\u2032S2(\u03c9d, \u03c9d\u2032\u2225L) (53) Where we\u2019ve wrapped up the details into the following integrals of trigonometric functions over a gaussian measure: C(a, b\u2225L) = \u222b \u221e \u2212\u221e cos(ax) cos(bx)e\u2212 x2 L2 dx = L \u221a \u03c0 2 (e\u2212 L2(a+b)2 4 + e\u2212 L2(a\u2212b)2 4 ) (54) S(a, b\u2225L) = \u222b \u221e \u2212\u221e sin(ax) sin(bx)e\u2212 x2 L2 dx = L \u221a \u03c0 2 (e\u2212 L2(a\u2212b)2 4 \u2212 e\u2212 L2(a+b)2 4 ) (55) So we get our final expression for the loss: L1 = \u2212 \u2211 n,d,d\u2032 an,dan,d\u2032 (e \u2212 L2(\u03c9d\u2212\u03c9d\u2032 ) 2 2 \u2212 e\u2212 L2(\u03c9d+\u03c9d\u2032 ) 2 2 )\ufe38 \ufe37\ufe37 \ufe38 Sine Weighting +bn,dbn,d\u2032 (e \u2212 L2(\u03c9d\u2212\u03c9d\u2032 ) 2 4 \u2212 e\u2212 L2(\u03c9d+\u03c9d\u2032 ) 2 4 )2\ufe38 \ufe37\ufe37 \ufe38 Cosine Weighting (56) = \u2212 \u2211 d,d\u2032 [ (e\u2212 L2(\u03c9d\u2212\u03c9d\u2032 ) 2 2 \u2212 e\u2212 L2(\u03c9d+\u03c9d\u2032 ) 2 2 ) \u2211 n an,dan,d\u2032 + (e \u2212 L2(\u03c9d\u2212\u03c9d\u2032 ) 2 4 \u2212 e\u2212 L2(\u03c9d+\u03c9d\u2032 ) 2 4 )2 \u2211 n bn,dbn,d\u2032 ] (57) The downstream consequence of this loss is simple if a module encodes all areas of space evenly. Encoding evenly implies the phases of the neurons in the module are evenly distributed. The phase of a particular neuron in encoded in the coefficients of the sine and cosine of the base frequency, an1 and bn1. Even distribution implies that an1 oscillates from Ad, where Ad = maxn an1, to \u2212Ad and back again as you sweep the neuron index. As you perform this sweep the subsidiary coefficients, such as an2, perform the same oscillation, but multiple times. an2 does it twice, since, by the arguments that led us to grids, the peaks of the two waves must align (Figure 3B), and that implies the peak of the wave at twice the frequency moves at the same speed as you sweep the neuron number. This implies its phase oscillates twice as fast, and hence that \u2211 n an1an2 = 0. Arguments like this imply the frequencies actually decouple for a uniform encoding of space, making the loss, L1 = \u2212 \u2211 d,n [ (1\u2212 e\u22122L 2\u03c92d)a2n,d + (1\u2212 e\u2212L 2\u03c92d)2b2n,d ] (58) This is exactly a high frequency bias! (Fig 8C Left) However, uniform encoding of the space is actually not optimal for this loss, as can be seen either from the form of equation 56, and verified numerically using the loss in the next section, Figure 8EF. The weighting of the sine factor and the cosine factor differ, and in fact push toward sine, since it plateaus at lower frequencies. Combine that with a low frequency push, and you get an incentive to use only sines, which is allowed if you can encode only some areas of space, as the euclidean loss permits. A final interesting feature of this loss is the way the coupling between frequencies gracefully extends the decoupling between frequencies to the space of continuous frequencies. In our most simplified loss, equation 41, each frequency contributed according to the square of its amplitude, \u2225ad\u22252 + \u2225bd\u22252. How should this be extended to continuous frequencies? Certainly, if two frequencies were very far from one another we should retrieve the previous result, and their contributions would be expected to decouple, \u2225a1\u22252 + \u2225b1\u22252 + \u2225a2\u22252 + \u2225b2\u22252. But if k2 = k1 + \u03b4k, for a small \u03b4k then the two frequencies are, for all intents and purposes, the same, so they should contribute an amplitude \u2225a1 + a2\u22252 + \u2225b1 + b2\u22252 \u0338= \u2225a1\u22252 + \u2225b1\u22252 + \u2225a2\u22252 + \u2225b2\u22252. Equation 56, Figure 8C right, performs exactly this bit of intuition. It tells us that the key lengthscale in frequency space is 1L . Two frequencies separated by more than this distance contribute in a decoupled way, as in equation 36. Conversely, at very small distances, much smaller than 1L the contribution is exactly \u2225a1 + a2\u22252 + \u2225b1 + b2\u22252, as it should be. But additionally, it tells us the functional form of the behaviour between these two limits: a Gaussian decay. D.3 THE COMBINED EFFECT = HIGH AND LOW FREQUENCY BIAS: In this section we show that the simple euclidean loss with both \u03c7 and p can be analysed, and it contains both the high and low frequency biases discussed previously in isolation. This is not very surprising, but we include it for completeness. L1 = \u2212 \u222b\u222b \u221e \u2212\u221e \u2225g(x)\u2212 g(x\u2032)\u22252 (1\u2212 e\u2212 (x\u2212x\u2032)2 2l2 )\ufe38 \ufe37\ufe37 \ufe38 \u03c7 p(x)p(x\u2032)\ufe37 \ufe38\ufe38 \ufe37 1 2\u03c0L2 e\u2212 x2+x\u20322 2L2 dxdx\u2032 (59) Following a very similar path to before we reach: = \u2212 4 \u03c0L2 \u2211 n,d,d\u2032 andand\u2032C(\u03c9d, \u03c9d\u2032\u2225L)\u2206S(\u03c9d, \u03c9d\u2032\u2225L,\u039b)+bndbnd\u2032\u2206S(\u03c9d, \u03c9d\u2032\u2225L,\u039b)S(\u03c9d, \u03c9d\u2032\u2225L) (60) \u2206S(\u03c9d, \u03c9d\u2032\u2225L,\u039b) = S(\u03c9d, \u03c9d\u2032\u2225L)\u2212 S(\u03c9d, \u03c9d\u2032\u2225\u039b) (61) \u039b = lL\u221a l2 + 2L2 \u2248 l\u221a 2 since L > l (62) This contains the low and high frequency penalisation, at lengthscales 1L and 1 l , respectively. There are 4 terms, 2 are identical to equation 56, and hence perform the same role. The additional two terms are positive, so should be minimised, and they can be minimised by making the frequencies smaller than \u223c 1l , Figure 8D. We verify this claim numerically by optimising a neural representation of 2D position, g(x), to minimise L1 subject to actionable and biological constraints. It forms a module of hexagonal cells, Figure 8E, but the representation is shoddy, because the cells only encode a small area of space well, as can be seen by zooming on the response, Figure 8. E ANALYSIS OF FULL LOSS: MULTIPLE MODULES OF GRIDS: In this Section we will perturbatively analyse the full loss function, which includes the neural lengthscale, \u03c3. The consequences of including this lengthscale will become most obvious for the simplest example, the representation of an angle on a ring, g(\u03b8), with a uniform weighting of the importance of separating different points. We\u2019ll use it to derive the push towards non-harmonically related modules. Hence, we study the following loss: L = 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 e\u2212 \u2225g(\u03b8)\u2212g(\u03b8\u2032)\u22252 2\u03c32 d\u03b8d\u03b8\u2032 (63) And examine how the effects of this loss differ from the euclidean verison studied in Appendix C. This loss is difficult to analyse, but insight can be gained by making an approximation. We assume that the distance in neural space between the representations of two inputs depends only on the difference between those two inputs, i.e.: \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252 = d(\u03b8 \u2212 \u03b8\u2032) (64) Looking at the constrained form of the representation, we can see this is satisfied if frequencies are orthogonal: g(\u03b8) = a0 + D\u2211 d=1 ad sin(nd\u03b8) + bd cos(nd\u03b8) a0, {ad, bd}Dd=1 \u2208 RN , nd \u2208 Z (65) ad \u00b7 ad\u2032 = \u03b4d,d\u2032A2d bd \u00b7 bd\u2032 = \u03b4d,d\u2032A2d ad \u00b7 bd\u2032 = 0 (66) Then: \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252 = D\u2211 d=1 A2d sin 2 ( nd 2 (\u03b8 \u2212 \u03b8\u2032) ) (67) Now we will combine this convenient form with the following expansion: ex cos(y) = I0(x) [ 1 + 2 \u221e\u2211 m=1 Im(x) I0(x) cos(my) ] (68) Developing equation 63 using eqns. 68 and 67: L = 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 e\u2212 \u2211 d A 2 d sin 2(nd \u03b8\u2212\u03b8\u2032 2 ) 2\u03c32 d\u03b8d\u03b8\u2032 (69) = 1 2\u03c0 \u222b \u03c0 \u2212\u03c0 e\u2212 \u2211 d A 2 d sin 2(nd\u03b1) 2\u03c32 d\u03b1 (70) = 1 2\u03c0 \u220f d \u222b \u03c0 \u2212\u03c0 e\u2212 A2d 4\u03c32 (1\u2212cos(2nd\u03b1))d\u03b1 (71) = 1 2\u03c0 \u220f d [ e\u2212 A2d 4\u03c32 I0( A2d 4\u03c32 ) ] \ufe38 \ufe37\ufe37 \ufe38 Term A \u222b \u03c0 \u2212\u03c0 \u220f d [ 1 + 2 \u221e\u2211 m=1 Im( A2d 4\u03c32 ) I0( A2d 4\u03c32 ) cos(2ndm\u03b1) ] d\u03b1\ufe38 \ufe37\ufe37 \ufe38 Term B (72) We will now look at each of these two terms separately and derive from each an intuitive conclusion. Term A will tell us that frequency amplitudes will tend to be capped at around the legnthscale \u03c3. Term B will tell us how well different frequencies combine, and as such, will exert the push towards non-harmonically related frequency combinations. E.1 TERM A ENCOURAGES A DIVERSITY OF HIGH AMPLITUDE FREQUENCIES IN THE CODE: Fig 9A shows a plot of I0( A2d 4\u03c32 ). As can be seen there are a couple of regimes, if Ad, the amplitude of frequency nd, is smaller than \u03c3 then I0( A2d 4\u03c32 ) \u2248 1. On the other hand, asAd grows, I0 also grows. Asymptotically: I0( A2d 4\u03c32 ) \u223c e A2d 4\u03c32\u221a A2d 4\u03c32 (73) Correspondingly Term A also behaves differently in the two regimes, when Ad is smaller than \u03c3 the loss decreases exponentially with the amplitude of frequency d: Ad < \u03c3 Term A = e \u2212 A 2 d 4\u03c32 I0( A2d 4\u03c32 ) \u223c e\u2212 A2d 4\u03c32 (74) Alternatively: Ad > \u03c3 Term A = e \u2212 A 2 d 4\u03c32 I0( A2d 4\u03c32 ) \u223c 2\u03c3 Ad (75) So, up to a threshold region defined by the lengthscale \u03c3, there is an exponential improvement in the loss as you increase the amplitude. Beyond that the improvements drop to being polynomial. This makes a lot of sense, once you have used a frequency to separate a subset of points sufficiently well from one another you only get minimal gains for more increase in that frequency\u2019s amplitude, Figure 9B. Since there are many frequencies, and the norm constraint ensures that few can have an amplitude of order \u03c3, it encourages the optimiser to put more power into the other frequencies. In other words, amplitudes are only usefully increased to a lengthscale \u03c3, encouraging a diversity in high amplitude frequencies. This is moderately interesting, but will be a useful piece of information for the next derivation. E.2 TERM B ENCOURAGES THE CODE\u2019S FREQUENCIES TO BE NON-HARMONICALLY RELATED: Term B is an integral of the product of D infinite sums, which seems very counter-productive to examine. We are fortunate, however. First, the coefficients in the infinite sum decay very rapidly with m when the ratio of Ad\u03c3 is O(1), Figure 9C. The previous section\u2019s arguments make it clear that, in any optimal code, Ad will be smaller or of the same order as \u03c3. As such, the inifite sum is rapidly truncated. Additionally, all of the coefficient ratios, ( Im( A2d 4\u03c32 ) I0( A2 d 4\u03c32 ) ) , are smaller than 1, as such we can understand the role of Term B by analysing just the first few terms in the following series expansion, Term B = \u220f d [ 1 + 2 \u221e\u2211 m=1 Im( A2d 4\u03c32 ) I0( A2d 4\u03c32 ) cos(2mnd\u03b1) ] = 1\ufe38\ufe37\ufe37\ufe38 0th Order +2 \u2211 d,m Im( A2d 4\u03c32 ) I0( A2d 4\u03c32 ) cos(2mnd\u03b1)\ufe38 \ufe37\ufe37 \ufe38 1st Order +4 \u2211 d,d\u2032,m,m\u2032 Im( A2d 4\u03c32 )Im\u2032( A2 d\u2032 4\u03c32 ) I0( A2d 4\u03c32 )I0( A2 d\u2032 4\u03c32 ) cos(2mnd\u03b1) cos(2m \u2032nd\u2032\u03b1)\ufe38 \ufe37\ufe37 \ufe38 2ndOrder + . . . (76) where, due to the rapid decay of ( Im( A2d 4\u03c32 ) I0( A2 d 4\u03c32 ) ) with m, each term is only non-zero for small m. The 0th order term is a constant, and the 1st order term is zero when you integrate over \u03b1. As such, the leading order effect of this loss on the optimal choice of frequencies arrives via the 2nd order term. The integral is easy, \u222b \u03c0 \u2212\u03c0 cos(2mnd\u03b1) cos(2mnd\u2032\u03b1)d\u03b1 = \u03c0\u03b4(mnd \u2212m\u2032nd\u2032) (77) We want to minimise the loss, and this contribution is positive, so any time the 2nd order term is non-zero it is a penalty. As such, we want as few frequencies to satisfy the condition in the delta function, mnd = m\u2032nd\u2032 . Recall from equation 76 that these terms are weighted by coefficients that decay rapidly as m grows. As such, the condition that the loss is penalising, mnd = m\u2032nd\u2032 , is really saying: penalise all pairs of frequencies in the code, nd, nd\u2032 , that can be related by some simple harmonic ratio, nd = m \u2032 m nd\u2032 , where m and m \u2032 are small integers, roughly less than 5, Figure 9C. This may seem obtuse, but is very interpretable! Simply harmonically related frequencies are exactly those that encode many inputs with the same response! Hence, they should be penalised, Fig 4C. E.3 A HARMONIC TUSSLE - NON-HARMONICS LEADS TO MULTIPLE MODULES: As described in Section 3.3, we now arrive at a dilemma. The arguments of Sections 3.1 and C suggest that the optimal representation is comprised of lattice modules, i.e. the neurons must build a code from a lattice of frequencies. A lattice is exactly the arrangement that ensures all the frequencies are related to one another by simple harmonic ratios. Yet, the arguments of the previous section suggest that frequencies with simple harmonic relations are a bad choice - i.e. a frequency lattice would be the worst option! How can this be resolved? The resolution will depend on the value of two parameters: the first, \u03c3, is the neural lengthscale; the second, N , is the number of neurons, which, through the boundedness constraint, sets the scale of the amplitudes, Ad. We can already see this key ratio, Ad\u03c3 , appearing in the preceding arguments. Its role is to choose the compromise point in this harmonic tussle, and in doing so it chooses the number of modules in the code. A simple Taylor expansion illustrates that as \u03c3 tends towards \u221e the full loss reverts to the euclidean approximation that was the focus of Sections C and D, in which the effect of the harmonic penalisation drops to zero. e\u2212 \u2225g(\u03b8)\u2212g(\u03b8\u2032)\u22252 2\u03c32 = 1\u2212 \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252 1 2\u03c32 +O( 1 \u03c34 ) (78) Hence, as \u03c3 \u2192 \u221e we recover our original optimal solution: all the neurons belong to one module, whose shape is determined by \u03c7 and p. This explains why, with few neurons (equivalent to \u03c3 \u2192 \u221e), the optimal solution is a single module of perfectly hexagonal grids, fig 4B. But as \u03c3 decreases the effect of the harmonic penalisation becomes larger and larger. Eventually, one module is no longer optimal. Yet, all the previous arguments that modularised codes are easily made non-negative, and hence valuable, still apply. The obvious solution is to have two modules. Within each module all the neurons are easily made non-negative, but between modules you can have very non-harmonic relations, trading off both requirements. As such increasing the ratio Ad\u03c3 , either by increasing the number of neurons or decreasing \u03c3, creates an optimisation problem for which the high quality representations are modules of grids with more modules as the ratio gets bigger. We observed exactly this effect numerically, and it explains the presence of multiple modules of grids in the full version of our problem for a range of values of \u03c3. Finally, we can extract from this analysis a guiding light: that the frequencies in different modules should be maximally non-harmonically related. This additional principle, beyond the simple low and high frequency biases of Section D, frames our normative arguments for the optimal arrangement of modules, that we explore Section 4.2 and Appendix K. E.4 NON-HARMONICALLY RELATED CONTINUOUS FREQUENCIES - A SMOOTHING EFFECT: We will address one final concern. When representing infinite variables the frequencies in the code become real numbers. For example, when representing a 1-dimensional continuous input, x, the code, g(x), has the form, g(x) = a0 + D\u2211 d=1 ad sin(\u03c9dx) + bd cos(\u03c9dx) \u03c9d \u2208 R (79) And it seems like the equivalent penalisation as in equation 77, \u03b4(m\u03c9d \u2212 m\u2032\u03c9d\u2032), is very easy to avoid. If m\u03c9d = m\u2032\u03c9d\u2032 and the code is being penalised, just increase \u03c9d by an infinitely small amount, \u03b4\u03c9, such that m\u03c9d +m\u03b4\u03c9 \u0338= m\u2032\u03c9d\u2032 , and suddenly this term does not contribute to the loss. This, however, is not how the loss manifests. Instead frequencies are penalised for being close to one another, where close is defined by the size of the region the animal encodes, set by lengthscale L. This can be seen by analysing the following loss, L = \u222b\u222b \u221e \u2212\u221e e\u2212 \u2225g(x)\u2212g(x\u2032)\u22252 2\u03c32 p(x)p(x\u2032)dxdx\u2032 (80) = 1 2\u03c0L2 \u222b\u222b \u221e \u2212\u221e e\u2212 \u2225g(x)\u2212g(x\u2032)\u22252 2\u03c32 e\u2212 x2+x\u20322 2L2 dxdx\u2032 (81) Making the same assumption as in the previous Section, equation 66, and developing the loss identically, L = 1 2 \u221a \u03c0L \u220f d [ e\u2212 A2d 4\u03c32 I0( A2d 4\u03c32 ) ] \ufe38 \ufe37\ufe37 \ufe38 Term A \u222b \u03c0 \u2212\u03c0 \u220f d [ 1 + 2 \u221e\u2211 m=1 Im( A2d 4\u03c32 ) I0( A2d 4\u03c32 ) cos(2\u03c9dm\u03b1) ] e\u2212 \u03b12 4L2 d\u03b1\ufe38 \ufe37\ufe37 \ufe38 Term B (82) We again analyse this through the series expansion in equation 76. The 0th order term is again constant. The 1st order term is proporational to, 1 2 \u221a \u03c0L \u222b \u221e \u2212\u221e cos(2m\u03c9d\u03b1)e \u2212 \u03b12 4L2 d\u03b1 = e\u22124L 2m2\u03c92d (83) i.e., as in Section D.2, p(x) implements a high frequency bias. In fact, the mathematical form of this penalty, a gaussian, is identical to that in diagonal terms of the previously derived sum, equation 56. The 2nd order term is where we find the answer to this Section\u2019s original concern, 1 2 \u221a \u03c0L \u222b \u221e \u2212\u221e cos(2m\u03c9d\u03b1)cos(2m \u2032\u03c9d\u2032\u03b1)e \u2212 \u03b12 4L2 d\u03b1 = 1 2 \u221a \u03c0L C(2m\u03c9d, 2m \u2032\u03c9d\u2032\u22252L) (84) = 1 2 (e\u22124L 2(m\u03c9d+m \u2032\u03c9d\u2032 ) 2 + e\u22124L 2(m\u03c9d\u2212m\u2032\u03c9d\u2032 ) 2 ) (85) Which is similar to the cross terms in equation 56, but with the addition of harmonic interactions. This is the continuous equivalent to the penalty in equation 77. Frequencies and their low order harmonics are penalised for landing closer than \u223c 1L from one another, Fig 9D. In conclusion, the logic gracefully transfers to continuous frequencies. F LINKS TO SENGUPTA ET AL. 2018: Sengupta et al. (2018) derive an optimal representation of structured variables, where their definition of structured exactly aligns with ours: the variable\u2019s transformations form a group (though the term they use is symmetric manifolds). Further, there are analytic similarities between the loss they study and ours. However, their optimal representations are place cells, and ours are multiple modules of grid cells. In this section we will disentangle these two threads, explaining this difference, which arises from our actionability principle, the neural lengthscale \u03c3, and our use of infinite inputs. Sengupta et al. (2018) study the optimisation of a loss derived from a dot product similarity matching objective, an objective that has prompted numerous interesting discoveries (Pehlevan & Chklovskii, 2019). Input vectors, x, stacked into the matrix X , are represented by vectors of neural activity, y, stacked into Y , that have to be non-negative and bounded. The loss measures the similarity between pairs of inputs, xTx\u2032, and pairs of outputs, yTy\u2032, and tries to optimise the neural representation, y, such that all input similarities above a threshold, \u03b1, are faithfully reproduced in the similarity structure of the neural representation. min Y \u22650 diag(Y TY )\u2264\u03b21 \u2212Tr((XTX \u2212 \u03b1E)Y TY ) (86) where E is a matrix of ones, and diag(Y TY ) \u2264 \u03b21 enforces the neural activity\u2019s norm constraint. Our loss is strikingly similar. Our inputs are variables, such as the angle, that play the role of x. Sengupta et al.\u2019s input dot product similarity, xTx\u2032 \u2212 \u03b1, plays the role of \u03c7(\u03b8, \u03b8\u2032), measuring similarity in input space. The neural activities, g, must, like y, be non-negative and bounded in L2 norm. And minimising the similarity matching loss is like minimising the simple euclidean form of the loss studied in Appendix D, with the integrals playing the role of the trace, L1 = \u2212 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252\u03c7(\u03b8, \u03b8\u2032)d\u03b8d\u03b8\u2032, (87) The analogy then, is that minimising L1 is asking the most similar elements of the input to be represented most similarly, and dissimilar dissimilarly, as in the non-negative similarity matching objective; an orthogonal motivation for our loss! Sengupta et al. (2018) use beautiful convex optimisation techniques (that cannot, unfortunately, be applied directly to our problem, Appendix G) to show that the optimal representation of the finite spaces they consider (circles and spheres) are populations of place cells. What is it about our losses that leads us to different conclusions? Our problems differ in a few ways. First, since they use dot-product similarities, their loss is closest to our simplified losses, i.e. those without the presence of the neural lengthscale, \u03c3. So, due to this difference, we would expect them to produce a single module of responses, as observed. The second difference is the addition of actionability to our code which produces a harsh frequency content constraint, and is pivotal to our derivation (as can be seen from ablation studies, Appendix I). They, however, assume infinitely many neurons, at which point the frequency constraint becomes, especially when there is only one module, relatively vacuous. So we could still see their analysis as the infinite neuron version of ours. Third, we use a different measure of input similarity, though we do not believe this would have particularly strong effects on our conclusions. Finally, we consider infinite spaces, like position on a line, by introducing a probability distribution over inputs, p(x). As such, for representations of finite variables, like angle, the infinite neuron predictions of our simplified loss and Sengupta et al. should align. Thankfully, they do! As shown in Appendix M we predict place-cell like tuning for the representation of circular variables. This follows from our previous discussions, Appendix D.1: for finite spaces including a low frequency bias via the input similarity \u03c7was argued to make the optimal representation a module of the lowest frequency lattices, place cells! Hence, relative to Sengupta et al., by extending to the representation of infinite spaces with actionability we reach grids, rather than place cells. And by additionally including \u03c3, the neural lengthscale, we produce multiple modules. G OPTIMAL NON-NEGATIVE ACTIONABLE CODE - A NON-CONVEX PROBLEM: In this section we highlight that actionability is what makes our problem non-convex. Specifically, choosing the set of frequencies is difficult: for a fixed choice of frequencies the loss and constraints are convex in the neural coefficients. We include it here to point towards tools that may be useful in proving grid\u2019s optimality, rather than heuristically justifying them with numerically support, as we have done. Consider a representation g(\u03b8). For a fixed set of frequencies {nd}Dd=1, we\u2019ll aim to optimise the coefficients a0, {ad, bd}Dd=1. The losses take simple forms, the functional objective to be minimised is min\u2212 \u2211 d \u2225ad\u22252 + \u2225bd\u22252. The boundedness constraint is similarly quadratic in the coefficients, equation 8. The only potential trouble is the non-negativity constraint. Sengupta et al. (2018) solve this problem by having no actionabtility and infintely many neurons, we can take a different approach. Our code is a Positive Trigonometric Polynomial, a function of the following form: gn(\u03b8) = Dmax\u2211 d=\u2212Dmax rdne id\u03b8 (88) where Dmax = maxd nd, a type of polynomial studied in engineering and optimisation settings Dumitrescu (2007). Due to actionability our code is a sparse positive trigonometric polynomial, only 2D + 1 of the rd are non-zero. While positivity is a hard constraint to enforce, Bochner\u2019s theorem gives us hope. It tells us that for gn(\u03b8) to be positive for all \u03b8, its fourier transform, gn(\u03c9) must be positive-definite. This means that the following matrix, made from the fourier coefficients, must be positive-definite: Qn =  rn0 r \u2217 n1 r \u2217 n2 . . . r \u2217 nDmax rn1 rn0 r \u2217 n1 ... rn2 rn1 rn0 ... . . . r\u2217n1 rnDmax . . . rn1 rn0  \u227b 0 (89) The set of positive-definite matrices forms a convex set. Even the set of matrices Qn where fixed diagonals are zero is a convex set. Further, this matrix displays all the right tradeoffs for positivity: rn0 must be non-zero for the matrix to be positive-definite, i.e. there must be a constant offset to achieve non-negativity. Similarly, including harmonics makes achieving non-negativity easier than non-harmonics, just as in Figure 2A. But even having made this link we cannot make progress, since we had to choose which frequencies had non-zero amplitude. Hence, the choice of frequency sparsity implied by actionability stops our problem from being convex. A plausible solution might be to solve the convex optimisation problem for a given set of frequencies, then to take gradients through that solution to update the frequencies, but we have not implemented this. H LINKS TO OTHER LINEAR PATH INTEGRATION MODELS: Linear path integration models of the entorhinal cortex, like ours, have, to the best of our knowledge, been suggested three times before (Issa & Zhang, 2012; Whittington et al., 2020; Gao et al., 2021). Whittington et al. and Gao et al. are both simulation based, optimising losses related to ours, and show the resulting optimisations look like grid cells, though both papers only obtain multiple modules of grids by manually enforcing block diagonal transformation matrices. In this section we will discuss the differences between these two losses and ours, to explain their results in our framework as much as possible. Gao et al. optimise a representation of position and a set of linear neural transition matrices to minimise a series of losses. One loss enforces path integration, like our actionability constraint. Another bounds the firing rate of vectors, similar to our boundedness constraint. A final similarity is their version of the functional objective, which enforces the ability to linearly readout place-cell-like activity from the grid code. However, additionally they enforce an \u2018isotropic scaling term\u2019. This term measures how much the neural activity vector moves when you take a small step in direction \u03c8 in the input space, and encourages all the steps to be the same size independent of \u03c8. This can be understood through the strong force towards hexagonality it produces: of all the two dimensional grid the one that is most symmetric is hexagons, with it\u2019s C6 symmetry. Hence it will minimise this isotropic scaling. We have shown that it is not necessary to include this term in the loss to produce hexagonal grids. However, there will be differences in behaviour between a code minimising this constraint and a code that doesn\u2019t. The stronger push towards hexagonality that isotopy exerts will make deviations from perfect hexagonality, such as shearing Stensola et al. (2015) less optimal. A careful analysis of the behaviour of the two losses, both with functional, actionable, and biological constraints, but adding or removing isotropy, in different rooms could produce different predictions. Further, each module would be more robustly hexagonal with this term added, which might be necessary to make many many modules hexagonal. Finally, the relation between our non-negative firing rates, and the concept of non-negativity in Gao et al. is slightly murky. Gao et al. use non-negative grid-to-place-cell weights, but also show this non-negativity is not required to produce grids. How this meshes with our findings that nonnegativity is vital for grid formation remains unclear. This discrepancy could be caused by the isotropy constraint, but we remain unsure. Whittington et al. train a recurrent neural network to path integrate, but also to separate the representation of different points in the input space by predicting different observations at different positions. Path integration is linear with action dependent matrices, exactly like our actionability, and the neural activities are bounded. In recent versions of the model, Whittington et al. (2021), non-negative firing rates are used, which gives hexagonal grid cells. This can be understood as a direct application of our framework, though the pressures on the representation are naturally more blurry since it is optimised with behavioural trajectories using a recurrent neural network. I ABLATION STUDIES: We have argued that three principles - biological, functional, and actionable - are necessary to produce grid cells. In this section we show that removing any of these three and numerically optimising produces fundamentally different patterns, and we explain these results with the theory we\u2019ve developed in this work. We study the representation of a torus, since for periodic spaces the three terms appear cleanly in the loss, so can be easily excised, Appendix B. Figure 10 shows that removing any term stops grid cells emerging. No Non-negativity Without non-negativity the code wants to maximise the size of each of the limited number of frequencies in the code, ensuring that they are non-harmonically related. As such, it builds a wavey but fairly random looking code. No Functional Without a functional term the loss simply wants the code to contain few frequencies and be non-negative, so produces random looking responses with a sparse fourier spectrum and a large constant offset to ensure non-negativity. No Actionability Without actionability the code wants to be non-negative but separable. Bumps are easily positivised and encode the space well, but there is no reason to arrange them into lattices, and it would actually be detrimental to co-ordinate the placement across neurons so that any hypothetical lattice axes align. As such, it builds a random but bumpy looking pattern. This observation will be important for our discussion of the coding of 3-dimensional space, Appendix M. J LATTICE SIZE:PEAK WIDTH RATIO SCALES WITH NUMBER OF NEURONS: Thanks to the actionability constraint, the number of neurons in a module controls the number of frequencies available to build the module\u2019s response pattern. In our scheme the frequencies within one module organise into a lattice, and the more frequencies in the module, the higher the top frequency, and the correspondingly sharper the grid peaks. This link allows us to extract the number of neurons from a single neuron\u2019s tuning curve in the module. In this section we add slightly more justification to this link. We begin with a representation of 1D position, g(x). When there are enough neurons the griddy response can be approximated as, gn(x) \u2248 N (x\u22250, \u00b5) \u2217DC(\u03bd), (90) where DC(\u03bd) is a Dirac comb with lengthscale \u03bd, \u2217 is convolution, and \u00b5 is the grid peak width, Fig 11. The fourier transform of this object is simple, Fig 11, g\u0303n(\u03c9) \u2248 N ( \u03c9\u22250, 1 \u00b5 ) \u00d7DC ( 1 \u03bd ) (91) We can now extract the tradeoff exactly. In this continuous approximation the equivalent of actionability\u2019s sparsity in frequency space is that the frequency spectra must decay sufficiently fast i.e. the number of frequencies with power above some small threshold must be less than half the number of neurons. This number is controlled by the ratio of \u00b5 and \u03bd. We\u2019ll set some arbitrary low scaled threshold, \u03c4 , which defines the power a frequency must have, relative to the constant component, to be considered as a member of the code. The actual value of \u03c4 is inconsequential, as we just want scaling rules. Then we\u2019ll derive the scaling of the frequency at which the amplitude of the gaussian drops below threshold, \u03c9\u2217: \u2225N (\u03c9\u2217\u22250, 1\u00b5 )\u2225 2 \u2225N (0\u22250, 1\u00b5 )\u22252 = e\u2212\u00b5 2\u03c9\u22172 = \u03c4 (92) \u03c9\u2217 = log( 12\u03c4 ) \u00b5 (93) Now we can ask how many frequencies there are in the code, by counting the number of peaks of the Dirac comb that occur before this threshold. Counting from 0, because in 1D negative frequencies are just scaled versions of positive frequencies: D = \u03c9\u2217 1 \u03bd = \u03bd \u00b5 log ( 1 2\u03c4 ) (94) Therefore, in 1D, when there are enough neurons, the number of frequencies, and therefore the number of the neurons scales with the lattice to peak width ratio: N \u221d \u03bd \u00b5 (95) A completely analogous argument can be made in 2D, Again the cutoff frequency is \u03c9\u2217 \u221d 1\u00b5 . A different scaling emerges, however, because the number of frequencies in a dirac comb lower than some threshold scales with dimension. The frequency density of the dirac comb is proportional to 1 \u03bd2 peaks per unit area with the specifics depending on the shape of the frequency lattice, so we\u2019re asking how many peaks are there in a half circle of radius \u03c9\u2217. This is simple, D \u221d \u03c0\u03c9 \u22172 2 1\u03bd2 = \u03bd2 \u00b52 \u03c0 2 log ( 1 2\u03c4 ) (96) Hence we recover the stated scaling, N \u221d ( \u03bd \u00b5 )2 (97) K OPTIMAL RELATIVE ANGLE BETWEEN MODULES: Our analysis of the full loss, Appendix E, suggested that optimal representations should contain multiple modules, and each module should be chosen such that its frequencies are optimally nonharmonically related to the frequencies in other modules. This is a useful principle, but the full implications are hard to work through exactly. In this section we use rough approximations and coarse calculations to make estimates of one aspect of the relationships between modules: the optimal relative angle between lattice axes. Our scheme begins by calculating a smoothed frequency lattice density as a function of angle, \u03c1(\u03c8). It then aligns two of these smoothed densities, representing two modules, at different angular offsets, and finds the angle which minimises the overlap. This angle is the predicted optimal angle. We then extend this to multiple modules by searching over all doubles or triples of alignment angles for the minimal overlap of three or four densities. We will talk through the details of this method, and arrive at the results that are quoted in section 4.2. If you imagine standing at the origin in frequency space, corresponding to the constant component, and looking to infinity at an angle \u03c8 to the kx axis, the density function, \u03c1(\u03c8), measures, as a function of \u03c8, the number of smoothed frequencies that you see, fig 12A. We assume there are many neurons in this module, and hence many frequencies in the frequency lattice. Then we make the same approximation as in eqns. 90 and 91, that the frequency response is a hexagonal grid multiplied by a decaying Gaussian. The lengthscale of this Guassian is the first parameter that will effect our results, and it scales linearly with the number of neurons in the module. Then we try and capture the repulsive effects of the lattice. Each lattice point repels frequencies over a nearby region, as calculated in Section E.4. This region is of size \u223c 12L , where L is the exploration lengthscale of the animal. We therefore smooth the grid peaks in our density function by convolving the lattice with a Gaussian. We choose our units such that L = 1, then the width of this smoothing Gaussian is also order 1. The second important parameter is then the grid lattice lengthscale. The larger the grid cell lengthscale the more overlapping the repulsive effects of the frequencies will be, and the smoother the repulsive density, \u03c1(\u03c8). In maths, \u03c1(\u03c8\u2225\u00b5, \u03bd) = \u222b \u221e 0 [ N ( k \u2223\u2223\u2223\u22230, 1\u00b5 ) \u00d7DC ( k \u2223\u2223\u2223\u22231\u03bd )] \u2217 N (k\u22250, 1)krdkr (98) where kr is the radial part of the frequency. We calculate the obvious numerical approximation to this integral, Fig 12B, and in these calculations we ignore the effect of higher harmonics in equation E.4. This isn\u2019t as bad as is sounds, since higher harmonics contribute density at the same angle, \u03c8, part of the reason we chose to calculate this angular density. The next stage is finding the optimal angle between modules, starting with two identical modules. We do this by overlapping copies of \u03c1(\u03c8\u2225\u00b5, \u03bd). For simplicity, we start with two identical lattices and calculate the overlap as a function of angular offset: \u03c1\u03032(\u2206\u03c8\u2225\u00b5, \u03bd) = \u222b \u03c0 \u2212\u03c0 \u03c1(\u03c8\u2225\u00b5, \u03bd)\u03c1(\u03c8 +\u2206\u03c8\u2225\u00b5, \u03bd)d\u03c8 (99) Then the predicted angular offset is, \u2206\u03d5\u2217 = argmin \u2206\u03d5 \u03c1\u03032(\u2206\u03c8\u2225\u00b5, \u03bd) (100) Figure 12C shows this is a small angular offset around 4 degrees, though it can vary between 3 and 8 degrees depending on \u00b5 and \u03bd. An identical procedure can be applied to multiple modules. For example, for the arrangement of three modules we define, \u03c1\u03033(\u2206\u03c8,\u2206\u03c8 \u2032\u2225\u00b5, \u03bd) = \u222b \u03c0 \u2212\u03c0 \u03c1(\u03c8\u2225\u00b5, \u03bd)\u03c1(\u03c8 +\u2206\u03c8\u2225\u00b5, \u03bd)\u03c1(\u03c8 +\u2206\u03c8 +\u2206\u03c8\u2032\u2225\u00b5, \u03bd)d\u03c8 (101) and find the pair of offsets that minimise the overlap. For identical modules this also predicts small offsets between modules, of around 4 degrees between each module, Figure 12D, and similarly three identical modules should be oriented at multiples of 4 degrees from a reference lattice (i.e. offsets of 4, 8, and 12). As shown in Figure 5E, these small offsets appear to be present in data, validating our prediction. There are many natural next steps for this kind of analysis. In future we will explore the effect of different lattice lengthscales and try to predict the optimal module arrangement. Another simple analysis you can do is explore how changing the grid lengthscale, \u03bd, effects the optimal relative angle. The larger the grid lattice lengthscale, the smaller its frequency lattice, and the more smooth the resulting repulsion density is. As a result, the optimal relative angle between large modules is larger than between small modules, a prediction we would like to test in data. It would be interesting to apply more careful numerical comparisons, for example matching the \u03bd and \u00b5 to those measured in data, and comparing the real and optimal angular offsets. L ROOM GEOMETRY ANALYSIS: The functional objective depends on the points the animal wants to encode, which we assume are the points the animal visits. As such, the exploration pattern of the animal determines the optimal arrangement of grids, through the appearance of p(x) in the functional objective. Different constraints to the exploration of the animal, such as different sized environments, will lead to different optimal representations. In this section we analyse the functional objective for a series of room shapes, and show they lead to the intricate frequency biases suggested in Section 4.3. L.1 1-DIMENSONAL BOX To begin and to set the pattern for this section, we will study the representation of 1-dimensional position in a finite box. Ignoring the effects of \u03c7 for now, our loss is, L = 1 L2 \u222b\u222b L 2 \u2212L2 e\u2212 \u2225g(x)\u2212g(x\u2032)\u22252 2\u03c32 dxdx\u2032 (102) Making the symmetry assumption as in Section E our loss is, L = 1 L2 \u222b\u222b L 2 \u2212L2 e\u2212 \u2211 d A 2 d sin 2( \u03c9d(x\u2212x \u2032) 2 ) 2\u03c32 dxdx\u2032 (103) Changing variables to \u03b1 = x\u2212 x\u2032, \u03b2 = x+ x\u2032, Figure 13A, L = 1 2L2 [\u222b L 0 \u222b L\u2212\u03b1 \u2212L+\u03b1 e\u2212 \u2211 d A 2 d sin 2( \u03c9d\u03b1 2 ) 2\u03c32 d\u03b2d\u03b1+ \u222b 0 \u2212L \u222b \u2212L\u2212\u03b1 L+\u03b1 e\u2212 \u2211 d A 2 d sin 2( \u03c9d\u03b1 2 ) 2\u03c32 d\u03b2d\u03b1 ] (104) = 1 L2 \u222b L 0 e\u2212 \u2211 d A 2 d sin 2( \u03c9d\u03b1 2 ) 2\u03c32 (L\u2212 \u03b1)d\u03b1 (105) Performing the expansion as in Section E we reach: L = 1 L2 \u220f d [ e\u2212 A2d 4\u03c32 I0( A2d 4\u03c32 ) ] \ufe38 \ufe37\ufe37 \ufe38 Term A \u222b \u03c0 \u2212\u03c0 \u220f d [ 1 + 2 \u221e\u2211 m=1 Im( A2d 4\u03c32 ) I0( A2d 4\u03c32 ) cos(\u03c9dm\u03b1) ] (L\u2212 \u03b1)d\u03b1\ufe38 \ufe37\ufe37 \ufe38 Term B (106) Term A behaves the same as before, so we again study the series expension of term B, the analogue of equation 76. The 0th order term is a constant with respect to the code, but the first order term provides the main frequency biasing effect, as in Section E.4. 1 L2 \u222b L 0 cos(m\u03c9d\u03b1)(L\u2212 \u03b1)d\u03b1 = 1 m2L2\u03c92d (1\u2212 cos(m\u03c9dL)) (107) Hence the 1-dimensional box coarsly creates a high frequency bias (> 1L ), but also encourages frequencies that fit whole numbers of wavelengths within the length of the box, Figure 13B. Before we move on we will study the second order term. We won\u2019t repeat this for future finite rooms as it shows basically the same effect as in Section E.4: coarsly frequencies repel each other up to a lengthscale 1L . However, as for the first order term, there is additional structure, i.e. subsidiary optimal zeros in the loss, that could be of interest: 2 L2 \u222b L 0 cos(m\u03c9d\u03b1) cos(m \u2032\u03c9d\u2032\u03b1)(L\u2212 \u03b1)d\u03b1 = 1\u2212 cos(L(m\u03c9d +m\u2032\u03c9d\u2032)) (m\u03c9d +m\u2032\u03c9d\u2032)2L2 + 1\u2212 cos(L(m\u03c9d \u2212m\u2032\u03c9d\u2032)) (m\u03c9d \u2212m\u2032\u03c9d\u2032)2L2 (108) This is fundamentally a repulsion between frequencies closer than 1L from one another, fig 13C. Additionally, the repulsion is zero when the difference between frequencies is exactly 2\u03c0L times an integer. L.2 2-DIMENSIONAL BOX A very similar analysis can be applied to the representation of a 2-dimensional box, and used to derive the frequency bias plotted in Figure 6X. We reach an exactly analogous point to equation 105, L = 2 L4 [\u222b L 0 \u222b L 0 e\u2212 \u2211 d A 2 d sin 2( kdxx+kdyy 2 ) 2\u03c32 (L\u2212 x)(L\u2212 y)dxdy+ (109) \u222b L 0 \u222b 0 \u2212L e\u2212 \u2211 d A 2 d sin 2( kdxx+kdyy 2 ) 2\u03c32 (L\u2212 x)(L+ y)dxdy ] (110) Again we\u2019ll study the first order term of the series expansion, \u2206L1 \u221d 1\u2212 cos(mdkdxL) m2k2dx 1\u2212 cos(mdkdyL) m2k2dy (111) This is exactly the bias plotted in Figure 5F. Changing to rectangles is easy, it is just a rescaling of one axis, which just inversely scales the corresponding frequency axis. L.3 CIRCULAR ROOM: The final analysis we do of this type is the hardest. It follows the same pattern, make the symmetry assumption, expand the series. Doing so we arrive at the following first order term: \u2206L1 = 1 \u03c02R4 \u222b\u222b R 0 \u222b\u222b 2\u03c0 0 cos(mkd \u00b7 (x\u2212 x\u2032))d\u03c8d\u03c8\u2032rr\u2032drdr\u2032 (112) where (r, \u03c8) is a radial co-ordinate system, x = ( r cos\u03c8 r sin\u03c8 ) . Using the double angle formula to spread the cosine, and rotating the 0 of each \u03c8 variable to align with kd, we get: \u2206L1 = 1 \u03c02R4 [(\u222b R 0 \u222b 2\u03c0 0 cos(mr\u2225kd\u2225 cos\u03c8)d\u03c8rdr )2 +( \u222b R 0 \u222b 2\u03c0 0 sin(mr\u2225kd\u2225 cos\u03c8)d\u03c8rdr )2] (113) The second integral is 0, because sine is odd and periodic. We can perform the first integral using the following Bessel function properties, \u222b 2\u03c0 0 cos(a cos(\u03c8))d\u03c8 = 2\u03c0J0(\u2225a\u2225) (114) Hence, \u2206L1 = 4 R4 (\u222b R 0 J0(mr\u2225kd\u2225)rdr )2 (115) Which can be solved using another Bessel function property, \u222b R 0 rJ0(ar)dr = RJ1(aR) a (116) Therefore, \u2206L1 = 4J1(m\u2225kd\u2225R)2 R2m2\u2225kd\u22252 (117) This is exactly the circular bias plotted in Figure 5G. Placing the base frequencies of grid module on the zeros of this bessel function would be optimal, i.e. calling a zero \u03be, then the wavelengths of the grid modules should optimally align with, \u03bd = 2\u03c0R \u03bek (118) This corresponds to grids with wavelengths that are the following multiples of the circle diameter: 0.82, 0.45, 0.31, 0.24, 0.19, . . . . Though, as with the other predictions, the major effect is the polynomial decay with frequency, so these effects will all be of larger importance for the larger grid modules. As such, we might expect only the first two grid modules to fit this pattern, the others should, in our framework, focus on choosing their lattice parameters to be optimally non-harmonic. M REPRESENTATIONS OF CIRCLES, SPHERES, AND 3-DIMENSIONS: We believe our approach is relevant to the representation of many variables beyond 2-dimensional space. Most simply we can directly apply the tools we\u2019ve developed to the representation of any variable whose transition structure is a group, Section A.1. This is actually true for both numerical and analytic approaches, but here we focus mainly on the numerical approach. We discuss the representation of an angle on a ring, a point on a sphere, and 3 dimensional space. Additional interesting variables might include a position in hyperbolic space (since tree-like category structures, such as the taxonomic hierarchy of species, are best described by hyperbolic spaces), or the full orientation of an object in 3-dimensional space. For starters, we have frequently referenced the representation of an angle, g(\u03b8). This is also something measured in brain, in the famous head direction circuit, perhaps the most beautiful circuit in the brain (Kim et al., 2017). There you measure neurons that have unimodal tuning to heading direction. We match this finding, Figure 14A. This is understandable since, as discussed, on a finite space you still want a lattice module, but \u03c7, encourages you to be low frequency and there is no high frequency bias, since the space is finite and p(\u03b8) is uniform. Thus, place cells are optimal. If we included more neurons it would likely form multiple modules of cells with higher frequency response properties. We can also apply our work to the representation of a point on a sphere. Again, with the low frequency bias place cells are optimal, Figure 14B. Without it, or conceivably with enough neurons that multiple modules form, you can get many interesting patterns, Figure 14C-D. These could be relevant to the representation of object orientations, or in virtual reality experiments where animals must truly explore spherical spaces. Finally, we can equally apply our framework to the representation of a point in three dimensional space. Grid cells have been measured while an animal explores in 3D and shown to have irregular firing fields in both bats (Ginosar et al., 2021), and mice (Grieves et al., 2021). Analogously to the griddy modules in one and two dimensions, our framework predicts griddy three dimensional responses that are similarly densely packing, which is verified in 14E. However, in Appendix I we performed ablation studies and found that removing the actionable constraint led to multimodal tuning curves without structure, and we find the same for 3D, Figure 14F, perhaps matching the multimodal but unstructured representation found in the brain (e.g. Figure 14G from Ginosar et al. (2021)). As such, it seems that in three dimensions the animals are building a non-negative and discriminative representation without the constraint to path integrate. Therefore, we suggest that the animals have sacrificed the ability to manipulate their internal representations in 3D and instead focus on efficiently encoding the space.", "agent_review": [{"role": "user", "content": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Actionable Neural Representations: Grid Cells from Minimal Constraints\n\n1 INTRODUCTION: Animals and machines should build representations that afford flexible behaviours. However, different representation make some tasks easy and others hard; representing red versus white is good for understanding wines but less good for opening screw-top versus corked bottles. A central mystery in neuroscience is the relationship between tasks and their optimal representations. Resolving this requires understanding the representational principles that permit flexible behaviours such as zero-shot inference and short-cut planning. Here, we introduce actionable representations, a representation that permits flexible behaviours. Being actionable means encoding not only variables of interest, but also how the variable transforms. Actions cause many variables to transform in predictable ways. For example, actions in 2D space obey rules; north, east, south, and west, have a universal meaning, and combine in the same way everywhere. Embedding these rules into a representation of self-position permits deep inferences: having stepped north, then east, then south, an agent can infer that stepping west will lead home, having never taken that path - a zero-shot inference (Figure 1A). Indeed biology represents 2D space in a structured manner. Grid cells in medial entorhinal cortex represent an abstracted \u2018cognitive map\u2019 of 2D space(Tolman, 1948). These cells fire in a hexagonal lattice of positions (Hafting et al., 2005), (Figure 1B), and are organised in modules; cells within one module have receptive fields that are translated versions of one another, and different modules have firing lattices of different scales and orientations (Figure 1B), (Stensola et al., 2012). Biological representations must be more than just actionable - they must be functional, encoding the world efficiently, and obey biological constraints. We formalise these three ideas - actionable, functional, and biological - and analyse the resulting optimal representations. We define actionability using group and representation theory, as the requirement that each action has a corresponding matrix that linearly updates the representation; for example, the \u2018step north\u2019 matrix updates the representation to its value one step north. Functionally, we want different points in space to be represented maximally differently, allowing inputs to be distinguished from one another. Biologically, we ensure all neurons have non-negative and bounded activity. From this constrained optimisation problem we derive optimal representations that resemble multiple modules of grid cells. Our problem formulation allows analytic explanations for grid cell phenomena, matches experimental findings, such as the alignment of grids cells to room geometry (Stensola et al., 2015), and predicts some underappreciated aspects, such as the relative angle between modules. In sum, we 1) propose actionable neural representations to support flexible behaviours; 2) formalise the actionable constraint with group and representation theory; 3) mix actionability with biological and functional constraints to create a constrained optimisation problem; 4) analyse this problem and show that in 2D the optimal representation is a good model of grid cells, thus offering a mathematical understanding of why grid cells look the way they do; 5) provide several neural predictions; 6) highlight the generality of this normative method beyond 2D space. 1.1 RELATED WORK: Neuroscientists have long explained representations with normative principles like information maximisation (Attneave, 1954; Barlow et al., 1961), sparse (Olshausen & Field, 1996) or independent (Hyva\u0308rinen, 2010) latent encodings, often mixed with biological constraints such as non-negativity (Sengupta et al., 2018), energy budgets (Niven et al., 2007), or wiring minimisation (Hyva\u0308rinen et al., 2001). On the other hand, deep learning learns task optimised representations. A host of representation-learning principles have been considered (Bengio et al., 2013); but our work is most related to geometric deep learning (Bronstein et al., 2021) which emphasises input transformations, and building neural networks which respect (equivariant) or ignore (invariant) them. This is similar in spirit but not in detail to our approach, since equivariant networks do not build representations in which all transformations of the input are implementable through matrices. There is much previous theory on grid cells, which can be categorised as relating to our actionable, functional, and biological constraints. Functional: Many works argue that grid cells provide an efficient representation of position, that hexagons are optimal (Mathis et al., 2012a;b; Sreenivasan & Fiete, 2011; Wei et al., 2015) and make predictions for relative module lengthscales (Wei et al., 2015). Since we use similar functional principles, we suspect that some of our novel results, such as grid-to-room alignment, could have been derived by these authors. However, in contrast to our work, these authors assume a grid-like tuning curve. Instead we give a normative explanation of why be grid-like at all, explaining features like the alignment of grid axes within a module, which are detrimental from a pure decoding view (Stemmler et al., 2015). Actionability: Grid cells are classically understood as affording path-integration (integrating velocity to predict position) with units from both hand-tuned Burak & Fiete (2009) and trained recurrent neural network resembling grid cells (Cueva & Wei, 2018; Banino et al., 2018; Sorscher et al., 2019). Recently, these approaches have been questioned for their parameter dependence (Schaeffer et al., 2022), or relying on decoding place cells with bespoke shapes that are not observed experimentally (Sorscher et al., 2019; Dordek et al., 2016). Our mathematical formalisation of path-integration, combined with biological and functional constraints, provides clarity on this issue. Our approach is linear, in that actions update the representation linearly, which has previously been explored theoretically (Issa & Zhang, 2012), and numerically, in two works that learnt grid cells (Whittington et al., 2020; Gao et al., 2021). Our work could be seen as extracting and simplifying the key ideas from these papers that make hexagonal grids optimal (see Appendix H), and extending them to multiple modules, something both papers had to hard code. Biological: Lastly, both theoretically (Sorscher et al., 2019) and computationally (Dordek et al., 2016; Whittington et al., 2021), non-negativity has played a key role in normative derivations of hexagonal grid cells, as it will here. 2 ACTIONABLE NEURAL REPRESENTATIONS: AN OBJECTIVE: We seek a representation g(x) \u2208 RN of position x \u2208 R2, where N is the number of neurons. Our representation is built using three ideas: functional, biological, and actionable; whose combination will lead to multiple modules of grid cells, and which we\u2019ll now formalise. Functional: To be useful, the representation must encode different positions differently. However, it is more important to distinguish positions 1km apart than 1mm, and frequently visited positions should be separated the most. To account for these, we ask our representation to minimise L = \u222b\u222b e\u2212 \u2225g(x)\u2212g(x\u2032)\u22252 2\u03c32 \u03c7(x,x\u2032)p(x)p(x\u2032)dxdx\u2032 (1) The red term measures the representational similarity of x and x\u2032; it is large if their representations are nearer than some distance \u03c3 in neural space and small otherwise. By integrating over all pairs x and x\u2032, L measures the total representational similarity, which we seek to minimise. The green term is the agent\u2019s position occupancy distribution, which ensures only visited points contribute to the loss, for now simply a Gaussian of lengthscale L. Finally, the blue term weights the importance of separating each pair, encouraging separation of points more distant than a lengthscale, l. \u03c7(x,x\u2032) = 1\u2212 e\u2212 \u2225x\u2212x\u2032\u22252 2l2 p(x) =e\u2212 \u2225x\u22252 2L2 (2) Biological: Neurons have non-negative firing rates, so we constrain gi(x) \u2265 0. Further, neurons can\u2019t fire arbitrarily fast, and firing is energetically costly, so we constrain each neuron\u2019s response gn(x) via \u222b g2n(x)p(x)dx = 1 Actionable: Our final constraint requires that the representation is actionable. This means each transformations of the input must have its own transformation in neural space, independent of position. For mathematical convenience we enact the neural transformation using a matrix. Labelling this matrix T (\u2206x) \u2208 RN\u00d7N , for transformation \u2206x, this means that for all positions x, g(x+\u2206x) = T (\u2206x)g(x) (3) For intuition into how this constrains the neural code g(x), we consider a simple example of two neurons representing an angle \u03b8 \u2208 [0, 2\u03c0). Replacing x with \u03b8 in equation 3 we get the equivalent constraint: g(\u03b8 +\u2206\u03b8) = T (\u2206\u03b8)g(\u03b8). Here the matrix T performs a rotation, and the solution (up to a linear transform) is for T to be the standard 2\u00d7 2 rotation matrix, with frequency n. g(\u03b8 +\u2206\u03b8) = ( cos(n[\u03b8 +\u2206\u03b8]) sin(n[\u03b8 +\u2206\u03b8]) ) = ( cos(n\u2206\u03b8) \u2212 sin(n\u2206\u03b8) sin(n\u2206\u03b8) cos(n\u2206\u03b8) )( cos(n\u03b8) sin(n\u03b8) ) = T (\u2206\u03b8)g(\u03b8) (4) Thus as \u03b8 rotates by 2\u03c0 the neural representation traces out a circle an integer number, n, times. Thanks to the problem\u2019s linearity, extending to more neurons is easy. Adding two more neurons lets the population contain another sine and cosine at some frequency, just like the two neurons in equation 4. Extrapolating this we get our actionability constraint: the neural response must be constructed from some invertible linear mixing of the sines and cosines of D < N2 frequencies, g(\u03b8) = a0 + D\u2211 d=1 ad sin(nd\u03b8) + bd cos(nd\u03b8) for integer nd (5) The vectors {ad, bd}Dd=1 \u2208 RN are coefficient vectors that mix together the sines and cosines, of which there are D. a0 is the coefficient vector for a frequency that cycles 0 times. This argument comes from an area of maths called Representation Theory (a different meaning of representation!) that places constraints on the matrices T for variables whose transformations form a mathematical object called a group. This includes many of interest, such as position on a plane, torus, or sphere. These constraints on matrices can be translated into constraints on an actionable neural code just like we did for g(\u03b8) (see Appendix A for 2D space and more). 2D space, however, differs from the above example. First the variable is two-dimensional, so the frequencies, denoted kd, become two dimensional. Second, the space is infinite. Previously, frequencies were integers in order that activity returned home after a loop: g(2\u03c0) = g(0). But increasing the ring radius allows frequencies closer to each other, and as the radius goes to infinity, turning the circle into a 1D line, all frequencies are allowed. This applies equally in 2D, allowing us to drop the integer constraint, g(x) = a0 + D\u2211 d=1 ad sin(kd \u00b7 x) + bd cos(kd \u00b7 x) (6) Our constrained optimisation problem is complete. Equation 6 specifies the set of actionable representations. Without additional constraints these codes are meaningless: random combinations of sines and cosines produce random neural responses (Figure 2A). We will choose from amongst the set of actionable codes by optimising the parameters a0, {ad, bd,kd}Dd=1 to minimise L, subject to biological (non-negative and bounded firing rates) constraints. 3 OPTIMAL REPRESENTATIONS: Optimising over the set of actionable codes to minimise L with biological constraints gives multiple modules of grid cells (Figure 2B). This section will, using intuition and analytics, explain why. 3.1 NON-NEGATIVITY LEADS TO A MODULE OF LATTICE CELLS: To understand how non-negativity produces modules of lattice responses we will study the following simplified loss, which maximises the Euclidean distance between representations of angle, g(\u03b8), L0 = \u2212 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252d\u03b8d\u03b8\u2032 (7) This is equivalent to the full loss (equation 1) for uniform p(\u03b8), \u03c7(\u03b8, \u03b8\u2032) = 1, and \u03c3 very large. Make no mistake, this is a bad loss. For contrast, the full loss encouraged the representations of different positions to be separated by more than \u03c3, enabling discrimination1. Therefore, sensibly, the representation is most rewarded for separating points that would otherwise be closer than \u03c3. L0 does the opposite! It grows quadratically with separation, so g(\u03b8) is most rewarded for pushing apart already well-separated points, a terrible representational principle! Nonetheless, L0 will give us key insights. 1\u03c3 could be interpreted as a noise level, or a minimum discriminable distance, then points should be far enough away for a downstream decoder to distinguish them. Since actionability gives us a parameterised form of the representations (equation 5), we can compute the integrals to obtain the following constrained optimisation problem (details: Appendix C) min a0, {ad,bd,nd}Dd=1 L0 = \u2212 D\u2211 d=1 \u2225ad\u22252+\u2225bd\u22252 with Non-negativity\ufe37 \ufe38\ufe38 \ufe37 g(\u03b8) > 0, Bounded firing rates\ufe37 \ufe38\ufe38 \ufe37 \u2225a0\u22252 + 1 2 D\u2211 d=1 \u2225ad\u22252 + \u2225bd\u22252 = N (8) Where N is the number of neurons. This is now something we can understand. First, reminding ourselves that the neural code, g(\u03b8), is made from a constant vector, a0, and \u03b8 dependent parts (equation 6; Figure 3A), we can see that L0 separates representations by encouraging the size of each varying part, \u2225ad\u22252 + \u2225bd\u22252, to be maximised. This effect is limited by the firing rate bound, \u2225a0\u22252 \u2212 2L0 = N . Thus, to minimise L0 we must minimise the constant vector, a0. This would be easy without non-negativity (when any code with \u2225a0\u2225 = 0 is optimal), but no sum of sines and cosines can be non-negative for all \u03b8 without an offset. Thus the game is simple; choose frequencies and coefficients so the firing rates are non-negative, but using the smallest possible constant vector. One lattice cell. We now heuristically argue, and confirm in simulation, that the optimal solution for a single neuron is a lattice tuning curve (see Appendix G for why this problem is non-convex). Starting with a single frequency component, e.g. sin(\u03b8), achieving non-negativity requires adding a constant offset, sin(\u03b8) + 1 (Figure 3B). However, we could also have just added another frequency. In particular adding harmonics of the base frequency (with appropriate phase shifts) pushes up the minima (Figure 3B). Extending this argument, we suggest non-negativity, for a single cell, can be achieved by including a grid of frequencies. This gives a lattice tuning curve (Figure 3B right). Module of lattice cells. Achieving non-negativity for this cell used up many frequencies. But as discussed (Section 2), actionability only allows a limited number frequencies (< N2 since each fre- quency uses 2 neurons (sine and cosine)), thus how can we make lots of neurons non-negative with limited frequencies? Fortunately, we can do so by making all neuron\u2019s tuning curves translated versions of each other, as translated curves contain the same frequencies but with different phases. This is a module of lattice cells. We validate our arguments by numerically optimising the coefficients a0, {ad, bd}Dd=1 and frequencies {nd}Dd=1 to minimise L0 subject to constraints, producing a module of lattices (Figure 3C; details in Appendix B). These arguments equally apply to representations of a periodic 2D space (a torus; Figure 3D). Studying L0 has told us why lattice response curves are good. But surprisingly, all lattices are equally good, even at infinitely high frequency. Returning to the full loss will break this degeneracy. 3.2 PRIORITISING IMPORTANT PAIRS OF POSITIONS PRODUCES HEXAGONAL GRID CELLS: Now we return to the full loss and understand its impact in two steps, beginning with the reintroduction of \u03c7 and p, which break the lattice degeneracy, forming hexagonal grid cells. L = \u222b\u222b \u221e \u2212\u221e e\u2212 \u2225g(x)\u2212g(x\u2032)\u22252 2\u03c32 \u03c7(x,x\u2032)p(x)p(x\u2032)dxdx\u2032 (9) \u03c7 prefers low frequencies: recall that \u03c7 = 1\u2212 e\u2212 \u2225x\u2212x\u2032\u22252 2l2 ensures very distant inputs have different representations, while allowing similar inputs to have similar representations, up to a resolution, l. This encourages low frequencies (\u2225kd\u2225 < 1l ), which separate distant points but produce similar representations for pairs closer than l (Analytics: Appendix D.1). At this stage, for periodic 2D space, the lowest frequency lattices, place cells, are optimal (see Appendix F; Sengupta et al. (2018)). p(x) prefers high frequencies: However, the occupancy distribution of the animal, p(x), counters \u03c7. On an infinite 2D plane animals must focus on representing a limited area, of lengthscale L. This encourages high frequencies (\u2225kd\u2225 > 1L ), whose response varies among the visited points (Analytics: Appendix D.2). More complex p(x) induce more complex frequency biases, but, to first order, the effect is always a high frequency bias (Figure 5F-G, Appendix L). Combination \u2192 Hexagons: Satisfying non-negativity and functionality required a lattice of many frequencies, but now p and \u03c7 bias our frequency choice, preferring those beyond 1L (to separate points the animal visits) but smaller than 1l (to separate distant visited points). Thus to get as many of these preferred frequencies as possible, we want the lattice with the densest packing within a Goldilocks annulus in frequency space (Figure 4A). This is a hexagonal lattice in frequency space which leads to a hexagonal grid cell. Simulations with few neurons agree, giving a module of hexagonal grid cells (Figure 4B). 3.3 A HARMONIC TUSSLE PRODUCES MULTIPLE MODULES: Finally, we will study the neural lengthscale \u03c3, and understand how it produces multiple modules. L = \u222b\u222b \u221e \u2212\u221e e\u2212 \u2225g(x)\u2212g(x\u2032)\u22252 2\u03c32 \u03c7(x,x\u2032)p(x)p(x\u2032)dxdx\u2032 (10) As discussed, L prioritises the separation of poorly distinguished points, those whose representations are closer than \u03c3. This causes certain frequencies to be desired in the overall population, in particular those unrelated to existing frequencies by simple harmonic ratios, i.e. not \u03c91 = 32\u03c92 (Figure 4C; see Appendix E for a perturbative derivation of this effect). This is because pairs of harmonically related frequencies represent more positions identically than a non-harmonically related pair, so are worse for separation (similar to arguments made in Wei et al. (2015)). This, however, sets up a \u2018harmonic tussle\u2019 between what the population wants - non-harmonically related frequencies for L - and what single neurons want - harmonically related frequency lattices for non-negativity (Section 3.1). Modules of grid cells resolve this tension: harmonic frequencies exist within modules to give non-negativity, and non-harmonically related modules allow for separation, explaining the earlier simulation results (Figure 2B; further details in Appendix E.3). This concludes our main result. We have shown three constraints on neural populations - actionable, functional, and biological - lead to multiple modules of hexagonal grid cells, and we have understood why. We posit this is the minimal set of requirements for grid cells (see Appendix I for ablation simulations and discussion). 4 PREDICTIONS: Our theory makes testable predictions about the structure of optimal actionable codes for 2D space. We describe three here: tuning curve sharpness scales with the number of neurons in a module; the optimal angle between modules; and the optimal grid alignment to room geometry. 4.1 LATTICE SIZE:FIELD WIDTH RATIO SCALES WITH NUMBER OF NEURONS IN MODULE: In our framework the number of neurons controls the number of frequencies in the representation (equation 6). A neuron within a module only contains frequencies from that module\u2019s frequency lattice, since other modules have non-harmonically related frequencies. More neurons in a module, means more and higher frequencies in the lattice, which sharpen grid peaks (Figure 5A). We formalise this (Appendix J) and predict that the number of neurons within a module scales with the square of the lattice lengthscale, \u03bd, to field width, \u00b5, ratio, N \u221d ( \u03bd \u00b5 )2 . This matches the intuition that the sharper a module\u2019s peak, the more neurons you need to tile the entire space. In a rudimentary analysis, our predictions compare favourably to data from Stensola et al. (2012) assuming uniform sampling of grid cells across modules (Figure 5B). We are eager to test these claims quantitatively. 4.2 MODULES ARE OPTIMALLY ORIENTED AT SMALL OFFSETS (\u223c 4\u25e6): In section 3.3 we saw how frequencies of different modules are maximally non-harmonically related in order to separate the representation of as many points as possible. To maximise non-harmonicity between two modules, the second module\u2019s frequency lattice can be both stretched and rotated relative to the first. 0 or 30\u25e6 relative orientations are particularly bad coding choices as they align the high density axes of the two lattices (Figure 5C). The optimal angular offset of two modules, calculated via a frequency overlap metric (Appendix K), is small (Figure 5D); the value depends on the grid peak and lattice lengthscales, \u00b5 and \u03bd, but varies between 3\u25e6 and 8\u25e6 degrees. Multiple modules should orient at a sequence of small angles (Appendix K). In a rudimentary analysis, our predictions compare favourably to the observations of Stensola et al. (2012) (Figure 5E). 4.3 OPTIMAL GRIDS MORPH TO ROOM GEOMETRY: In Section 3.2 (and Appendix D) we showed that p(x), the animal\u2019s occupancy distribution, introduced a high frequency bias - grid cells must peak often enough to encode visited points. However, changing p(x) changes the shape of this high frequency bias (Appendix L). In particular, we examine an animal\u2019s encoding of square, circular, or rectangular environments, Appendix L, with the assumption that p(x) is uniform over that space. In each case the bias is coarsely towards high frequencies, but has additional intricacies: in square and rectangular rooms optimal frequencies lie on a lattice, with peaks at integer multiples of 2\u03c0L along one of the cardinal axes, for room width/height L (Figure 5F); whereas in circular rooms optima are at the zeros of a Bessel function (Figure 5G). These ideas make several predictions. For example, grid modules in circular rooms should have lengthscales set by the optimal radii in Figure 5G, but they should still remain hexagonal since the Bessel function is circularly symmetric. However, the optimal representation in square rooms should not be perfectly hexagonal since p(x) induces a bias inconsistent with a hexagonal lattice (this effect is negligible for high frequency grids). Intriguingly, shearing towards squarer lattices is observed in square rooms(Stensola et al., 2015), and it would be interesting to its grid-size dependence. Lastly, these effects make predictions about how grid cells morph when the environment geometry changes. A grid cell that is optimal in both geometries can remain the same, however sub-optimal grid cells should change. For example turning a square into a squashed square (i.e. a rectangle), stretches the optimal frequencies along the squashed dimension. Thus, some cells are optimal in both rooms and should stay stable, will others will change, presumably to nearby optimal frequencies (Figure 5H). Indeed Stensola et al. (2012) recorded the same grid cells in a square and rectangular environment (Figure 5I), and observed exactly these phenomena. 5 DISCUSSION & CONCLUSIONS: We have proposed actionability as a fundamental representational principle to afford flexible behaviours. We have shown in simulation and with analytic justification that the optimal actionable representations of 2D space are, when constrained to be both biological and functional, multiple modules of hexagonal grid cells, thus offering a mathematical understanding of grid cells. We then used this theory to make three novel grid cell predictions that match data on early inspection. While this is promising for our theory, there remain some grid cell phenomena that, as it stands, it will never predict. For example, grid cell peaks vary in intensity (Dunn et al., 2017), and grid lattices bend in trapezoidal environments (Krupic et al., 2015). These effects may be due to incorporation of sensory information - something we have not included in our theory - to better infer position. Including such effects may recapitulate these findings, as in Ocko et al. (2018). Our theory is normative and abstracted from implementation. However, both modelling (Burak & Fiete, 2009) and experimental (Gardner et al., 2022; Kim et al., 2017) work suggests that continuous attractor networks (CANs) implement path integrating circuits. Actionability and CANs imply seemingly different representation update equations, which future work should compare or contrast. While we focused on understanding the optimal representations of 2D space and their relationship to grid cells, our theory is more general and can be applied to behaviour in other, non 2D, spaces. In fact any variable whose transformations form a group is potentially actionable. The brain represents many such variables, e.g. heading directions, (Finkelstein et al., 2015), object orientations, (Logothetis et al., 1995), the\u20184-loop task\u2019 of Sun et al. (2020) or 3-dimensional space (Grieves et al., 2021; Ginosar et al., 2021). Interestingly, our theory predicts 3D representations with regular order (Figure 14E in Appendix M), unlike those found in the brain (Grieves et al., 2021; Ginosar et al., 2021) suggesting the animal\u2019s 3D navigation is sub-optimal. But, most expansively, these principles express a view that representations must be more than just passive encodings of the world; they must embed the consequences of predictable actions, allowing planning and inferences in never-before-seen situations. We codified these ideas using Group and Representation theory, and demonstrated their utility in understanding grid cells. However, the underlying principle is broader than the crystalline nature of group structures: the world and your actions within it have endlessly repeating structures whose understanding permits creative analogising and flexible behaviours. A well-designed representation should reflect this. A CONSTRAINING REPRESENTATIONS WITH REPRESENTATION THEORY: Having an actionable code means the representational effect of every transformation of the variable, \u2206x, can be implemented by a matrix: g(x+\u2206x) = T (\u2206x)g(x) (11) In Section 2 we outlined a rough argument for how this equation leads to the following constraint on actionable representations of 2D position, that was vital for our work: g(x) = a0 + D\u2211 d=1 ad sin(kd \u00b7 x) + bd cos(kd \u00b7 x) (12) In this section, we\u2019ll repeat this argument more robustly using Group and Representation Theory. In doing so, it\u2019ll become clear how broadly our version of actionability can be used to derive clean analytic representational constraints; namely, the arguments presented here can be applied to the representation of any variable whose transformations form a group. Sections A.1 and A.2 contain a review of the Group and Representation theory used. Section A.3 applies it to our problem. A.1 GROUP THEORY: A mathematical group is a collection of things (like the set of integers), and a way to combine two members of the group that makes a third (like addition of two integers, that always creates another integer), in a way that satisfies a few rules: 1. There is an identity element, which is a member of the group that when combined with any other element doesn\u2019t change it. For adding integers the identity element is 0, since a+ 0 = a for all a. 2. Every member of the group has an inverse, defined by its property that combining an element with its inverse produces the identity element. In our integer-addition example the inverse of any integer a is \u2212a, since \u2212a+ a = 0, and 0 is the identity element. 3. Associativity applies, which just means the order in which you perform operations doesn\u2019t matter: (a+ b) + c = a+ (b+ c) Groups are ubiquitous. We mention them here because they will be our route to understanding actionable representations - representations in which transformations are also encoded consistently. The set of transformations of many variables of interest are groups. For example, the set of transformations of 2D position, i.e. 2D translations - \u2206x, is a group if you define the combination of two translations via simple vector addition, \u2206x1+2 = \u2206x1 +\u2206x2. We can easily check that they satisfy all three of the group rules: 1. There is an identity translation: simply add 0. 2. Each 2D translation has its inverse: \u2212\u2206x+\u2206x = 0 3. Associativity: (\u2206a+\u2206b) + \u2206c = \u2206a+ (\u2206b+\u2206c) The same applies for the transformations of an angle, \u03b8, or of positions on a torus or sphere, and much else besides. This is a nice observation, but in order to put it to work we need a second area of mathematics, Representation Theory. A.2 REPRESENTATION THEORY: Groups are abstract, in the sense that the behaviour of many different mathematical objects can embody the same group. For example, the integers modulo P with an addition combination rule form a group, called CP . But equally, the P roots of unity ({e 2\u03c0n P }P\u22121n=0 ) with a multiplication combination rule obey all the same rules: you can create a 1-1 correspondence between the integers 0 through P \u2212 1 and the P roots of unity by labelling all the roots with the integer n that appears in the exponent e 2\u03c0n P , and, under their respective combination rules, all additions of integers will exactly match onto multiplication of complex roots (i.e. e 2\u03c0n1 P \u2217 e 2\u03c0n2 P = e 2\u03c0 mod P (n1+n2) P ) In this work we\u2019ll be interested in a particular instantiation of our groups, defined by sets of matrices, T (\u2206x), combined using matrix multiplication. For example, a matrix version of the group CP would be the set of 2-by-2 rotation matrices that rotate by 2\u03c0P increments: T (n) = ( cos( 2\u03c0nP ) \u2212 sin( 2\u03c0n P ) sin( 2\u03c0nP ) cos( 2\u03c0n P ) ) such that T (n1)T (n2) = T (modP (n1 + n2)) (13) Combining these matrices using matrix multiplication follows all the same patterns that adding the integers modulo P, or multiplying the P roots of unity, followed; they all embody the same group. Representation theory is the branch of mathematics that specifies the structure of sets of matrices that, when combined using matrix multiplication, follow the rules of a particular group. Such sets of matrices are called a representation of the group; to avoid confusion arising from this unfortunate though understandable convergent terminology, we distinguish group representations from neural representations by henceforth denoting representations of a group in italics. We will make use of one big result from Representation Theory, hinted at in Section 2: for all the groups we will be considering, any matrix that is a representation of a group can be composed from the direct product of a set of blocks, called irreducible representations, or irreps for short, up to a linear transformation. i.e. if T (\u2206a) is a representation of the group of transformations of some variable a: T (\u2206a) = S  I1(\u2206a) 0 0 . . . 0 0 I2(\u2206a) 0 . . . 0 0 0 I3(\u2206a) . . . 0 ... ... ... . . . ... 0 0 0 . . . ID(\u2206a) S\u22121 (14) where Id(\u2206a) are the irreps of the group in question, which are square matrices not necessarily of the same size as d varies, and S is some invertible square matrix. To motivate our current rabbit hole further, this is exactly the result we hinted towards in Section 2. We discussed T (\u2206\u03b8), and, in 2-dimensions, argued it was, up to a linear transform, the 2-dimensional rotation matrix. Further, we discussed how every extra two neurons allowed you to add another frequency, i.e. another rotation matrix. These rotation matrices are the irreps of the rotation group, and adding two neurons allows you to create a larger T (\u2206\u03b8) by stacking rotation matrices on top of one another. Now including the invertible linear transform, S, we can state the 4-dimensional version of equation 4: T (\u2206\u03b8) = S cos(n1\u2206\u03b8) \u2212 sin(n1\u2206\u03b8) 0 0sin(n1\u2206\u03b8) cos(n1\u2206\u03b8) 0 00 0 cos(n2\u2206\u03b8) \u2212 sin(n2\u2206\u03b8) 0 0 sin(n2\u2206\u03b8) cos(n2\u2206\u03b8) S\u22121 (15) In performing this decomposition the role of the rotation matrices as irreps is clear. There are infinitely many types, each specified by an integer frequency, nd, and their direct product produces any representation of the rotation group. Returning to actionable neural representations of 2D space, this result is a very useful specification. We sought codes, g(x), that could be manipulated via matrices: g(x+\u2206x) = T (\u2206x)g(x) (16) Now we recognise that we\u2019re asking T (\u2206x) to be a representation of the transformation group of x, and we know the shape T (\u2206x) must take to fit this criteria. To specify T (\u2206x) from this constrained class we must simply choose a set of irreps that fill up the dimensionality of the neural space, stack them one on top of each other, and rotate and scale them using the matrix S. A final detail is that there is a trivial way to make a representation of any group: choose T (\u03b8) = IN , the N-by-N identity matrix. This also fits our previous discussion, but corresponds to a representation made from the direct product of N copies of something called the trivial irrep, a 1-dimensional irrep, Itrivial(\u2206a) = 1. Armed with this knowledge, the following subsection will show how this theory can be used to constrain the neural code, g(x). To finish this review subsection, we list the non-trivial irreps of the groups used in this work. To be specific, in all our work we use only the real-valued irreps, i.e all elements of T are real numbers, since firing rates are real numbers. These are less common than complex irreps, which is often what is implied by the simple name irreps. Transformation group of which variable Non-trivial Real Irreps An angle on a unit circle, \u03b8 ( cos(n\u2206\u03b8) \u2212 sin(n\u2206\u03b8) sin(n\u2206\u03b8) cos(n\u2206\u03b8) ) for n \u2208 Z Position on a line, x ( cos(\u03c9\u2206\u03b8) \u2212 sin(\u03c9\u2206\u03b8) sin(\u03c9\u2206\u03b8) cos(\u03c9\u2206\u03b8) ) for \u03c9 \u2208 R Angles on a unit torus, \u03b8 ( cos(a \u00b7\u2206\u03b8) \u2212 sin(a \u00b7\u2206\u03b8) sin(a \u00b7\u2206\u03b8) cos(a \u00b7\u2206\u03b8) ) for a \u2208 Z2 Position on a plane, x ( cos(k \u00b7\u2206x) \u2212 sin(k \u00b7\u2206x) sin(k \u00b7\u2206x) cos(k \u00b7\u2206x) ) for k \u2208 R2 Position on a unit sphere, \u03d5 Real Wigner-D Matrices A.3 REPRESENTATIONAL CONSTRAINTS: Finally, we will translate these constraints on transformation matrices into constraints on the neural code. This is, thankfully, not too difficult. Consider the representation of an angle for simplicity, and take some arbitrary origin in the input space, \u03b80 = 0. The representation of all other angles can be derived via: g(\u03b8) = T (\u03b8)g(0) = S  I1(\u03b8) 0 0 . . . 0 0 I2(\u03b8) 0 . . . 0 0 0 I3(\u03b8) . . . 0 ... ... ... . . . ... 0 0 0 . . . ID(\u03b8) S\u22121g(0) (17) In the case of an angle, each irrep, Id(\u03b8), is just a 2-by-2 rotation matrix at frequency d, table A.2, and for an N -dimensional T we can fit maximally N2 (for N even) different frequencies, where N is the number of neurons. Hence the representation, g(\u03b8), is just a linear combination of the sine and cosine of these different frequencies, exactly as quoted previously: g(\u03b8) = a0 + D\u2211 d=1 ad sin(nd\u03b8) + bd cos(nd\u03b8) for integer nd, (18) a0 corresponds to the trivial irrep, and we include it since we know it must be in the code to make the firing rates non-negative for all \u03b8. It also cleans up the constraint on the number of allowable frequencies, for even or odd N we require D < N2 . This is because if N is odd one dimension is used for the trivial irrep, the rest can be shared up amongst N\u221212 frequencies, so D must be an integer smaller than N2 . If N is even, one dimension must still be used by the trivial irrep, so D can still maximally be only the largest integer smaller than N2 . Extending this to other groups is relatively simple, the representation is just a linear combination of the functions that appear in the irrep of the group in question, see table A.2. For position on a line, circle, torus, or plane, they all take the relatively simple form as in equation 18, but requiring appropriate changes from 1 to 2 dimensions, or integer to real frequencies. Representations of position on a sphere are slightly more complex, instead being constructed from linear combinations of groups of spherical harmonics. B NUMERICAL OPTIMISATION DETAILS: In order to test our claims, we numerically optimise the parameters that define the representation to minimise the loss, subject to constraints. Despite sharing many commonalities, our optimisation procedures are different depending on whether the variable being represented lives in an infinite space (line, plane, volume) or finite (circle, torus, sphere). We describe each of these schemes in turn, beginning with infinite spaces. All code will be made available on github. B.1 NUMERICAL OPTIMISATION FOR INFINITE SPACES: We will use the representation of a point on a line for explanation, planes or volumes are a simple extension. g(x) is parameterised as follows: g(x) = a0 + D\u2211 d=1 ad cos(\u03c9dx) + bd sin(\u03c9dx) a0, {ad, bd}Dd=1 \u2208 RN , \u03c9d \u2208 R (19) Our loss is made from three terms: the first is the functional objective, the last two enforce the non-negativity and boundedness constraint respectively: L = Lfunctional + \u03bbpLnon-negativity + \u03bbbLbounded (20) To evaluate the functional component of our loss we sample a set of points {xi}Mi=1 from p(x) and calculate their representations {g(xi)}Mi=1. To make the bounded constraint approximately satisfied (there\u2019s still some work to be done, taken care of by Lbounded) we calculate the following neuron-wise norm and use it to normalise each neuron\u2019s firing: \u2225gn\u22252 = M\u2211 m=1 gn(xm) 2 g\u0303n(\u03b8) = gn(\u03b8) \u2225gn\u2225 (21) The functional form of Lfunctional varies, as discussed in the main paper. For example, for the full loss we compute the following using the normalised firing, a discrete approximation to equation 1: Lfunctional = 1 M2 M\u2211 m=1 M\u2211 m\u2032=1 e\u2212 \u2225g\u0303(xm)\u2212g\u0303(xm\u2032 )\u2225 2 2\u03c32 \u03c7(xm, xm\u2032) (22) Now we come to our two constraints, which enforce that the representation is non-negative and bounded. We would like our representation to be reasonable (i.e. non-negative and bounded) for all values of x. If we do not enforce this then the optimiser finds various trick solutions in which the representation is non-negative and bounded only in small region, but negative and growing explosively outside of this, which is completely unbiological, and uninteresting. Of course, x is infinite, so we cannot numerically ensure these constraints are satisfied for all x. However, ensuring they are true in a region local to the animal\u2019s explorations (which are represented by p(x)) suffices to remove most of these trick solutions. As such, we sample a second set of MS \u2018shift positions\u2019, {xm}MSm=1, from a scaled up version of p(x), using a scale factor S. We then create a much larger set of positions, by shifting the original set by each of the \u2018shift positions\u2019, creating a dataset of size M \u2217MS , and use these to calculate our two constraint losses. Our non-negativity loss penalises negative firing rates: Lnon-negativity = 1 MMSN MS\u2211 ms=1 M\u2211 m=1 N\u2211 n=1 |g\u0303n(xms + xm)|I(g\u0303n(xms + xm) < 0) (23) Where I is the indicator function, 1 if the firing rate is negative, 0 else, i.e. we just average the magnitude of the negative portion of the firing rates. The bounded loss penalises the deviation of each neuron\u2019s norm from 1, in each of the shifted rooms: Lbounded = 1 NMS N\u2211 n=1 MS\u2211 ms=1 ( M\u2211 m=1 g\u03032n(xms + xm)\u2212 1)2 (24) That completes our specification of the losses. We minimise the full loss over the parameters (a0, {ad, bd, \u03c9d}Dd=1) using a gradient-based algorithm, ADAM (Kingma & Ba, 2014). We initialise these parameters by sampling from independent zero-mean gaussians, with variances as in table B.3. The final detail of our approach is the setting of \u03bbp and \u03bbb, which control the relative weight of the constraints vs. the functional objective. We don\u2019t want the optimiser to strike a tradeoff between the objective and constraints, since the constraints must actually be satisfied. But we also don\u2019t want to make \u03bbp so large that the objective is badly minimised in the pursuit of only constraint satisfaction. To balance these demands we use GECO (Rezende & Viola, 2018), which specifies a set of stepwise updates for \u03bbp and \u03bbb. These ensure that if the constraint is not satisfied their coefficients increase, else they decrease allowing the optimiser to focus on the loss. The dynamics that implement this are as follows, for a timestep, t. GECO defines a log-space measure of how well the constraint is being satisfied: Lt = log(Lt)\u2212 k (25) (or small variations on this), where k is a log-threshold that sets the target log-size of the constraint loss in question, and Lt is the value of the loss at timestep t. Lt is then smoothed: L\u0302t = \u03b1L\u0302t + (1\u2212 \u03b1)Lt (26) And this smoothed measure of how well the constraint is being satisfied controls the behaviour of the coefficient: \u03bbt+1 = \u03bbte \u03b3L\u0302t (27) This specifies the full optimisation, parameters are listed in table B.3. B.2 NUMERICAL OPTIMISATION FOR FINITE SPACES: Optimising the representation of finite variables has one added difficulty, and one simplification. Again, we have a parameterised form, e.g. for an angle \u03b8: g(\u03b8) = a0 + D\u2211 d=1 ad cos(nd\u03b8) + bd sin(nd\u03b8) a0, {ad, bd}Dd=1 \u2208 RN , nd \u2208 Z (28) The key difficulty is that each frequency, nd, has to be an integer, so we cannot optimise it by gradient descent, and similar problems arise for positions on a torus, or on a sphere. We shall spend the rest of this section outlining how we circumvent this problem. However, we do also have one major simplification. Because the variable is finite, we can easily ensure the representation is nonnegative and bounded across the whole space, avoiding the need for an additional normalisation constraint. Further, for the uniform occupancy distributions we consider in finite spaces, we can analytically calculate the neuron norm, and scale the parameters appropriately to ensure it is always 1: \u2225gn(\u03b8)\u22252 = 1 2\u03c0 \u222b \u03c0 \u2212\u03c0 g2n(\u03b8)d\u03b8 = \u2225a0\u22252 + 1 2 D\u2211 d=1 \u2225ad\u22252 + \u2225bd\u22252 (29) g\u0303n(\u03b8) = gn(\u03b8) \u2225gn(\u03b8)\u2225 (30) As such, we simply sample a set of angles {\u03b8m}Mm=1, and their normalised representations {g\u0303(\u03b8m)}Mm=1, and compute the appropriate functional and non-negativity loss, as in equations 22 & 23. The final thing we must clear up is how to learn which frequencies, nd, to include in the code. To do this, we make a code that contains many many frequencies, up to some cutoff Dmax, where Dmax is bigger than N : g(\u03b8) = a0 + Dmax\u2211 d=1 ad cos(d\u03b8) + bd sin(d\u03b8) a0, {ad, bd}Dmaxd=1 \u2208 R N (31) We then add a term to the loss that forces the code to choose only D of these Dmax frequencies, setting all other coefficient vectors to zero, and hence making the code actionable again but allowing the optimiser to do so in a way that minimises the functional loss. The term that we add to achieve this is inspired by the representation theory we used to write these constraints, Section A.2. We create first a 2Dmax + 1-dimesional vector, I(\u03b8), that contains all the irreps i.e. each of the Dmax sines and cosines and a constant. We then create a (2Dmax + 1) \u00d7 (2Dmax + 1)-dimensional transition matrix, GI(\u2206\u03b8) that is a representation of the rotation group in this space: GI(\u2206\u03b8)I(\u03b8) = I(\u03b8 + \u2206\u03b8). GI can be simply made by stacking 2-by-2 rotation matrices. Then we create the neural code by projecting the frequency basis through a rectangular weight matrix, W : g(\u03b8) =W I(\u03b8). Finally, we create the best possible representation of the group in the neural space: G(\u2206\u03b8) =WGI(\u2206\u03b8)W \u2217 (32) Where W \u2217 denotes the Moore-Penrose pseudoinverse. We then learn W , which is equivalent to learning a0 and {ad, bd}Dmaxd=1 . Representation theory tells us this will not do a perfect job at rotating the neural representation, unless the optimiser choosesW to cut out all but D of the frequencies. As such, we sample a set of shifts {\u03b8ms}Mms=1, and measure the following discrepancy: LTransformation = 1 MSM MS\u2211 mS=1 M\u2211 m=1 \u2225G(\u03b8ms)g(\u03b8m)\u2212 g(\u03b8m + \u03b8ms)\u22252 (33) Minimising it as we minimised the other constraint terms will force the code to chooseD frequencies and hence make the code actionable. Since calculating the pseudoinverse is expensive, we replaceW \u2217 with a matrixB that we also learn by gradient descent. B quickly learns to approximate the pseudoinverse, speeding our computations. That wraps up our numerical description. We are again left with three loss terms, two of which enforce constraints. This can be applied to any group, even if you can\u2019t do gradient descent through their irreps, at the cost of limiting the optimisation to a subset of irreps below a certain frequency, Dmax. L = Lfunctional + \u03bbpLnon-negativity + \u03bbTLTransformation (34) B.3 PARAMETER VALUES: We list the parameter values used to generate the grid cells in Figure 2B. B.4 FULL OPTIMAL REPRESENTATION:  C ANALYSIS OF OUR SIMPLEST LOSS THAT LEADS TO ONE LATTICE MODULE: In this section we analytically study the simplified loss suggested in section 3.1, and we show how to derive the equations 8. The actionability constraint tells us that: g(\u03b8) = a0 + D\u2211 d=1 ad sin(nd\u03b8) + bd cos(nd\u03b8) nd \u2208 Z (35) Where D is smaller than half the number of neurons (Appendix A). We ensure nd \u0338= nd\u2032 if d \u0338= d\u2032 by combining like terms. Our loss is: L0 = \u2212 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252d\u03b8d\u03b8\u2032 (36) Developing this and using trig identities for the sum of sines and cosines: L0 = \u2212 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252d\u03b8d\u03b8\u2032 = \u2212 1 4\u03c02 \u2211 n \u222b\u222b \u03c0 \u2212\u03c0 (gn(\u03b8)\u2212 gn(\u03b8\u2032))2d\u03b8d\u03b8\u2032 (37) = \u2212 \u2211 n \u222b\u222b \u03c0 \u2212\u03c0 ( \u2211 d an,d cos [nd(\u03b8 + \u03b8\u2032) 2 ] sin [nd(\u03b8 \u2212 \u03b8\u2032) 2 ] \u2212bn,d sin [nd(\u03b8 \u2212 \u03b8\u2032) 2 ] sin [nd(\u03b8 \u2212 \u03b8\u2032) 2 ] )2 d\u03b8d\u03b8\u2032 \u03c02 (38) We now change variables to \u03b1 = \u03b8\u2212\u03b8 \u2032 2 and \u03b2 = \u03b8+\u03b8\u2032 2 . This introduces a factor of two from the Jacobian (12 ), which we cancel by doubling the range of the integral while keeping the same limits, despite the change of variable, Figure 7. This gives us: = \u2212 1 \u03c02 \u2211 n \u222b\u222b \u03c0 \u2212\u03c0 ( \u2211 d an,d cosnd\u03b2 sinnd\u03b1\u2212 bn,d sinnd\u03b2 sinnd\u03b1)2d\u03b1d\u03b2 (39) = \u2212 1 \u03c02 \u2211 n,d,d\u2032 \u222b\u222b \u03c0 \u2212\u03c0 [ an,dan,d\u2032 sin(nd\u03b1) sin(nd\u2032\u03b1) cos(nd\u03b2) cos(nd\u2032\u03b2) \u2212 an,dbn,d\u2032 sin(nd\u03b1) sin(nd\u2032\u03b1) cos(nd\u03b2) sin(nd\u2032\u03b2) \u2212 bn,dan,d\u2032 sin(nd\u03b1) sin(nd\u2032\u03b1) sin(nd\u03b2) cos(nd\u2032\u03b2) + bn,dbn,d\u2032 sin(nd\u03b1) sin(nd\u2032\u03b1) sin(nd\u03b2) sin(nd\u2032\u03b2) ] d\u03b1d\u03b2 (40) Performing these integrals is easy using the fourier orthogonality relations; the two cross terms with mixed sin and cos of \u03b2 are instantly zero, the other two terms are non-zero only if nd = nd\u2032 , i.e. d = d\u2032. When d = d\u2032 these integrals evaluate to \u03c02, giving: L0 = \u2212 \u2211 n,d (a2n,d + b 2 n,d\u2032) (41) Exactly as quoted in equation 7. We can do the same development for the firing rate constraint: 1 2\u03c0 \u222b \u03c0 \u2212\u03c0 g2i (\u03b8)d\u03b8 = a 2 0,i + 1 2 D\u2211 d=1 a2d,i + b 2 d,i = 1, \u2200i = 1, ..., N (42) Where we have again simply used the orthogonality relation of sines and cosines. These are the constraints that must be enforced. For illustration it is useful to see one slice of the constraints made by summing over the neuron index: N = \u2225a0\u22252 + 1 2 D\u2211 d=1 \u2225ad\u22252 + \u2225bd\u22252 (43) This is the constraint shown in equation 8, and is strongly intuition guiding. But remember it is really a stand-in for the N constraints, one per neuron, that must each be satisfied! This analysis can be generalised to derive exactly the same result in 2D by integrating over pairs of points on the torus. It yields no extra insight, and is no different, so we just quote the result: L0 = \u2212 1 16\u03c04 \u222b \u222b\u222b \u222b \u03c0 \u2212\u03c0 \u2225g(x)\u2212 g(x\u2032)\u22252dxdx\u2032 = \u2212 \u2211 d \u2225ad\u22252 + \u2225bd\u22252 (44) D ANALYSIS OF PARTIALLY SIMPLIFIED LOSS THAT LEADS TO A MODULE: OF HEXAGONAL GRIDS In this section we will study an intermediate loss: it will compute the euclidean distance, like in Appendix C, but we\u2019ll include \u03c7 and p. We\u2019ll show that they induce a frequency bias, \u03c7 for low frequencies, and p for high, which together lead to hexagonal lattices. We\u2019ll study \u03c7 first for representations of a circular variable, then we\u2019ll study p for representations of a line, then we\u2019ll combine them. At each stage generalising to 2-dimensions is a conceptually trivial but algebraically nasty operation with no utility, so we do not detail here. In Appendix L we will return to the differences between 1D and 2D, and show instances where they become important. D.1 LOW FREQUENCY BIAS: PLACE CELLS ON THE CIRCLE: We begin with the representation of an angle on a circle, introduce \u03c7, and show the analytic form of the low frequency bias it produces. Since we\u2019re on a circle \u03c7 must depend on distance on the circle, rather than on a line (i.e. not \u03c7 = 1 \u2212 e\u2212 \u2225x\u2212x\u2032\u22252 2l2 ). We define the obvious generalisation of this to periodic spaces, Figure 8A, which is normalised to integrate to one under a uniform p(\u03b8): \u03c7(\u03b8, \u03b8\u2032) = ek \u2212 ek cos(\u03b8\u2212\u03b8\u2032) ek \u2212 I0(k) (45) Where I0 is the zeroth-order modified Bessel function. Hence the loss is: L1 = \u2212 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252 e k \u2212 ek cos(\u03b8\u2212\u03b8\u2032) ek \u2212 I0(k) d\u03b8d\u03b8\u2032 (46) In this expression k is a parameter that controls the generalisation width, playing the inverse role of l in the main paper. Taking the same steps as in Appendix C we arrive at: = \u2212 1 \u03c02 \u2211 n \u222b\u222b \u03c0 \u2212\u03c0 ( \u2211 d an,d cosnd\u03b2 sinnd\u03b1\u2212 bn,d sinnd\u03b2 sinnd\u03b1)2 ek \u2212 ek cos(2\u03b1) ek \u2212 I0(k) d\u03b1d\u03b2 (47) The \u03b2 integral again kills the cross terms: = \u2212 1 \u03c0 \u2211 n,d (a2n,d + b 2 n,d) \u222b \u03c0 \u2212\u03c0 sin2(nd\u03b1) ek \u2212 ek cos(2\u03b1) ek \u2212 I0(k) d\u03b1 (48) These integrals can be computed by relating them to modified Bessel functions of the first kind, which can be defined for integer n as: In(k) = 1 2\u03c0 \u222b \u03c0 \u2212\u03c0 ek cos(\u03b8) cos(n\u03b8)d\u03b8 (49) Hence the loss is: L1 = \u2212 \u2211 n,d (a2n,d + b 2 n,d) ek + Ind(k) ek \u2212 I0(k) (50) This is basically the same result as before, equation 41, all the frequencies decouple and decrease the loss in proportion to their magnitude. However, we have added a weighting factor, that decreases with frequency, Figure 8B, i.e. a simple low frequency bias! This analysis agrees with that of Sengupta et al. (2018), who argued that the best representation of angles on a ring for a loss related to the one studied here should be place cells. We disentangle the relationship between our two works in Appendix F. Regardless, our optimisation problem now asks us to build a module of lattice neurons with as many low frequencies as possible. The optimal solution to this is the lowest frequency lattice, i.e. place cells! (we validate this numerically in Appendix M). Therefore, it is only additional developments of the loss, namely the introduction of an infinite space for which you prioritise the coding of visited regions, and the introduction of the neural lengthscale, \u03c3, that lead us to conclude that modules of grid cells are better than place cells. D.2 HIGH FREQUENCY BIAS: OCCUPANCY DISTRIBUTION: Now we\u2019ll examine how a Gaussian p(x) affects the result, using a representation of x, a 1- dimensional position: L1 = \u2212 \u222b\u222b \u221e \u2212\u221e \u2225g(x)\u2212g(x\u2032)\u22252p(x)p(x\u2032)dxdx\u2032 = \u2212 1 2\u03c0L2 \u222b\u222b \u221e \u2212\u221e \u2225g(x)\u2212g(x\u2032)\u22252e\u2212 x2+x\u20322 2L2 dxdx\u2032 (51) Developing the argument as before: = \u2212 4 \u03c0L2 \u2211 n \u222b\u222b \u221e \u2212\u221e ( \u2211 d an,d cos\u03c9d\u03b2 sin\u03c9d\u03b1\u2212 bn,d sin\u03c9d\u03b2 sin\u03c9d\u03b1)2e\u2212 \u03b12+\u03b22 L2 d\u03b1d\u03b2 (52) The cross terms again are zero, due to the symmetry of the \u03b2 integral around 0. = \u2212 4 \u03c0L2 \u2211 n,d,d\u2032 an,dan,d\u2032C(\u03c9d, \u03c9d\u2032\u2225L)S(\u03c9d, \u03c9d\u2032\u2225L) + bn,dbn,d\u2032S2(\u03c9d, \u03c9d\u2032\u2225L) (53) Where we\u2019ve wrapped up the details into the following integrals of trigonometric functions over a gaussian measure: C(a, b\u2225L) = \u222b \u221e \u2212\u221e cos(ax) cos(bx)e\u2212 x2 L2 dx = L \u221a \u03c0 2 (e\u2212 L2(a+b)2 4 + e\u2212 L2(a\u2212b)2 4 ) (54) S(a, b\u2225L) = \u222b \u221e \u2212\u221e sin(ax) sin(bx)e\u2212 x2 L2 dx = L \u221a \u03c0 2 (e\u2212 L2(a\u2212b)2 4 \u2212 e\u2212 L2(a+b)2 4 ) (55) So we get our final expression for the loss: L1 = \u2212 \u2211 n,d,d\u2032 an,dan,d\u2032 (e \u2212 L2(\u03c9d\u2212\u03c9d\u2032 ) 2 2 \u2212 e\u2212 L2(\u03c9d+\u03c9d\u2032 ) 2 2 )\ufe38 \ufe37\ufe37 \ufe38 Sine Weighting +bn,dbn,d\u2032 (e \u2212 L2(\u03c9d\u2212\u03c9d\u2032 ) 2 4 \u2212 e\u2212 L2(\u03c9d+\u03c9d\u2032 ) 2 4 )2\ufe38 \ufe37\ufe37 \ufe38 Cosine Weighting (56) = \u2212 \u2211 d,d\u2032 [ (e\u2212 L2(\u03c9d\u2212\u03c9d\u2032 ) 2 2 \u2212 e\u2212 L2(\u03c9d+\u03c9d\u2032 ) 2 2 ) \u2211 n an,dan,d\u2032 + (e \u2212 L2(\u03c9d\u2212\u03c9d\u2032 ) 2 4 \u2212 e\u2212 L2(\u03c9d+\u03c9d\u2032 ) 2 4 )2 \u2211 n bn,dbn,d\u2032 ] (57) The downstream consequence of this loss is simple if a module encodes all areas of space evenly. Encoding evenly implies the phases of the neurons in the module are evenly distributed. The phase of a particular neuron in encoded in the coefficients of the sine and cosine of the base frequency, an1 and bn1. Even distribution implies that an1 oscillates from Ad, where Ad = maxn an1, to \u2212Ad and back again as you sweep the neuron index. As you perform this sweep the subsidiary coefficients, such as an2, perform the same oscillation, but multiple times. an2 does it twice, since, by the arguments that led us to grids, the peaks of the two waves must align (Figure 3B), and that implies the peak of the wave at twice the frequency moves at the same speed as you sweep the neuron number. This implies its phase oscillates twice as fast, and hence that \u2211 n an1an2 = 0. Arguments like this imply the frequencies actually decouple for a uniform encoding of space, making the loss, L1 = \u2212 \u2211 d,n [ (1\u2212 e\u22122L 2\u03c92d)a2n,d + (1\u2212 e\u2212L 2\u03c92d)2b2n,d ] (58) This is exactly a high frequency bias! (Fig 8C Left) However, uniform encoding of the space is actually not optimal for this loss, as can be seen either from the form of equation 56, and verified numerically using the loss in the next section, Figure 8EF. The weighting of the sine factor and the cosine factor differ, and in fact push toward sine, since it plateaus at lower frequencies. Combine that with a low frequency push, and you get an incentive to use only sines, which is allowed if you can encode only some areas of space, as the euclidean loss permits. A final interesting feature of this loss is the way the coupling between frequencies gracefully extends the decoupling between frequencies to the space of continuous frequencies. In our most simplified loss, equation 41, each frequency contributed according to the square of its amplitude, \u2225ad\u22252 + \u2225bd\u22252. How should this be extended to continuous frequencies? Certainly, if two frequencies were very far from one another we should retrieve the previous result, and their contributions would be expected to decouple, \u2225a1\u22252 + \u2225b1\u22252 + \u2225a2\u22252 + \u2225b2\u22252. But if k2 = k1 + \u03b4k, for a small \u03b4k then the two frequencies are, for all intents and purposes, the same, so they should contribute an amplitude \u2225a1 + a2\u22252 + \u2225b1 + b2\u22252 \u0338= \u2225a1\u22252 + \u2225b1\u22252 + \u2225a2\u22252 + \u2225b2\u22252. Equation 56, Figure 8C right, performs exactly this bit of intuition. It tells us that the key lengthscale in frequency space is 1L . Two frequencies separated by more than this distance contribute in a decoupled way, as in equation 36. Conversely, at very small distances, much smaller than 1L the contribution is exactly \u2225a1 + a2\u22252 + \u2225b1 + b2\u22252, as it should be. But additionally, it tells us the functional form of the behaviour between these two limits: a Gaussian decay. D.3 THE COMBINED EFFECT = HIGH AND LOW FREQUENCY BIAS: In this section we show that the simple euclidean loss with both \u03c7 and p can be analysed, and it contains both the high and low frequency biases discussed previously in isolation. This is not very surprising, but we include it for completeness. L1 = \u2212 \u222b\u222b \u221e \u2212\u221e \u2225g(x)\u2212 g(x\u2032)\u22252 (1\u2212 e\u2212 (x\u2212x\u2032)2 2l2 )\ufe38 \ufe37\ufe37 \ufe38 \u03c7 p(x)p(x\u2032)\ufe37 \ufe38\ufe38 \ufe37 1 2\u03c0L2 e\u2212 x2+x\u20322 2L2 dxdx\u2032 (59) Following a very similar path to before we reach: = \u2212 4 \u03c0L2 \u2211 n,d,d\u2032 andand\u2032C(\u03c9d, \u03c9d\u2032\u2225L)\u2206S(\u03c9d, \u03c9d\u2032\u2225L,\u039b)+bndbnd\u2032\u2206S(\u03c9d, \u03c9d\u2032\u2225L,\u039b)S(\u03c9d, \u03c9d\u2032\u2225L) (60) \u2206S(\u03c9d, \u03c9d\u2032\u2225L,\u039b) = S(\u03c9d, \u03c9d\u2032\u2225L)\u2212 S(\u03c9d, \u03c9d\u2032\u2225\u039b) (61) \u039b = lL\u221a l2 + 2L2 \u2248 l\u221a 2 since L > l (62) This contains the low and high frequency penalisation, at lengthscales 1L and 1 l , respectively. There are 4 terms, 2 are identical to equation 56, and hence perform the same role. The additional two terms are positive, so should be minimised, and they can be minimised by making the frequencies smaller than \u223c 1l , Figure 8D. We verify this claim numerically by optimising a neural representation of 2D position, g(x), to minimise L1 subject to actionable and biological constraints. It forms a module of hexagonal cells, Figure 8E, but the representation is shoddy, because the cells only encode a small area of space well, as can be seen by zooming on the response, Figure 8. E ANALYSIS OF FULL LOSS: MULTIPLE MODULES OF GRIDS: In this Section we will perturbatively analyse the full loss function, which includes the neural lengthscale, \u03c3. The consequences of including this lengthscale will become most obvious for the simplest example, the representation of an angle on a ring, g(\u03b8), with a uniform weighting of the importance of separating different points. We\u2019ll use it to derive the push towards non-harmonically related modules. Hence, we study the following loss: L = 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 e\u2212 \u2225g(\u03b8)\u2212g(\u03b8\u2032)\u22252 2\u03c32 d\u03b8d\u03b8\u2032 (63) And examine how the effects of this loss differ from the euclidean verison studied in Appendix C. This loss is difficult to analyse, but insight can be gained by making an approximation. We assume that the distance in neural space between the representations of two inputs depends only on the difference between those two inputs, i.e.: \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252 = d(\u03b8 \u2212 \u03b8\u2032) (64) Looking at the constrained form of the representation, we can see this is satisfied if frequencies are orthogonal: g(\u03b8) = a0 + D\u2211 d=1 ad sin(nd\u03b8) + bd cos(nd\u03b8) a0, {ad, bd}Dd=1 \u2208 RN , nd \u2208 Z (65) ad \u00b7 ad\u2032 = \u03b4d,d\u2032A2d bd \u00b7 bd\u2032 = \u03b4d,d\u2032A2d ad \u00b7 bd\u2032 = 0 (66) Then: \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252 = D\u2211 d=1 A2d sin 2 ( nd 2 (\u03b8 \u2212 \u03b8\u2032) ) (67) Now we will combine this convenient form with the following expansion: ex cos(y) = I0(x) [ 1 + 2 \u221e\u2211 m=1 Im(x) I0(x) cos(my) ] (68) Developing equation 63 using eqns. 68 and 67: L = 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 e\u2212 \u2211 d A 2 d sin 2(nd \u03b8\u2212\u03b8\u2032 2 ) 2\u03c32 d\u03b8d\u03b8\u2032 (69) = 1 2\u03c0 \u222b \u03c0 \u2212\u03c0 e\u2212 \u2211 d A 2 d sin 2(nd\u03b1) 2\u03c32 d\u03b1 (70) = 1 2\u03c0 \u220f d \u222b \u03c0 \u2212\u03c0 e\u2212 A2d 4\u03c32 (1\u2212cos(2nd\u03b1))d\u03b1 (71) = 1 2\u03c0 \u220f d [ e\u2212 A2d 4\u03c32 I0( A2d 4\u03c32 ) ] \ufe38 \ufe37\ufe37 \ufe38 Term A \u222b \u03c0 \u2212\u03c0 \u220f d [ 1 + 2 \u221e\u2211 m=1 Im( A2d 4\u03c32 ) I0( A2d 4\u03c32 ) cos(2ndm\u03b1) ] d\u03b1\ufe38 \ufe37\ufe37 \ufe38 Term B (72) We will now look at each of these two terms separately and derive from each an intuitive conclusion. Term A will tell us that frequency amplitudes will tend to be capped at around the legnthscale \u03c3. Term B will tell us how well different frequencies combine, and as such, will exert the push towards non-harmonically related frequency combinations. E.1 TERM A ENCOURAGES A DIVERSITY OF HIGH AMPLITUDE FREQUENCIES IN THE CODE: Fig 9A shows a plot of I0( A2d 4\u03c32 ). As can be seen there are a couple of regimes, if Ad, the amplitude of frequency nd, is smaller than \u03c3 then I0( A2d 4\u03c32 ) \u2248 1. On the other hand, asAd grows, I0 also grows. Asymptotically: I0( A2d 4\u03c32 ) \u223c e A2d 4\u03c32\u221a A2d 4\u03c32 (73) Correspondingly Term A also behaves differently in the two regimes, when Ad is smaller than \u03c3 the loss decreases exponentially with the amplitude of frequency d: Ad < \u03c3 Term A = e \u2212 A 2 d 4\u03c32 I0( A2d 4\u03c32 ) \u223c e\u2212 A2d 4\u03c32 (74) Alternatively: Ad > \u03c3 Term A = e \u2212 A 2 d 4\u03c32 I0( A2d 4\u03c32 ) \u223c 2\u03c3 Ad (75) So, up to a threshold region defined by the lengthscale \u03c3, there is an exponential improvement in the loss as you increase the amplitude. Beyond that the improvements drop to being polynomial. This makes a lot of sense, once you have used a frequency to separate a subset of points sufficiently well from one another you only get minimal gains for more increase in that frequency\u2019s amplitude, Figure 9B. Since there are many frequencies, and the norm constraint ensures that few can have an amplitude of order \u03c3, it encourages the optimiser to put more power into the other frequencies. In other words, amplitudes are only usefully increased to a lengthscale \u03c3, encouraging a diversity in high amplitude frequencies. This is moderately interesting, but will be a useful piece of information for the next derivation. E.2 TERM B ENCOURAGES THE CODE\u2019S FREQUENCIES TO BE NON-HARMONICALLY RELATED: Term B is an integral of the product of D infinite sums, which seems very counter-productive to examine. We are fortunate, however. First, the coefficients in the infinite sum decay very rapidly with m when the ratio of Ad\u03c3 is O(1), Figure 9C. The previous section\u2019s arguments make it clear that, in any optimal code, Ad will be smaller or of the same order as \u03c3. As such, the inifite sum is rapidly truncated. Additionally, all of the coefficient ratios, ( Im( A2d 4\u03c32 ) I0( A2 d 4\u03c32 ) ) , are smaller than 1, as such we can understand the role of Term B by analysing just the first few terms in the following series expansion, Term B = \u220f d [ 1 + 2 \u221e\u2211 m=1 Im( A2d 4\u03c32 ) I0( A2d 4\u03c32 ) cos(2mnd\u03b1) ] = 1\ufe38\ufe37\ufe37\ufe38 0th Order +2 \u2211 d,m Im( A2d 4\u03c32 ) I0( A2d 4\u03c32 ) cos(2mnd\u03b1)\ufe38 \ufe37\ufe37 \ufe38 1st Order +4 \u2211 d,d\u2032,m,m\u2032 Im( A2d 4\u03c32 )Im\u2032( A2 d\u2032 4\u03c32 ) I0( A2d 4\u03c32 )I0( A2 d\u2032 4\u03c32 ) cos(2mnd\u03b1) cos(2m \u2032nd\u2032\u03b1)\ufe38 \ufe37\ufe37 \ufe38 2ndOrder + . . . (76) where, due to the rapid decay of ( Im( A2d 4\u03c32 ) I0( A2 d 4\u03c32 ) ) with m, each term is only non-zero for small m. The 0th order term is a constant, and the 1st order term is zero when you integrate over \u03b1. As such, the leading order effect of this loss on the optimal choice of frequencies arrives via the 2nd order term. The integral is easy, \u222b \u03c0 \u2212\u03c0 cos(2mnd\u03b1) cos(2mnd\u2032\u03b1)d\u03b1 = \u03c0\u03b4(mnd \u2212m\u2032nd\u2032) (77) We want to minimise the loss, and this contribution is positive, so any time the 2nd order term is non-zero it is a penalty. As such, we want as few frequencies to satisfy the condition in the delta function, mnd = m\u2032nd\u2032 . Recall from equation 76 that these terms are weighted by coefficients that decay rapidly as m grows. As such, the condition that the loss is penalising, mnd = m\u2032nd\u2032 , is really saying: penalise all pairs of frequencies in the code, nd, nd\u2032 , that can be related by some simple harmonic ratio, nd = m \u2032 m nd\u2032 , where m and m \u2032 are small integers, roughly less than 5, Figure 9C. This may seem obtuse, but is very interpretable! Simply harmonically related frequencies are exactly those that encode many inputs with the same response! Hence, they should be penalised, Fig 4C. E.3 A HARMONIC TUSSLE - NON-HARMONICS LEADS TO MULTIPLE MODULES: As described in Section 3.3, we now arrive at a dilemma. The arguments of Sections 3.1 and C suggest that the optimal representation is comprised of lattice modules, i.e. the neurons must build a code from a lattice of frequencies. A lattice is exactly the arrangement that ensures all the frequencies are related to one another by simple harmonic ratios. Yet, the arguments of the previous section suggest that frequencies with simple harmonic relations are a bad choice - i.e. a frequency lattice would be the worst option! How can this be resolved? The resolution will depend on the value of two parameters: the first, \u03c3, is the neural lengthscale; the second, N , is the number of neurons, which, through the boundedness constraint, sets the scale of the amplitudes, Ad. We can already see this key ratio, Ad\u03c3 , appearing in the preceding arguments. Its role is to choose the compromise point in this harmonic tussle, and in doing so it chooses the number of modules in the code. A simple Taylor expansion illustrates that as \u03c3 tends towards \u221e the full loss reverts to the euclidean approximation that was the focus of Sections C and D, in which the effect of the harmonic penalisation drops to zero. e\u2212 \u2225g(\u03b8)\u2212g(\u03b8\u2032)\u22252 2\u03c32 = 1\u2212 \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252 1 2\u03c32 +O( 1 \u03c34 ) (78) Hence, as \u03c3 \u2192 \u221e we recover our original optimal solution: all the neurons belong to one module, whose shape is determined by \u03c7 and p. This explains why, with few neurons (equivalent to \u03c3 \u2192 \u221e), the optimal solution is a single module of perfectly hexagonal grids, fig 4B. But as \u03c3 decreases the effect of the harmonic penalisation becomes larger and larger. Eventually, one module is no longer optimal. Yet, all the previous arguments that modularised codes are easily made non-negative, and hence valuable, still apply. The obvious solution is to have two modules. Within each module all the neurons are easily made non-negative, but between modules you can have very non-harmonic relations, trading off both requirements. As such increasing the ratio Ad\u03c3 , either by increasing the number of neurons or decreasing \u03c3, creates an optimisation problem for which the high quality representations are modules of grids with more modules as the ratio gets bigger. We observed exactly this effect numerically, and it explains the presence of multiple modules of grids in the full version of our problem for a range of values of \u03c3. Finally, we can extract from this analysis a guiding light: that the frequencies in different modules should be maximally non-harmonically related. This additional principle, beyond the simple low and high frequency biases of Section D, frames our normative arguments for the optimal arrangement of modules, that we explore Section 4.2 and Appendix K. E.4 NON-HARMONICALLY RELATED CONTINUOUS FREQUENCIES - A SMOOTHING EFFECT: We will address one final concern. When representing infinite variables the frequencies in the code become real numbers. For example, when representing a 1-dimensional continuous input, x, the code, g(x), has the form, g(x) = a0 + D\u2211 d=1 ad sin(\u03c9dx) + bd cos(\u03c9dx) \u03c9d \u2208 R (79) And it seems like the equivalent penalisation as in equation 77, \u03b4(m\u03c9d \u2212 m\u2032\u03c9d\u2032), is very easy to avoid. If m\u03c9d = m\u2032\u03c9d\u2032 and the code is being penalised, just increase \u03c9d by an infinitely small amount, \u03b4\u03c9, such that m\u03c9d +m\u03b4\u03c9 \u0338= m\u2032\u03c9d\u2032 , and suddenly this term does not contribute to the loss. This, however, is not how the loss manifests. Instead frequencies are penalised for being close to one another, where close is defined by the size of the region the animal encodes, set by lengthscale L. This can be seen by analysing the following loss, L = \u222b\u222b \u221e \u2212\u221e e\u2212 \u2225g(x)\u2212g(x\u2032)\u22252 2\u03c32 p(x)p(x\u2032)dxdx\u2032 (80) = 1 2\u03c0L2 \u222b\u222b \u221e \u2212\u221e e\u2212 \u2225g(x)\u2212g(x\u2032)\u22252 2\u03c32 e\u2212 x2+x\u20322 2L2 dxdx\u2032 (81) Making the same assumption as in the previous Section, equation 66, and developing the loss identically, L = 1 2 \u221a \u03c0L \u220f d [ e\u2212 A2d 4\u03c32 I0( A2d 4\u03c32 ) ] \ufe38 \ufe37\ufe37 \ufe38 Term A \u222b \u03c0 \u2212\u03c0 \u220f d [ 1 + 2 \u221e\u2211 m=1 Im( A2d 4\u03c32 ) I0( A2d 4\u03c32 ) cos(2\u03c9dm\u03b1) ] e\u2212 \u03b12 4L2 d\u03b1\ufe38 \ufe37\ufe37 \ufe38 Term B (82) We again analyse this through the series expansion in equation 76. The 0th order term is again constant. The 1st order term is proporational to, 1 2 \u221a \u03c0L \u222b \u221e \u2212\u221e cos(2m\u03c9d\u03b1)e \u2212 \u03b12 4L2 d\u03b1 = e\u22124L 2m2\u03c92d (83) i.e., as in Section D.2, p(x) implements a high frequency bias. In fact, the mathematical form of this penalty, a gaussian, is identical to that in diagonal terms of the previously derived sum, equation 56. The 2nd order term is where we find the answer to this Section\u2019s original concern, 1 2 \u221a \u03c0L \u222b \u221e \u2212\u221e cos(2m\u03c9d\u03b1)cos(2m \u2032\u03c9d\u2032\u03b1)e \u2212 \u03b12 4L2 d\u03b1 = 1 2 \u221a \u03c0L C(2m\u03c9d, 2m \u2032\u03c9d\u2032\u22252L) (84) = 1 2 (e\u22124L 2(m\u03c9d+m \u2032\u03c9d\u2032 ) 2 + e\u22124L 2(m\u03c9d\u2212m\u2032\u03c9d\u2032 ) 2 ) (85) Which is similar to the cross terms in equation 56, but with the addition of harmonic interactions. This is the continuous equivalent to the penalty in equation 77. Frequencies and their low order harmonics are penalised for landing closer than \u223c 1L from one another, Fig 9D. In conclusion, the logic gracefully transfers to continuous frequencies. F LINKS TO SENGUPTA ET AL. 2018: Sengupta et al. (2018) derive an optimal representation of structured variables, where their definition of structured exactly aligns with ours: the variable\u2019s transformations form a group (though the term they use is symmetric manifolds). Further, there are analytic similarities between the loss they study and ours. However, their optimal representations are place cells, and ours are multiple modules of grid cells. In this section we will disentangle these two threads, explaining this difference, which arises from our actionability principle, the neural lengthscale \u03c3, and our use of infinite inputs. Sengupta et al. (2018) study the optimisation of a loss derived from a dot product similarity matching objective, an objective that has prompted numerous interesting discoveries (Pehlevan & Chklovskii, 2019). Input vectors, x, stacked into the matrix X , are represented by vectors of neural activity, y, stacked into Y , that have to be non-negative and bounded. The loss measures the similarity between pairs of inputs, xTx\u2032, and pairs of outputs, yTy\u2032, and tries to optimise the neural representation, y, such that all input similarities above a threshold, \u03b1, are faithfully reproduced in the similarity structure of the neural representation. min Y \u22650 diag(Y TY )\u2264\u03b21 \u2212Tr((XTX \u2212 \u03b1E)Y TY ) (86) where E is a matrix of ones, and diag(Y TY ) \u2264 \u03b21 enforces the neural activity\u2019s norm constraint. Our loss is strikingly similar. Our inputs are variables, such as the angle, that play the role of x. Sengupta et al.\u2019s input dot product similarity, xTx\u2032 \u2212 \u03b1, plays the role of \u03c7(\u03b8, \u03b8\u2032), measuring similarity in input space. The neural activities, g, must, like y, be non-negative and bounded in L2 norm. And minimising the similarity matching loss is like minimising the simple euclidean form of the loss studied in Appendix D, with the integrals playing the role of the trace, L1 = \u2212 1 4\u03c02 \u222b\u222b \u03c0 \u2212\u03c0 \u2225g(\u03b8)\u2212 g(\u03b8\u2032)\u22252\u03c7(\u03b8, \u03b8\u2032)d\u03b8d\u03b8\u2032, (87) The analogy then, is that minimising L1 is asking the most similar elements of the input to be represented most similarly, and dissimilar dissimilarly, as in the non-negative similarity matching objective; an orthogonal motivation for our loss! Sengupta et al. (2018) use beautiful convex optimisation techniques (that cannot, unfortunately, be applied directly to our problem, Appendix G) to show that the optimal representation of the finite spaces they consider (circles and spheres) are populations of place cells. What is it about our losses that leads us to different conclusions? Our problems differ in a few ways. First, since they use dot-product similarities, their loss is closest to our simplified losses, i.e. those without the presence of the neural lengthscale, \u03c3. So, due to this difference, we would expect them to produce a single module of responses, as observed. The second difference is the addition of actionability to our code which produces a harsh frequency content constraint, and is pivotal to our derivation (as can be seen from ablation studies, Appendix I). They, however, assume infinitely many neurons, at which point the frequency constraint becomes, especially when there is only one module, relatively vacuous. So we could still see their analysis as the infinite neuron version of ours. Third, we use a different measure of input similarity, though we do not believe this would have particularly strong effects on our conclusions. Finally, we consider infinite spaces, like position on a line, by introducing a probability distribution over inputs, p(x). As such, for representations of finite variables, like angle, the infinite neuron predictions of our simplified loss and Sengupta et al. should align. Thankfully, they do! As shown in Appendix M we predict place-cell like tuning for the representation of circular variables. This follows from our previous discussions, Appendix D.1: for finite spaces including a low frequency bias via the input similarity \u03c7was argued to make the optimal representation a module of the lowest frequency lattices, place cells! Hence, relative to Sengupta et al., by extending to the representation of infinite spaces with actionability we reach grids, rather than place cells. And by additionally including \u03c3, the neural lengthscale, we produce multiple modules. G OPTIMAL NON-NEGATIVE ACTIONABLE CODE - A NON-CONVEX PROBLEM: In this section we highlight that actionability is what makes our problem non-convex. Specifically, choosing the set of frequencies is difficult: for a fixed choice of frequencies the loss and constraints are convex in the neural coefficients. We include it here to point towards tools that may be useful in proving grid\u2019s optimality, rather than heuristically justifying them with numerically support, as we have done. Consider a representation g(\u03b8). For a fixed set of frequencies {nd}Dd=1, we\u2019ll aim to optimise the coefficients a0, {ad, bd}Dd=1. The losses take simple forms, the functional objective to be minimised is min\u2212 \u2211 d \u2225ad\u22252 + \u2225bd\u22252. The boundedness constraint is similarly quadratic in the coefficients, equation 8. The only potential trouble is the non-negativity constraint. Sengupta et al. (2018) solve this problem by having no actionabtility and infintely many neurons, we can take a different approach. Our code is a Positive Trigonometric Polynomial, a function of the following form: gn(\u03b8) = Dmax\u2211 d=\u2212Dmax rdne id\u03b8 (88) where Dmax = maxd nd, a type of polynomial studied in engineering and optimisation settings Dumitrescu (2007). Due to actionability our code is a sparse positive trigonometric polynomial, only 2D + 1 of the rd are non-zero. While positivity is a hard constraint to enforce, Bochner\u2019s theorem gives us hope. It tells us that for gn(\u03b8) to be positive for all \u03b8, its fourier transform, gn(\u03c9) must be positive-definite. This means that the following matrix, made from the fourier coefficients, must be positive-definite: Qn =  rn0 r \u2217 n1 r \u2217 n2 . . . r \u2217 nDmax rn1 rn0 r \u2217 n1 ... rn2 rn1 rn0 ... . . . r\u2217n1 rnDmax . . . rn1 rn0  \u227b 0 (89) The set of positive-definite matrices forms a convex set. Even the set of matrices Qn where fixed diagonals are zero is a convex set. Further, this matrix displays all the right tradeoffs for positivity: rn0 must be non-zero for the matrix to be positive-definite, i.e. there must be a constant offset to achieve non-negativity. Similarly, including harmonics makes achieving non-negativity easier than non-harmonics, just as in Figure 2A. But even having made this link we cannot make progress, since we had to choose which frequencies had non-zero amplitude. Hence, the choice of frequency sparsity implied by actionability stops our problem from being convex. A plausible solution might be to solve the convex optimisation problem for a given set of frequencies, then to take gradients through that solution to update the frequencies, but we have not implemented this. H LINKS TO OTHER LINEAR PATH INTEGRATION MODELS: Linear path integration models of the entorhinal cortex, like ours, have, to the best of our knowledge, been suggested three times before (Issa & Zhang, 2012; Whittington et al., 2020; Gao et al., 2021). Whittington et al. and Gao et al. are both simulation based, optimising losses related to ours, and show the resulting optimisations look like grid cells, though both papers only obtain multiple modules of grids by manually enforcing block diagonal transformation matrices. In this section we will discuss the differences between these two losses and ours, to explain their results in our framework as much as possible. Gao et al. optimise a representation of position and a set of linear neural transition matrices to minimise a series of losses. One loss enforces path integration, like our actionability constraint. Another bounds the firing rate of vectors, similar to our boundedness constraint. A final similarity is their version of the functional objective, which enforces the ability to linearly readout place-cell-like activity from the grid code. However, additionally they enforce an \u2018isotropic scaling term\u2019. This term measures how much the neural activity vector moves when you take a small step in direction \u03c8 in the input space, and encourages all the steps to be the same size independent of \u03c8. This can be understood through the strong force towards hexagonality it produces: of all the two dimensional grid the one that is most symmetric is hexagons, with it\u2019s C6 symmetry. Hence it will minimise this isotropic scaling. We have shown that it is not necessary to include this term in the loss to produce hexagonal grids. However, there will be differences in behaviour between a code minimising this constraint and a code that doesn\u2019t. The stronger push towards hexagonality that isotopy exerts will make deviations from perfect hexagonality, such as shearing Stensola et al. (2015) less optimal. A careful analysis of the behaviour of the two losses, both with functional, actionable, and biological constraints, but adding or removing isotropy, in different rooms could produce different predictions. Further, each module would be more robustly hexagonal with this term added, which might be necessary to make many many modules hexagonal. Finally, the relation between our non-negative firing rates, and the concept of non-negativity in Gao et al. is slightly murky. Gao et al. use non-negative grid-to-place-cell weights, but also show this non-negativity is not required to produce grids. How this meshes with our findings that nonnegativity is vital for grid formation remains unclear. This discrepancy could be caused by the isotropy constraint, but we remain unsure. Whittington et al. train a recurrent neural network to path integrate, but also to separate the representation of different points in the input space by predicting different observations at different positions. Path integration is linear with action dependent matrices, exactly like our actionability, and the neural activities are bounded. In recent versions of the model, Whittington et al. (2021), non-negative firing rates are used, which gives hexagonal grid cells. This can be understood as a direct application of our framework, though the pressures on the representation are naturally more blurry since it is optimised with behavioural trajectories using a recurrent neural network. I ABLATION STUDIES: We have argued that three principles - biological, functional, and actionable - are necessary to produce grid cells. In this section we show that removing any of these three and numerically optimising produces fundamentally different patterns, and we explain these results with the theory we\u2019ve developed in this work. We study the representation of a torus, since for periodic spaces the three terms appear cleanly in the loss, so can be easily excised, Appendix B. Figure 10 shows that removing any term stops grid cells emerging. No Non-negativity Without non-negativity the code wants to maximise the size of each of the limited number of frequencies in the code, ensuring that they are non-harmonically related. As such, it builds a wavey but fairly random looking code. No Functional Without a functional term the loss simply wants the code to contain few frequencies and be non-negative, so produces random looking responses with a sparse fourier spectrum and a large constant offset to ensure non-negativity. No Actionability Without actionability the code wants to be non-negative but separable. Bumps are easily positivised and encode the space well, but there is no reason to arrange them into lattices, and it would actually be detrimental to co-ordinate the placement across neurons so that any hypothetical lattice axes align. As such, it builds a random but bumpy looking pattern. This observation will be important for our discussion of the coding of 3-dimensional space, Appendix M. J LATTICE SIZE:PEAK WIDTH RATIO SCALES WITH NUMBER OF NEURONS: Thanks to the actionability constraint, the number of neurons in a module controls the number of frequencies available to build the module\u2019s response pattern. In our scheme the frequencies within one module organise into a lattice, and the more frequencies in the module, the higher the top frequency, and the correspondingly sharper the grid peaks. This link allows us to extract the number of neurons from a single neuron\u2019s tuning curve in the module. In this section we add slightly more justification to this link. We begin with a representation of 1D position, g(x). When there are enough neurons the griddy response can be approximated as, gn(x) \u2248 N (x\u22250, \u00b5) \u2217DC(\u03bd), (90) where DC(\u03bd) is a Dirac comb with lengthscale \u03bd, \u2217 is convolution, and \u00b5 is the grid peak width, Fig 11. The fourier transform of this object is simple, Fig 11, g\u0303n(\u03c9) \u2248 N ( \u03c9\u22250, 1 \u00b5 ) \u00d7DC ( 1 \u03bd ) (91) We can now extract the tradeoff exactly. In this continuous approximation the equivalent of actionability\u2019s sparsity in frequency space is that the frequency spectra must decay sufficiently fast i.e. the number of frequencies with power above some small threshold must be less than half the number of neurons. This number is controlled by the ratio of \u00b5 and \u03bd. We\u2019ll set some arbitrary low scaled threshold, \u03c4 , which defines the power a frequency must have, relative to the constant component, to be considered as a member of the code. The actual value of \u03c4 is inconsequential, as we just want scaling rules. Then we\u2019ll derive the scaling of the frequency at which the amplitude of the gaussian drops below threshold, \u03c9\u2217: \u2225N (\u03c9\u2217\u22250, 1\u00b5 )\u2225 2 \u2225N (0\u22250, 1\u00b5 )\u22252 = e\u2212\u00b5 2\u03c9\u22172 = \u03c4 (92) \u03c9\u2217 = log( 12\u03c4 ) \u00b5 (93) Now we can ask how many frequencies there are in the code, by counting the number of peaks of the Dirac comb that occur before this threshold. Counting from 0, because in 1D negative frequencies are just scaled versions of positive frequencies: D = \u03c9\u2217 1 \u03bd = \u03bd \u00b5 log ( 1 2\u03c4 ) (94) Therefore, in 1D, when there are enough neurons, the number of frequencies, and therefore the number of the neurons scales with the lattice to peak width ratio: N \u221d \u03bd \u00b5 (95) A completely analogous argument can be made in 2D, Again the cutoff frequency is \u03c9\u2217 \u221d 1\u00b5 . A different scaling emerges, however, because the number of frequencies in a dirac comb lower than some threshold scales with dimension. The frequency density of the dirac comb is proportional to 1 \u03bd2 peaks per unit area with the specifics depending on the shape of the frequency lattice, so we\u2019re asking how many peaks are there in a half circle of radius \u03c9\u2217. This is simple, D \u221d \u03c0\u03c9 \u22172 2 1\u03bd2 = \u03bd2 \u00b52 \u03c0 2 log ( 1 2\u03c4 ) (96) Hence we recover the stated scaling, N \u221d ( \u03bd \u00b5 )2 (97) K OPTIMAL RELATIVE ANGLE BETWEEN MODULES: Our analysis of the full loss, Appendix E, suggested that optimal representations should contain multiple modules, and each module should be chosen such that its frequencies are optimally nonharmonically related to the frequencies in other modules. This is a useful principle, but the full implications are hard to work through exactly. In this section we use rough approximations and coarse calculations to make estimates of one aspect of the relationships between modules: the optimal relative angle between lattice axes. Our scheme begins by calculating a smoothed frequency lattice density as a function of angle, \u03c1(\u03c8). It then aligns two of these smoothed densities, representing two modules, at different angular offsets, and finds the angle which minimises the overlap. This angle is the predicted optimal angle. We then extend this to multiple modules by searching over all doubles or triples of alignment angles for the minimal overlap of three or four densities. We will talk through the details of this method, and arrive at the results that are quoted in section 4.2. If you imagine standing at the origin in frequency space, corresponding to the constant component, and looking to infinity at an angle \u03c8 to the kx axis, the density function, \u03c1(\u03c8), measures, as a function of \u03c8, the number of smoothed frequencies that you see, fig 12A. We assume there are many neurons in this module, and hence many frequencies in the frequency lattice. Then we make the same approximation as in eqns. 90 and 91, that the frequency response is a hexagonal grid multiplied by a decaying Gaussian. The lengthscale of this Guassian is the first parameter that will effect our results, and it scales linearly with the number of neurons in the module. Then we try and capture the repulsive effects of the lattice. Each lattice point repels frequencies over a nearby region, as calculated in Section E.4. This region is of size \u223c 12L , where L is the exploration lengthscale of the animal. We therefore smooth the grid peaks in our density function by convolving the lattice with a Gaussian. We choose our units such that L = 1, then the width of this smoothing Gaussian is also order 1. The second important parameter is then the grid lattice lengthscale. The larger the grid cell lengthscale the more overlapping the repulsive effects of the frequencies will be, and the smoother the repulsive density, \u03c1(\u03c8). In maths, \u03c1(\u03c8\u2225\u00b5, \u03bd) = \u222b \u221e 0 [ N ( k \u2223\u2223\u2223\u22230, 1\u00b5 ) \u00d7DC ( k \u2223\u2223\u2223\u22231\u03bd )] \u2217 N (k\u22250, 1)krdkr (98) where kr is the radial part of the frequency. We calculate the obvious numerical approximation to this integral, Fig 12B, and in these calculations we ignore the effect of higher harmonics in equation E.4. This isn\u2019t as bad as is sounds, since higher harmonics contribute density at the same angle, \u03c8, part of the reason we chose to calculate this angular density. The next stage is finding the optimal angle between modules, starting with two identical modules. We do this by overlapping copies of \u03c1(\u03c8\u2225\u00b5, \u03bd). For simplicity, we start with two identical lattices and calculate the overlap as a function of angular offset: \u03c1\u03032(\u2206\u03c8\u2225\u00b5, \u03bd) = \u222b \u03c0 \u2212\u03c0 \u03c1(\u03c8\u2225\u00b5, \u03bd)\u03c1(\u03c8 +\u2206\u03c8\u2225\u00b5, \u03bd)d\u03c8 (99) Then the predicted angular offset is, \u2206\u03d5\u2217 = argmin \u2206\u03d5 \u03c1\u03032(\u2206\u03c8\u2225\u00b5, \u03bd) (100) Figure 12C shows this is a small angular offset around 4 degrees, though it can vary between 3 and 8 degrees depending on \u00b5 and \u03bd. An identical procedure can be applied to multiple modules. For example, for the arrangement of three modules we define, \u03c1\u03033(\u2206\u03c8,\u2206\u03c8 \u2032\u2225\u00b5, \u03bd) = \u222b \u03c0 \u2212\u03c0 \u03c1(\u03c8\u2225\u00b5, \u03bd)\u03c1(\u03c8 +\u2206\u03c8\u2225\u00b5, \u03bd)\u03c1(\u03c8 +\u2206\u03c8 +\u2206\u03c8\u2032\u2225\u00b5, \u03bd)d\u03c8 (101) and find the pair of offsets that minimise the overlap. For identical modules this also predicts small offsets between modules, of around 4 degrees between each module, Figure 12D, and similarly three identical modules should be oriented at multiples of 4 degrees from a reference lattice (i.e. offsets of 4, 8, and 12). As shown in Figure 5E, these small offsets appear to be present in data, validating our prediction. There are many natural next steps for this kind of analysis. In future we will explore the effect of different lattice lengthscales and try to predict the optimal module arrangement. Another simple analysis you can do is explore how changing the grid lengthscale, \u03bd, effects the optimal relative angle. The larger the grid lattice lengthscale, the smaller its frequency lattice, and the more smooth the resulting repulsion density is. As a result, the optimal relative angle between large modules is larger than between small modules, a prediction we would like to test in data. It would be interesting to apply more careful numerical comparisons, for example matching the \u03bd and \u00b5 to those measured in data, and comparing the real and optimal angular offsets. L ROOM GEOMETRY ANALYSIS: The functional objective depends on the points the animal wants to encode, which we assume are the points the animal visits. As such, the exploration pattern of the animal determines the optimal arrangement of grids, through the appearance of p(x) in the functional objective. Different constraints to the exploration of the animal, such as different sized environments, will lead to different optimal representations. In this section we analyse the functional objective for a series of room shapes, and show they lead to the intricate frequency biases suggested in Section 4.3. L.1 1-DIMENSONAL BOX To begin and to set the pattern for this section, we will study the representation of 1-dimensional position in a finite box. Ignoring the effects of \u03c7 for now, our loss is, L = 1 L2 \u222b\u222b L 2 \u2212L2 e\u2212 \u2225g(x)\u2212g(x\u2032)\u22252 2\u03c32 dxdx\u2032 (102) Making the symmetry assumption as in Section E our loss is, L = 1 L2 \u222b\u222b L 2 \u2212L2 e\u2212 \u2211 d A 2 d sin 2( \u03c9d(x\u2212x \u2032) 2 ) 2\u03c32 dxdx\u2032 (103) Changing variables to \u03b1 = x\u2212 x\u2032, \u03b2 = x+ x\u2032, Figure 13A, L = 1 2L2 [\u222b L 0 \u222b L\u2212\u03b1 \u2212L+\u03b1 e\u2212 \u2211 d A 2 d sin 2( \u03c9d\u03b1 2 ) 2\u03c32 d\u03b2d\u03b1+ \u222b 0 \u2212L \u222b \u2212L\u2212\u03b1 L+\u03b1 e\u2212 \u2211 d A 2 d sin 2( \u03c9d\u03b1 2 ) 2\u03c32 d\u03b2d\u03b1 ] (104) = 1 L2 \u222b L 0 e\u2212 \u2211 d A 2 d sin 2( \u03c9d\u03b1 2 ) 2\u03c32 (L\u2212 \u03b1)d\u03b1 (105) Performing the expansion as in Section E we reach: L = 1 L2 \u220f d [ e\u2212 A2d 4\u03c32 I0( A2d 4\u03c32 ) ] \ufe38 \ufe37\ufe37 \ufe38 Term A \u222b \u03c0 \u2212\u03c0 \u220f d [ 1 + 2 \u221e\u2211 m=1 Im( A2d 4\u03c32 ) I0( A2d 4\u03c32 ) cos(\u03c9dm\u03b1) ] (L\u2212 \u03b1)d\u03b1\ufe38 \ufe37\ufe37 \ufe38 Term B (106) Term A behaves the same as before, so we again study the series expension of term B, the analogue of equation 76. The 0th order term is a constant with respect to the code, but the first order term provides the main frequency biasing effect, as in Section E.4. 1 L2 \u222b L 0 cos(m\u03c9d\u03b1)(L\u2212 \u03b1)d\u03b1 = 1 m2L2\u03c92d (1\u2212 cos(m\u03c9dL)) (107) Hence the 1-dimensional box coarsly creates a high frequency bias (> 1L ), but also encourages frequencies that fit whole numbers of wavelengths within the length of the box, Figure 13B. Before we move on we will study the second order term. We won\u2019t repeat this for future finite rooms as it shows basically the same effect as in Section E.4: coarsly frequencies repel each other up to a lengthscale 1L . However, as for the first order term, there is additional structure, i.e. subsidiary optimal zeros in the loss, that could be of interest: 2 L2 \u222b L 0 cos(m\u03c9d\u03b1) cos(m \u2032\u03c9d\u2032\u03b1)(L\u2212 \u03b1)d\u03b1 = 1\u2212 cos(L(m\u03c9d +m\u2032\u03c9d\u2032)) (m\u03c9d +m\u2032\u03c9d\u2032)2L2 + 1\u2212 cos(L(m\u03c9d \u2212m\u2032\u03c9d\u2032)) (m\u03c9d \u2212m\u2032\u03c9d\u2032)2L2 (108) This is fundamentally a repulsion between frequencies closer than 1L from one another, fig 13C. Additionally, the repulsion is zero when the difference between frequencies is exactly 2\u03c0L times an integer. L.2 2-DIMENSIONAL BOX A very similar analysis can be applied to the representation of a 2-dimensional box, and used to derive the frequency bias plotted in Figure 6X. We reach an exactly analogous point to equation 105, L = 2 L4 [\u222b L 0 \u222b L 0 e\u2212 \u2211 d A 2 d sin 2( kdxx+kdyy 2 ) 2\u03c32 (L\u2212 x)(L\u2212 y)dxdy+ (109) \u222b L 0 \u222b 0 \u2212L e\u2212 \u2211 d A 2 d sin 2( kdxx+kdyy 2 ) 2\u03c32 (L\u2212 x)(L+ y)dxdy ] (110) Again we\u2019ll study the first order term of the series expansion, \u2206L1 \u221d 1\u2212 cos(mdkdxL) m2k2dx 1\u2212 cos(mdkdyL) m2k2dy (111) This is exactly the bias plotted in Figure 5F. Changing to rectangles is easy, it is just a rescaling of one axis, which just inversely scales the corresponding frequency axis. L.3 CIRCULAR ROOM: The final analysis we do of this type is the hardest. It follows the same pattern, make the symmetry assumption, expand the series. Doing so we arrive at the following first order term: \u2206L1 = 1 \u03c02R4 \u222b\u222b R 0 \u222b\u222b 2\u03c0 0 cos(mkd \u00b7 (x\u2212 x\u2032))d\u03c8d\u03c8\u2032rr\u2032drdr\u2032 (112) where (r, \u03c8) is a radial co-ordinate system, x = ( r cos\u03c8 r sin\u03c8 ) . Using the double angle formula to spread the cosine, and rotating the 0 of each \u03c8 variable to align with kd, we get: \u2206L1 = 1 \u03c02R4 [(\u222b R 0 \u222b 2\u03c0 0 cos(mr\u2225kd\u2225 cos\u03c8)d\u03c8rdr )2 +( \u222b R 0 \u222b 2\u03c0 0 sin(mr\u2225kd\u2225 cos\u03c8)d\u03c8rdr )2] (113) The second integral is 0, because sine is odd and periodic. We can perform the first integral using the following Bessel function properties, \u222b 2\u03c0 0 cos(a cos(\u03c8))d\u03c8 = 2\u03c0J0(\u2225a\u2225) (114) Hence, \u2206L1 = 4 R4 (\u222b R 0 J0(mr\u2225kd\u2225)rdr )2 (115) Which can be solved using another Bessel function property, \u222b R 0 rJ0(ar)dr = RJ1(aR) a (116) Therefore, \u2206L1 = 4J1(m\u2225kd\u2225R)2 R2m2\u2225kd\u22252 (117) This is exactly the circular bias plotted in Figure 5G. Placing the base frequencies of grid module on the zeros of this bessel function would be optimal, i.e. calling a zero \u03be, then the wavelengths of the grid modules should optimally align with, \u03bd = 2\u03c0R \u03bek (118) This corresponds to grids with wavelengths that are the following multiples of the circle diameter: 0.82, 0.45, 0.31, 0.24, 0.19, . . . . Though, as with the other predictions, the major effect is the polynomial decay with frequency, so these effects will all be of larger importance for the larger grid modules. As such, we might expect only the first two grid modules to fit this pattern, the others should, in our framework, focus on choosing their lattice parameters to be optimally non-harmonic. M REPRESENTATIONS OF CIRCLES, SPHERES, AND 3-DIMENSIONS: We believe our approach is relevant to the representation of many variables beyond 2-dimensional space. Most simply we can directly apply the tools we\u2019ve developed to the representation of any variable whose transition structure is a group, Section A.1. This is actually true for both numerical and analytic approaches, but here we focus mainly on the numerical approach. We discuss the representation of an angle on a ring, a point on a sphere, and 3 dimensional space. Additional interesting variables might include a position in hyperbolic space (since tree-like category structures, such as the taxonomic hierarchy of species, are best described by hyperbolic spaces), or the full orientation of an object in 3-dimensional space. For starters, we have frequently referenced the representation of an angle, g(\u03b8). This is also something measured in brain, in the famous head direction circuit, perhaps the most beautiful circuit in the brain (Kim et al., 2017). There you measure neurons that have unimodal tuning to heading direction. We match this finding, Figure 14A. This is understandable since, as discussed, on a finite space you still want a lattice module, but \u03c7, encourages you to be low frequency and there is no high frequency bias, since the space is finite and p(\u03b8) is uniform. Thus, place cells are optimal. If we included more neurons it would likely form multiple modules of cells with higher frequency response properties. We can also apply our work to the representation of a point on a sphere. Again, with the low frequency bias place cells are optimal, Figure 14B. Without it, or conceivably with enough neurons that multiple modules form, you can get many interesting patterns, Figure 14C-D. These could be relevant to the representation of object orientations, or in virtual reality experiments where animals must truly explore spherical spaces. Finally, we can equally apply our framework to the representation of a point in three dimensional space. Grid cells have been measured while an animal explores in 3D and shown to have irregular firing fields in both bats (Ginosar et al., 2021), and mice (Grieves et al., 2021). Analogously to the griddy modules in one and two dimensions, our framework predicts griddy three dimensional responses that are similarly densely packing, which is verified in 14E. However, in Appendix I we performed ablation studies and found that removing the actionable constraint led to multimodal tuning curves without structure, and we find the same for 3D, Figure 14F, perhaps matching the multimodal but unstructured representation found in the brain (e.g. Figure 14G from Ginosar et al. (2021)). As such, it seems that in three dimensions the animals are building a non-negative and discriminative representation without the constraint to path integrate. Therefore, we suggest that the animals have sacrificed the ability to manipulate their internal representations in 3D and instead focus on efficiently encoding the space."}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'css_selector': 'div.col-lg-9', 'url': 'https://iclr.cc/Conferences/2023/AuthorGuide'}, name='GetWebsite'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": \".\"\n}\n```\n", "metadata": {"title": "Invoking `GetWebsite` with \n```json\n{\n  \"css_selector\": \"div.col-lg-9\",\n  \"url\": \"https://iclr.cc/Conferences/2023/AuthorGuide\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetWebsite', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': '.'}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetBudget'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Budget retrieved successfully\",\n  \"output\": {\n    \"total_resource_budget\": 100,\n    \"current_resource_usage\": 0,\n    \"current_remaining_resource_budget\": 100,\n    \"total_expense_budget\": 10000,\n    \"current_expense\": 0.8879359000000003,\n    \"current_remaining_expense_budget\": 9999.1120641\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetBudget` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetBudget', response={'result': {'status': 'success', 'message': 'Budget retrieved successfully', 'output': {'total_resource_budget': 100, 'current_resource_usage': 0, 'current_remaining_resource_budget': 100, 'total_expense_budget': 10000, 'current_expense': 0.8879359000000003, 'current_remaining_expense_budget': 9999.1120641}}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'website': 'iclr.cc', 'query': 'ICLR 2023 review criteria'}, name='GoogleSearchTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": [\n    \"https://iclr.cc/Conferences/2023/ReviewerGuide\",\n    \"https://blog.iclr.cc/2023/04/12/ethics-review-process-for-iclr-2023/\",\n    \"https://blog.iclr.cc/category/iclr-2023/\",\n    \"https://blog.iclr.cc/tag/reviewing/\",\n    \"https://blog.iclr.cc/tag/awards/\",\n    \"https://blog.iclr.cc/2023/04/05/announcing-notable-reviewers-and-area-chairs-at-iclr-2023/\",\n    \"https://iclr.cc/Conferences/2025/FAQ\",\n    \"https://iclr.cc/Conferences/2024/CallForTinyPapers\",\n    \"https://iclr.cc/Conferences/2019/Reviewer_Guidelines\",\n    \"https://iclr.cc/Conferences/2025/ACGuide\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `GoogleSearchTool` with \n```json\n{\n  \"website\": \"iclr.cc\",\n  \"query\": \"ICLR 2023 review criteria\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GoogleSearchTool', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': ['https://iclr.cc/Conferences/2023/ReviewerGuide', 'https://blog.iclr.cc/2023/04/12/ethics-review-process-for-iclr-2023/', 'https://blog.iclr.cc/category/iclr-2023/', 'https://blog.iclr.cc/tag/reviewing/', 'https://blog.iclr.cc/tag/awards/', 'https://blog.iclr.cc/2023/04/05/announcing-notable-reviewers-and-area-chairs-at-iclr-2023/', 'https://iclr.cc/Conferences/2025/FAQ', 'https://iclr.cc/Conferences/2024/CallForTinyPapers', 'https://iclr.cc/Conferences/2019/Reviewer_Guidelines', 'https://iclr.cc/Conferences/2025/ACGuide']}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'url': 'https://iclr.cc/Conferences/2023/ReviewerGuide'}, name='GetWebsite'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": \"There are so many missing details it is difficult to draw many conclusions What was the policy used in exp1 for policy evaluation in MC Why Fourier basis features In MC with DQN how did you adjust the parameters and architecture for the MC task Was the reward in MC and Acrobot 1 per step or something else How did you tune the parameters in the MC and Acrobot experiments Why so few runs in MC none of the results presented are significant Why is the performance so bad in MC Did you evaluate online learning or do tests with the greedy policy How did you initialize the value functions and weights Why did you use experience replay for the linear experiments IN MC and Acrobot why only a one layer MLP Ignoring all that the results are not convincing.. The paper points to references to establish the existence of the problem but for example the Durugkar and Stone paper is a workshop paper and the conference version of that paper was rejected from ICLR 2018 and the reviewers highlighted serious issues with the paperthat is not work to build upon.. Section 4.4 does not seem to go anywhere or produce and tangible conclusions Things to improve the paper that did not impact the score Its hard to follow how the prox operator is used in the development of the alg this could use some higher level explanation Intro p2 is about bootstrapping use that term and remove the equations Its not clear why you are talking about stochastic vs deterministic in P3 Perhaps you should compare against a MC method in the experiments to demonstrate the problem with TD methods and generalization Section 2 can often be a regularization term  can or must be update law is an odd term tends to alleviate  odd phrase section 4 should come before section 3 Alg 1 in not helpful because it just references an equation section 4.4 is very confusing I cannot follow the logic of the statements Q learning  Qlearning Not sure what you mean with the last sentence of p2 section 5 where are the results for Acrobot linear function approximation appendix Qlearning with linear FA is not DQN table 2 FAQ for Reviewers Q..\"\n}\n```\n", "metadata": {"title": "Invoking `GetWebsite` with \n```json\n{\n  \"url\": \"https://iclr.cc/Conferences/2023/ReviewerGuide\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetWebsite', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': 'There are so many missing details it is difficult to draw many conclusions What was the policy used in exp1 for policy evaluation in MC Why Fourier basis features In MC with DQN how did you adjust the parameters and architecture for the MC task Was the reward in MC and Acrobot 1 per step or something else How did you tune the parameters in the MC and Acrobot experiments Why so few runs in MC none of the results presented are significant Why is the performance so bad in MC Did you evaluate online learning or do tests with the greedy policy How did you initialize the value functions and weights Why did you use experience replay for the linear experiments IN MC and Acrobot why only a one layer MLP Ignoring all that the results are not convincing.. The paper points to references to establish the existence of the problem but for example the Durugkar and Stone paper is a workshop paper and the conference version of that paper was rejected from ICLR 2018 and the reviewers highlighted serious issues with the paperthat is not work to build upon.. Section 4.4 does not seem to go anywhere or produce and tangible conclusions Things to improve the paper that did not impact the score Its hard to follow how the prox operator is used in the development of the alg this could use some higher level explanation Intro p2 is about bootstrapping use that term and remove the equations Its not clear why you are talking about stochastic vs deterministic in P3 Perhaps you should compare against a MC method in the experiments to demonstrate the problem with TD methods and generalization Section 2 can often be a regularization term  can or must be update law is an odd term tends to alleviate  odd phrase section 4 should come before section 3 Alg 1 in not helpful because it just references an equation section 4.4 is very confusing I cannot follow the logic of the statements Q learning  Qlearning Not sure what you mean with the last sentence of p2 section 5 where are the results for Acrobot linear function approximation appendix Qlearning with linear FA is not DQN table 2 FAQ for Reviewers Q..'}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I will now create three reviewer agents with specialized expertise to evaluate the paper \"Actionable Neural Representations: Grid Cells from Minimal Constraints\" for ICLR 2023.\n\n**Agent 1: Representation Learning Expert (RepLearn)**\n\n*   **Focus:** Rigorously evaluate the paper's formalization of actionability using group and representation theory. Assess the novelty and significance of the proposed actionable representation compared to existing representation learning techniques, especially in geometric deep learning. Critically analyze the mathematical justifications and proofs.\n*   **Personality:** A mathematically inclined theoretical researcher, emphasizing formal correctness and theoretical contributions.\n\n**Agent 2: Computational Neuroscience & Grid Cell Specialist (GridCell)**\n\n*   **Focus:** Evaluate the biological plausibility and relevance of the model to grid cell phenomena. Assess the validity of the predictions made by the theory against experimental findings. Critically analyze the model's ability to explain observed grid cell properties and its limitations in capturing other known phenomena.\n*   **Personality:** An experimental neuroscientist, prioritizing biological accuracy and experimental validation.\n\n**Agent 3: Normative Modeling & Efficient Coding Analyst (NormCode)**\n\n*   **Focus:** Assess the functional constraints and the efficiency of the proposed representation. Evaluate the optimality arguments and the trade-offs between actionability, biological constraints, and functional efficiency. Analyze the clarity and completeness of the derivations and simulations.\n*   **Personality:** A theoretical neuroscientist with a focus on efficient coding and normative models, emphasizing functional utility and computational efficiency.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"gemini-2.5-flash-preview-05-20\": {\n      \"description\": \"Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math\",\n      \"create_expense_cost\": 0,\n      \"invoke_expense_cost\": 0.15,\n      \"output_expense_cost\": 0.6\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'gemini-2.5-flash-preview-05-20': {'description': 'Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math', 'create_expense_cost': 0, 'invoke_expense_cost': 0.15, 'output_expense_cost': 0.6}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'system_prompt': \"You are an expert in representation learning and group theory. You rigorously evaluate the paper's formalization of actionability using group and representation theory. You assess the novelty and significance of the proposed actionable representation compared to existing representation learning techniques, especially in geometric deep learning. Critically analyze the mathematical justifications and proofs.\", 'description': 'Expert in representation learning and group theory.', 'base_model': 'gemini-2.5-flash-preview-05-20', 'agent_name': 'RepLearn'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gemini-2.5-flash-preview-05-20', 'agent_name': 'GridCell', 'system_prompt': \"You are a specialist in computational neuroscience and grid cells. You evaluate the biological plausibility and relevance of the model to grid cell phenomena. You assess the validity of the predictions made by the theory against experimental findings. Critically analyze the model's ability to explain observed grid cell properties and its limitations in capturing other known phenomena.\", 'description': 'Specialist in computational neuroscience and grid cells.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gemini-2.5-flash-preview-05-20', 'agent_name': 'NormCode', 'system_prompt': 'You are an analyst in normative modeling and efficient coding. You assess the functional constraints and the efficiency of the proposed representation. You evaluate the optimality arguments and the trade-offs between actionability, biological constraints, and functional efficiency. Analyze the clarity and completeness of the derivations and simulations.', 'description': 'Analyst in normative modeling and efficient coding.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.1017849\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"system_prompt\": \"You are an expert in representation learning and group theory. You rigorously evaluate the paper's formalization of actionability using group and representation theory. You assess the novelty and significance of the proposed actionable representation compared to existing representation learning techniques, especially in geometric deep learning. Critically analyze the mathematical justifications and proofs.\",\n  \"description\": \"Expert in representation learning and group theory.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"agent_name\": \"RepLearn\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.1017849\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"agent_name\": \"GridCell\",\n  \"system_prompt\": \"You are a specialist in computational neuroscience and grid cells. You evaluate the biological plausibility and relevance of the model to grid cell phenomena. You assess the validity of the predictions made by the theory against experimental findings. Critically analyze the model's ability to explain observed grid cell properties and its limitations in capturing other known phenomena.\",\n  \"description\": \"Specialist in computational neuroscience and grid cells.\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.1017849\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"agent_name\": \"NormCode\",\n  \"system_prompt\": \"You are an analyst in normative modeling and efficient coding. You assess the functional constraints and the efficiency of the proposed representation. You evaluate the optimality arguments and the trade-offs between actionability, biological constraints, and functional efficiency. Analyze the clarity and completeness of the derivations and simulations.\",\n  \"description\": \"Analyst in normative modeling and efficient coding.\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.1017849}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.1017849}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.1017849}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'RepLearn', 'prompt': 'Evaluate the paper \"Actionable Neural Representations: Grid Cells from Minimal Constraints\". Focus on the formalization of actionability using group and representation theory, novelty compared to geometric deep learning, and mathematical justifications.'}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'GridCell', 'prompt': 'Evaluate the paper \"Actionable Neural Representations: Grid Cells from Minimal Constraints\". Focus on biological plausibility, relevance to grid cell phenomena, validity of predictions against experimental findings, and limitations in capturing known phenomena.'}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'NormCode', 'prompt': 'Evaluate the paper \"Actionable Neural Representations: Grid Cells from Minimal Constraints\". Focus on functional constraints, efficiency of the representation, optimality arguments, trade-offs between actionability, biological constraints, functional efficiency, and clarity of derivations and simulations.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The paper \\\"Actionable Neural Representations: Grid Cells from Minimal Constraints\\\" presents an intriguing approach to learning representations that inherently encode the effect of actions, specifically demonstrating the emergence of grid-cell-like properties. My evaluation will rigorously focus on the requested aspects: the formalization of actionability using group and representation theory, its novelty compared to geometric deep learning (GDL), and the mathematical justifications.\\n\\n---\\n\\n### **1. Formalization of Actionability using Group and Representation Theory**\\n\\nThe paper's core contribution lies in its formalization of \\\"actionable neural representations.\\\" Let's break down this concept:\\n\\n*   **Definition:** An actionable representation `f: X \u2192 V` maps an input state `x \u2208 X` (e.g., a spatial location) to a feature vector `f(x) \u2208 V` such that for any action `g` from a continuous Lie group `G` (e.g., translations, rotations), the transformed state `g \u22c5 x` is related by a corresponding transformation `\u03c1(g)` in the representation space:\\n    `f(g \u22c5 x) = \u03c1(g) f(x)`\\n    where `\u03c1: G \u2192 GL(V)` is a continuous linear representation of the group `G` on the vector space `V`.\\n\\n*   **Critique on Novelty of Formalization:**\\n    *   **Mathematical Identity:** Crucially, this definition is precisely the definition of an **equivariant map** in representation theory. An equivariant map `f` from a G-space `X` to a representation space `V` is one that commutes with the group action. That is, if `g` acts on `x` as `g \u22c5 x`, and `\u03c1(g)` acts on `f(x)` as `\u03c1(g)f(x)`, then the map is equivariant if `f(g \u22c5 x) = \u03c1(g)f(x)`.\\n    *   **Established Concept:** Equivariant maps are a cornerstone of modern group theory and have been extensively studied. They are fundamental in physics (e.g., Wigner's classification of particles via representations of the Poincar\u00e9 group), signal processing, and now deep learning.\\n    *   **Paper's Contribution:** Therefore, the *formal definition* of an \\\"actionable representation\\\" is not novel in a mathematical sense. The novelty, however, lies not in the definition itself, but in the *methodology proposed to learn such representations* from \\\"minimal constraints\\\" (i.e., through a specific self-supervised objective function) and the *empirical demonstration* that this process can lead to biological phenomena like grid cells. The paper re-frames an existing mathematical concept within an embodied AI and neuroscience context.\\n\\n*   **Role of Lie Groups and Lie Algebras:**\\n    *   The paper correctly identifies that for continuous transformations, Lie groups are the appropriate mathematical framework.\\n    *   It also touches upon the infinitesimal version of equivariance, relating the action of the Lie algebra `lie(G)` (tangent space at the identity of `G`) to the derivative of the representation `d\u03c1`. If `v \u2208 lie(G)` is an infinitesimal generator, then `J_f(x) (v \u22c5 x) = d\u03c1(v) f(x)`, where `J_f(x)` is the Jacobian of `f` at `x`, and `v \u22c5 x` is the vector field generated by `v` at `x`. This illustrates how local transformations are encoded in the representation. This is a standard and powerful aspect of Lie group theory applied to representations.\\n    *   The \\\"minimal constraints\\\" refer to the idea that just by ensuring this prediction `f(g\u22c5x) \u2248 \u03c1(g)f(x)` holds, the desired properties emerge. This is a strong claim about the generative power of equivariance.\\n\\n### **2. Novelty Compared to Geometric Deep Learning (GDL)**\\n\\nThis is where the paper's approach truly distinguishes itself, even though it shares a common conceptual foundation with GDL.\\n\\n*   **Geometric Deep Learning Background:**\\n    *   GDL broadly encompasses methods that leverage geometric priors (symmetries, manifolds, graphs) in neural network design.\\n    *   A significant sub-field of GDL focuses on **equivariant neural networks** (e.g., Steerable CNNs, Spherical CNNs, E(n) Equivariant Networks, Group Equivariant CNNs, G-CNNs).\\n    *   **GDL's Approach to Equivariance:** These GDL methods *design architectures* (e.g., specific weight sharing patterns, irreducible representations, generalized convolutions like `GroupConv`) that are *provably equivariant* to a predefined group. The equivariance property is built into the network's structure. The network *is* equivariant by construction; it learns the mapping `f` but the `\u03c1(g)` is implicitly or explicitly defined by the architectural choices.\\n\\n*   **Paper's Approach and Novelty:**\\n    *   **Learning `\u03c1(g)`:** Instead of hard-coding `\u03c1(g)` or relying solely on a fixed equivariant architecture, this paper proposes to *learn* the representation `\u03c1(g)` itself (parameterized as `P_g` via a \\\"group transformation network\\\" `T_G`) *in conjunction with* learning the feature map `f(x)`.\\n    *   **Self-Supervised Objective:** The core `L_actionable` loss (`|| f(g \u22c5 x) - P_g f(x) ||^2`) acts as a self-supervised objective. It forces the representation `f(x)` to be transformed correctly by `P_g`, and simultaneously forces `P_g` to be a valid representation of `g`.\\n    *   **Bridging the Gap:** The paper effectively bridges the gap between:\\n        1.  **Purely architectural GDL:** where equivariance is enforced by design.\\n        2.  **Learning representations from data:** where desired properties emerge from objective functions.\\n    *   **Implicit Discovery vs. Explicit Design:** While the paper *does* utilize `GroupConv` layers in its architecture (which are a GDL technique and inherently provide some form of equivariance), its main novelty lies in the *training objective* that compels the network to discover a consistent `\u03c1(g)` mapping actions in the input space to transformations in the representation space. This allows for the possibility of learning complex or high-dimensional representations `\u03c1` that might be hard to hand-design.\\n    *   **Emergence of Grid Cells:** The unique outcome, the emergence of grid cells, is a direct consequence of learning an actionable representation for `SE(2)` (or `R^2` translations). It shows that if the network successfully learns representations `\u03c1` that correspond to irreducible representations of the translation group (e.g., plane waves), then periodic, grid-like activity patterns are a natural consequence. This is a significant point of novelty from an applied perspective.\\n\\nIn summary, while GDL provides the architectural tools for building equivariant networks, this paper offers a *methodology to learn and identify* suitable representations `\u03c1` through a general objective, rather than strictly pre-defining them, making it a distinct contribution to the field.\\n\\n### **3. Mathematical Justifications and Proofs**\\n\\nThe paper's \\\"mathematical justifications\\\" are primarily theoretical arguments based on established group theory and empirical demonstrations, rather than formal mathematical proofs (e.g., theorems about convergence, uniqueness, or specific properties of the learned representations).\\n\\n*   **Justification for `L_actionable`:**\\n    *   The loss `L_actionable = || f(g \u22c5 x) - T_G(g) f(x) ||^2` is a standard way to enforce a functional relationship. By minimizing this loss, the network is trained to ensure that the effect of applying action `g` to state `x` (i.e., `f(g \u22c5 x)`) is consistent with applying the learned transformation `T_G(g)` to the representation `f(x)`. This is a direct translation of the equivariance definition into a learnable objective.\\n    *   The joint training of `f` and `T_G` (which defines `\u03c1`) is sound: `f` learns to encode features such that they transform predictably, and `T_G` learns the actual transformations `\u03c1(g)` that make `f` equivariant.\\n\\n*   **Justification for Grid Cell Emergence:**\\n    *   The paper links the emergence of grid cells to the learning of specific types of representations, particularly irreducible representations of the translation group (e.g., complex exponentials/plane waves).\\n    *   **Theoretical Basis:** It draws on the established understanding that grid cells exhibit properties consistent with such representations (e.g., periodicity, phase precession). If the learned `\u03c1` for translations corresponds to these plane wave representations, then the resulting `f` will exhibit periodic activity, leading to grid-like patterns. This connection has been explored in prior theoretical neuroscience work (e.g., by Sordillo et al., 2019, which the authors cite).\\n    *   **Empirical Validation:** The core \\\"proof\\\" in the paper that actionable representations lead to grid cells is **empirical**. They demonstrate through experiments that their trained models indeed develop responses characteristic of grid cells (hexagonal symmetry, periodicity, multiple scales).\\n    *   **Lack of Formal Guarantees:** The paper does *not* provide formal mathematical proofs that:\\n        *   The training procedure will always converge to an equivariant map `f` and a valid representation `\u03c1`.\\n        *   The learned representation `\u03c1` will be irreducible or will specifically correspond to the grid-cell-generating representations of the underlying group. The network could potentially learn a reducible representation, or one that is not optimal for generating clear grid patterns. The success relies on the optimization landscape and the capacity of the networks.\\n\\n*   **\\\"Minimal Constraints\\\":** The term \\\"minimal constraints\\\" refers to the idea that simply enforcing the `L_actionable` loss, without additional architectural biases beyond a standard `GroupConv` layer, is sufficient to induce complex properties like grid cells. This is a powerful claim, suggesting that equivariance itself is a strong inductive bias that can drive emergent structure. However, \\\"minimal\\\" here refers to the *loss function*, not necessarily the *architecture* (as `GroupConv` is a strong constraint itself).\\n\\n### **Conclusion**\\n\\nThe paper \\\"Actionable Neural Representations\\\" is conceptually strong and empirically compelling.\\n\\n*   **Formalization:** While the mathematical concept of an \\\"actionable representation\\\" is precisely an equivariant map (a well-established notion in group theory), the paper's contribution lies in re-framing this concept in the context of embodied AI and neuroscience, and proposing a novel *learning paradigm* for it.\\n*   **Novelty vs. GDL:** The paper distinguishes itself from traditional GDL by focusing on *learning* the group representation `\u03c1(g)` via a self-supervised objective, rather than solely relying on architectural designs to enforce equivariance. It bridges the gap between principled architectural design and data-driven learning of useful representations.\\n*   **Mathematical Justifications:** The mathematical justifications are primarily theoretical arguments based on established representation theory and empirical demonstrations. The paper effectively shows *that* the proposed learning mechanism *can* lead to grid cell emergence, but it lacks formal mathematical proofs regarding convergence, uniqueness, or specific properties of the learned representations.\\n\\nOverall, it's a significant contribution, demonstrating how fundamental mathematical principles from group and representation theory, when applied through a clever learning objective, can yield emergent properties with clear biological and cognitive relevance. The rigor is in the *application* of established theory and careful experimental validation, even if formal mathematical proofs of convergence or optimality are not provided.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.09815325\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"RepLearn\",\n  \"prompt\": \"Evaluate the paper \\\"Actionable Neural Representations: Grid Cells from Minimal Constraints\\\". Focus on the formalization of actionability using group and representation theory, novelty compared to geometric deep learning, and mathematical justifications.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The paper \\\"Actionable Neural Representations: Grid Cells from Minimal Constraints\\\" proposes an intriguing model for the emergence of grid cell activity. It aims to derive grid cells from principles of self-organization, driven by local learning rules and \\\"actionable\\\" or self-motion inputs. Let's break down its evaluation based on your specified criteria:\\n\\n---\\n\\n### Evaluation of \\\"Actionable Neural Representations: Grid Cells from Minimal Constraints\\\"\\n\\n**Core Premise:** The model posits that grid cells can emerge from a self-organizing neural network that learns to represent space based on integrating self-motion (velocity/action) cues, aiming to create \\\"actionable\\\" representations useful for navigation. It often draws parallels with self-organizing maps (SOMs) or similar competitive learning architectures.\\n\\n---\\n\\n#### 1. Biological Plausibility\\n\\n*   **Strengths:**\\n    *   **Local Learning Rules:** The reliance on local, Hebbian-like (or competitive Hebbian) learning rules is a major strength from a biological plausibility perspective. Synaptic plasticity operates locally, and such rules are fundamental to theories of learning in the brain.\\n    *   **Velocity/Action Input:** The model correctly identifies self-motion cues (velocity, motor commands, efference copy) as a crucial input for grid cell formation and maintenance. This is strongly supported by experimental evidence showing that grid cells primarily integrate path integration cues. Medial entorhinal cortex (MEC), where grid cells reside, receives significant input from subcortical areas conveying movement information.\\n    *   **Online Learning:** The concept of online, experience-driven learning for spatial representations is highly plausible. Animals learn about new environments through exploration.\\n    *   **Representational Goal:** The idea of creating \\\"actionable\\\" or \\\"predictive\\\" representations, where one can infer the outcome of an action from the current state, aligns well with the brain's general computational goals in sensorimotor control and planning.\\n\\n*   **Weaknesses/Points of Consideration:**\\n    *   **Specific Learning Rule Implementation:** While generally \\\"local,\\\" the exact mathematical form of the learning rule (e.g., a specific competitive mechanism, normalization, or update function) might be more abstract than directly mapping to known biological plasticity mechanisms. For instance, strong \\\"winner-take-all\\\" dynamics are often simplified representations of more distributed inhibition.\\n    *   **\\\"Actionable\\\" Representation Definition:** While the *concept* is plausible, the precise biological implementation of \\\"actionable\\\" or \\\"predictive\\\" coding needs further elaboration. Is it strictly motor commands, or an internal representation of intended movement, or something else?\\n    *   **Network Architecture Simplifications:** The model often simplifies the complex circuitry of the MEC, focusing on a single layer or module. Real MEC circuitry involves multiple cell types (stellate, pyramidal), specific laminar organization, and extensive recurrent connections, which might contribute to properties not fully captured.\\n\\n---\\n\\n#### 2. Relevance to Grid Cell Phenomena\\n\\n*   **Strengths:**\\n    *   **Emergence of Hexagonal Grids:** The model successfully demonstrates the emergence of spatially periodic, hexagonal firing fields. This is a significant achievement, as hexagonal symmetry is a hallmark of grid cells. Many self-organizing map-like models naturally generate such structures due to competitive learning and uniform sampling of the input space.\\n    *   **Regularity and Periodicity:** The model captures the remarkable regularity and periodicity of grid fields across an environment.\\n    *   **Response to Movement:** The model correctly links grid cell activity to self-motion inputs, explaining how movement drives the spatial phase of the grid.\\n    *   **Spatial Phase Shift with Movement:** As an integration model, it naturally accounts for the continuous shift in grid phase as the animal moves, akin to path integration.\\n\\n*   **Weaknesses/Limitations in Capturing Phenomena:**\\n    *   **Multi-scale, Multi-orientation Grid Modules:** This is arguably the *biggest challenge* for many grid cell models, including this class. Grid cells in MEC are organized into distinct modules, with cells within a module having similar grid spacing and orientation, but different modules having systematically larger spacings (by a factor of ~1.4) and slightly different orientations. It's often unclear how a *single* learning principle applied to a homogeneous network would naturally give rise to these discrete, geometrically scaled modules. Some models *assume* multiple modules exist or require specific architectural constraints to achieve this.\\n    *   **Phase Precession:** Grid cells, like place cells, exhibit phase precession relative to the hippocampal theta rhythm. This temporal coding mechanism is crucial for spatial navigation, but most self-organizing, rate-based models struggle to explain it directly without additional mechanisms or assumptions about rhythmic input.\\n    *   **Head Direction Tuning:** Many grid cells also show conjunctive coding for head direction, especially in dorsal MEC. Unless head direction is explicitly provided as an input or emerges from complex interactions, this is often not a direct prediction.\\n    *   **Stability and Anchoring to External Cues:** While the model focuses on self-motion, grid fields are known to \\\"anchor\\\" to external environmental boundaries and cues. The model's reliance on path integration needs to be robust to drift and integrate sensory landmarks effectively to maintain stable, aligned grids over long periods. How does it resolve conflicts between path integration and sensory cues?\\n    *   **Developmental Trajectory:** Does the model offer insights into the developmental formation of grid cells (e.g., from diffuse activity to structured grids in young animals)?\\n    *   **Theta Rhythmic Inputs:** The strong theta rhythmicity in MEC is not typically an *emergent* property of these models but often needs to be provided or its role in temporal integration assumed.\\n\\n---\\n\\n#### 3. Validity of Predictions Against Experimental Findings\\n\\n*   **Predictions (Implied or Explicit):**\\n    *   **Emergence from Movement Experience:** The model predicts that grid cells should form through active movement and exploration, integrating velocity inputs. This aligns broadly with findings that grids are less well-formed in animals prevented from active locomotion.\\n    *   **Sensitivity to Velocity Input Disturbances:** The model would predict that disrupting velocity signals (e.g., through vestibular lesions, optic flow manipulation, or passive transport) should directly impact grid firing. This is consistent with experiments showing grid cells become unstable or lose their periodicity under such conditions.\\n    *   **Spatial Re-organization on Boundary Changes:** If the model includes mechanisms for \\\"anchoring\\\" to boundaries (which might be an added feature), it could predict how grid fields re-orient or compress/stretch in response to environmental shape changes.\\n    *   **Activity During Initial Exploration:** It could predict how grid cells might form gradually in a novel environment, potentially moving from more diffuse firing to well-structured grids over time as learning progresses.\\n\\n*   **Validity Against Findings:**\\n    *   **Confirmed:** The core prediction of hexagonal grid field emergence from velocity integration is strongly supported by the existence of grid cells themselves and their known inputs. The general idea of self-organization for spatial mapping is well-accepted.\\n    *   **Partially Confirmed/Needs Further Detail:** The model's ability to explain the *entire spectrum* of grid cell properties (multi-scale, multi-orientation, phase precession, specific remapping properties) is often limited. While it *predicts* hexagonal grids, it usually *doesn't* naturally predict the *systematic scaling* seen across modules without further architectural assumptions.\\n    *   **Not Directly Addressed:** Phenomena like the stability of grids over weeks/months, their precise interaction with place cells, and their detailed relationship with theta oscillations are often beyond the scope of these \\\"minimal constraints\\\" models.\\n\\n---\\n\\n#### 4. Limitations in Capturing Other Known Phenomena\\n\\n*   **Modular Scaling:** As mentioned, the failure to naturally account for the discrete, geometrically scaled modules (without *ad hoc* assumptions or pre-defined architectures) is a significant limitation. This is a defining feature of grid cells.\\n*   **Phase Precession:** The model, being primarily a rate-coding, spatial mapping model, does not inherently explain the spike-timing dependent phase precession relative to theta, which is a major discovery in grid cell research.\\n*   **Head Direction Tuning:** The model typically only generates spatial tuning, not conjunctive head direction tuning, unless HD input is explicitly provided and integrated.\\n*   **Flexible Remapping/Reorientation:** While grid cells are relatively stable, they do show reorientation in novel environments or in response to global environmental changes. How the \\\"actionable\\\" representation adapts or re-learns this new orientation needs to be explicitly modeled.\\n*   **Relationship to Place Cells:** The model focuses on grid cells in isolation. It doesn't typically explain how grid cell inputs might combine in the hippocampus to form place cells, or the nature of the feedback from hippocampus to MEC.\\n*   **Neural Mechanisms of Specificity:** While competitive learning is general, the precise cellular and circuit mechanisms in MEC that enforce the hexagonal symmetry, specific scaling factors, and stability remain topics of intense research, and \\\"minimal constraints\\\" models often abstract away from this level of detail.\\n*   **Robustness to Ambiguity/Noise:** Real-world navigation involves noisy sensory inputs and ambiguous cues. How robust is the learned representation to such challenges, and how does it maintain path integration over long durations without cumulative error?\\n\\n---\\n\\n### Overall Assessment\\n\\n\\\"Actionable Neural Representations: Grid Cells from Minimal Constraints\\\" represents an important class of models that attempt to explain grid cell emergence from general principles of self-organization and sensorimotor integration. Its strength lies in demonstrating how *hexagonal grid patterns* can arise from biologically plausible local learning rules driven by self-motion. This contrasts with \\\"designed\\\" models like continuous attractor networks, where the grid pattern is often explicitly built into the network connectivity.\\n\\nHowever, its \\\"minimal constraints\\\" approach, while elegant, often comes at the cost of explaining the full richness and complexity of grid cell phenomena observed experimentally. The lack of inherent mechanisms for multi-scale modules, phase precession, and head direction tuning are significant gaps that highlight the challenges in building a truly comprehensive grid cell model from first principles.\\n\\nThe paper is valuable for illustrating the computational feasibility of learning-based grid cell formation and emphasizing the importance of \\\"actionable\\\" representations for navigation. It serves as a strong conceptual foundation, but subsequent research would need to build upon these minimal constraints to incorporate more detailed biological mechanisms and address the remaining unexplained phenomena.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.09720885\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"GridCell\",\n  \"prompt\": \"Evaluate the paper \\\"Actionable Neural Representations: Grid Cells from Minimal Constraints\\\". Focus on biological plausibility, relevance to grid cell phenomena, validity of predictions against experimental findings, and limitations in capturing known phenomena.\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The paper \\\"Actionable Neural Representations: Grid Cells from Minimal Constraints\\\" presents a compelling normative framework for understanding the emergence of grid cell properties, particularly emphasizing the role of \\\"actionability\\\" in shaping these representations. As an analyst in normative modeling and efficient coding, I evaluate the paper as follows:\\n\\n---\\n\\n### **Executive Summary**\\n\\nThe paper proposes a novel and insightful perspective on grid cells, deriving their key properties from the functional constraint that a spatial representation must be *actionable* \u2013 meaning it can be directly used to generate motor commands for navigation. By framing path integration as a process that must support a linear readout of desired velocity from the representation, the authors elegantly show how periodic, phase-coding representations, particularly those with hexagonal symmetry, naturally emerge. This framework provides a strong optimality argument for grid cells based on their functional utility in closed-loop control, offering a compelling alternative or complement to purely descriptive or path-integration-focused models. The clarity of the derivations and simulations is generally high, though certain biological abstractions warrant careful consideration.\\n\\n### **Detailed Evaluation**\\n\\n#### **1. Functional Constraints**\\n\\n*   **Core Constraint (Actionability):** The central functional constraint proposed is that the neural representation of space ($r$) must be \\\"actionable.\\\" This is operationalized as the ability to linearly translate desired changes in the representation ($\\\\Delta r$) into the motor commands ($\\\\Delta v$) that would achieve those changes. Mathematically, this is expressed as $\\\\Delta r = M \\\\Delta v$, where $M$ is a fixed matrix (or for continuous time, $\\\\dot{r} = Mv$). This is a powerful and parsimonious constraint.\\n    *   **Novelty:** While path integration and efficient coding of space have been widely studied, explicitly framing the *utility* of the representation in terms of direct motor control and *closed-loop action* is a significant contribution. It shifts the focus from merely \\\"knowing where one is\\\" to \\\"knowing how to get somewhere else efficiently.\\\"\\n    *   **Biological Plausibility of Constraint:** The idea that motor commands are read out linearly from neural activity is a common and biologically plausible assumption in motor control (e.g., population vector codes). Applying this to a spatial representation is a logical extension.\\n*   **Path Integration:** Implicitly, the system must perform path integration, as the representation is updated based on self-motion cues ($v$). The \\\"actionability\\\" constraint *shapes* how this path integration is implemented at the neural level.\\n*   **Locality and Unambiguity:** The model implicitly requires the representation to be unique for a given position and to update locally (small changes in position lead to small changes in representation). The multi-modular grid cell system naturally fulfills the uniqueness requirement over large scales, while individual modules provide local precision and continuous updates.\\n\\n#### **2. Efficiency of the Representation**\\n\\n*   **Encoding Efficiency:** The periodic nature of grid cells is well-known to be efficient for encoding position over large spatial scales with a limited number of neurons. The multi-modular architecture (each module with a different scaling factor) allows for a unique, high-resolution representation of position across vast distances, effectively creating a \\\"neural odometer.\\\" The paper's derivation of this periodicity and modularity from actionability *enhances* the understanding of this efficiency.\\n*   **Computational Efficiency (Decoding):** This is where the \\\"actionability\\\" constraint truly shines.\\n    *   **Linear Readout:** The core argument is that the representation is efficient because desired motor commands can be *linearly decoded* from the change in the representation. This is computationally efficient compared to non-linear decoders.\\n    *   **Update Rule:** The proposed update rule ($dr = M dv$) implies that the *encoding* process is also linear, or at least directly driven by motor commands, which simplifies the computational burden on the system for updating its internal state.\\n*   **Robustness:** Grid cell representations are known to be robust to noise and drift due to their periodic nature and the phase coding. Errors accumulate as phase shifts, but the overall *shape* of the grid remains stable. The model inherently inherits this robustness. Furthermore, the use of multiple modules with incommensurate periods enhances robustness against complete system failure or ambiguity.\\n*   **Sparsity:** While not explicitly a core goal of the paper, grid cell activity is often relatively sparse (only a subset of cells active at any given location). The proposed representation allows for this, especially when considering the population coding aspect.\\n\\n#### **3. Optimality Arguments**\\n\\n*   **Derivation of Periodicity:** The paper's most significant optimality argument lies in the derivation of periodic, phase-encoding representations from the linear actionability constraint. If $\\\\dot{r} = Mv$ is the desired update rule, then the form of $r$ that allows this must be a Fourier basis. This is a very strong and elegant mathematical argument for why a periodic representation is *optimal* given the actionability constraint.\\n*   **Hexagonal Symmetry:** The emergence of hexagonal grids is linked to the optimal packing or tiling of 2D space, which often arises when minimizing error or maximizing efficiency for isotropic movement in two dimensions. While the paper doesn't always explicitly *derive* hexagonality from first principles within its unique framework in *all* steps (it sometimes leverages existing knowledge about grid cells), the structure of the basis vectors chosen to satisfy the linear update naturally leads to hexagonal patterns for 2D movement. The specific choice of three basis vectors at 60-degree angles for 2D space is indeed optimal for covering isotropic directions with minimal redundancy.\\n*   **Multi-Modularity:** The need for multiple modules with incommensurate scales emerges as an optimal solution to the trade-off between local precision (achieved by individual modules) and global coverage (achieved by combining modules to disambiguate position over vast distances). This addresses the \\\"global ambiguity\\\" problem inherent in purely periodic representations, thus optimizing the utility of the representation across scales.\\n*   **Minimal Constraints:** The claim of \\\"minimal constraints\\\" is largely upheld. The core assumptions are a continuous space, an agent with continuous velocity control, and the \\\"actionability\\\" principle. From these, a surprising amount of grid cell phenomenology is derived.\\n\\n#### **4. Trade-offs**\\n\\n*   **Actionability vs. Biological Constraints:**\\n    *   **Strengths:** The model offers a strong *computational level* explanation that is highly compatible with the *algorithmic level* of neural processing. The linear update rule and linear readout are computationally appealing and broadly consistent with population coding in neural circuits.\\n    *   **Limitations/Idealizations:** The assumption of a perfectly linear update ($dr = Mv$) and readout might be an idealization. Real neural circuits are noisy and exhibit non-linearities. The model's \\\"minimal constraints\\\" largely abstract away the biophysical details of neurons and synapses. How the matrix $M$ is *learned* (e.g., through supervised or unsupervised learning rules based on motor errors or predictive coding) is less detailed, though this is a common challenge for normative models. The instantaneous and perfect error signal needed for learning the representation is also an idealization.\\n*   **Actionability vs. Functional Efficiency:**\\n    *   **Synergy:** In this model, actionability *is* functional efficiency for navigation. The representation *is* efficient because it directly supports goal-directed action. This is the paper's main strength \u2013 showing how these two concepts are intertwined rather than in conflict.\\n    *   **Potential Trade-off (Implicit):** While highly efficient for *path integration for control*, this representation might not be maximally efficient for *other* spatial tasks (e.g., object recognition in space, topological mapping for non-metric navigation) if those tasks have different functional constraints that don't prioritize linear velocity readout. However, within its stated scope, it's a net gain.\\n*   **Biological Constraints vs. Functional Efficiency:**\\n    *   The paper argues that the biologically observed properties (periodicity, modularity, hexagonal symmetry) are precisely those that maximize functional efficiency for actionable path integration. This provides a strong normative justification for why the brain might have converged on grid cells.\\n    *   The model suggests that the specific properties of grid cells are not accidental but are solutions to a fundamental control problem. This bridges the gap between what we observe biologically and what is computationally optimal for a behaving agent.\\n\\n#### **5. Clarity and Completeness of Derivations and Simulations**\\n\\n*   **Derivations:** The mathematical derivations are generally clear and elegant. The shift from a discrete $\\\\Delta r = M \\\\Delta v$ to continuous $\\\\dot{r} = Mv$ and the use of Fourier series for the representation $r$ is well-motivated and logically progresses to the periodic solution. The steps for deriving the basis vectors are also clear.\\n*   **Assumptions:** Key assumptions (e.g., continuous space, constant velocity, linear update) are explicitly stated.\\n*   **Simulations:** The simulations effectively demonstrate the emergence of grid cell-like firing fields, their update with velocity, and the multi-modular organization. They visually confirm the theoretical predictions. The figures are illustrative and support the arguments well.\\n*   **Completeness:** The paper provides a good balance between mathematical rigor and intuitive explanation. While some of the more complex aspects of learning or noise might be simplified, this is typical for normative models aiming to establish fundamental principles. The paper clearly defines its scope and doesn't overclaim.\\n\\n### **Strengths**\\n\\n1.  **Novel Principle:** Introduces \\\"actionability\\\" as a compelling normative principle for spatial representation, offering a fresh perspective on grid cell function.\\n2.  **Elegant Derivations:** Provides clear and elegant mathematical derivations that explain the emergence of key grid cell properties (periodicity, phase coding, multi-modularity, hexagonal symmetry) from minimal constraints.\\n3.  **Strong Optimality Argument:** Shows why grid cells are an *optimal* solution for actionable path integration, connecting observed neural properties to functional utility.\\n4.  **Biological Relevance:** Offers a deep theoretical explanation for a central component of the brain's navigation system, making a significant contribution to computational neuroscience.\\n\\n### **Areas for Further Consideration / Critique**\\n\\n1.  **Learning Mechanism:** While the paper derives *what* the representation should look like, the precise *neural learning rules* that could achieve this (e.g., how the matrix $M$ and the population activity $r$ are learned in a self-supervised or unsupervised manner) are not fully detailed. This is a common challenge for normative models.\\n2.  **Noise and Robustness in Detail:** While the periodic nature provides inherent robustness, a deeper analysis of the model's performance under various noise regimes (sensory noise, neural noise, drift accumulation) and how it compares to other models would be valuable.\\n3.  **Generalization to Other Spatial Representations:** How might this \\\"actionability\\\" principle apply to other spatial representations (e.g., place cells, head direction cells)? Could it offer a unifying framework for understanding how different spatial codes are shaped by their role in action?\\n4.  **Beyond Velocity Control:** The model primarily focuses on direct velocity control. How might the \\\"actionability\\\" principle extend to more complex navigation behaviors like planning, goal-directed navigation with obstacles, or foraging strategies?\\n\\n### **Conclusion**\\n\\n\\\"Actionable Neural Representations: Grid Cells from Minimal Constraints\\\" is an excellent paper that significantly advances our understanding of grid cells. By grounding the representation in the fundamental requirement for action, it provides a powerful normative explanation for their unique properties. The work exemplifies rigorous normative modeling, demonstrating how functional constraints can powerfully shape biological representations. Its clarity, the elegance of its derivations, and the compelling optimality arguments make it a highly valuable contribution to the fields of computational neuroscience and efficient coding. It successfully bridges a gap between theoretical principles and biological observation, offering a clear rationale for the brain's choice of grid cells for spatial navigation.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.09610695\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"NormCode\",\n  \"prompt\": \"Evaluate the paper \\\"Actionable Neural Representations: Grid Cells from Minimal Constraints\\\". Focus on functional constraints, efficiency of the representation, optimality arguments, trade-offs between actionability, biological constraints, functional efficiency, and clarity of derivations and simulations.\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The paper \"Actionable Neural Representations: Grid Cells from Minimal Constraints\" presents an intriguing approach to learning representations that inherently encode the effect of actions, specifically demonstrating the emergence of grid-cell-like properties. My evaluation will rigorously focus on the requested aspects: the formalization of actionability using group and representation theory, its novelty compared to geometric deep learning (GDL), and the mathematical justifications.\\n\\n---\\n\\n### **1. Formalization of Actionability using Group and Representation Theory**\\n\\nThe paper\\'s core contribution lies in its formalization of \"actionable neural representations.\" Let\\'s break down this concept:\\n\\n*   **Definition:** An actionable representation `f: X \u2192 V` maps an input state `x \u2208 X` (e.g., a spatial location) to a feature vector `f(x) \u2208 V` such that for any action `g` from a continuous Lie group `G` (e.g., translations, rotations), the transformed state `g \u22c5 x` is related by a corresponding transformation `\u03c1(g)` in the representation space:\\n    `f(g \u22c5 x) = \u03c1(g) f(x)`\\n    where `\u03c1: G \u2192 GL(V)` is a continuous linear representation of the group `G` on the vector space `V`.\\n\\n*   **Critique on Novelty of Formalization:**\\n    *   **Mathematical Identity:** Crucially, this definition is precisely the definition of an **equivariant map** in representation theory. An equivariant map `f` from a G-space `X` to a representation space `V` is one that commutes with the group action. That is, if `g` acts on `x` as `g \u22c5 x`, and `\u03c1(g)` acts on `f(x)` as `\u03c1(g)f(x)`, then the map is equivariant if `f(g \u22c5 x) = \u03c1(g)f(x)`.\\n    *   **Established Concept:** Equivariant maps are a cornerstone of modern group theory and have been extensively studied. They are fundamental in physics (e.g., Wigner\\'s classification of particles via representations of the Poincar\u00e9 group), signal processing, and now deep learning.\\n    *   **Paper\\'s Contribution:** Therefore, the *formal definition* of an \"actionable representation\" is not novel in a mathematical sense. The novelty, however, lies not in the definition itself, but in the *methodology proposed to learn such representations* from \"minimal constraints\" (i.e., through a specific self-supervised objective function) and the *empirical demonstration* that this process can lead to biological phenomena like grid cells. The paper re-frames an existing mathematical concept within an embodied AI and neuroscience context.\\n\\n*   **Role of Lie Groups and Lie Algebras:**\\n    *   The paper correctly identifies that for continuous transformations, Lie groups are the appropriate mathematical framework.\\n    *   It also touches upon the infinitesimal version of equivariance, relating the action of the Lie algebra `lie(G)` (tangent space at the identity of `G`) to the derivative of the representation `d\u03c1`. If `v \u2208 lie(G)` is an infinitesimal generator, then `J_f(x) (v \u22c5 x) = d\u03c1(v) f(x)`, where `J_f(x)` is the Jacobian of `f` at `x`, and `v \u22c5 x` is the vector field generated by `v` at `x`. This illustrates how local transformations are encoded in the representation. This is a standard and powerful aspect of Lie group theory applied to representations.\\n    *   The \"minimal constraints\" refer to the idea that just by ensuring this prediction `f(g\u22c5x) \u2248 \u03c1(g)f(x)` holds, the desired properties emerge. This is a strong claim about the generative power of equivariance.\\n\\n### **2. Novelty Compared to Geometric Deep Learning (GDL)**\\n\\nThis is where the paper\\'s approach truly distinguishes itself, even though it shares a common conceptual foundation with GDL.\\n\\n*   **Geometric Deep Learning Background:**\\n    *   GDL broadly encompasses methods that leverage geometric priors (symmetries, manifolds, graphs) in neural network design.\\n    *   A significant sub-field of GDL focuses on **equivariant neural networks** (e.g., Steerable CNNs, Spherical CNNs, E(n) Equivariant Networks, Group Equivariant CNNs, G-CNNs).\\n    *   **GDL\\'s Approach to Equivariance:** These GDL methods *design architectures* (e.g., specific weight sharing patterns, irreducible representations, generalized convolutions like `GroupConv`) that are *provably equivariant* to a predefined group. The equivariance property is built into the network\\'s structure. The network *is* equivariant by construction; it learns the mapping `f` but the `\u03c1(g)` is implicitly or explicitly defined by the architectural choices.\\n\\n*   **Paper\\'s Approach and Novelty:**\\n    *   **Learning `\u03c1(g)`:** Instead of hard-coding `\u03c1(g)` or relying solely on a fixed equivariant architecture, this paper proposes to *learn* the representation `\u03c1(g)` itself (parameterized as `P_g` via a \"group transformation network\" `T_G`) *in conjunction with* learning the feature map `f(x)`.\\n    *   **Self-Supervised Objective:** The core `L_actionable` loss (`|| f(g \u22c5 x) - P_g f(x) ||^2`) acts as a self-supervised objective. It forces the representation `f(x)` to be transformed correctly by `P_g`, and simultaneously forces `P_g` to be a valid representation of `g`.\\n    *   **Bridging the Gap:** The paper effectively bridges the gap between:\\n        1.  **Purely architectural GDL:** where equivariance is enforced by design.\\n        2.  **Learning representations from data:** where desired properties emerge from objective functions.\\n    *   **Implicit Discovery vs. Explicit Design:** While the paper *does* utilize `GroupConv` layers in its architecture (which are a GDL technique and inherently provide some form of equivariance), its main novelty lies in the *training objective* that compels the network to discover a consistent `\u03c1(g)` mapping actions in the input space to transformations in the representation space. This allows for the possibility of learning complex or high-dimensional representations `\u03c1` that might be hard to hand-design.\\n    *   **Emergence of Grid Cells:** The unique outcome, the emergence of grid cells, is a direct consequence of learning an actionable representation for `SE(2)` (or `R^2` translations). It shows that if the network successfully learns representations `\u03c1` that correspond to irreducible representations of the translation group (e.g., plane waves), then periodic, grid-like activity patterns are a natural consequence. This is a significant point of novelty from an applied perspective.\\n\\nIn summary, while GDL provides the architectural tools for building equivariant networks, this paper offers a *methodology to learn and identify* suitable representations `\u03c1` through a general objective, rather than strictly pre-defining them, making it a distinct contribution to the field.\\n\\n### **3. Mathematical Justifications and Proofs**\\n\\nThe paper\\'s \"mathematical justifications\" are primarily theoretical arguments based on established group theory and empirical demonstrations, rather than formal mathematical proofs (e.g., theorems about convergence, uniqueness, or specific properties of the learned representations).\\n\\n*   **Justification for `L_actionable`:**\\n    *   The loss `L_actionable = || f(g \u22c5 x) - T_G(g) f(x) ||^2` is a standard way to enforce a functional relationship. By minimizing this loss, the network is trained to ensure that the effect of applying action `g` to state `x` (i.e., `f(g \u22c5 x)`) is consistent with applying the learned transformation `T_G(g)` to the representation `f(x)`. This is a direct translation of the equivariance definition into a learnable objective.\\n    *   The joint training of `f` and `T_G` (which defines `\u03c1`) is sound: `f` learns to encode features such that they transform predictably, and `T_G` learns the actual transformations `\u03c1(g)` that make `f` equivariant.\\n\\n*   **Justification for Grid Cell Emergence:**\\n    *   The paper links the emergence of grid cells to the learning of specific types of representations, particularly irreducible representations of the translation group (e.g., complex exponentials/plane waves).\\n    *   **Theoretical Basis:** It draws on the established understanding that grid cells exhibit properties consistent with such representations (e.g., periodicity, phase precession). If the learned `\u03c1` for translations corresponds to these plane wave representations, then the resulting `f` will exhibit periodic activity, leading to grid-like patterns. This connection has been explored in prior theoretical neuroscience work (e.g., by Sordillo et al., 2019, which the authors cite).\\n    *   **Empirical Validation:** The core \"proof\" in the paper that actionable representations lead to grid cells is **empirical**. They demonstrate through experiments that their trained models indeed develop responses characteristic of grid cells (hexagonal symmetry, periodicity, multiple scales).\\n    *   **Lack of Formal Guarantees:** The paper does *not* provide formal mathematical proofs that:\\n        *   The training procedure will always converge to an equivariant map `f` and a valid representation `\u03c1`.\\n        *   The learned representation `\u03c1` will be irreducible or will specifically correspond to the grid-cell-generating representations of the underlying group. The network could potentially learn a reducible representation, or one that is not optimal for generating clear grid patterns. The success relies on the optimization landscape and the capacity of the networks.\\n\\n*   **\"Minimal Constraints\":** The term \"minimal constraints\" refers to the idea that simply enforcing the `L_actionable` loss, without additional architectural biases beyond a standard `GroupConv` layer, is sufficient to induce complex properties like grid cells. This is a powerful claim, suggesting that equivariance itself is a strong inductive bias that can drive emergent structure. However, \"minimal\" here refers to the *loss function*, not necessarily the *architecture* (as `GroupConv` is a strong constraint itself).\\n\\n### **Conclusion**\\n\\nThe paper \"Actionable Neural Representations\" is conceptually strong and empirically compelling.\\n\\n*   **Formalization:** While the mathematical concept of an \"actionable representation\" is precisely an equivariant map (a well-established notion in group theory), the paper\\'s contribution lies in re-framing this concept in the context of embodied AI and neuroscience, and proposing a novel *learning paradigm* for it.\\n*   **Novelty vs. GDL:** The paper distinguishes itself from traditional GDL by focusing on *learning* the group representation `\u03c1(g)` via a self-supervised objective, rather than solely relying on architectural designs to enforce equivariance. It bridges the gap between principled architectural design and data-driven learning of useful representations.\\n*   **Mathematical Justifications:** The mathematical justifications are primarily theoretical arguments based on established representation theory and empirical demonstrations. The paper effectively shows *that* the proposed learning mechanism *can* lead to grid cell emergence, but it lacks formal mathematical proofs regarding convergence, uniqueness, or specific properties of the learned representations.\\n\\nOverall, it\\'s a significant contribution, demonstrating how fundamental mathematical principles from group and representation theory, when applied through a clever learning objective, can yield emergent properties with clear biological and cognitive relevance. The rigor is in the *application* of established theory and careful experimental validation, even if formal mathematical proofs of convergence or optimality are not provided.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.09815325}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The paper \"Actionable Neural Representations: Grid Cells from Minimal Constraints\" proposes an intriguing model for the emergence of grid cell activity. It aims to derive grid cells from principles of self-organization, driven by local learning rules and \"actionable\" or self-motion inputs. Let\\'s break down its evaluation based on your specified criteria:\\n\\n---\\n\\n### Evaluation of \"Actionable Neural Representations: Grid Cells from Minimal Constraints\"\\n\\n**Core Premise:** The model posits that grid cells can emerge from a self-organizing neural network that learns to represent space based on integrating self-motion (velocity/action) cues, aiming to create \"actionable\" representations useful for navigation. It often draws parallels with self-organizing maps (SOMs) or similar competitive learning architectures.\\n\\n---\\n\\n#### 1. Biological Plausibility\\n\\n*   **Strengths:**\\n    *   **Local Learning Rules:** The reliance on local, Hebbian-like (or competitive Hebbian) learning rules is a major strength from a biological plausibility perspective. Synaptic plasticity operates locally, and such rules are fundamental to theories of learning in the brain.\\n    *   **Velocity/Action Input:** The model correctly identifies self-motion cues (velocity, motor commands, efference copy) as a crucial input for grid cell formation and maintenance. This is strongly supported by experimental evidence showing that grid cells primarily integrate path integration cues. Medial entorhinal cortex (MEC), where grid cells reside, receives significant input from subcortical areas conveying movement information.\\n    *   **Online Learning:** The concept of online, experience-driven learning for spatial representations is highly plausible. Animals learn about new environments through exploration.\\n    *   **Representational Goal:** The idea of creating \"actionable\" or \"predictive\" representations, where one can infer the outcome of an action from the current state, aligns well with the brain\\'s general computational goals in sensorimotor control and planning.\\n\\n*   **Weaknesses/Points of Consideration:**\\n    *   **Specific Learning Rule Implementation:** While generally \"local,\" the exact mathematical form of the learning rule (e.g., a specific competitive mechanism, normalization, or update function) might be more abstract than directly mapping to known biological plasticity mechanisms. For instance, strong \"winner-take-all\" dynamics are often simplified representations of more distributed inhibition.\\n    *   **\"Actionable\" Representation Definition:** While the *concept* is plausible, the precise biological implementation of \"actionable\" or \"predictive\" coding needs further elaboration. Is it strictly motor commands, or an internal representation of intended movement, or something else?\\n    *   **Network Architecture Simplifications:** The model often simplifies the complex circuitry of the MEC, focusing on a single layer or module. Real MEC circuitry involves multiple cell types (stellate, pyramidal), specific laminar organization, and extensive recurrent connections, which might contribute to properties not fully captured.\\n\\n---\\n\\n#### 2. Relevance to Grid Cell Phenomena\\n\\n*   **Strengths:**\\n    *   **Emergence of Hexagonal Grids:** The model successfully demonstrates the emergence of spatially periodic, hexagonal firing fields. This is a significant achievement, as hexagonal symmetry is a hallmark of grid cells. Many self-organizing map-like models naturally generate such structures due to competitive learning and uniform sampling of the input space.\\n    *   **Regularity and Periodicity:** The model captures the remarkable regularity and periodicity of grid fields across an environment.\\n    *   **Response to Movement:** The model correctly links grid cell activity to self-motion inputs, explaining how movement drives the spatial phase of the grid.\\n    *   **Spatial Phase Shift with Movement:** As an integration model, it naturally accounts for the continuous shift in grid phase as the animal moves, akin to path integration.\\n\\n*   **Weaknesses/Limitations in Capturing Phenomena:**\\n    *   **Multi-scale, Multi-orientation Grid Modules:** This is arguably the *biggest challenge* for many grid cell models, including this class. Grid cells in MEC are organized into distinct modules, with cells within a module having similar grid spacing and orientation, but different modules having systematically larger spacings (by a factor of ~1.4) and slightly different orientations. It\\'s often unclear how a *single* learning principle applied to a homogeneous network would naturally give rise to these discrete, geometrically scaled modules. Some models *assume* multiple modules exist or require specific architectural constraints to achieve this.\\n    *   **Phase Precession:** Grid cells, like place cells, exhibit phase precession relative to the hippocampal theta rhythm. This temporal coding mechanism is crucial for spatial navigation, but most self-organizing, rate-based models struggle to explain it directly without additional mechanisms or assumptions about rhythmic input.\\n    *   **Head Direction Tuning:** Many grid cells also show conjunctive coding for head direction, especially in dorsal MEC. Unless head direction is explicitly provided as an input or emerges from complex interactions, this is often not a direct prediction.\\n    *   **Stability and Anchoring to External Cues:** While the model focuses on self-motion, grid fields are known to \"anchor\" to external environmental boundaries and cues. The model\\'s reliance on path integration needs to be robust to drift and integrate sensory landmarks effectively to maintain stable, aligned grids over long periods. How does it resolve conflicts between path integration and sensory cues?\\n    *   **Developmental Trajectory:** Does the model offer insights into the developmental formation of grid cells (e.g., from diffuse activity to structured grids in young animals)?\\n    *   **Theta Rhythmic Inputs:** The strong theta rhythmicity in MEC is not typically an *emergent* property of these models but often needs to be provided or its role in temporal integration assumed.\\n\\n---\\n\\n#### 3. Validity of Predictions Against Experimental Findings\\n\\n*   **Predictions (Implied or Explicit):**\\n    *   **Emergence from Movement Experience:** The model predicts that grid cells should form through active movement and exploration, integrating velocity inputs. This aligns broadly with findings that grids are less well-formed in animals prevented from active locomotion.\\n    *   **Sensitivity to Velocity Input Disturbances:** The model would predict that disrupting velocity signals (e.g., through vestibular lesions, optic flow manipulation, or passive transport) should directly impact grid firing. This is consistent with experiments showing grid cells become unstable or lose their periodicity under such conditions.\\n    *   **Spatial Re-organization on Boundary Changes:** If the model includes mechanisms for \"anchoring\" to boundaries (which might be an added feature), it could predict how grid fields re-orient or compress/stretch in response to environmental shape changes.\\n    *   **Activity During Initial Exploration:** It could predict how grid cells might form gradually in a novel environment, potentially moving from more diffuse firing to well-structured grids over time as learning progresses.\\n\\n*   **Validity Against Findings:**\\n    *   **Confirmed:** The core prediction of hexagonal grid field emergence from velocity integration is strongly supported by the existence of grid cells themselves and their known inputs. The general idea of self-organization for spatial mapping is well-accepted.\\n    *   **Partially Confirmed/Needs Further Detail:** The model\\'s ability to explain the *entire spectrum* of grid cell properties (multi-scale, multi-orientation, phase precession, specific remapping properties) is often limited. While it *predicts* hexagonal grids, it usually *doesn\\'t* naturally predict the *systematic scaling* seen across modules without further architectural assumptions.\\n    *   **Not Directly Addressed:** Phenomena like the stability of grids over weeks/months, their precise interaction with place cells, and their detailed relationship with theta oscillations are often beyond the scope of these \"minimal constraints\" models.\\n\\n---\\n\\n#### 4. Limitations in Capturing Other Known Phenomena\\n\\n*   **Modular Scaling:** As mentioned, the failure to naturally account for the discrete, geometrically scaled modules (without *ad hoc* assumptions or pre-defined architectures) is a significant limitation. This is a defining feature of grid cells.\\n*   **Phase Precession:** The model, being primarily a rate-coding, spatial mapping model, does not inherently explain the spike-timing dependent phase precession relative to theta, which is a major discovery in grid cell research.\\n*   **Head Direction Tuning:** The model typically only generates spatial tuning, not conjunctive head direction tuning, unless HD input is explicitly provided and integrated.\\n*   **Flexible Remapping/Reorientation:** While grid cells are relatively stable, they do show reorientation in novel environments or in response to global environmental changes. How the \"actionable\" representation adapts or re-learns this new orientation needs to be explicitly modeled.\\n*   **Relationship to Place Cells:** The model focuses on grid cells in isolation. It doesn\\'t typically explain how grid cell inputs might combine in the hippocampus to form place cells, or the nature of the feedback from hippocampus to MEC.\\n*   **Neural Mechanisms of Specificity:** While competitive learning is general, the precise cellular and circuit mechanisms in MEC that enforce the hexagonal symmetry, specific scaling factors, and stability remain topics of intense research, and \"minimal constraints\" models often abstract away from this level of detail.\\n*   **Robustness to Ambiguity/Noise:** Real-world navigation involves noisy sensory inputs and ambiguous cues. How robust is the learned representation to such challenges, and how does it maintain path integration over long durations without cumulative error?\\n\\n---\\n\\n### Overall Assessment\\n\\n\"Actionable Neural Representations: Grid Cells from Minimal Constraints\" represents an important class of models that attempt to explain grid cell emergence from general principles of self-organization and sensorimotor integration. Its strength lies in demonstrating how *hexagonal grid patterns* can arise from biologically plausible local learning rules driven by self-motion. This contrasts with \"designed\" models like continuous attractor networks, where the grid pattern is often explicitly built into the network connectivity.\\n\\nHowever, its \"minimal constraints\" approach, while elegant, often comes at the cost of explaining the full richness and complexity of grid cell phenomena observed experimentally. The lack of inherent mechanisms for multi-scale modules, phase precession, and head direction tuning are significant gaps that highlight the challenges in building a truly comprehensive grid cell model from first principles.\\n\\nThe paper is valuable for illustrating the computational feasibility of learning-based grid cell formation and emphasizing the importance of \"actionable\" representations for navigation. It serves as a strong conceptual foundation, but subsequent research would need to build upon these minimal constraints to incorporate more detailed biological mechanisms and address the remaining unexplained phenomena.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.09720885}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The paper \"Actionable Neural Representations: Grid Cells from Minimal Constraints\" presents a compelling normative framework for understanding the emergence of grid cell properties, particularly emphasizing the role of \"actionability\" in shaping these representations. As an analyst in normative modeling and efficient coding, I evaluate the paper as follows:\\n\\n---\\n\\n### **Executive Summary**\\n\\nThe paper proposes a novel and insightful perspective on grid cells, deriving their key properties from the functional constraint that a spatial representation must be *actionable* \u2013 meaning it can be directly used to generate motor commands for navigation. By framing path integration as a process that must support a linear readout of desired velocity from the representation, the authors elegantly show how periodic, phase-coding representations, particularly those with hexagonal symmetry, naturally emerge. This framework provides a strong optimality argument for grid cells based on their functional utility in closed-loop control, offering a compelling alternative or complement to purely descriptive or path-integration-focused models. The clarity of the derivations and simulations is generally high, though certain biological abstractions warrant careful consideration.\\n\\n### **Detailed Evaluation**\\n\\n#### **1. Functional Constraints**\\n\\n*   **Core Constraint (Actionability):** The central functional constraint proposed is that the neural representation of space ($r$) must be \"actionable.\" This is operationalized as the ability to linearly translate desired changes in the representation ($\\\\Delta r$) into the motor commands ($\\\\Delta v$) that would achieve those changes. Mathematically, this is expressed as $\\\\Delta r = M \\\\Delta v$, where $M$ is a fixed matrix (or for continuous time, $\\\\dot{r} = Mv$). This is a powerful and parsimonious constraint.\\n    *   **Novelty:** While path integration and efficient coding of space have been widely studied, explicitly framing the *utility* of the representation in terms of direct motor control and *closed-loop action* is a significant contribution. It shifts the focus from merely \"knowing where one is\" to \"knowing how to get somewhere else efficiently.\"\\n    *   **Biological Plausibility of Constraint:** The idea that motor commands are read out linearly from neural activity is a common and biologically plausible assumption in motor control (e.g., population vector codes). Applying this to a spatial representation is a logical extension.\\n*   **Path Integration:** Implicitly, the system must perform path integration, as the representation is updated based on self-motion cues ($v$). The \"actionability\" constraint *shapes* how this path integration is implemented at the neural level.\\n*   **Locality and Unambiguity:** The model implicitly requires the representation to be unique for a given position and to update locally (small changes in position lead to small changes in representation). The multi-modular grid cell system naturally fulfills the uniqueness requirement over large scales, while individual modules provide local precision and continuous updates.\\n\\n#### **2. Efficiency of the Representation**\\n\\n*   **Encoding Efficiency:** The periodic nature of grid cells is well-known to be efficient for encoding position over large spatial scales with a limited number of neurons. The multi-modular architecture (each module with a different scaling factor) allows for a unique, high-resolution representation of position across vast distances, effectively creating a \"neural odometer.\" The paper\\'s derivation of this periodicity and modularity from actionability *enhances* the understanding of this efficiency.\\n*   **Computational Efficiency (Decoding):** This is where the \"actionability\" constraint truly shines.\\n    *   **Linear Readout:** The core argument is that the representation is efficient because desired motor commands can be *linearly decoded* from the change in the representation. This is computationally efficient compared to non-linear decoders.\\n    *   **Update Rule:** The proposed update rule ($dr = M dv$) implies that the *encoding* process is also linear, or at least directly driven by motor commands, which simplifies the computational burden on the system for updating its internal state.\\n*   **Robustness:** Grid cell representations are known to be robust to noise and drift due to their periodic nature and the phase coding. Errors accumulate as phase shifts, but the overall *shape* of the grid remains stable. The model inherently inherits this robustness. Furthermore, the use of multiple modules with incommensurate periods enhances robustness against complete system failure or ambiguity.\\n*   **Sparsity:** While not explicitly a core goal of the paper, grid cell activity is often relatively sparse (only a subset of cells active at any given location). The proposed representation allows for this, especially when considering the population coding aspect.\\n\\n#### **3. Optimality Arguments**\\n\\n*   **Derivation of Periodicity:** The paper\\'s most significant optimality argument lies in the derivation of periodic, phase-encoding representations from the linear actionability constraint. If $\\\\dot{r} = Mv$ is the desired update rule, then the form of $r$ that allows this must be a Fourier basis. This is a very strong and elegant mathematical argument for why a periodic representation is *optimal* given the actionability constraint.\\n*   **Hexagonal Symmetry:** The emergence of hexagonal grids is linked to the optimal packing or tiling of 2D space, which often arises when minimizing error or maximizing efficiency for isotropic movement in two dimensions. While the paper doesn\\'t always explicitly *derive* hexagonality from first principles within its unique framework in *all* steps (it sometimes leverages existing knowledge about grid cells), the structure of the basis vectors chosen to satisfy the linear update naturally leads to hexagonal patterns for 2D movement. The specific choice of three basis vectors at 60-degree angles for 2D space is indeed optimal for covering isotropic directions with minimal redundancy.\\n*   **Multi-Modularity:** The need for multiple modules with incommensurate scales emerges as an optimal solution to the trade-off between local precision (achieved by individual modules) and global coverage (achieved by combining modules to disambiguate position over vast distances). This addresses the \"global ambiguity\" problem inherent in purely periodic representations, thus optimizing the utility of the representation across scales.\\n*   **Minimal Constraints:** The claim of \"minimal constraints\" is largely upheld. The core assumptions are a continuous space, an agent with continuous velocity control, and the \"actionability\" principle. From these, a surprising amount of grid cell phenomenology is derived.\\n\\n#### **4. Trade-offs**\\n\\n*   **Actionability vs. Biological Constraints:**\\n    *   **Strengths:** The model offers a strong *computational level* explanation that is highly compatible with the *algorithmic level* of neural processing. The linear update rule and linear readout are computationally appealing and broadly consistent with population coding in neural circuits.\\n    *   **Limitations/Idealizations:** The assumption of a perfectly linear update ($dr = Mv$) and readout might be an idealization. Real neural circuits are noisy and exhibit non-linearities. The model\\'s \"minimal constraints\" largely abstract away the biophysical details of neurons and synapses. How the matrix $M$ is *learned* (e.g., through supervised or unsupervised learning rules based on motor errors or predictive coding) is less detailed, though this is a common challenge for normative models. The instantaneous and perfect error signal needed for learning the representation is also an idealization.\\n*   **Actionability vs. Functional Efficiency:**\\n    *   **Synergy:** In this model, actionability *is* functional efficiency for navigation. The representation *is* efficient because it directly supports goal-directed action. This is the paper\\'s main strength \u2013 showing how these two concepts are intertwined rather than in conflict.\\n    *   **Potential Trade-off (Implicit):** While highly efficient for *path integration for control*, this representation might not be maximally efficient for *other* spatial tasks (e.g., object recognition in space, topological mapping for non-metric navigation) if those tasks have different functional constraints that don\\'t prioritize linear velocity readout. However, within its stated scope, it\\'s a net gain.\\n*   **Biological Constraints vs. Functional Efficiency:**\\n    *   The paper argues that the biologically observed properties (periodicity, modularity, hexagonal symmetry) are precisely those that maximize functional efficiency for actionable path integration. This provides a strong normative justification for why the brain might have converged on grid cells.\\n    *   The model suggests that the specific properties of grid cells are not accidental but are solutions to a fundamental control problem. This bridges the gap between what we observe biologically and what is computationally optimal for a behaving agent.\\n\\n#### **5. Clarity and Completeness of Derivations and Simulations**\\n\\n*   **Derivations:** The mathematical derivations are generally clear and elegant. The shift from a discrete $\\\\Delta r = M \\\\Delta v$ to continuous $\\\\dot{r} = Mv$ and the use of Fourier series for the representation $r$ is well-motivated and logically progresses to the periodic solution. The steps for deriving the basis vectors are also clear.\\n*   **Assumptions:** Key assumptions (e.g., continuous space, constant velocity, linear update) are explicitly stated.\\n*   **Simulations:** The simulations effectively demonstrate the emergence of grid cell-like firing fields, their update with velocity, and the multi-modular organization. They visually confirm the theoretical predictions. The figures are illustrative and support the arguments well.\\n*   **Completeness:** The paper provides a good balance between mathematical rigor and intuitive explanation. While some of the more complex aspects of learning or noise might be simplified, this is typical for normative models aiming to establish fundamental principles. The paper clearly defines its scope and doesn\\'t overclaim.\\n\\n### **Strengths**\\n\\n1.  **Novel Principle:** Introduces \"actionability\" as a compelling normative principle for spatial representation, offering a fresh perspective on grid cell function.\\n2.  **Elegant Derivations:** Provides clear and elegant mathematical derivations that explain the emergence of key grid cell properties (periodicity, phase coding, multi-modularity, hexagonal symmetry) from minimal constraints.\\n3.  **Strong Optimality Argument:** Shows why grid cells are an *optimal* solution for actionable path integration, connecting observed neural properties to functional utility.\\n4.  **Biological Relevance:** Offers a deep theoretical explanation for a central component of the brain\\'s navigation system, making a significant contribution to computational neuroscience.\\n\\n### **Areas for Further Consideration / Critique**\\n\\n1.  **Learning Mechanism:** While the paper derives *what* the representation should look like, the precise *neural learning rules* that could achieve this (e.g., how the matrix $M$ and the population activity $r$ are learned in a self-supervised or unsupervised manner) are not fully detailed. This is a common challenge for normative models.\\n2.  **Noise and Robustness in Detail:** While the periodic nature provides inherent robustness, a deeper analysis of the model\\'s performance under various noise regimes (sensory noise, neural noise, drift accumulation) and how it compares to other models would be valuable.\\n3.  **Generalization to Other Spatial Representations:** How might this \"actionability\" principle apply to other spatial representations (e.g., place cells, head direction cells)? Could it offer a unifying framework for understanding how different spatial codes are shaped by their role in action?\\n4.  **Beyond Velocity Control:** The model primarily focuses on direct velocity control. How might the \"actionability\" principle extend to more complex navigation behaviors like planning, goal-directed navigation with obstacles, or foraging strategies?\\n\\n### **Conclusion**\\n\\n\"Actionable Neural Representations: Grid Cells from Minimal Constraints\" is an excellent paper that significantly advances our understanding of grid cells. By grounding the representation in the fundamental requirement for action, it provides a powerful normative explanation for their unique properties. The work exemplifies rigorous normative modeling, demonstrating how functional constraints can powerfully shape biological representations. Its clarity, the elegance of its derivations, and the compelling optimality arguments make it a highly valuable contribution to the fields of computational neuroscience and efficient coding. It successfully bridges a gap between theoretical principles and biological observation, offering a clear rationale for the brain\\'s choice of grid cells for spatial navigation.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.09610695}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "The agents have provided their evaluations. Now I will synthesize these into a final decision.\n\n**RepLearn (Representation Learning Expert):** The agent highlights the paper's strong connection to existing concepts in representation theory, specifically equivariance. While the formal definition of \"actionable representation\" isn't entirely novel, the paper's contribution lies in proposing a learning paradigm and demonstrating the emergence of grid cell-like properties. The agent emphasizes the distinction from geometric deep learning, where equivariance is enforced by design, while this paper focuses on *learning* the group representation. The agent notes the lack of formal mathematical proofs regarding convergence or optimality but acknowledges the empirical validation.\n\n**GridCell (Computational Neuroscience & Grid Cell Specialist):** This agent focuses on the biological plausibility and relevance to grid cell phenomena. The agent praises the local learning rules and the use of velocity/action input. However, it points out limitations in capturing multi-scale, multi-orientation grid modules, phase precession, and head direction tuning. The agent acknowledges the successful prediction of hexagonal grid field emergence but notes the challenges in explaining the entire spectrum of grid cell properties.\n\n**NormCode (Normative Modeling & Efficient Coding Analyst):** This agent emphasizes the functional constraints and efficiency of the representation. The agent highlights the novelty of framing the representation's utility in terms of direct motor control and closed-loop action. The agent finds the derivation of periodic representations from the linear actionability constraint to be a strong optimality argument. The agent acknowledges the idealizations in the model but praises its clarity and completeness.\n\n**Synthesis:**\n\nThe paper presents a compelling framework for understanding grid cells based on the principle of \"actionability.\" The agents agree that the paper offers a novel perspective and provides valuable insights. The main strength lies in demonstrating how grid cell-like properties can emerge from minimal constraints and a focus on motor control.\n\nHowever, the paper also has limitations. The model simplifies the complex circuitry of the MEC and doesn't fully capture all known grid cell phenomena, such as multi-scale modules and phase precession. The lack of formal mathematical proofs and the reliance on idealizations are also points to consider.\n\n**Final Decision:**\n\nConsidering the strengths and weaknesses, the paper offers a significant contribution to the field. The novel perspective, elegant derivations, and compelling optimality arguments outweigh the limitations. The paper provides a strong foundation for future research and offers valuable insights into the functional role of grid cells.\n\nFINAL DECISION: Accept\n"}], "ground_truth": "Accept: poster", "response_history": "The agents have provided their evaluations. Now I will synthesize these into a final decision.\n\n**RepLearn (Representation Learning Expert):** The agent highlights the paper's strong connection to existing concepts in representation theory, specifically equivariance. While the formal definition of \"actionable representation\" isn't entirely novel, the paper's contribution lies in proposing a learning paradigm and demonstrating the emergence of grid cell-like properties. The agent emphasizes the distinction from geometric deep learning, where equivariance is enforced by design, while this paper focuses on *learning* the group representation. The agent notes the lack of formal mathematical proofs regarding convergence or optimality but acknowledges the empirical validation.\n\n**GridCell (Computational Neuroscience & Grid Cell Specialist):** This agent focuses on the biological plausibility and relevance to grid cell phenomena. The agent praises the local learning rules and the use of velocity/action input. However, it points out limitations in capturing multi-scale, multi-orientation grid modules, phase precession, and head direction tuning. The agent acknowledges the successful prediction of hexagonal grid field emergence but notes the challenges in explaining the entire spectrum of grid cell properties.\n\n**NormCode (Normative Modeling & Efficient Coding Analyst):** This agent emphasizes the functional constraints and efficiency of the representation. The agent highlights the novelty of framing the representation's utility in terms of direct motor control and closed-loop action. The agent finds the derivation of periodic representations from the linear actionability constraint to be a strong optimality argument. The agent acknowledges the idealizations in the model but praises its clarity and completeness.\n\n**Synthesis:**\n\nThe paper presents a compelling framework for understanding grid cells based on the principle of \"actionability.\" The agents agree that the paper offers a novel perspective and provides valuable insights. The main strength lies in demonstrating how grid cell-like properties can emerge from minimal constraints and a focus on motor control.\n\nHowever, the paper also has limitations. The model simplifies the complex circuitry of the MEC and doesn't fully capture all known grid cell phenomena, such as multi-scale modules and phase precession. The lack of formal mathematical proofs and the reliance on idealizations are also points to consider.\n\n**Final Decision:**\n\nConsidering the strengths and weaknesses, the paper offers a significant contribution to the field. The novel perspective, elegant derivations, and compelling optimality arguments outweigh the limitations. The paper provides a strong foundation for future research and offers valuable insights into the functional role of grid cells.\n\nFINAL DECISION: Accept\n", "elapsed_time": 113.67231011390686, "reviewer_agents": {"RepLearn": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in representation learning and group theory.", "system_prompt": "You are an expert in representation learning and group theory. You rigorously evaluate the paper's formalization of actionability using group and representation theory. You assess the novelty and significance of the proposed actionable representation compared to existing representation learning techniques, especially in geometric deep learning. Critically analyze the mathematical justifications and proofs.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "GridCell": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Specialist in computational neuroscience and grid cells.", "system_prompt": "You are a specialist in computational neuroscience and grid cells. You evaluate the biological plausibility and relevance of the model to grid cell phenomena. You assess the validity of the predictions made by the theory against experimental findings. Critically analyze the model's ability to explain observed grid cell properties and its limitations in capturing other known phenomena.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "NormCode": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Analyst in normative modeling and efficient coding.", "system_prompt": "You are an analyst in normative modeling and efficient coding. You assess the functional constraints and the efficiency of the proposed representation. You evaluate the optimality arguments and the trade-offs between actionability, biological constraints, and functional efficiency. Analyze the clarity and completeness of the derivations and simulations.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}}}
