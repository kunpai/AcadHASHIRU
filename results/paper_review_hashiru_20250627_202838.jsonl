{"paper_id": "OM7doLjQbOQ", "prompt": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\n\n1 INTRODUCTION: Recent studies (Szegedy et al., 2013; Goodfellow et al., 2015) showed that deep neural network (DNN) models are vulnerable to adversarial attacks, where perturbations are added to the clean data to fool the models in making erroneous classification. Such adversarial perturbations are usually crafted to be almost imperceptible by humans, yet causing apparent fluctuations in the model output. The effectiveness of adversarial attacks on deep learning models raises concerns in multiple fields, especially for security-sensitive applications. Besides being effective to the victim model, adversarial attacks are found to be capable of transferring across models (Papernot et al., 2016). One explanation for this phenomenon is the overlapping decision boundaries shared by different models (Liu et al., 2017; Dong et al., 2018). Such behavior not only aggravates concerns on the reliability and robustness of deep learning models, but also enables various black-box attacks which leverage the transferring behavior, such as directly generating attacks from a source (or surrogate) model (Zhou et al., 2018) or acting as a gradient prior to reduce the number of model queries (Guo et al., 2019). Intermediate Level Attack (ILA) is a method proposed by Huang et al. (2019) to fine-tune an existing adversarial attack as a reference, thereby raising its attack transferability across different models. Formulated to maximize the intermediate feature map discrepancy represented in the models, ILA achieves remarkable black-box transferability, outperforming various attacks that are directly generated (Zhou et al., 2018; Xie et al., 2019a). On the other hand, many of the transfer-based attacks empirically show that simple image transformations, including padding (Xie et al., 2019a), image translation (Dong et al., 2019), and scaling (Lin et al., 2020), are effective in strengthening the transferability of an adversarial attack. Similar ideas can be applied to the ILA framework, which takes three input images as references, including a clean image, a reference adversarial image, and the updated image from the previous iteration. This motivates us to advance the current transfer-based ILA attack by promoting reference diversity using three carefully-designed augmentation techniques. In this paper, we revisit ILA and propose ILA-DA, which employs three novel augmentation techniques to advance ILA attack transferability across different model architectures. First, we develop an automated way to apply effective image transformations for more diverse augmented references. Unlike recent research on Automated Data Augmentation (AutoDA) (Cubuk et al., 2019; Li et al., 2020c) which constructs useful augmented data for model training, our approach applies learnable augmentations to create more powerful adversarial examples. Second, we propose an efficient augmentation method, called the reverse adversarial update, to create new augmented references by utilizing adversarial noises. Nevertheless, ILA-DA forms new augmented references by interpolating images in the direction that leads to larger intermediate feature discrepancies. Through fine-tuning simple baseline attacks, such as the Iterative Fast Gradient Sign Method (I-FGSM), we show that ILA-DA outperforms state-of-the-art transfer-based attacks on both defended and undefended models by large margins. Visualizations of some adversarial examples created by ILA-DA are illustrated in Figure 1 and Appendix L. Our main contributions can be summarized as follows: \u2022 We propose ILA-DA, which applies three novel augmentation techniques, including automated data augmentation, reverse adversarial update and attack interpolation, to considerably strengthen the current ILA attack. \u2022 We demonstrate that ILA-DA outperforms various ILA attacks and other state-of-the-art approaches. On ImageNet, we attain an average attack success rate of 84.5%, which is 19.5% better than ILA and 4.7% better than the previous state-of-the-art across nine undefended models. \u2022 We show that ILA-DA with simple I-FGSM attack references can exceed state-of-the-art attacks on six defended models. We also find that incorporating ILA-DA into existing attacks can further increase their attack transferability. 2 RELATED WORKS: White-box and black-box attacks are two common threat models used in adversarial attack research. The white-box setting assumes that the attacker has access to the victim model\u2019s internal state, including its gradient, parameters, training dataset, etc. The black-box setting, on the other hand, only allows model queries without accessing any subsequent model information. While more variations such as the grey-box and no-box settings exist, they are generally not considered in this work. Typical white-box attacks exploit the gradient of the model to generate adversarial examples. The Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) generates attacks by adding the signed gradient of the loss with respect to the image back to the image, obtaining a higher loss and possibly incorrect prediction. The Iterative Fast Gradient Sign Method (I-FGSM, also known as BIM) (Kurakin et al., 2017a) performs FGSM iteratively while clipping the difference between the adversarial example and the original image within a perturbation budget. Projected Gradient Descent (PGD) (Madry et al., 2018) first initiates a random start within the perturbation budget and runs I-FGSM to update the attack. Under the black-box setting, the model information can be estimated by repeated model queries with varying inputs (Chen et al., 2017; Brendel et al., 2018). Recently, there are multiple attempts to reduce the number of model queries (Bhagoji et al., 2018; Ilyas et al., 2018; Li et al., 2020a). However, in order to provide an accurate estimation to the model information, the required number of model queries is still abnormally large, causing the attacks to be detected by real-world systems easily. This turns the spotlight back to transfer-based attacks, which create adversarial examples through black-box transferability instead of repeated attempts. 2.1 TRANSFER-BASED ATTACKS: Since strong gradient-based white-box attacks usually do not exhibit high transferability (Kurakin et al., 2017b), notable efforts have been devoted to improve transferability apart from increasing the attack strength. An observable direction of the previous works is to incorporate carefully-designed data augmentation techniques into existing attacks. MI-FGSM. Dong et al. (2018) first introduced the Momentum Iterative Fast Gradient Sign Method (MI-FGSM) to incorporate momentum to stabilize the update direction in iterative attacks, resulting in more transferable attacks. DIM. The Diverse Input Method (DI2-FGSM, or DIM) (Xie et al., 2019a) applies random resizing and zero-padding with a certain probability to the image in each iteration of I-FGSM. TIM. Dong et al. (2019) pointed out the discretion of attention map between the defended and undefended models and proposed the Translation-Invariant Method (TIM) by attacking an ensemble of translated images. To reduce the computational overhead, it was shown that kernel convolution can be applied to the gradient to achieve similar effects. SIM and NI-FGSM. Lin et al. (2020) introduced the scale-invariant property of DNN and proposed the Scale-Invariant Method (SIM) to attack an ensemble of images with a scaling factor of 0.5 on the pixel values. In the same paper, the Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) was introduced, aiming to escape from poor local maxima with the Nesterov accelerated gradient. Variance Tuning. Wang & He (2021) proposed variance tuning. For each iteration, the gradient is adjusted with an expectation of gradient sampled from the image\u2019s neighborhood. Combining variance tuning with the composition of previous methods, denoted as the Composite Transformation Method (CTM), they formed the attack VMI-CT-FGSM and VNI-CT-FGSM, resulting in one of the strongest transfer-based black-box attacks on defended models. Following the convention, we will name this series of work the CTM family. A table matching the names and the alias of the CTM family is shown in Appendix H. Admix. Wang et al. (2021) recognized the effectiveness of input transformation in attack transferability and proposed Admix. Inspired by Mixup (Zhang et al., 2018), Admix computes the gradient on a set of mixed images with different categories, which can be viewed as a special case of SIM, but with higher empirical contribution to transferability. SGM and LinBP. Unlike the CTM family, another type of works successfully raise the attack transferability by exploiting CNN model architectures. Wu et al. (2020) found that attack transferability can be enhanced by scaling up the gradient in the skip connection and referred to the technique as Skip Gradient Method (SGM). Guo et al. (2020) revised the linear property of DNN models and proposed LinBP to propagate backward without considering the non-linear layers. 2.2 INTERMEDIATE LEVEL ATTACK FOR FINE-TUNING ADVERSARIAL EXAMPLES: Consider an input image x, an existing adversarial attack x\u2032, a model F , and a function Fl that outputs the feature maps at layer l of the model. The original ILA projection loss proposed by Huang et al. (2019) minimizes the dot product: L(x, x\u2032, x\u2032\u2032) = \u2212\u2206y\u2032\u2032l \u00b7\u2206y\u2032l (1) where \u2206y\u2032l and \u2206y\u2032\u2032l are two vectors defined as follows: \u2206y\u2032l = Fl(x \u2032)\u2212 Fl(x) (2) \u2206y\u2032\u2032l = Fl(x \u2032\u2032)\u2212 Fl(x) (3) This is equivalent to maximizing \u2206y\u2032\u2032l \u00b7\u2206y\u2032l. The idea behind the formulation is to increase the norm without sacrificing the perturbation direction that causes misclassification. Due to the constraint of the perturbation budget, there is hardly room for fine-tuning in the image space. Instead, projection maximization is carried out in the feature space, specifically at layer l of the model. Based on the same framework, Li et al. (2020b) proposed another formulation of ILA: max x\u2032\u2032 (Fl(x\u2032\u2032)\u2212 Fl(x))Tw* (4) where w* is a pre-computed parameter vector that directly maps the intermediate-level discrepancies to predict the adversarial loss, skipping the remaining model layers. One notable trick of the design is that the computation of w* involves every feature map discrepancy Fl(x\u2032t)\u2212Fl(x) in each iteration t during the generation of x\u2032. We refer to this attack as ILA++ in the rest of this paper. 3 OUR METHOD: Based on what we have observed from the previous work, adversarial attacks tend to exhibit higher transferability when more references are involved. The increase of references can be achieved by data augmentation and reusing the temporal information over multiple iterations. From Equation (1), all three ILA input references x, x\u2032 and x\u2032\u2032 are images, suggesting the possibility of applying image augmentation like that for representation learning to improve model generalization. One natural way to augment the images is to apply image transformation to the input before retrieving the intermediate feature maps. Consequently, the function Fl can be substituted by F \u2032l (x) = Fl(T (x)) (5) where T can be any image transformation function. Different from conventional data augmentation that generalizes model representation, our task is to improve attack transferability, which can be interpreted as generalizing an adversarial example to succeed in attacking more models. To achieve the goal, we propose three augmentation techniques: automated data augmentation, reverse adversarial update, and attack interpolation. The algorithmic details for applying these augmentation techniques in ILA-DA are depicted in Algorithm 1. The clip function is to ensure that the first argument is within the range defined by the second and third arguments. Algorithm 1 ILA-DA Require: x, x\u2032, Fl, perturbation budget \u03f5, step size a, \u03b7, number of iterations n, a set of transforma- tions {\u03c6i}ki=1, augmentation parameter \u03b1 Initialize \u03b1 x\u2032\u2032 \u2190 x\u2032 for i\u2190 1 to n do xrev \u2190 2x\u2212 x\u2032 \u25b7 reverse adversarial update z \u223c p\u03b1(z) \u2206y\u2032l \u2190 Fl(Tz(x\u2032))\u2212 Fl(Tz(xrev)) \u2206y\u2032\u2032l \u2190 Fl(Tz(x\u2032\u2032))\u2212 Fl(Tz(xrev)) x\u2032\u2032 \u2190 x\u2032\u2032 \u2212 a sign(\u2207x\u2032\u2032(\u2212\u2206y\u2032\u2032l \u00b7\u2206y\u2032l)) x\u2032\u2032 \u2190 clip(x\u2032\u2032, x\u2212 \u03f5, x + \u03f5) x\u2032\u2032 \u2190 clip(x\u2032\u2032, 0, 1) \u03bb\u2190 \u2225 \u2206y\u2032\u2032l \u22252 \u2225\u2206y\u2032\u2032l \u22252+\u2225\u2206y\u2032l\u22252 x\u2032 \u2190 \u03bbx\u2032\u2032 + (1\u2212 \u03bb)x\u2032 \u25b7 attack interpolation \u03b1\u2190 \u03b1\u2212 \u03b7\u2207x\u2032\u2032(\u2212\u2206y\u2032\u2032l \u00b7\u2206y\u2032l) \u25b7 augmentation parameter update end for return x\u2032\u2032 3.1 AUGMENTATION LEARNING: Data augmentation is an effective technique to improve model generalization. However, it is nontrivial to select the most effective augmentation method for a specific task and dataset. As an Auto- mated Data Augmentation approach, AutoAugment (Cubuk et al., 2019) uses reinforcement learning to search for the optimal augmentation policy for a dataset. With similar motivation, we propose an automated way to learn effective image transformations that lead to a more vigorous ILA attack. We model the augmentation selection process Tz as sampling from a distribution p\u03b1(z) with the learnable parameter vector \u03b1 in Equation (6): Tz(x) = k\u2211 i=1 zi\u03c6i(x), z \u223c p\u03b1(z) (6) Sampled from p\u03b1(z), z is a one-hot vector that indicates the selection of the augmentation from a set of k transformations {\u03c6i}ki=1. The overall goal is to maximize the ILA objective in Equation (1) using the augmentation sampled from p\u03b1(z). We use the Gumbel-Softmax reparameterization trick to relax the discrete augmentation distribution to be continuous (Jang et al., 2017). In so doing, we can utilize stochastic gradient descent, with a learning weight parameter \u03b7, to optimize the objective. The technique is also adopted in neural architecture search (Xie et al., 2019b) and AutoDA (Li et al., 2020c). With the reparameterization trick, we can soften zi as: zi = f\u03b1i(gi) = exp ((log\u03b1i + gi)/\u03c4)\u2211k j=1 exp ((log\u03b1j + gj)/\u03c4) , (7) where gi = \u2212 log(\u2212 log(ui)) is the i-th Gumbel random variable, ui is a uniform random variable and \u03c4 is the temperature parameter of the softmax function. In our implementation, we set \u03c4 = 1 and sample from three common image transformation operations: translation, cropping, and rotation. It is worth noting that a perfect pixel alignment among x, x\u2032 and x\u2032\u2032 is required in ILA-DA, as a tiny shift in the image pixel induces misalignment in the feature maps. In other words, translation, cropping and rotation are required to be applied identically to all three ILA references. Otherwise, the optimization of ILA ends up being trivial due to the distorted feature discrepancies. 3.2 REVERSE ADVERSARIAL UPDATE ON THE CLEAN EXAMPLE: Let us reconsider Equation (2). To perform ILA, we need to obtain the feature map discrepancies between the reference attack x\u2032 and the unperturbed image x. Although x is correctly classified by the model, there is no information on how confident the model is and how well the input can represent the ground-truth class. In contrast, if x is not only correctly classified, but it is also done with a high confidence, the discrepancy of the feature maps is expected to intrinsically reflect more useful information about the attack. In order to boost the confidence of x, we propose to add a negative perturbation to it, which is exactly opposite to applying an adversarial attack. If we consider the entire model, this operation is expected to decrease the loss of the classifier, making the classification task easier by emphasizing features that are more significant for the task. Thus, we also expect the input to highly activate the intermediate layers, providing a better guidance for image update in ILA. Our preliminary study on the effect of reverse adversarial update can be found in Appendix E, which verifies the benign behavior brought by the reverse adversarial update. With the reverse adversarial perturbation, as expected, the model gives a lower loss and higher confidence on the correct class. Instead of crafting a new adversarial attack which would incur extra computational demand, we can extract the perturbation from the existing reference attack x\u2032, turning the transformation into Tadv(x) = x\u2212 (x\u2032 \u2212 x) = 2x\u2212 x\u2032 (8) The notion of reverse adversarial update shares some similarities with DeepDream (Mordvintsev et al., 2015), in which an image is updated towards the models\u2019 interpretation of the target class. After the transformation, we look for the feature discrepancies caused by a bi-directional perturbation \u00b1\u03f5 sign(\u2207xJ(\u03b8, x, y)), with the positive side being the adversarial attack and the negative side being the augmentation. 3.3 ATTACK INTERPOLATION ON THE REFERENCE ATTACK: Besides the clean image x, the reference attack x\u2032 is another factor to remain unchanged throughout all the ILA iterations. Given the image transformation in Equation (5), it is unlikely to work with another image transformation unaligned with the other images. Similar to Section 3.2, we instead exploit the adversarial noise to be the augmentation. According to the intuition of Best Transfer Direction (BTD) in Huang et al. (2019), the stronger the reference attack is, the better ILA performs. This inspires us to strengthen the reference attack during the iterations of ILA, by interpolating the reference attack itself with a stronger attack. The output of the previous iteration, x\u2032\u2032, is a good candidate for a strong attack. At iteration t, we set x\u2032t+1 = \u03bbx \u2032\u2032 t + (1\u2212 \u03bb)x\u2032t (9) where \u03bb is a weighting factor in the range [0, 1] to control the proportion of the two attacks. This augmentation is similar to mixup (Zhang et al., 2018), except that we are mixing two adversarial examples, rather than two images from different classes. With the assumption that the attack is finetuned to be more transferable in every iteration, by interpolating two attacks, we can strengthen the transferability of the reference attack while preserving a part of its original characteristics. Based on our preliminary findings, setting \u03bb to non-trivial values such as 0.5 suffices to yield satisfactory performance. However, a constant value lacks consideration of the behavior of the two attacks. Precisely, we hope to perform an adaptive interpolation depending on the effectiveness of x\u2032\u2032t . If x\u2032\u2032t is weak after a single update, \u03bb should bias towards preserving the reference attack more than mixing with the new attack. Since ILA focuses on maximizing the linear projection between feature map discrepancies, the norm of the feature map discrepancy can be a good indicator of the performance. Consequently, we set \u03bb = \u2225\u2206y\u2032\u2032l \u22252 \u2225\u2206y\u2032\u2032l \u22252 + \u2225\u2206y \u2032 l\u22252 (10) If we neglect the transformation in Section 3.2 for simplicity, \u2206y\u2032l and \u2206y\u2032\u2032l will remain the same as what we obtained from Equations (2) and (3), respectively. Note that the value of \u03bb is recomputed in each iteration before interpolation is applied. A similar approach of reusing previous attacks in the temporal domain can be applied to the reverse adversarial update in Section 3.2. The reference attack x\u2032 in Equation (8) can be further extended to a collection of x\u2032t for every iteration t. This is not the only work to consider the temporary values of the reference attack before its convergence. ILA++ (Li et al., 2020b) also collects all losses computed during the generation of the reference attack, achieving superiority over the standard ILA. Different from Li et al. (2020b) which uses past losses (with respect to x\u2032) to help update x\u2032\u2032t , we make use of the past x\u2032t directly as an augmentation to enrich the information regarding feature discrepancies under different attacks. 4 EXPERIMENTAL RESULTS: In this section, we evaluate the attack transferability of ILA-DA in comparison with previous methods based on fine-tuning as well as state-of-the-art transfer-based attack generation methods on multiple defended and undefended models. Next, we investigate the effects of the three augmentation techniques and identify influential factors for the transferability of an attack. 4.1 SETUP: Undefended models. Following the line of work in transfer-based attacks, a total of nine classification models are used to evaluate attack transferability, including ResNet50 (He et al., 2016), VGG19 (Simonyan & Zisserman, 2015), Inception V3 (Inc-v3) (Szegedy et al., 2016), Wide ResNet50 2x (WRN) (Zagoruyko & Komodakis, 2016), DenseNet161 (DenseNet) (Huang et al., 2017), ResNeXt101 (ResNeXt) (Xie et al., 2017), MobileNet V2 (MobileNet) (Sandler et al., 2018), PNASNet5 (PNASNet) (Liu et al., 2018) and SENet50 (SENet) (Hu et al., 2018). Among these models, we use ResNet50, VGG19 and Inception V3 as the source models of the attack. All the models are pretrained on ImageNet (Russakovsky et al., 2015), with the model parameters of PNASNet1 and SENet2 obtained from public repositories and the remaining from Torchvision (Paszke et al., 2019). 1https://github.com/Cadene/pretrained-models.pytorch 2https://github.com/moskomule/senet.pytorch For the choice of the intermediate layer, we opt the layer 3-1 for ResNet50, layer 9 for VGG19, and layer 6a for Inception V3, where the former two have been shown to result in good performance by Li et al. (2020b). Due to the ineffectiveness of transfer-based targeted attacks (Liu et al., 2017), we report the untargeted attacks with \u03f5 values of 8/255 (0.03) under the \u2113\u221e norm constraint. To measure the attack success rate, we randomly sample 5000 images from the ILSVRC2012 validation set with all images being classified correctly by the nine models. The default number of iterations of I-FGSM is 10 and the attack step size is set to max( 1255 , \u03f5 no. of iterations ). To mount a complete attack, we first run I-FGSM for 10 iterations on the source model, and then pass the example as the reference attack to ILA-DA to perform fine-tuning for 50 iterations. The same model is used as both the source model of I-FGSM and surrogate model of ILA-DA. To demonstrate the superiority of ILA-DA over the ILA family, we include the result of ILA (Huang et al., 2019) and ILA++ (Li et al., 2020b). Besides the ILA family, we also append the results of other state-of-the-art transfer-based attacks, namely MI-CT-FGSM and NI-CT-FGSM, with their corresponding variations with variance tuning, SGM and LinBP as introduced in Section 2.1. Since the experimental setting in the CTM family is slightly different from ours and LinBP, we reproduce the works in the CTM family and apply in our experimental environment. The results of the attacks on undefended models are shown in Table 1, with the details of the hyper-parameters listed in Appendix F. We also conduct extensive experiments using other perturbation budgets, source models and datasets. The results can be found in Appendix D, G and I respectively. Defended models. We follow the experimental setup used in the previous works (Dong et al., 2018; 2019; Wang & He, 2021), which includes 1000 pre-sampled test images and a set of models which are pre-trained under ensemble adversarial training (Tramer et al., 2018). Then we compare ILADA with the baselines introduced in Section 2.1, including I-FGSM, MI-FGSM, MI-CT-FGSM, NI-CT-FGSM, Admix and variance tuning. We pick Inception V3 as the source model and an adversarially trained Inception V3 as the surrogate model of ILA-DA, and all attacks are performed with perturbation size \u03f5 = 16/255(\u2248 0.063). The attacks are evaluated against three robust models: an Inception V3 trained adversarially with an ensemble of 3 models (Inc-v3ens3), an Inception V3 trained adversarially with an ensemble of 4 models (Inc-v3ens4), and an Inception-ResNet V2 trained adversarially with an ensemble of 3 models (IncRes-v2ens). Besides, we also test the attacks against the top three submitted defenses in the NIPS 2017 adversarial competition: HGD (Liao et al., 2018), R&P (Xie et al., 2018) and MMD3. For all three defenses, we adopt the default models and hyper-parameters used in their corresponding repositories. Since the gradients of the adversarially trained models are more difficult to exploit, we increase the number of ILA-DA iterations to 500 in order to guarantee convergence of the attack. The attack success rate against each of the defenses is reported in Table 2, while a study of the number of iterations is reported in Appendix A. 4.2 BLACK-BOX TRANSFERABILITY OF ILA-DA ON UNDEFENDED MODELS: ILA-DA outperforms ILA and ILA++ on 8 transferred models by a large margin. Although the adversarial examples generated by ILA-DA are slightly less effective on the source model (ResNet50), the examples achieve remarkable attack success rates on other transferred models (see Table 1). In particular, ILA-DA attains an average attack success rate of 84.5%, which is 19.5% better than ILA and 4.7% better than the previous state-of-the-art (VNI-CT-FGSM). The improved attack transferability validates the effectiveness of our proposed augmentation techniques. It is worth mentioning that the attack difficulty increases with the differences in model architectures. For example, the attacks generated from ResNet (with skip connections) tend to transfer better to models also with skip connections, such as WRN, DenseNet, ResNeXt, compared to Inception V3 and PNASNet. Even under such model dissimilarity, the improvement in attack transferability caused by ILA-DA is more prominent than the ILA baselines. We believe such a phenomenon can be attributed to the extensive augmentations that enable both state-of-the-art attacks and ILA-DA to exploit gradients with better resemblance to different architectures. 4.3 BLACK-BOX TRANSFERABILITY OF ILA-DA ON DEFENDED MODELS: In Table 2, ILA-DA also shows outstanding attack transferability against six defended models. Unlike the CTM family, ILA-DA does not require stacking multiple attack methods to achieve high 3https://github.com/anlthms/nips-2017/tree/master/mmd attack transferability. Based on a weak I-FGSM reference attack, I-FGSM + ILA-DA yields an average attack success rate of 75.9%, which outperforms the strongest state-of-the-art (VNI-CTFGSM) by 4.4%. Moreover, incorporating ILA-DA to existing methods brings further improvements. Specifically, using the attack reference generated by MI-Admix-TI-DIM, ILA-DA can attain the highest attack success rate of 81.9%. As the defended models are trained to be robust to adversarial inputs, more ILA-DA fine-tuning is required in comparison to undefended models (see Appendix A and Appendix K). Although using a larger number of ILA-DA iterations induces higher computational overhead, ILA-DA could still achieve comparable performance with state-of-the-art attacks such as VNI-CT-FGSM and LinBP under similar running time, while ILA-DA also exhibits a great degree of improvement when the time budget is up-scaled. 4.4 ABLATION STUDY: ILA-DA can be regarded as an extension of the original ILA, equipped with three ILA augmentation techniques: automated data augmentation, reverse adversarial update, and attack interpolation. We conduct an ablation study on the effects of the three augmentations by removing each of them from ILA-DA and reporting the performance of the reduced methods in Table 3. In addition, we also study two non-automated ways of applying translation, cropping and rotation to the ILA references. In particular, Random Augmentation randomly applies one of the three augmentations; All Augmentation applies all three augmentations to the reference images. While all the three proposed augmentation techniques contribute to the improved attack transferability on average, Table 3 reveals that applying translation, cropping and rotation to the ILA references increases the attack success rate the most, followed by the use of attack interpolation and reverse adversarial update. Although reverse adversarial update and attack interpolation slightly lower the performance by 0.5% in 2 out of 16 settings, the improvements in transferability across different models brought by the two augmentations are evident. The study also validates the use of the proposed automated data augmentation scheme. It is shown that applying all augmentations may cause excessive distortion to the images and lead to a significant drop in the attack success rate. Nevertheless, using the learned augmentation parameter shows non-trivial improvements over random augmentation. 5 CONCLUSION: In this paper, we presented ILA-DA, which applies three types of data augmentation techniques to the original ILA formulation. Specifically, we proposed an automated data augmentation method which is searched by learnable parameters, reverse adversarial update which exploits the adversarial noise to augment the reference, and attack interpolation which strengthens the reference attack across iterations. Incorporating all the proposed augmentations, ILA-DA outperforms state-of-theart methods for transfer-based attacks on both undefended and defended models. We also studied the contribution of each augmentation technique to ILA-DA attack transferability. This work showcases how data augmentation techniques could benefit the generation of transfer-based adversarial attacks. We believe that it is beneficial to extend the proposed automated method to search for more types of data augmentation, other augmentation parameters, and even the optimal ILA layer to generate the attack. A NUMBER OF ILA ITERATIONS: With extensive augmentation, we also search for the number of iterations for ILA-DA to result in a stable attack. We vary the number of iterations from 5 to 2000, in both the setup of undefended (\u03f5 = 8/255) and defended models (\u03f5 = 16/255). A plot of the result is shown in Figure 2. For undefended models, ILA-DA could yield attacks with high transferability within 50 iterations. However, for defended models, it requires more iterations to converge to an effective attack. B EFFECT OF THE CHOICE OF INTERMEDIATE LAYER: We apply ILA and ILA-DA at several layers near the suggested default layer of ILA. As shown in Figure 3, ILA-DA is strictly better than ILA in terms of attack success rate in every layer tested. Furthermore, the inverted U-curve patterns exhibited in Figure 3 suggest that the method is not very sensitive to the selection of layer around the layer proposed by ILA. C ANALYSIS OF THE LEARNED AUGMENTATION POLICY: ILA-DA optimizes three image transformation operations that are commonly used, including translation, cropping and rotation. On the ImageNet dataset, we discovered that the cropping operation yields the strongest attack. Empirical results also show that the augmentation parameters converged to certain fixed points. In Figure 4a, ILA-DA explores different augmentations in the first few iterations and discovers random cropping as the best transformations for maximizing the ILA objective. Figure 4b further shows that the converged augmentation parameters are applied to the attack references throughout the ILA-DA fine-tuning. D RESULTS ON UNDEFENDED MODELS WITH LARGER PERTURBATION BUDGETS: As a supplementary to Table 1 which shows the attack results with \u03f5 = 8/255 (0.03) on undefended models, Table 4 compares the performance of ILA-DA with other baselines with \u03f5 = 13/255 (0.05). The experimental results show that ILA-DA leads I-FGSM, ILA, ILA++ and VMI-CT-FGSM by a large margin. E EFFECT OF REVERSE ADVERSARIAL UPDATE ON THE MODEL PERFORMANCE: We would like to verify the intuition of using reverse adversarial update, specifically, whether in practice such operation can decrease the loss and increase the model confidence. Therefore, we randomly sample 5000 images, including those that are misclassified by the model. Then we apply I-FGSM10 in the form of reverse adversarial update to the images, and pass them back to the model for classification. We record the loss and apply softmax to the logits to obtain the confidence, and report the average result of all 5000 images. The result is summarized in Table 5. Furthermore, we also inspect the class activation map (CAM) of the source model. Figure 5 visualizes some of the results. For some images (the first two), updating reversely helps the model focus on more features. For the remaining (the latter two), adversarial reverse update does not exhibit a significant change of intermediate layers\u2019 contribution to the ground-truth class. To test the effect of reversely updated examples on targeted models, we also feed them into the remaining undefended models and observe the change in accuracy. The result is reported in Table 6, which shows that reverse adversarial update not only benefits the source model, but also many of the other models with similar architecture. F HYPER-PARAMETERS OF THE BASELINES: We reproduced the algorithms of CTM (DIM, TIM, SIM), MI-FGSM, NI-FGSM and variance tuning according to the repository released by Wang & He (2021), and tested the attacks under our experimental settings. The hyper-parameters used in the experiments are shown in Table 7, which are the default values in their corresponding papers. For the implementation of SGM and LinBP, we directly use the official implementation released by Guo et al. (2020). G ATTACKS USING OTHER SOURCE MODELS: In this section, we test the effectiveness of ILA-DA with other source models. In particular, we adopt VGG19 and Inception V3 as the source models for both the reference attack and ILA-DA, and re-evaulate the attack success rate on the remaining eight undefended models. The results are shown in Table 8 and Table 9. For Inception V3, ILA-DA still outperforms every other baseline. For VGG19, the performance of I-FGSM + ILA-DA is slightly worse than that of the best attack, VNI-CT-FGSM. However, when we fine-tune ILA-DA on a stronger reference attack such as MICT-FGSM, our method regains the best average attack success rate among all other methods. H NAME MATCHING OF THE CTM FAMILY:  I EXPERIMENTS WITH MORE DATASETS: To show the generalization capability of ILA-DA, we conduct experiments with two more datasets, namely CIFAR-10 and CIFAR-100 (Krizhevsky, 2012). For CIFAR-10, we follow Li et al. (2020b), which considers six models: VGG19 with batch normalization (Simonyan & Zisserman, 2015), Wide ResNet-28-10 (WRN) (Zagoruyko & Komodakis, 2016), ResNeXt-29 (ResNeXt) (Xie et al., 2017), DenseNet-190 (DenseNet) (Huang et al., 2017), PyramidNet\u2020 (Han et al., 2017) (with ShakeDrop (Yamada et al., 2018) and AutoAugment (Cubuk et al., 2019)), and also GDAS (Dong & Yang, 2019). The former four models are also selected to be used in the CIFAR-100 setup, with the model parameters obtained from a public repository4. For both datasets, we randomly sample 5000 images from the test set that are classified correctly by all four models and we pick VGG19 to be the source model. The experimental results on CIFAR-10 and CIFAR-100 are shown in Table 11. For both CIFAR-10 and CIFAR-100, the attack success rates of the baselines are much higher than that of ImageNet. Although ILA-DA mostly attains the highest attack success rate among the base- 4https://github.com/bearpaw/pytorch-classification lines, we observe that the degree of improvement is not as high as that in Table 1. It may be because of the tiny size and low resolution (32\u00d732 for both CIFAR-10 and CIFAR-100) of the images, leading to a reduction in the effect of data augmentation. J EFFECT OF \u03bb ON ATTACK INTERPOLATION In this section, we study the effect of \u03bb used for attack interpolation, with fixed values between 0 and 1, in addition to the adaptive choice computed from the norm of the feature map discrepancy. The result is shown in Figure 6. While a non-trivial value of \u03bb suffices to improve the performance, the adaptive selection using Equation (10) results in optimal or sub-optimal attack transferability in most cases. K STUDY ON THE RUNNING TIME OF THE ALGORITHMS: In this section, we compare the running time between ILA-DA and other state-of-the-art attacks. We report the average running time required to run attack on a batch from the ILSVRC2012 validation set, with a batch size of 100. The remaining setup is the same as the previous experiments, where all hyper-parameters are specified in Appendix F. We measure the running time required for attacking a batch and report the mean and standard deviation of the running time across 5 batches. All the experiments are performed on a machine with an Intel Xeon Silver 4214R CPU and Nvidia GeForce RTX3090 GPU. The result is summarized in Table 12. L MORE VISUALIZATIONS ON THE GENERATED EXAMPLES: ", "agent_review": [{"role": "user", "content": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\n\n1 INTRODUCTION: Recent studies (Szegedy et al., 2013; Goodfellow et al., 2015) showed that deep neural network (DNN) models are vulnerable to adversarial attacks, where perturbations are added to the clean data to fool the models in making erroneous classification. Such adversarial perturbations are usually crafted to be almost imperceptible by humans, yet causing apparent fluctuations in the model output. The effectiveness of adversarial attacks on deep learning models raises concerns in multiple fields, especially for security-sensitive applications. Besides being effective to the victim model, adversarial attacks are found to be capable of transferring across models (Papernot et al., 2016). One explanation for this phenomenon is the overlapping decision boundaries shared by different models (Liu et al., 2017; Dong et al., 2018). Such behavior not only aggravates concerns on the reliability and robustness of deep learning models, but also enables various black-box attacks which leverage the transferring behavior, such as directly generating attacks from a source (or surrogate) model (Zhou et al., 2018) or acting as a gradient prior to reduce the number of model queries (Guo et al., 2019). Intermediate Level Attack (ILA) is a method proposed by Huang et al. (2019) to fine-tune an existing adversarial attack as a reference, thereby raising its attack transferability across different models. Formulated to maximize the intermediate feature map discrepancy represented in the models, ILA achieves remarkable black-box transferability, outperforming various attacks that are directly generated (Zhou et al., 2018; Xie et al., 2019a). On the other hand, many of the transfer-based attacks empirically show that simple image transformations, including padding (Xie et al., 2019a), image translation (Dong et al., 2019), and scaling (Lin et al., 2020), are effective in strengthening the transferability of an adversarial attack. Similar ideas can be applied to the ILA framework, which takes three input images as references, including a clean image, a reference adversarial image, and the updated image from the previous iteration. This motivates us to advance the current transfer-based ILA attack by promoting reference diversity using three carefully-designed augmentation techniques. In this paper, we revisit ILA and propose ILA-DA, which employs three novel augmentation techniques to advance ILA attack transferability across different model architectures. First, we develop an automated way to apply effective image transformations for more diverse augmented references. Unlike recent research on Automated Data Augmentation (AutoDA) (Cubuk et al., 2019; Li et al., 2020c) which constructs useful augmented data for model training, our approach applies learnable augmentations to create more powerful adversarial examples. Second, we propose an efficient augmentation method, called the reverse adversarial update, to create new augmented references by utilizing adversarial noises. Nevertheless, ILA-DA forms new augmented references by interpolating images in the direction that leads to larger intermediate feature discrepancies. Through fine-tuning simple baseline attacks, such as the Iterative Fast Gradient Sign Method (I-FGSM), we show that ILA-DA outperforms state-of-the-art transfer-based attacks on both defended and undefended models by large margins. Visualizations of some adversarial examples created by ILA-DA are illustrated in Figure 1 and Appendix L. Our main contributions can be summarized as follows: \u2022 We propose ILA-DA, which applies three novel augmentation techniques, including automated data augmentation, reverse adversarial update and attack interpolation, to considerably strengthen the current ILA attack. \u2022 We demonstrate that ILA-DA outperforms various ILA attacks and other state-of-the-art approaches. On ImageNet, we attain an average attack success rate of 84.5%, which is 19.5% better than ILA and 4.7% better than the previous state-of-the-art across nine undefended models. \u2022 We show that ILA-DA with simple I-FGSM attack references can exceed state-of-the-art attacks on six defended models. We also find that incorporating ILA-DA into existing attacks can further increase their attack transferability. 2 RELATED WORKS: White-box and black-box attacks are two common threat models used in adversarial attack research. The white-box setting assumes that the attacker has access to the victim model\u2019s internal state, including its gradient, parameters, training dataset, etc. The black-box setting, on the other hand, only allows model queries without accessing any subsequent model information. While more variations such as the grey-box and no-box settings exist, they are generally not considered in this work. Typical white-box attacks exploit the gradient of the model to generate adversarial examples. The Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) generates attacks by adding the signed gradient of the loss with respect to the image back to the image, obtaining a higher loss and possibly incorrect prediction. The Iterative Fast Gradient Sign Method (I-FGSM, also known as BIM) (Kurakin et al., 2017a) performs FGSM iteratively while clipping the difference between the adversarial example and the original image within a perturbation budget. Projected Gradient Descent (PGD) (Madry et al., 2018) first initiates a random start within the perturbation budget and runs I-FGSM to update the attack. Under the black-box setting, the model information can be estimated by repeated model queries with varying inputs (Chen et al., 2017; Brendel et al., 2018). Recently, there are multiple attempts to reduce the number of model queries (Bhagoji et al., 2018; Ilyas et al., 2018; Li et al., 2020a). However, in order to provide an accurate estimation to the model information, the required number of model queries is still abnormally large, causing the attacks to be detected by real-world systems easily. This turns the spotlight back to transfer-based attacks, which create adversarial examples through black-box transferability instead of repeated attempts. 2.1 TRANSFER-BASED ATTACKS: Since strong gradient-based white-box attacks usually do not exhibit high transferability (Kurakin et al., 2017b), notable efforts have been devoted to improve transferability apart from increasing the attack strength. An observable direction of the previous works is to incorporate carefully-designed data augmentation techniques into existing attacks. MI-FGSM. Dong et al. (2018) first introduced the Momentum Iterative Fast Gradient Sign Method (MI-FGSM) to incorporate momentum to stabilize the update direction in iterative attacks, resulting in more transferable attacks. DIM. The Diverse Input Method (DI2-FGSM, or DIM) (Xie et al., 2019a) applies random resizing and zero-padding with a certain probability to the image in each iteration of I-FGSM. TIM. Dong et al. (2019) pointed out the discretion of attention map between the defended and undefended models and proposed the Translation-Invariant Method (TIM) by attacking an ensemble of translated images. To reduce the computational overhead, it was shown that kernel convolution can be applied to the gradient to achieve similar effects. SIM and NI-FGSM. Lin et al. (2020) introduced the scale-invariant property of DNN and proposed the Scale-Invariant Method (SIM) to attack an ensemble of images with a scaling factor of 0.5 on the pixel values. In the same paper, the Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) was introduced, aiming to escape from poor local maxima with the Nesterov accelerated gradient. Variance Tuning. Wang & He (2021) proposed variance tuning. For each iteration, the gradient is adjusted with an expectation of gradient sampled from the image\u2019s neighborhood. Combining variance tuning with the composition of previous methods, denoted as the Composite Transformation Method (CTM), they formed the attack VMI-CT-FGSM and VNI-CT-FGSM, resulting in one of the strongest transfer-based black-box attacks on defended models. Following the convention, we will name this series of work the CTM family. A table matching the names and the alias of the CTM family is shown in Appendix H. Admix. Wang et al. (2021) recognized the effectiveness of input transformation in attack transferability and proposed Admix. Inspired by Mixup (Zhang et al., 2018), Admix computes the gradient on a set of mixed images with different categories, which can be viewed as a special case of SIM, but with higher empirical contribution to transferability. SGM and LinBP. Unlike the CTM family, another type of works successfully raise the attack transferability by exploiting CNN model architectures. Wu et al. (2020) found that attack transferability can be enhanced by scaling up the gradient in the skip connection and referred to the technique as Skip Gradient Method (SGM). Guo et al. (2020) revised the linear property of DNN models and proposed LinBP to propagate backward without considering the non-linear layers. 2.2 INTERMEDIATE LEVEL ATTACK FOR FINE-TUNING ADVERSARIAL EXAMPLES: Consider an input image x, an existing adversarial attack x\u2032, a model F , and a function Fl that outputs the feature maps at layer l of the model. The original ILA projection loss proposed by Huang et al. (2019) minimizes the dot product: L(x, x\u2032, x\u2032\u2032) = \u2212\u2206y\u2032\u2032l \u00b7\u2206y\u2032l (1) where \u2206y\u2032l and \u2206y\u2032\u2032l are two vectors defined as follows: \u2206y\u2032l = Fl(x \u2032)\u2212 Fl(x) (2) \u2206y\u2032\u2032l = Fl(x \u2032\u2032)\u2212 Fl(x) (3) This is equivalent to maximizing \u2206y\u2032\u2032l \u00b7\u2206y\u2032l. The idea behind the formulation is to increase the norm without sacrificing the perturbation direction that causes misclassification. Due to the constraint of the perturbation budget, there is hardly room for fine-tuning in the image space. Instead, projection maximization is carried out in the feature space, specifically at layer l of the model. Based on the same framework, Li et al. (2020b) proposed another formulation of ILA: max x\u2032\u2032 (Fl(x\u2032\u2032)\u2212 Fl(x))Tw* (4) where w* is a pre-computed parameter vector that directly maps the intermediate-level discrepancies to predict the adversarial loss, skipping the remaining model layers. One notable trick of the design is that the computation of w* involves every feature map discrepancy Fl(x\u2032t)\u2212Fl(x) in each iteration t during the generation of x\u2032. We refer to this attack as ILA++ in the rest of this paper. 3 OUR METHOD: Based on what we have observed from the previous work, adversarial attacks tend to exhibit higher transferability when more references are involved. The increase of references can be achieved by data augmentation and reusing the temporal information over multiple iterations. From Equation (1), all three ILA input references x, x\u2032 and x\u2032\u2032 are images, suggesting the possibility of applying image augmentation like that for representation learning to improve model generalization. One natural way to augment the images is to apply image transformation to the input before retrieving the intermediate feature maps. Consequently, the function Fl can be substituted by F \u2032l (x) = Fl(T (x)) (5) where T can be any image transformation function. Different from conventional data augmentation that generalizes model representation, our task is to improve attack transferability, which can be interpreted as generalizing an adversarial example to succeed in attacking more models. To achieve the goal, we propose three augmentation techniques: automated data augmentation, reverse adversarial update, and attack interpolation. The algorithmic details for applying these augmentation techniques in ILA-DA are depicted in Algorithm 1. The clip function is to ensure that the first argument is within the range defined by the second and third arguments. Algorithm 1 ILA-DA Require: x, x\u2032, Fl, perturbation budget \u03f5, step size a, \u03b7, number of iterations n, a set of transforma- tions {\u03c6i}ki=1, augmentation parameter \u03b1 Initialize \u03b1 x\u2032\u2032 \u2190 x\u2032 for i\u2190 1 to n do xrev \u2190 2x\u2212 x\u2032 \u25b7 reverse adversarial update z \u223c p\u03b1(z) \u2206y\u2032l \u2190 Fl(Tz(x\u2032))\u2212 Fl(Tz(xrev)) \u2206y\u2032\u2032l \u2190 Fl(Tz(x\u2032\u2032))\u2212 Fl(Tz(xrev)) x\u2032\u2032 \u2190 x\u2032\u2032 \u2212 a sign(\u2207x\u2032\u2032(\u2212\u2206y\u2032\u2032l \u00b7\u2206y\u2032l)) x\u2032\u2032 \u2190 clip(x\u2032\u2032, x\u2212 \u03f5, x + \u03f5) x\u2032\u2032 \u2190 clip(x\u2032\u2032, 0, 1) \u03bb\u2190 \u2225 \u2206y\u2032\u2032l \u22252 \u2225\u2206y\u2032\u2032l \u22252+\u2225\u2206y\u2032l\u22252 x\u2032 \u2190 \u03bbx\u2032\u2032 + (1\u2212 \u03bb)x\u2032 \u25b7 attack interpolation \u03b1\u2190 \u03b1\u2212 \u03b7\u2207x\u2032\u2032(\u2212\u2206y\u2032\u2032l \u00b7\u2206y\u2032l) \u25b7 augmentation parameter update end for return x\u2032\u2032 3.1 AUGMENTATION LEARNING: Data augmentation is an effective technique to improve model generalization. However, it is nontrivial to select the most effective augmentation method for a specific task and dataset. As an Auto- mated Data Augmentation approach, AutoAugment (Cubuk et al., 2019) uses reinforcement learning to search for the optimal augmentation policy for a dataset. With similar motivation, we propose an automated way to learn effective image transformations that lead to a more vigorous ILA attack. We model the augmentation selection process Tz as sampling from a distribution p\u03b1(z) with the learnable parameter vector \u03b1 in Equation (6): Tz(x) = k\u2211 i=1 zi\u03c6i(x), z \u223c p\u03b1(z) (6) Sampled from p\u03b1(z), z is a one-hot vector that indicates the selection of the augmentation from a set of k transformations {\u03c6i}ki=1. The overall goal is to maximize the ILA objective in Equation (1) using the augmentation sampled from p\u03b1(z). We use the Gumbel-Softmax reparameterization trick to relax the discrete augmentation distribution to be continuous (Jang et al., 2017). In so doing, we can utilize stochastic gradient descent, with a learning weight parameter \u03b7, to optimize the objective. The technique is also adopted in neural architecture search (Xie et al., 2019b) and AutoDA (Li et al., 2020c). With the reparameterization trick, we can soften zi as: zi = f\u03b1i(gi) = exp ((log\u03b1i + gi)/\u03c4)\u2211k j=1 exp ((log\u03b1j + gj)/\u03c4) , (7) where gi = \u2212 log(\u2212 log(ui)) is the i-th Gumbel random variable, ui is a uniform random variable and \u03c4 is the temperature parameter of the softmax function. In our implementation, we set \u03c4 = 1 and sample from three common image transformation operations: translation, cropping, and rotation. It is worth noting that a perfect pixel alignment among x, x\u2032 and x\u2032\u2032 is required in ILA-DA, as a tiny shift in the image pixel induces misalignment in the feature maps. In other words, translation, cropping and rotation are required to be applied identically to all three ILA references. Otherwise, the optimization of ILA ends up being trivial due to the distorted feature discrepancies. 3.2 REVERSE ADVERSARIAL UPDATE ON THE CLEAN EXAMPLE: Let us reconsider Equation (2). To perform ILA, we need to obtain the feature map discrepancies between the reference attack x\u2032 and the unperturbed image x. Although x is correctly classified by the model, there is no information on how confident the model is and how well the input can represent the ground-truth class. In contrast, if x is not only correctly classified, but it is also done with a high confidence, the discrepancy of the feature maps is expected to intrinsically reflect more useful information about the attack. In order to boost the confidence of x, we propose to add a negative perturbation to it, which is exactly opposite to applying an adversarial attack. If we consider the entire model, this operation is expected to decrease the loss of the classifier, making the classification task easier by emphasizing features that are more significant for the task. Thus, we also expect the input to highly activate the intermediate layers, providing a better guidance for image update in ILA. Our preliminary study on the effect of reverse adversarial update can be found in Appendix E, which verifies the benign behavior brought by the reverse adversarial update. With the reverse adversarial perturbation, as expected, the model gives a lower loss and higher confidence on the correct class. Instead of crafting a new adversarial attack which would incur extra computational demand, we can extract the perturbation from the existing reference attack x\u2032, turning the transformation into Tadv(x) = x\u2212 (x\u2032 \u2212 x) = 2x\u2212 x\u2032 (8) The notion of reverse adversarial update shares some similarities with DeepDream (Mordvintsev et al., 2015), in which an image is updated towards the models\u2019 interpretation of the target class. After the transformation, we look for the feature discrepancies caused by a bi-directional perturbation \u00b1\u03f5 sign(\u2207xJ(\u03b8, x, y)), with the positive side being the adversarial attack and the negative side being the augmentation. 3.3 ATTACK INTERPOLATION ON THE REFERENCE ATTACK: Besides the clean image x, the reference attack x\u2032 is another factor to remain unchanged throughout all the ILA iterations. Given the image transformation in Equation (5), it is unlikely to work with another image transformation unaligned with the other images. Similar to Section 3.2, we instead exploit the adversarial noise to be the augmentation. According to the intuition of Best Transfer Direction (BTD) in Huang et al. (2019), the stronger the reference attack is, the better ILA performs. This inspires us to strengthen the reference attack during the iterations of ILA, by interpolating the reference attack itself with a stronger attack. The output of the previous iteration, x\u2032\u2032, is a good candidate for a strong attack. At iteration t, we set x\u2032t+1 = \u03bbx \u2032\u2032 t + (1\u2212 \u03bb)x\u2032t (9) where \u03bb is a weighting factor in the range [0, 1] to control the proportion of the two attacks. This augmentation is similar to mixup (Zhang et al., 2018), except that we are mixing two adversarial examples, rather than two images from different classes. With the assumption that the attack is finetuned to be more transferable in every iteration, by interpolating two attacks, we can strengthen the transferability of the reference attack while preserving a part of its original characteristics. Based on our preliminary findings, setting \u03bb to non-trivial values such as 0.5 suffices to yield satisfactory performance. However, a constant value lacks consideration of the behavior of the two attacks. Precisely, we hope to perform an adaptive interpolation depending on the effectiveness of x\u2032\u2032t . If x\u2032\u2032t is weak after a single update, \u03bb should bias towards preserving the reference attack more than mixing with the new attack. Since ILA focuses on maximizing the linear projection between feature map discrepancies, the norm of the feature map discrepancy can be a good indicator of the performance. Consequently, we set \u03bb = \u2225\u2206y\u2032\u2032l \u22252 \u2225\u2206y\u2032\u2032l \u22252 + \u2225\u2206y \u2032 l\u22252 (10) If we neglect the transformation in Section 3.2 for simplicity, \u2206y\u2032l and \u2206y\u2032\u2032l will remain the same as what we obtained from Equations (2) and (3), respectively. Note that the value of \u03bb is recomputed in each iteration before interpolation is applied. A similar approach of reusing previous attacks in the temporal domain can be applied to the reverse adversarial update in Section 3.2. The reference attack x\u2032 in Equation (8) can be further extended to a collection of x\u2032t for every iteration t. This is not the only work to consider the temporary values of the reference attack before its convergence. ILA++ (Li et al., 2020b) also collects all losses computed during the generation of the reference attack, achieving superiority over the standard ILA. Different from Li et al. (2020b) which uses past losses (with respect to x\u2032) to help update x\u2032\u2032t , we make use of the past x\u2032t directly as an augmentation to enrich the information regarding feature discrepancies under different attacks. 4 EXPERIMENTAL RESULTS: In this section, we evaluate the attack transferability of ILA-DA in comparison with previous methods based on fine-tuning as well as state-of-the-art transfer-based attack generation methods on multiple defended and undefended models. Next, we investigate the effects of the three augmentation techniques and identify influential factors for the transferability of an attack. 4.1 SETUP: Undefended models. Following the line of work in transfer-based attacks, a total of nine classification models are used to evaluate attack transferability, including ResNet50 (He et al., 2016), VGG19 (Simonyan & Zisserman, 2015), Inception V3 (Inc-v3) (Szegedy et al., 2016), Wide ResNet50 2x (WRN) (Zagoruyko & Komodakis, 2016), DenseNet161 (DenseNet) (Huang et al., 2017), ResNeXt101 (ResNeXt) (Xie et al., 2017), MobileNet V2 (MobileNet) (Sandler et al., 2018), PNASNet5 (PNASNet) (Liu et al., 2018) and SENet50 (SENet) (Hu et al., 2018). Among these models, we use ResNet50, VGG19 and Inception V3 as the source models of the attack. All the models are pretrained on ImageNet (Russakovsky et al., 2015), with the model parameters of PNASNet1 and SENet2 obtained from public repositories and the remaining from Torchvision (Paszke et al., 2019). 1https://github.com/Cadene/pretrained-models.pytorch 2https://github.com/moskomule/senet.pytorch For the choice of the intermediate layer, we opt the layer 3-1 for ResNet50, layer 9 for VGG19, and layer 6a for Inception V3, where the former two have been shown to result in good performance by Li et al. (2020b). Due to the ineffectiveness of transfer-based targeted attacks (Liu et al., 2017), we report the untargeted attacks with \u03f5 values of 8/255 (0.03) under the \u2113\u221e norm constraint. To measure the attack success rate, we randomly sample 5000 images from the ILSVRC2012 validation set with all images being classified correctly by the nine models. The default number of iterations of I-FGSM is 10 and the attack step size is set to max( 1255 , \u03f5 no. of iterations ). To mount a complete attack, we first run I-FGSM for 10 iterations on the source model, and then pass the example as the reference attack to ILA-DA to perform fine-tuning for 50 iterations. The same model is used as both the source model of I-FGSM and surrogate model of ILA-DA. To demonstrate the superiority of ILA-DA over the ILA family, we include the result of ILA (Huang et al., 2019) and ILA++ (Li et al., 2020b). Besides the ILA family, we also append the results of other state-of-the-art transfer-based attacks, namely MI-CT-FGSM and NI-CT-FGSM, with their corresponding variations with variance tuning, SGM and LinBP as introduced in Section 2.1. Since the experimental setting in the CTM family is slightly different from ours and LinBP, we reproduce the works in the CTM family and apply in our experimental environment. The results of the attacks on undefended models are shown in Table 1, with the details of the hyper-parameters listed in Appendix F. We also conduct extensive experiments using other perturbation budgets, source models and datasets. The results can be found in Appendix D, G and I respectively. Defended models. We follow the experimental setup used in the previous works (Dong et al., 2018; 2019; Wang & He, 2021), which includes 1000 pre-sampled test images and a set of models which are pre-trained under ensemble adversarial training (Tramer et al., 2018). Then we compare ILADA with the baselines introduced in Section 2.1, including I-FGSM, MI-FGSM, MI-CT-FGSM, NI-CT-FGSM, Admix and variance tuning. We pick Inception V3 as the source model and an adversarially trained Inception V3 as the surrogate model of ILA-DA, and all attacks are performed with perturbation size \u03f5 = 16/255(\u2248 0.063). The attacks are evaluated against three robust models: an Inception V3 trained adversarially with an ensemble of 3 models (Inc-v3ens3), an Inception V3 trained adversarially with an ensemble of 4 models (Inc-v3ens4), and an Inception-ResNet V2 trained adversarially with an ensemble of 3 models (IncRes-v2ens). Besides, we also test the attacks against the top three submitted defenses in the NIPS 2017 adversarial competition: HGD (Liao et al., 2018), R&P (Xie et al., 2018) and MMD3. For all three defenses, we adopt the default models and hyper-parameters used in their corresponding repositories. Since the gradients of the adversarially trained models are more difficult to exploit, we increase the number of ILA-DA iterations to 500 in order to guarantee convergence of the attack. The attack success rate against each of the defenses is reported in Table 2, while a study of the number of iterations is reported in Appendix A. 4.2 BLACK-BOX TRANSFERABILITY OF ILA-DA ON UNDEFENDED MODELS: ILA-DA outperforms ILA and ILA++ on 8 transferred models by a large margin. Although the adversarial examples generated by ILA-DA are slightly less effective on the source model (ResNet50), the examples achieve remarkable attack success rates on other transferred models (see Table 1). In particular, ILA-DA attains an average attack success rate of 84.5%, which is 19.5% better than ILA and 4.7% better than the previous state-of-the-art (VNI-CT-FGSM). The improved attack transferability validates the effectiveness of our proposed augmentation techniques. It is worth mentioning that the attack difficulty increases with the differences in model architectures. For example, the attacks generated from ResNet (with skip connections) tend to transfer better to models also with skip connections, such as WRN, DenseNet, ResNeXt, compared to Inception V3 and PNASNet. Even under such model dissimilarity, the improvement in attack transferability caused by ILA-DA is more prominent than the ILA baselines. We believe such a phenomenon can be attributed to the extensive augmentations that enable both state-of-the-art attacks and ILA-DA to exploit gradients with better resemblance to different architectures. 4.3 BLACK-BOX TRANSFERABILITY OF ILA-DA ON DEFENDED MODELS: In Table 2, ILA-DA also shows outstanding attack transferability against six defended models. Unlike the CTM family, ILA-DA does not require stacking multiple attack methods to achieve high 3https://github.com/anlthms/nips-2017/tree/master/mmd attack transferability. Based on a weak I-FGSM reference attack, I-FGSM + ILA-DA yields an average attack success rate of 75.9%, which outperforms the strongest state-of-the-art (VNI-CTFGSM) by 4.4%. Moreover, incorporating ILA-DA to existing methods brings further improvements. Specifically, using the attack reference generated by MI-Admix-TI-DIM, ILA-DA can attain the highest attack success rate of 81.9%. As the defended models are trained to be robust to adversarial inputs, more ILA-DA fine-tuning is required in comparison to undefended models (see Appendix A and Appendix K). Although using a larger number of ILA-DA iterations induces higher computational overhead, ILA-DA could still achieve comparable performance with state-of-the-art attacks such as VNI-CT-FGSM and LinBP under similar running time, while ILA-DA also exhibits a great degree of improvement when the time budget is up-scaled. 4.4 ABLATION STUDY: ILA-DA can be regarded as an extension of the original ILA, equipped with three ILA augmentation techniques: automated data augmentation, reverse adversarial update, and attack interpolation. We conduct an ablation study on the effects of the three augmentations by removing each of them from ILA-DA and reporting the performance of the reduced methods in Table 3. In addition, we also study two non-automated ways of applying translation, cropping and rotation to the ILA references. In particular, Random Augmentation randomly applies one of the three augmentations; All Augmentation applies all three augmentations to the reference images. While all the three proposed augmentation techniques contribute to the improved attack transferability on average, Table 3 reveals that applying translation, cropping and rotation to the ILA references increases the attack success rate the most, followed by the use of attack interpolation and reverse adversarial update. Although reverse adversarial update and attack interpolation slightly lower the performance by 0.5% in 2 out of 16 settings, the improvements in transferability across different models brought by the two augmentations are evident. The study also validates the use of the proposed automated data augmentation scheme. It is shown that applying all augmentations may cause excessive distortion to the images and lead to a significant drop in the attack success rate. Nevertheless, using the learned augmentation parameter shows non-trivial improvements over random augmentation. 5 CONCLUSION: In this paper, we presented ILA-DA, which applies three types of data augmentation techniques to the original ILA formulation. Specifically, we proposed an automated data augmentation method which is searched by learnable parameters, reverse adversarial update which exploits the adversarial noise to augment the reference, and attack interpolation which strengthens the reference attack across iterations. Incorporating all the proposed augmentations, ILA-DA outperforms state-of-theart methods for transfer-based attacks on both undefended and defended models. We also studied the contribution of each augmentation technique to ILA-DA attack transferability. This work showcases how data augmentation techniques could benefit the generation of transfer-based adversarial attacks. We believe that it is beneficial to extend the proposed automated method to search for more types of data augmentation, other augmentation parameters, and even the optimal ILA layer to generate the attack. A NUMBER OF ILA ITERATIONS: With extensive augmentation, we also search for the number of iterations for ILA-DA to result in a stable attack. We vary the number of iterations from 5 to 2000, in both the setup of undefended (\u03f5 = 8/255) and defended models (\u03f5 = 16/255). A plot of the result is shown in Figure 2. For undefended models, ILA-DA could yield attacks with high transferability within 50 iterations. However, for defended models, it requires more iterations to converge to an effective attack. B EFFECT OF THE CHOICE OF INTERMEDIATE LAYER: We apply ILA and ILA-DA at several layers near the suggested default layer of ILA. As shown in Figure 3, ILA-DA is strictly better than ILA in terms of attack success rate in every layer tested. Furthermore, the inverted U-curve patterns exhibited in Figure 3 suggest that the method is not very sensitive to the selection of layer around the layer proposed by ILA. C ANALYSIS OF THE LEARNED AUGMENTATION POLICY: ILA-DA optimizes three image transformation operations that are commonly used, including translation, cropping and rotation. On the ImageNet dataset, we discovered that the cropping operation yields the strongest attack. Empirical results also show that the augmentation parameters converged to certain fixed points. In Figure 4a, ILA-DA explores different augmentations in the first few iterations and discovers random cropping as the best transformations for maximizing the ILA objective. Figure 4b further shows that the converged augmentation parameters are applied to the attack references throughout the ILA-DA fine-tuning. D RESULTS ON UNDEFENDED MODELS WITH LARGER PERTURBATION BUDGETS: As a supplementary to Table 1 which shows the attack results with \u03f5 = 8/255 (0.03) on undefended models, Table 4 compares the performance of ILA-DA with other baselines with \u03f5 = 13/255 (0.05). The experimental results show that ILA-DA leads I-FGSM, ILA, ILA++ and VMI-CT-FGSM by a large margin. E EFFECT OF REVERSE ADVERSARIAL UPDATE ON THE MODEL PERFORMANCE: We would like to verify the intuition of using reverse adversarial update, specifically, whether in practice such operation can decrease the loss and increase the model confidence. Therefore, we randomly sample 5000 images, including those that are misclassified by the model. Then we apply I-FGSM10 in the form of reverse adversarial update to the images, and pass them back to the model for classification. We record the loss and apply softmax to the logits to obtain the confidence, and report the average result of all 5000 images. The result is summarized in Table 5. Furthermore, we also inspect the class activation map (CAM) of the source model. Figure 5 visualizes some of the results. For some images (the first two), updating reversely helps the model focus on more features. For the remaining (the latter two), adversarial reverse update does not exhibit a significant change of intermediate layers\u2019 contribution to the ground-truth class. To test the effect of reversely updated examples on targeted models, we also feed them into the remaining undefended models and observe the change in accuracy. The result is reported in Table 6, which shows that reverse adversarial update not only benefits the source model, but also many of the other models with similar architecture. F HYPER-PARAMETERS OF THE BASELINES: We reproduced the algorithms of CTM (DIM, TIM, SIM), MI-FGSM, NI-FGSM and variance tuning according to the repository released by Wang & He (2021), and tested the attacks under our experimental settings. The hyper-parameters used in the experiments are shown in Table 7, which are the default values in their corresponding papers. For the implementation of SGM and LinBP, we directly use the official implementation released by Guo et al. (2020). G ATTACKS USING OTHER SOURCE MODELS: In this section, we test the effectiveness of ILA-DA with other source models. In particular, we adopt VGG19 and Inception V3 as the source models for both the reference attack and ILA-DA, and re-evaulate the attack success rate on the remaining eight undefended models. The results are shown in Table 8 and Table 9. For Inception V3, ILA-DA still outperforms every other baseline. For VGG19, the performance of I-FGSM + ILA-DA is slightly worse than that of the best attack, VNI-CT-FGSM. However, when we fine-tune ILA-DA on a stronger reference attack such as MICT-FGSM, our method regains the best average attack success rate among all other methods. H NAME MATCHING OF THE CTM FAMILY:  I EXPERIMENTS WITH MORE DATASETS: To show the generalization capability of ILA-DA, we conduct experiments with two more datasets, namely CIFAR-10 and CIFAR-100 (Krizhevsky, 2012). For CIFAR-10, we follow Li et al. (2020b), which considers six models: VGG19 with batch normalization (Simonyan & Zisserman, 2015), Wide ResNet-28-10 (WRN) (Zagoruyko & Komodakis, 2016), ResNeXt-29 (ResNeXt) (Xie et al., 2017), DenseNet-190 (DenseNet) (Huang et al., 2017), PyramidNet\u2020 (Han et al., 2017) (with ShakeDrop (Yamada et al., 2018) and AutoAugment (Cubuk et al., 2019)), and also GDAS (Dong & Yang, 2019). The former four models are also selected to be used in the CIFAR-100 setup, with the model parameters obtained from a public repository4. For both datasets, we randomly sample 5000 images from the test set that are classified correctly by all four models and we pick VGG19 to be the source model. The experimental results on CIFAR-10 and CIFAR-100 are shown in Table 11. For both CIFAR-10 and CIFAR-100, the attack success rates of the baselines are much higher than that of ImageNet. Although ILA-DA mostly attains the highest attack success rate among the base- 4https://github.com/bearpaw/pytorch-classification lines, we observe that the degree of improvement is not as high as that in Table 1. It may be because of the tiny size and low resolution (32\u00d732 for both CIFAR-10 and CIFAR-100) of the images, leading to a reduction in the effect of data augmentation. J EFFECT OF \u03bb ON ATTACK INTERPOLATION In this section, we study the effect of \u03bb used for attack interpolation, with fixed values between 0 and 1, in addition to the adaptive choice computed from the norm of the feature map discrepancy. The result is shown in Figure 6. While a non-trivial value of \u03bb suffices to improve the performance, the adaptive selection using Equation (10) results in optimal or sub-optimal attack transferability in most cases. K STUDY ON THE RUNNING TIME OF THE ALGORITHMS: In this section, we compare the running time between ILA-DA and other state-of-the-art attacks. We report the average running time required to run attack on a batch from the ILSVRC2012 validation set, with a batch size of 100. The remaining setup is the same as the previous experiments, where all hyper-parameters are specified in Appendix F. We measure the running time required for attacking a batch and report the mean and standard deviation of the running time across 5 batches. All the experiments are performed on a machine with an Intel Xeon Silver 4214R CPU and Nvidia GeForce RTX3090 GPU. The result is summarized in Table 12. L MORE VISUALIZATIONS ON THE GENERATED EXAMPLES: "}, {"role": "assistant", "content": "Okay, I will review the paper \"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\" for ICLR 2023.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetBudget'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Budget retrieved successfully\",\n  \"output\": {\n    \"total_resource_budget\": 100,\n    \"current_resource_usage\": 0,\n    \"current_remaining_resource_budget\": 100,\n    \"total_expense_budget\": 10000,\n    \"current_expense\": 0.09427139999999998,\n    \"current_remaining_expense_budget\": 9999.9057286\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetBudget` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetBudget', response={'result': {'status': 'success', 'message': 'Budget retrieved successfully', 'output': {'total_resource_budget': 100, 'current_resource_usage': 0, 'current_remaining_resource_budget': 100, 'total_expense_budget': 10000, 'current_expense': 0.09427139999999998, 'current_remaining_expense_budget': 9999.9057286}}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'directory': '.'}, name='ListFiles'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Files listed successfully\",\n  \"error\": null,\n  \"output\": [\n    \"favicon.ico\",\n    \"requirements.txt\",\n    \"HASHIRU_LOGO.png\",\n    \"HASHIRU_ARCH.png\",\n    \"deleteAgents.py\",\n    \"CITATION.cff\",\n    \"tests\",\n    \"__pycache__\",\n    \"start.py\",\n    \"README.md\",\n    \"results\",\n    \"HASHIRU.png\",\n    \".gitignore\",\n    \".env\",\n    \"static\",\n    \".gitattributes\",\n    \"app.py\",\n    \"HASHIRU_2.png\",\n    \".git\",\n    \"paper_review_hashiru_20250627_201357.jsonl\",\n    \"src\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ListFiles` with \n```json\n{\n  \"directory\": \".\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ListFiles', response={'result': {'status': 'success', 'message': 'Files listed successfully', 'error': None, 'output': ['favicon.ico', 'requirements.txt', 'HASHIRU_LOGO.png', 'HASHIRU_ARCH.png', 'deleteAgents.py', 'CITATION.cff', 'tests', '__pycache__', 'start.py', 'README.md', 'results', 'HASHIRU.png', '.gitignore', '.env', 'static', '.gitattributes', 'app.py', 'HASHIRU_2.png', '.git', 'paper_review_hashiru_20250627_201357.jsonl', 'src']}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'website': 'iclr.cc', 'query': 'ICLR 2023 review criteria'}, name='GoogleSearchTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": [\n    \"https://iclr.cc/Conferences/2023/ReviewerGuide\",\n    \"https://blog.iclr.cc/2023/04/12/ethics-review-process-for-iclr-2023/\",\n    \"https://blog.iclr.cc/category/iclr-2023/\",\n    \"https://blog.iclr.cc/tag/reviewing/\",\n    \"https://blog.iclr.cc/tag/awards/\",\n    \"https://blog.iclr.cc/2023/04/05/announcing-notable-reviewers-and-area-chairs-at-iclr-2023/\",\n    \"https://iclr.cc/Conferences/2025/FAQ\",\n    \"https://iclr.cc/Conferences/2023/CallForTinyPapers\",\n    \"https://iclr.cc/Conferences/2019/Reviewer_Guidelines\",\n    \"https://iclr.cc/Conferences/2022/ReviewerGuide\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `GoogleSearchTool` with \n```json\n{\n  \"website\": \"iclr.cc\",\n  \"query\": \"ICLR 2023 review criteria\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GoogleSearchTool', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': ['https://iclr.cc/Conferences/2023/ReviewerGuide', 'https://blog.iclr.cc/2023/04/12/ethics-review-process-for-iclr-2023/', 'https://blog.iclr.cc/category/iclr-2023/', 'https://blog.iclr.cc/tag/reviewing/', 'https://blog.iclr.cc/tag/awards/', 'https://blog.iclr.cc/2023/04/05/announcing-notable-reviewers-and-area-chairs-at-iclr-2023/', 'https://iclr.cc/Conferences/2025/FAQ', 'https://iclr.cc/Conferences/2023/CallForTinyPapers', 'https://iclr.cc/Conferences/2019/Reviewer_Guidelines', 'https://iclr.cc/Conferences/2022/ReviewerGuide']}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'output_type': 'full_text', 'url': 'https://iclr.cc/Conferences/2023/ReviewerGuide'}, name='GetWebsite'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR 2022 Reviewer Instructions\\n\\n\\n\\n\\n\\nSkip to yearly menu bar\\nSkip to main content\\n\\nMain Navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            ICLR\\n        \\n\\n\\n\\n\\n                        Help/FAQ\\n                    \\n\\n\\n\\n\\n\\n\\n                        Contact ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n                        Downloads\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Blog\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n                        Privacy Policy\\n                    \\n\\n\\n\\n\\n\\n\\n                        Create Profile\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reset Password\\n                    \\n\\n\\n\\n\\n\\n\\n                        Journal To Conference Track\\n                    \\n\\n\\n\\n\\n\\n\\n                        Diversity & Inclusion\\n                    \\n\\n\\n\\n\\n\\n\\n                        Proceedings at OpenReview\\n                    \\n\\n\\n\\n\\n\\n\\n                        Future Meetings\\n                    \\n\\n\\n\\n\\n\\n\\n                        Press\\n                    \\n\\n\\n\\n\\n\\n\\n                        Exhibitor Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Twitter\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        My Stuff\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Login\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Select Year: (2023)\\n        \\n\\n\\n2026\\n                \\n\\n\\n\\n2025\\n                \\n\\n\\n\\n2024\\n                \\n\\n\\n\\n2023\\n                \\n\\n\\n\\n2022\\n                \\n\\n\\n\\n2021\\n                \\n\\n\\n\\n2020\\n                \\n\\n\\n\\n2019\\n                \\n\\n\\n\\n2018\\n                \\n\\n\\n\\n2017\\n                \\n\\n\\n\\n2016\\n                \\n\\n\\n\\n2015\\n                \\n\\n\\n\\n2014\\n                \\n\\n\\n\\n2013\\n                \\n\\n\\n\\n\\n\\n\\n                        Dates\\n                    \\n\\n\\n\\n\\n            Calls\\n        \\n\\n\\n\\n\\n                        Call for Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Tiny Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Workshops\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Blog Posts\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Socials\\n                    \\n\\n\\n\\n\\n\\n\\n            Guides\\n        \\n\\n\\n\\n\\n                        Author Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewer Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Area Chair Guide\\n                    \\n\\n\\n\\n\\n\\n\\n            Attend\\n        \\n\\n\\n\\n\\n                        Register\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visa and Travel  Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        Hotels\\n                    \\n\\n\\n\\n\\n\\n\\n                        Child Care\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visiting Kigali\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n            Organization\\n        \\n\\n\\n\\n\\n                        ICLR Board\\n                    \\n\\n\\n\\n\\n\\n\\n                        Organizing Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Program Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Ethics Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        Conference Site\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nICLR 2023 Reviewer Guide\\nThank you for agreeing to serve as an ICLR 2023 reviewer. Your contribution as a reviewer is paramount to creating an exciting and high-quality program. We ask that:\\n\\nYour reviews are timely and substantive.\\nYou follow the reviewing guidelines below.\u00a0\\nYou adhere to our Code of Ethics in your role as a reviewer. You must also adhere to our Code of Conduct.\\n\\nThis guide is intended to help you understand the ICLR 2023 decision process and your role within it. It contains:\\n\\nAn outline of the main reviewer tasks\\nStep-by-step reviewing instructions (especially relevant for reviewers that are new to ICLR)\\nReview examples\\nAn FAQ.\\n\\n\u00a0\\n\\n\\nWe\u2019re counting on you\\nAs a reviewer you are central to the program creation process for ICLR 2023. Your Area Chairs (ACs), Senior Area Chairs (SACs) and the Program Chairs (PCs) will rely greatly on your expertise and your diligent and thorough reviews to make decisions on each paper. Therefore, your role as a reviewer is critical to ensuring a strong program for ICLR 2023.\\nHigh-quality reviews are also very valuable for helping authors improve their work, whether it is eventually accepted by ICLR 2023, or not. Therefore it is important to treat each valid ICLR 2023 submission with equal care.\\nAs a token of our appreciation for your essential work, top reviewers will be acknowledged permanently on the ICLR 2023 website. Furthermore, top and high quality reviewers will receive special acknowledgement during the opening ceremony and free registration to ICLR 2023.\\n\u00a0\\n\\nMain reviewer tasks\\nThe main reviewer tasks and dates are as follows:\\n\\nCreate or update your OpenReview profile (September 30 2022)\\nBid on papers (September 30 2022 - October 06 2022)\\nWrite a constructive, thorough and timely review (October 12 2022 - October 24 2022)\\nInitial paper reviews released (November 4 2022)\\nDiscuss with authors and other reviewers to clarify and improve the paper (November 4 2022 - November 18 2022)\\nFlag any potential CoE violations and/or concerns (by November 18 2022)\\nProvide a recommendation to the area chair assigned to the paper (by November 18 2022)\\nVirtual Meeting with AC if paper you reviewed falls into borderline papers (November 18 2022 - December 12 2022)\\nProvide a final recommendation to the area chair assigned to the paper (after Virtual Meeting)\\n\\n\u00a0\\n\\n\\n\u00a0\\nCode of Ethics\\nAll ICLR participants, including reviewers, are required to adhere to the ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics). All reviewers are required to read the Code of Ethics and adhere to it. The Code of Ethics applies to all conference participation, including paper submission, reviewing, and paper discussion.\u00a0\\nAs part of the review process, reviewers are asked to raise potential violations of the ICLR Code of Ethics. Note that authors are encouraged to discuss questions and potential issues regarding the Code of Ethics as part of their submission. This discussion is not counted against the maximum page limit of the paper and should be included as a separate section.\\n\u00a0\\n\\n\u00a0\\nReviewing a submission: step-by-step\\nSummarized in one sentence, a review aims to determine whether a submission will bring sufficient value to the community and contribute new knowledge. The process can be broken down into the following main reviewer tasks:\\n\u00a0\\n\\n\\nRead the paper: It\u2019s important to carefully read through the entire paper, and to look up any related work and citations that will help you comprehensively evaluate it. Be sure to give yourself sufficient time for this step.\\nWhile reading, consider the following:\\n\\nObjective of the work: What is the goal of the paper? Is it to better address a known application or problem, draw attention to a new application or problem, or to introduce and/or explain a new theoretical finding? A combination of these? Different objectives will require different considerations as to potential value and impact.\\nStrong points: is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g. theoretically, algorithmically, etc.)?\\nWeak points: is it weak in any of the aspects listed in b.?\\nBe mindful of potential biases and try to be open-minded about the value and interest a paper can hold for the entire ICLR community, even if it may not be very interesting for you.\\n\\n\\nAnswer four key questions for yourself, to make a recommendation to Accept or Reject:\\n\\nWhat is the specific question and/or problem tackled by the paper?\\nIs the approach well motivated, including being well-placed in the literature?\\nDoes the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\\nWhat is the significance of the work? Does it contribute new knowledge and sufficient value to the community? Note, this does not necessarily require state-of-the-art results. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge (incl., empirical, theoretical, for practitioners, etc).\\n\\n\\nWrite and submit your initial review, organizing it as follows:\u00a0\\n\\nSummarize what the paper claims to contribute. Be positive and constructive.\\nList strong and weak points of the paper. Be as comprehensive as possible.\\nClearly state your initial recommendation (accept or reject) with one or two key reasons for this choice.\\nProvide supporting arguments for your recommendation.\\nAsk questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.\u00a0\\nProvide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\\n\\n\\nComplete the CoE report: ICLR has adopted the following Code of Ethics (CoE). When submitting your review, you\u2019ll be asked to complete a CoE report for the paper. The report is a simple form with two questions. The first asks whether there is a potential violation of the CoE. The second is relevant only if there is a potential violation and asks the reviewer to explain why there may be a potential violation. In order to answer these questions, it is therefore important that you read the CoE before starting your reviews. \\n\\t\u00a0\\nEngage in discussion: The discussion phase at ICLR is different from most conferences in the AI/ML community. During this phase, reviewers, authors and area chairs engage in asynchronous discussion and authors are allowed to revise their submissions to address concerns that arise. It is crucial that you are actively engaged during this phase. Maintain a spirit of openness to changing your initial recommendation (either to a more positive or more negative) rating.\\nBorderline paper meeting: This year, we are introducing a new program for ACs to (virtually) meet and discuss with reviewers only for borderline cases. ACs will reach out to schedule this meeting. This is to ensure active discussions among reviewers, and well-thought-out decisions. ACs will schedule the meeting and facilitate the discussion. For a productive discussion, it is important to familiarize yourself with other reviewers' feedback prior to the meeting. Please note that we will be leveraging information for reviewers who failed to attend this meeting (excluding emergencies).\u00a0\\nProvide final recommendation: Update your review, taking into account the new information collected during the discussion phase, and any revisions to the submission. State your reasoning and what did/didn\u2019t change your recommendation throughout the discussion phase.\\n\\n\u00a0\\n\\n\u00a0\\nFor great in-depth resources on reviewing, see these resources:\\n\\nDaniel Dennet, Criticising with Kindness.\\nComprehensive advice: Mistakes Reviewers Make\\nViews from multiple reviewers: Last minute reviewing advice\\nPerspective from instructions to Area Chairs: Dear ACs.\\n\\n\u00a0\\n\\n\u00a0\\nReview Examples\\nHere are two sample reviews from previous conferences that give an example of what we consider a good review for the case of leaning-to-accept and leaning-to-reject.\\nReview for a Paper where Leaning-to-Accept\\nThis paper proposes a method, Dual-AC, for optimizing the actor (policy) and critic (value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor. In order to achieve that, they take the linear programming approach of solving the Bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems. The discussion on the deficiencies of the naive LP approach is mostly well done. Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without losing convexity by applying a regularization. They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements. Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms.\u00a0\\nOverall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments. Given these clarifications in an author response, I would be willing to increase the score.\u00a0\\nFor the theory, there are a few steps that need clarification and further clarification on novelty. For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results. It looks like Theorem 2 has already been shown in \\\"Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time\u201d. There is a statement that \u201cChen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting. However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization\u201d. Is your Theorem 2 somehow an extension? Is Theorem 3 completely new?\\nThis is particularly called into question due to the lack of assumptions about the function class for value functions. It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function). This assumption seems to be used right at the bottom of Page 17, where U^{pi*} = V^*. Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small. More about conditions on eta_v would be illuminating.\u00a0\\nThere is also one step in the theorem that I cannot verify. On Page 18, how is the squared removed for difference between U and Upi? The transition from the second line of the proof to the third line is not clear. It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2.\u00a0\\n\u00a0\\n\\nFor the experiments, the following should be addressed.\\n1. It would have been better to also show the performance graphs with and without the improvements for multiple domains.\\n2. The central contribution is extending the single step LP to a multi-step formulation. It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains.\\n3. Increasing k also comes at a computational cost. I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO).\\n4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization. It was also mentioned that increasing the regularization parameter size increases the convergence rate. Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization? In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned. Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity? A bit more discussion on these choices would be helpful.\u00a0\\n\u00a0\\n\\nMinor comments:\\n1. Page 2: In equation 5, there should not be a 'ds' in the dual variable constraint\\n\u00a0\\n\\n\u00a0\\nReview for a Paper where Leaning-to-Reject\\nThis paper introduces a variation on temporal difference learning for the function approximation case that attempts to resolve the issue of over-generalization across temporally-successive states. The new approach is applied to both linear and non-linear function approximation, and for prediction and control problems. The algorithmic contribution is demonstrated with a suite of experiments in classic benchmark control domains (Mountain Car and Acrobot), and in Pong.\\nThis paper should be rejected because (1) the algorithm is not well justified either by theory or practice, (2) the paper never clearly demonstrates the existence of problem they are trying to solve (nor differentiates it from the usual problem of generalizing well), (3) the experiments are difficult to understand, missing many details, and generally do not support a significant contribution, and (4) the paper is imprecise and unpolished.\\nMain argument\\nThe paper does not do a great job of demonstrating that the problem it is trying to solve is a real thing. There is no experiment in this paper that clearly shows how this temporal generalization problem is different from the need to generalize well with function approximation. The paper points to references to establish the existence of the problem, but for example the Durugkar and Stone paper is a workshop paper and the conference version of that paper was rejected from ICLR 2018 and the reviewers highlighted serious issues with the paper\u2014that is not work to build upon. Further the paper under review here claims this problem is most pressing in the non-linear case, but the analysis in section 4.1 is for the linear case.\u00a0\\nThe resultant algorithm does not seem well justified, and has a different fixed point than TD, but there is no discussion of this other than section 4.4, which does not make clear statements about the correctness of the algorithm or what it converges to. Can you provide a proof or any kind of evidence that the proposed approach is sound, or how it\u2019s fixed point relates to TD?\\nThe experiments do not provide convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches. There are so many missing details it is difficult to draw many conclusions:\\n\\nWhat was the policy used in exp1 for policy evaluation in MC?\\nWhy Fourier basis features?\\nIn MC with DQN how did you adjust the parameters and architecture for the MC task?\\nWas the reward in MC and Acrobot -1 per step or something else\\nHow did you tune the parameters in the MC and Acrobot experiments?\\nWhy so few runs in MC, none of the results presented are significant?\\nWhy is the performance so bad in MC?\\nDid you evaluate online learning or do tests with the greedy policy?\\nHow did you initialize the value functions and weights?\\nWhy did you use experience replay for the linear experiments?\\nIN MC and Acrobot why only a one layer MLP?\\n\\nIgnoring all that, the results are not convincing. Most of the results in the paper are not statistically significant. The policy evaluation results in MC show little difference to regular TD. The Pong results show DQN is actually better. This makes the reader wonder if the result with DQN on MC and Acrobot are only worse because you did not properly tune DQN for those domains, whereas the default DQN architecture is well tuned for Atari and that is why you method is competitive in the smaller domains.\u00a0\\nThe differences in the \u201caverage change in value plots\u201d are very small if the rewards are -1 per step. Can you provide some context to understand the significance of this difference? In the last experiment linear FA and MC, the step-size is set equal for all methods\u2014this is not a valid comparison. Your method may just work better with alpha = 0.1.\u00a0\\n\u00a0\\n\\nThe paper has many imprecise parts, here are a few:\\n\\nThe definition of the value function would be approximate not equals unless you specify some properties of the function approximation architecture. Same for the Bellman equation\\nequation 1 of section 2.1 is neither an algorithm or a loss function\\nTD does not minimize the squared TD. Saying that is the objective function of TD learning in not true\\nend of section 2.1 says \u201cIt is computed as\u201d but the following equation just gives a form for the partial derivative\\nequation 2, x is not bounded\u00a0\\nYou state TC-loss has an unclear solution property, I don\u2019t know what that means and I don\u2019t think your approach is well justified either\\nSection 4.1 assumes linear FA, but its implied up until paragraph 2 that it has not assumed linear\\ntreatment of n_t in alg differs from appendix (t is no time episode number)\\nYour method has a n_t parameter that is adapted according to a schedule seemingly giving it an unfair advantage over DQN.\\nOver-claim not supported by the results: \u201cwe see that HR-TD is able to find a representation that is better at keeping the target value separate than TC is \u201c. The results do not show this.\\nSection 4.4 does not seem to go anywhere or produce and tangible conclusions\\n\\n\u00a0\\n\\nThings to improve the paper that did not impact the score:\\n\\nIt\u2019s hard to follow how the prox operator is used in the development of the alg, this could use some higher level explanation\\nIntro p2 is about bootstrapping, use that term and remove the equations\\nIt\u2019s not clear why you are talking about stochastic vs deterministic in P3\\nPerhaps you should compare against a MC method in the experiments to demonstrate the problem with TD methods and generalization\\nSection 2: \u201ccan often be a regularization term\u201d >> can or must be?\\nupdate law is an odd term\\n\u00a0tends to alleviate\u201d >> odd phrase\\nsection 4 should come before section 3\\nAlg 1 in not helpful because it just references an equation\\nsection 4.4 is very confusing, I cannot follow the logic of the statements\u00a0\\nQ learning >> Q-learning\\nNot sure what you mean with the last sentence of p2 section 5\\nwhere are the results for Acrobot linear function approximation\\nappendix Q-learning with linear FA is not DQN (table 2)\\n\\n\u00a0\\n\\nFAQ for Reviewers\\nQ. The review I just submitted is not appearing! Did it go thru?\\nA. There seems to be some delay in the system for you to see your reviews. Just give a bit of time and refresh again.\\n\u00a0\\nQ. What is the borderline paper meeting between AC and reviewers about?\\nA. This year we require ACs to meet with reviewers for all borderline papers (defined and announced by PCs once all reviews are received). There will be one meeting per borderline paper. Once we receive all reviews, we will let ACs know which papers fall into this bucket. ACs will then schedule and run the meetings with the reviewers of the papers in question.\\n\u00a0\\n\\nQ: What if I can not attend the meeting?\\nA: Please work with your AC to find a meeting schedule that works for everyone. Note that the discussion phase is roughly 1 month long, and overlaps with the NeurIPS conference, in case in-person meeting is preferred. All reviewers agreed to participate in review meetings when they accepted the reviewer invitations. An AC has right to\u00a0 downweight the review of reviewers who refuse to attend the meeting.\\n\u00a0\\n\\nQ: How should I use supplementary material?\\nA: It is not necessary to read supplementary material but such material can often answer questions that arise while reading the main paper, so consider looking there before asking authors.\\n\u00a0\\n\\nQ: How should I handle a policy violation?\\nA: To flag a CoE violation related to a submission, please indicate it when submitting the CoE report for that paper. The AC will work with the PC and the ethics board to resolve the case. To discuss other violations (e.g. plagiarism, double submission, paper length, formatting, etc.), please contact either the AC/SAC or the PC as appropriate. You can do this by sending a confidential comment with the appropriate readership restrictions.\\n\u00a0\\n\\nQ: Am I allowed to ask for additional experiments?\\nA: You can ask for additional experiments. New experiments should not significantly change the content of the submission. Rather, they should be limited in scope and serve to more thoroughly validate existing results from the submission.\\n\u00a0\\n\\nQ: If a submission does not achieve state-of-the-art results, is that grounds for rejection?\\nA: No, a lack of state-of-the-art results does not by itself constitute grounds for rejection. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge. Submissions can achieve this without achieving state-of-the-art results.\\nQ: Are authors expected to cite and compare with very recent work? What about non peer-reviewed (e.g., ArXiv) papers? (updated on 7 November 2022)\\nA: We consider papers contemporaneous if they are published (available in online proceedings) within the last four months. That means, since our full paper deadline is September 28, if a paper was published (i.e., at a peer-reviewed venue) on or after May 28, 2022, authors are not required to compare their own work to that paper. Authors are encouraged to cite and discuss all relevant papers, but they may be excused for not knowing about papers not published in peer-reviewed conference proceedings or journals, which includes papers exclusively available on arXiv. Reviewers are encouraged to use their own good judgement and, if in doubt, discuss with their area chair.\u00a0\\n\u00a0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuccessful Page Load\\n\\n\\n\\n\\n\\n\\n\\n                        ICLR uses cookies for essential functions only. We do not sell your personal\\n                        information.\\n                        Our Privacy Policy \u00bb\u00a0\\n\\n\\nAccept\\n                            Cookies\\n                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            The ICLR Logo above may be used on presentations. Right-click and choose\\n                            download. It is a vector graphic and may be used at any scale.\\n                        \\n\\n\\n\\n\\n\\n                            Useful links\\n                        \\n\\n\\nSponsor / Exhibitor Information\\nPress\\n\\n\\n\\n\\n\\n\\n\\nContact\\n\\n 2710 E Corridor Dr, Appleton WI 54913\\n                            \\n\\n Email\\n\\n Phone: +1-920-268-4789\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR Proceedings at OpenReview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"\n}\n```\n", "metadata": {"title": "Invoking `GetWebsite` with \n```json\n{\n  \"output_type\": \"full_text\",\n  \"url\": \"https://iclr.cc/Conferences/2023/ReviewerGuide\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetWebsite', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR 2022 Reviewer Instructions\\n\\n\\n\\n\\n\\nSkip to yearly menu bar\\nSkip to main content\\n\\nMain Navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            ICLR\\n        \\n\\n\\n\\n\\n                        Help/FAQ\\n                    \\n\\n\\n\\n\\n\\n\\n                        Contact ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n                        Downloads\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Blog\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n                        Privacy Policy\\n                    \\n\\n\\n\\n\\n\\n\\n                        Create Profile\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reset Password\\n                    \\n\\n\\n\\n\\n\\n\\n                        Journal To Conference Track\\n                    \\n\\n\\n\\n\\n\\n\\n                        Diversity & Inclusion\\n                    \\n\\n\\n\\n\\n\\n\\n                        Proceedings at OpenReview\\n                    \\n\\n\\n\\n\\n\\n\\n                        Future Meetings\\n                    \\n\\n\\n\\n\\n\\n\\n                        Press\\n                    \\n\\n\\n\\n\\n\\n\\n                        Exhibitor Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Twitter\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        My Stuff\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Login\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Select Year: (2023)\\n        \\n\\n\\n2026\\n                \\n\\n\\n\\n2025\\n                \\n\\n\\n\\n2024\\n                \\n\\n\\n\\n2023\\n                \\n\\n\\n\\n2022\\n                \\n\\n\\n\\n2021\\n                \\n\\n\\n\\n2020\\n                \\n\\n\\n\\n2019\\n                \\n\\n\\n\\n2018\\n                \\n\\n\\n\\n2017\\n                \\n\\n\\n\\n2016\\n                \\n\\n\\n\\n2015\\n                \\n\\n\\n\\n2014\\n                \\n\\n\\n\\n2013\\n                \\n\\n\\n\\n\\n\\n\\n                        Dates\\n                    \\n\\n\\n\\n\\n            Calls\\n        \\n\\n\\n\\n\\n                        Call for Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Tiny Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Workshops\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Blog Posts\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Socials\\n                    \\n\\n\\n\\n\\n\\n\\n            Guides\\n        \\n\\n\\n\\n\\n                        Author Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewer Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Area Chair Guide\\n                    \\n\\n\\n\\n\\n\\n\\n            Attend\\n        \\n\\n\\n\\n\\n                        Register\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visa and Travel  Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        Hotels\\n                    \\n\\n\\n\\n\\n\\n\\n                        Child Care\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visiting Kigali\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n            Organization\\n        \\n\\n\\n\\n\\n                        ICLR Board\\n                    \\n\\n\\n\\n\\n\\n\\n                        Organizing Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Program Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Ethics Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        Conference Site\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nICLR 2023 Reviewer Guide\\nThank you for agreeing to serve as an ICLR 2023 reviewer. Your contribution as a reviewer is paramount to creating an exciting and high-quality program. We ask that:\\n\\nYour reviews are timely and substantive.\\nYou follow the reviewing guidelines below.\\xa0\\nYou adhere to our Code of Ethics in your role as a reviewer. You must also adhere to our Code of Conduct.\\n\\nThis guide is intended to help you understand the ICLR 2023 decision process and your role within it. It contains:\\n\\nAn outline of the main reviewer tasks\\nStep-by-step reviewing instructions (especially relevant for reviewers that are new to ICLR)\\nReview examples\\nAn FAQ.\\n\\n\\xa0\\n\\n\\nWe\u2019re counting on you\\nAs a reviewer you are central to the program creation process for ICLR 2023. Your Area Chairs (ACs), Senior Area Chairs (SACs) and the Program Chairs (PCs) will rely greatly on your expertise and your diligent and thorough reviews to make decisions on each paper. Therefore, your role as a reviewer is critical to ensuring a strong program for ICLR 2023.\\nHigh-quality reviews are also very valuable for helping authors improve their work, whether it is eventually accepted by ICLR 2023, or not. Therefore it is important to treat each valid ICLR 2023 submission with equal care.\\nAs a token of our appreciation for your essential work, top reviewers will be acknowledged permanently on the ICLR 2023 website. Furthermore, top and high quality reviewers will receive special acknowledgement during the opening ceremony and free registration to ICLR 2023.\\n\\xa0\\n\\nMain reviewer tasks\\nThe main reviewer tasks and dates are as follows:\\n\\nCreate or update your OpenReview profile (September 30 2022)\\nBid on papers (September 30 2022 - October 06 2022)\\nWrite a constructive, thorough and timely review (October 12 2022 - October 24 2022)\\nInitial paper reviews released (November 4 2022)\\nDiscuss with authors and other reviewers to clarify and improve the paper (November 4 2022 - November 18 2022)\\nFlag any potential CoE violations and/or concerns (by November 18 2022)\\nProvide a recommendation to the area chair assigned to the paper (by November 18 2022)\\nVirtual Meeting with AC if paper you reviewed falls into borderline papers (November 18 2022 - December 12 2022)\\nProvide a final recommendation to the area chair assigned to the paper (after Virtual Meeting)\\n\\n\\xa0\\n\\n\\n\\xa0\\nCode of Ethics\\nAll ICLR participants, including reviewers, are required to adhere to the ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics). All reviewers are required to read the Code of Ethics and adhere to it. The Code of Ethics applies to all conference participation, including paper submission, reviewing, and paper discussion.\\xa0\\nAs part of the review process, reviewers are asked to raise potential violations of the ICLR Code of Ethics. Note that authors are encouraged to discuss questions and potential issues regarding the Code of Ethics as part of their submission. This discussion is not counted against the maximum page limit of the paper and should be included as a separate section.\\n\\xa0\\n\\n\\xa0\\nReviewing a submission: step-by-step\\nSummarized in one sentence, a review aims to determine whether a submission will bring sufficient value to the community and contribute new knowledge. The process can be broken down into the following main reviewer tasks:\\n\\xa0\\n\\n\\nRead the paper: It\u2019s important to carefully read through the entire paper, and to look up any related work and citations that will help you comprehensively evaluate it. Be sure to give yourself sufficient time for this step.\\nWhile reading, consider the following:\\n\\nObjective of the work: What is the goal of the paper? Is it to better address a known application or problem, draw attention to a new application or problem, or to introduce and/or explain a new theoretical finding? A combination of these? Different objectives will require different considerations as to potential value and impact.\\nStrong points: is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g. theoretically, algorithmically, etc.)?\\nWeak points: is it weak in any of the aspects listed in b.?\\nBe mindful of potential biases and try to be open-minded about the value and interest a paper can hold for the entire ICLR community, even if it may not be very interesting for you.\\n\\n\\nAnswer four key questions for yourself, to make a recommendation to Accept or Reject:\\n\\nWhat is the specific question and/or problem tackled by the paper?\\nIs the approach well motivated, including being well-placed in the literature?\\nDoes the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\\nWhat is the significance of the work? Does it contribute new knowledge and sufficient value to the community? Note, this does not necessarily require state-of-the-art results. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge (incl., empirical, theoretical, for practitioners, etc).\\n\\n\\nWrite and submit your initial review, organizing it as follows:\\xa0\\n\\nSummarize what the paper claims to contribute. Be positive and constructive.\\nList strong and weak points of the paper. Be as comprehensive as possible.\\nClearly state your initial recommendation (accept or reject) with one or two key reasons for this choice.\\nProvide supporting arguments for your recommendation.\\nAsk questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.\\xa0\\nProvide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\\n\\n\\nComplete the CoE report: ICLR has adopted the following Code of Ethics (CoE). When submitting your review, you\u2019ll be asked to complete a CoE report for the paper. The report is a simple form with two questions. The first asks whether there is a potential violation of the CoE. The second is relevant only if there is a potential violation and asks the reviewer to explain why there may be a potential violation. In order to answer these questions, it is therefore important that you read the CoE before starting your reviews. \\n\\t\\xa0\\nEngage in discussion: The discussion phase at ICLR is different from most conferences in the AI/ML community. During this phase, reviewers, authors and area chairs engage in asynchronous discussion and authors are allowed to revise their submissions to address concerns that arise. It is crucial that you are actively engaged during this phase. Maintain a spirit of openness to changing your initial recommendation (either to a more positive or more negative) rating.\\nBorderline paper meeting: This year, we are introducing a new program for ACs to (virtually) meet and discuss with reviewers only for borderline cases. ACs will reach out to schedule this meeting. This is to ensure active discussions among reviewers, and well-thought-out decisions. ACs will schedule the meeting and facilitate the discussion. For a productive discussion, it is important to familiarize yourself with other reviewers\\' feedback prior to the meeting. Please note that we will be leveraging information for reviewers who failed to attend this meeting (excluding emergencies).\\xa0\\nProvide final recommendation: Update your review, taking into account the new information collected during the discussion phase, and any revisions to the submission. State your reasoning and what did/didn\u2019t change your recommendation throughout the discussion phase.\\n\\n\\xa0\\n\\n\\xa0\\nFor great in-depth resources on reviewing, see these resources:\\n\\nDaniel Dennet, Criticising with Kindness.\\nComprehensive advice: Mistakes Reviewers Make\\nViews from multiple reviewers: Last minute reviewing advice\\nPerspective from instructions to Area Chairs: Dear ACs.\\n\\n\\xa0\\n\\n\\xa0\\nReview Examples\\nHere are two sample reviews from previous conferences that give an example of what we consider a good review for the case of leaning-to-accept and leaning-to-reject.\\nReview for a Paper where Leaning-to-Accept\\nThis paper proposes a method, Dual-AC, for optimizing the actor (policy) and critic (value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor. In order to achieve that, they take the linear programming approach of solving the Bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems. The discussion on the deficiencies of the naive LP approach is mostly well done. Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without losing convexity by applying a regularization. They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements. Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms.\\xa0\\nOverall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments. Given these clarifications in an author response, I would be willing to increase the score.\\xa0\\nFor the theory, there are a few steps that need clarification and further clarification on novelty. For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results. It looks like Theorem 2 has already been shown in \"Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time\u201d. There is a statement that \u201cChen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting. However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization\u201d. Is your Theorem 2 somehow an extension? Is Theorem 3 completely new?\\nThis is particularly called into question due to the lack of assumptions about the function class for value functions. It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function). This assumption seems to be used right at the bottom of Page 17, where U^{pi*} = V^*. Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small. More about conditions on eta_v would be illuminating.\\xa0\\nThere is also one step in the theorem that I cannot verify. On Page 18, how is the squared removed for difference between U and Upi? The transition from the second line of the proof to the third line is not clear. It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2.\\xa0\\n\\xa0\\n\\nFor the experiments, the following should be addressed.\\n1. It would have been better to also show the performance graphs with and without the improvements for multiple domains.\\n2. The central contribution is extending the single step LP to a multi-step formulation. It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains.\\n3. Increasing k also comes at a computational cost. I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO).\\n4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization. It was also mentioned that increasing the regularization parameter size increases the convergence rate. Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization? In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned. Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity? A bit more discussion on these choices would be helpful.\\xa0\\n\\xa0\\n\\nMinor comments:\\n1. Page 2: In equation 5, there should not be a \\'ds\\' in the dual variable constraint\\n\\xa0\\n\\n\\xa0\\nReview for a Paper where Leaning-to-Reject\\nThis paper introduces a variation on temporal difference learning for the function approximation case that attempts to resolve the issue of over-generalization across temporally-successive states. The new approach is applied to both linear and non-linear function approximation, and for prediction and control problems. The algorithmic contribution is demonstrated with a suite of experiments in classic benchmark control domains (Mountain Car and Acrobot), and in Pong.\\nThis paper should be rejected because (1) the algorithm is not well justified either by theory or practice, (2) the paper never clearly demonstrates the existence of problem they are trying to solve (nor differentiates it from the usual problem of generalizing well), (3) the experiments are difficult to understand, missing many details, and generally do not support a significant contribution, and (4) the paper is imprecise and unpolished.\\nMain argument\\nThe paper does not do a great job of demonstrating that the problem it is trying to solve is a real thing. There is no experiment in this paper that clearly shows how this temporal generalization problem is different from the need to generalize well with function approximation. The paper points to references to establish the existence of the problem, but for example the Durugkar and Stone paper is a workshop paper and the conference version of that paper was rejected from ICLR 2018 and the reviewers highlighted serious issues with the paper\u2014that is not work to build upon. Further the paper under review here claims this problem is most pressing in the non-linear case, but the analysis in section 4.1 is for the linear case.\\xa0\\nThe resultant algorithm does not seem well justified, and has a different fixed point than TD, but there is no discussion of this other than section 4.4, which does not make clear statements about the correctness of the algorithm or what it converges to. Can you provide a proof or any kind of evidence that the proposed approach is sound, or how it\u2019s fixed point relates to TD?\\nThe experiments do not provide convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches. There are so many missing details it is difficult to draw many conclusions:\\n\\nWhat was the policy used in exp1 for policy evaluation in MC?\\nWhy Fourier basis features?\\nIn MC with DQN how did you adjust the parameters and architecture for the MC task?\\nWas the reward in MC and Acrobot -1 per step or something else\\nHow did you tune the parameters in the MC and Acrobot experiments?\\nWhy so few runs in MC, none of the results presented are significant?\\nWhy is the performance so bad in MC?\\nDid you evaluate online learning or do tests with the greedy policy?\\nHow did you initialize the value functions and weights?\\nWhy did you use experience replay for the linear experiments?\\nIN MC and Acrobot why only a one layer MLP?\\n\\nIgnoring all that, the results are not convincing. Most of the results in the paper are not statistically significant. The policy evaluation results in MC show little difference to regular TD. The Pong results show DQN is actually better. This makes the reader wonder if the result with DQN on MC and Acrobot are only worse because you did not properly tune DQN for those domains, whereas the default DQN architecture is well tuned for Atari and that is why you method is competitive in the smaller domains.\\xa0\\nThe differences in the \u201caverage change in value plots\u201d are very small if the rewards are -1 per step. Can you provide some context to understand the significance of this difference? In the last experiment linear FA and MC, the step-size is set equal for all methods\u2014this is not a valid comparison. Your method may just work better with alpha = 0.1.\\xa0\\n\\xa0\\n\\nThe paper has many imprecise parts, here are a few:\\n\\nThe definition of the value function would be approximate not equals unless you specify some properties of the function approximation architecture. Same for the Bellman equation\\nequation 1 of section 2.1 is neither an algorithm or a loss function\\nTD does not minimize the squared TD. Saying that is the objective function of TD learning in not true\\nend of section 2.1 says \u201cIt is computed as\u201d but the following equation just gives a form for the partial derivative\\nequation 2, x is not bounded\\xa0\\nYou state TC-loss has an unclear solution property, I don\u2019t know what that means and I don\u2019t think your approach is well justified either\\nSection 4.1 assumes linear FA, but its implied up until paragraph 2 that it has not assumed linear\\ntreatment of n_t in alg differs from appendix (t is no time episode number)\\nYour method has a n_t parameter that is adapted according to a schedule seemingly giving it an unfair advantage over DQN.\\nOver-claim not supported by the results: \u201cwe see that HR-TD is able to find a representation that is better at keeping the target value separate than TC is \u201c. The results do not show this.\\nSection 4.4 does not seem to go anywhere or produce and tangible conclusions\\n\\n\\xa0\\n\\nThings to improve the paper that did not impact the score:\\n\\nIt\u2019s hard to follow how the prox operator is used in the development of the alg, this could use some higher level explanation\\nIntro p2 is about bootstrapping, use that term and remove the equations\\nIt\u2019s not clear why you are talking about stochastic vs deterministic in P3\\nPerhaps you should compare against a MC method in the experiments to demonstrate the problem with TD methods and generalization\\nSection 2: \u201ccan often be a regularization term\u201d >> can or must be?\\nupdate law is an odd term\\n\\xa0tends to alleviate\u201d >> odd phrase\\nsection 4 should come before section 3\\nAlg 1 in not helpful because it just references an equation\\nsection 4.4 is very confusing, I cannot follow the logic of the statements\\xa0\\nQ learning >> Q-learning\\nNot sure what you mean with the last sentence of p2 section 5\\nwhere are the results for Acrobot linear function approximation\\nappendix Q-learning with linear FA is not DQN (table 2)\\n\\n\\xa0\\n\\nFAQ for Reviewers\\nQ. The review I just submitted is not appearing! Did it go thru?\\nA. There seems to be some delay in the system for you to see your reviews. Just give a bit of time and refresh again.\\n\\xa0\\nQ. What is the borderline paper meeting between AC and reviewers about?\\nA. This year we require ACs to meet with reviewers for all borderline papers (defined and announced by PCs once all reviews are received). There will be one meeting per borderline paper. Once we receive all reviews, we will let ACs know which papers fall into this bucket. ACs will then schedule and run the meetings with the reviewers of the papers in question.\\n\\xa0\\n\\nQ: What if I can not attend the meeting?\\nA: Please work with your AC to find a meeting schedule that works for everyone. Note that the discussion phase is roughly 1 month long, and overlaps with the NeurIPS conference, in case in-person meeting is preferred. All reviewers agreed to participate in review meetings when they accepted the reviewer invitations. An AC has right to\\xa0 downweight the review of reviewers who refuse to attend the meeting.\\n\\xa0\\n\\nQ: How should I use supplementary material?\\nA: It is not necessary to read supplementary material but such material can often answer questions that arise while reading the main paper, so consider looking there before asking authors.\\n\\xa0\\n\\nQ: How should I handle a policy violation?\\nA: To flag a CoE violation related to a submission, please indicate it when submitting the CoE report for that paper. The AC will work with the PC and the ethics board to resolve the case. To discuss other violations (e.g. plagiarism, double submission, paper length, formatting, etc.), please contact either the AC/SAC or the PC as appropriate. You can do this by sending a confidential comment with the appropriate readership restrictions.\\n\\xa0\\n\\nQ: Am I allowed to ask for additional experiments?\\nA: You can ask for additional experiments. New experiments should not significantly change the content of the submission. Rather, they should be limited in scope and serve to more thoroughly validate existing results from the submission.\\n\\xa0\\n\\nQ: If a submission does not achieve state-of-the-art results, is that grounds for rejection?\\nA: No, a lack of state-of-the-art results does not by itself constitute grounds for rejection. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge. Submissions can achieve this without achieving state-of-the-art results.\\nQ: Are authors expected to cite and compare with very recent work? What about non peer-reviewed (e.g., ArXiv) papers? (updated on 7 November 2022)\\nA: We consider papers contemporaneous if they are published (available in online proceedings) within the last four months. That means, since our full paper deadline is September 28, if a paper was published (i.e., at a peer-reviewed venue) on or after May 28, 2022, authors are not required to compare their own work to that paper. Authors are encouraged to cite and discuss all relevant papers, but they may be excused for not knowing about papers not published in peer-reviewed conference proceedings or journals, which includes papers exclusively available on arXiv. Reviewers are encouraged to use their own good judgement and, if in doubt, discuss with their area chair.\\xa0\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuccessful Page Load\\n\\n\\n\\n\\n\\n\\n\\n                        ICLR uses cookies for essential functions only. We do not sell your personal\\n                        information.\\n                        Our Privacy Policy \u00bb\\xa0\\n\\n\\nAccept\\n                            Cookies\\n                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            The ICLR Logo above may be used on presentations. Right-click and choose\\n                            download. It is a vector graphic and may be used at any scale.\\n                        \\n\\n\\n\\n\\n\\n                            Useful links\\n                        \\n\\n\\nSponsor / Exhibitor Information\\nPress\\n\\n\\n\\n\\n\\n\\n\\nContact\\n\\n 2710 E Corridor Dr, Appleton WI 54913\\n                            \\n\\n Email\\n\\n Phone: +1-920-268-4789\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR Proceedings at OpenReview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'max_results': 5, 'query': 'transferability of adversarial attacks'}, name='ArxivTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Found 5 paper(s) on arXiv\",\n  \"output\": [\n    {\n      \"title\": \"Efficient Generation of Targeted and Transferable Adversarial Examples for Vision-Language Models Via Diffusion Models\",\n      \"authors\": [\n        \"Qi Guo\",\n        \"Shanmin Pang\",\n        \"Xiaojun Jia\",\n        \"Yang Liu\",\n        \"Qing Guo\"\n      ],\n      \"published\": \"2024-04-16T07:19:52+00:00\",\n      \"summary\": \"Adversarial attacks, particularly \\\\textbf{targeted} transfer-based attacks,\\ncan be used to assess the adversarial robustness of large visual-language\\nmodels (VLMs), allowing for a more thorough examination of potential security\\nflaws before deployment. However, previous transfer-based adversarial attacks\\nincur high costs due to high iteration counts and complex method structure.\\nFurthermore, due to the unnaturalness of adversarial semantics, the generated\\nadversarial examples have low transferability. These issues limit the utility\\nof existing methods for assessing robustness. To address these issues, we\\npropose AdvDiffVLM, which uses diffusion models to generate natural,\\nunrestricted and targeted adversarial examples via score matching.\\nSpecifically, AdvDiffVLM uses Adaptive Ensemble Gradient Estimation to modify\\nthe score during the diffusion model's reverse generation process, ensuring\\nthat the produced adversarial examples have natural adversarial targeted\\nsemantics, which improves their transferability. Simultaneously, to improve the\\nquality of adversarial examples, we use the GradCAM-guided Mask method to\\ndisperse adversarial semantics throughout the image rather than concentrating\\nthem in a single area. Finally, AdvDiffVLM embeds more target semantics into\\nadversarial examples after multiple iterations. Experimental results show that\\nour method generates adversarial examples 5x to 10x faster than\\nstate-of-the-art transfer-based adversarial attacks while maintaining higher\\nquality adversarial examples. Furthermore, compared to previous transfer-based\\nadversarial attacks, the adversarial examples generated by our method have\\nbetter transferability. Notably, AdvDiffVLM can successfully attack a variety\\nof commercial VLMs in a black-box environment, including GPT-4V.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2404.10335v4\"\n    },\n    {\n      \"title\": \"Explainable and Transferable Adversarial Attack for ML-Based Network Intrusion Detectors\",\n      \"authors\": [\n        \"Hangsheng Zhang\",\n        \"Dongqi Han\",\n        \"Yinlong Liu\",\n        \"Zhiliang Wang\",\n        \"Jiyan Sun\",\n        \"Shangyuan Zhuang\",\n        \"Jiqiang Liu\",\n        \"Jinsong Dong\"\n      ],\n      \"published\": \"2024-01-19T13:43:09+00:00\",\n      \"summary\": \"espite being widely used in network intrusion detection systems (NIDSs),\\nmachine learning (ML) has proven to be highly vulnerable to adversarial\\nattacks. White-box and black-box adversarial attacks of NIDS have been explored\\nin several studies. However, white-box attacks unrealistically assume that the\\nattackers have full knowledge of the target NIDSs. Meanwhile, existing\\nblack-box attacks can not achieve high attack success rate due to the weak\\nadversarial transferability between models (e.g., neural networks and tree\\nmodels). Additionally, neither of them explains why adversarial examples exist\\nand why they can transfer across models. To address these challenges, this\\npaper introduces ETA, an Explainable Transfer-based Black-Box Adversarial\\nAttack framework. ETA aims to achieve two primary objectives: 1) create\\ntransferable adversarial examples applicable to various ML models and 2)\\nprovide insights into the existence of adversarial examples and their\\ntransferability within NIDSs. Specifically, we first provide a general\\ntransfer-based adversarial attack method applicable across the entire ML space.\\nFollowing that, we exploit a unique insight based on cooperative game theory\\nand perturbation interpretations to explain adversarial examples and\\nadversarial transferability. On this basis, we propose an Important-Sensitive\\nFeature Selection (ISFS) method to guide the search for adversarial examples,\\nachieving stronger transferability and ensuring traffic-space constraints.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2401.10691v1\"\n    },\n    {\n      \"title\": \"Transferability of Adversarial Attacks on Synthetic Speech Detection\",\n      \"authors\": [\n        \"Jiacheng Deng\",\n        \"Shunyi Chen\",\n        \"Li Dong\",\n        \"Diqun Yan\",\n        \"Rangding Wang\"\n      ],\n      \"published\": \"2022-05-16T14:24:56+00:00\",\n      \"summary\": \"Synthetic speech detection is one of the most important research problems in\\naudio security. Meanwhile, deep neural networks are vulnerable to adversarial\\nattacks. Therefore, we establish a comprehensive benchmark to evaluate the\\ntransferability of adversarial attacks on the synthetic speech detection task.\\nSpecifically, we attempt to investigate: 1) The transferability of adversarial\\nattacks between different features. 2) The influence of varying extraction\\nhyperparameters of features on the transferability of adversarial attacks. 3)\\nThe effect of clipping or self-padding operation on the transferability of\\nadversarial attacks. By performing these analyses, we summarise the weaknesses\\nof synthetic speech detectors and the transferability behaviours of adversarial\\nattacks, which provide insights for future research. More details can be found\\nat https://gitee.com/djc_QRICK/Attack-Transferability-On-Synthetic-Detection.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2205.07711v1\"\n    },\n    {\n      \"title\": \"Adversarial Imitation Attack\",\n      \"authors\": [\n        \"Mingyi Zhou\",\n        \"Jing Wu\",\n        \"Yipeng Liu\",\n        \"Xiaolin Huang\",\n        \"Shuaicheng Liu\",\n        \"Xiang Zhang\",\n        \"Ce Zhu\"\n      ],\n      \"published\": \"2020-03-28T10:02:49+00:00\",\n      \"summary\": \"Deep learning models are known to be vulnerable to adversarial examples. A\\npractical adversarial attack should require as little as possible knowledge of\\nattacked models. Current substitute attacks need pre-trained models to generate\\nadversarial examples and their attack success rates heavily rely on the\\ntransferability of adversarial examples. Current score-based and decision-based\\nattacks require lots of queries for the attacked models. In this study, we\\npropose a novel adversarial imitation attack. First, it produces a replica of\\nthe attacked model by a two-player game like the generative adversarial\\nnetworks (GANs). The objective of the generative model is to generate examples\\nthat lead the imitation model returning different outputs with the attacked\\nmodel. The objective of the imitation model is to output the same labels with\\nthe attacked model under the same inputs. Then, the adversarial examples\\ngenerated by the imitation model are utilized to fool the attacked model.\\nCompared with the current substitute attacks, imitation attacks can use less\\ntraining data to produce a replica of the attacked model and improve the\\ntransferability of adversarial examples. Experiments demonstrate that our\\nimitation attack requires less training data than the black-box substitute\\nattacks, but achieves an attack success rate close to the white-box attack on\\nunseen data with no query.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2003.12760v2\"\n    },\n    {\n      \"title\": \"Direction-Aggregated Attack for Transferable Adversarial Examples\",\n      \"authors\": [\n        \"Tianjin Huang\",\n        \"Vlado Menkovski\",\n        \"Yulong Pei\",\n        \"YuHao Wang\",\n        \"Mykola Pechenizkiy\"\n      ],\n      \"published\": \"2021-04-19T09:54:56+00:00\",\n      \"summary\": \"Deep neural networks are vulnerable to adversarial examples that are crafted\\nby imposing imperceptible changes to the inputs. However, these adversarial\\nexamples are most successful in white-box settings where the model and its\\nparameters are available. Finding adversarial examples that are transferable to\\nother models or developed in a black-box setting is significantly more\\ndifficult. In this paper, we propose the Direction-Aggregated adversarial\\nattacks that deliver transferable adversarial examples. Our method utilizes\\naggregated direction during the attack process for avoiding the generated\\nadversarial examples overfitting to the white-box model. Extensive experiments\\non ImageNet show that our proposed method improves the transferability of\\nadversarial examples significantly and outperforms state-of-the-art attacks,\\nespecially against adversarial robust models. The best averaged attack success\\nrates of our proposed method reaches 94.6\\\\% against three adversarial trained\\nmodels and 94.8\\\\% against five defense methods. It also reveals that current\\ndefense approaches do not prevent transferable adversarial attacks.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2104.09172v2\"\n    }\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ArxivTool` with \n```json\n{\n  \"max_results\": 5,\n  \"query\": \"transferability of adversarial attacks\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ArxivTool', response={'result': {'status': 'success', 'message': 'Found 5 paper(s) on arXiv', 'output': [{'title': 'Efficient Generation of Targeted and Transferable Adversarial Examples for Vision-Language Models Via Diffusion Models', 'authors': ['Qi Guo', 'Shanmin Pang', 'Xiaojun Jia', 'Yang Liu', 'Qing Guo'], 'published': '2024-04-16T07:19:52+00:00', 'summary': \"Adversarial attacks, particularly \\\\textbf{targeted} transfer-based attacks,\\ncan be used to assess the adversarial robustness of large visual-language\\nmodels (VLMs), allowing for a more thorough examination of potential security\\nflaws before deployment. However, previous transfer-based adversarial attacks\\nincur high costs due to high iteration counts and complex method structure.\\nFurthermore, due to the unnaturalness of adversarial semantics, the generated\\nadversarial examples have low transferability. These issues limit the utility\\nof existing methods for assessing robustness. To address these issues, we\\npropose AdvDiffVLM, which uses diffusion models to generate natural,\\nunrestricted and targeted adversarial examples via score matching.\\nSpecifically, AdvDiffVLM uses Adaptive Ensemble Gradient Estimation to modify\\nthe score during the diffusion model's reverse generation process, ensuring\\nthat the produced adversarial examples have natural adversarial targeted\\nsemantics, which improves their transferability. Simultaneously, to improve the\\nquality of adversarial examples, we use the GradCAM-guided Mask method to\\ndisperse adversarial semantics throughout the image rather than concentrating\\nthem in a single area. Finally, AdvDiffVLM embeds more target semantics into\\nadversarial examples after multiple iterations. Experimental results show that\\nour method generates adversarial examples 5x to 10x faster than\\nstate-of-the-art transfer-based adversarial attacks while maintaining higher\\nquality adversarial examples. Furthermore, compared to previous transfer-based\\nadversarial attacks, the adversarial examples generated by our method have\\nbetter transferability. Notably, AdvDiffVLM can successfully attack a variety\\nof commercial VLMs in a black-box environment, including GPT-4V.\", 'pdf_url': 'http://arxiv.org/pdf/2404.10335v4'}, {'title': 'Explainable and Transferable Adversarial Attack for ML-Based Network Intrusion Detectors', 'authors': ['Hangsheng Zhang', 'Dongqi Han', 'Yinlong Liu', 'Zhiliang Wang', 'Jiyan Sun', 'Shangyuan Zhuang', 'Jiqiang Liu', 'Jinsong Dong'], 'published': '2024-01-19T13:43:09+00:00', 'summary': 'espite being widely used in network intrusion detection systems (NIDSs),\\nmachine learning (ML) has proven to be highly vulnerable to adversarial\\nattacks. White-box and black-box adversarial attacks of NIDS have been explored\\nin several studies. However, white-box attacks unrealistically assume that the\\nattackers have full knowledge of the target NIDSs. Meanwhile, existing\\nblack-box attacks can not achieve high attack success rate due to the weak\\nadversarial transferability between models (e.g., neural networks and tree\\nmodels). Additionally, neither of them explains why adversarial examples exist\\nand why they can transfer across models. To address these challenges, this\\npaper introduces ETA, an Explainable Transfer-based Black-Box Adversarial\\nAttack framework. ETA aims to achieve two primary objectives: 1) create\\ntransferable adversarial examples applicable to various ML models and 2)\\nprovide insights into the existence of adversarial examples and their\\ntransferability within NIDSs. Specifically, we first provide a general\\ntransfer-based adversarial attack method applicable across the entire ML space.\\nFollowing that, we exploit a unique insight based on cooperative game theory\\nand perturbation interpretations to explain adversarial examples and\\nadversarial transferability. On this basis, we propose an Important-Sensitive\\nFeature Selection (ISFS) method to guide the search for adversarial examples,\\nachieving stronger transferability and ensuring traffic-space constraints.', 'pdf_url': 'http://arxiv.org/pdf/2401.10691v1'}, {'title': 'Transferability of Adversarial Attacks on Synthetic Speech Detection', 'authors': ['Jiacheng Deng', 'Shunyi Chen', 'Li Dong', 'Diqun Yan', 'Rangding Wang'], 'published': '2022-05-16T14:24:56+00:00', 'summary': 'Synthetic speech detection is one of the most important research problems in\\naudio security. Meanwhile, deep neural networks are vulnerable to adversarial\\nattacks. Therefore, we establish a comprehensive benchmark to evaluate the\\ntransferability of adversarial attacks on the synthetic speech detection task.\\nSpecifically, we attempt to investigate: 1) The transferability of adversarial\\nattacks between different features. 2) The influence of varying extraction\\nhyperparameters of features on the transferability of adversarial attacks. 3)\\nThe effect of clipping or self-padding operation on the transferability of\\nadversarial attacks. By performing these analyses, we summarise the weaknesses\\nof synthetic speech detectors and the transferability behaviours of adversarial\\nattacks, which provide insights for future research. More details can be found\\nat https://gitee.com/djc_QRICK/Attack-Transferability-On-Synthetic-Detection.', 'pdf_url': 'http://arxiv.org/pdf/2205.07711v1'}, {'title': 'Adversarial Imitation Attack', 'authors': ['Mingyi Zhou', 'Jing Wu', 'Yipeng Liu', 'Xiaolin Huang', 'Shuaicheng Liu', 'Xiang Zhang', 'Ce Zhu'], 'published': '2020-03-28T10:02:49+00:00', 'summary': 'Deep learning models are known to be vulnerable to adversarial examples. A\\npractical adversarial attack should require as little as possible knowledge of\\nattacked models. Current substitute attacks need pre-trained models to generate\\nadversarial examples and their attack success rates heavily rely on the\\ntransferability of adversarial examples. Current score-based and decision-based\\nattacks require lots of queries for the attacked models. In this study, we\\npropose a novel adversarial imitation attack. First, it produces a replica of\\nthe attacked model by a two-player game like the generative adversarial\\nnetworks (GANs). The objective of the generative model is to generate examples\\nthat lead the imitation model returning different outputs with the attacked\\nmodel. The objective of the imitation model is to output the same labels with\\nthe attacked model under the same inputs. Then, the adversarial examples\\ngenerated by the imitation model are utilized to fool the attacked model.\\nCompared with the current substitute attacks, imitation attacks can use less\\ntraining data to produce a replica of the attacked model and improve the\\ntransferability of adversarial examples. Experiments demonstrate that our\\nimitation attack requires less training data than the black-box substitute\\nattacks, but achieves an attack success rate close to the white-box attack on\\nunseen data with no query.', 'pdf_url': 'http://arxiv.org/pdf/2003.12760v2'}, {'title': 'Direction-Aggregated Attack for Transferable Adversarial Examples', 'authors': ['Tianjin Huang', 'Vlado Menkovski', 'Yulong Pei', 'YuHao Wang', 'Mykola Pechenizkiy'], 'published': '2021-04-19T09:54:56+00:00', 'summary': 'Deep neural networks are vulnerable to adversarial examples that are crafted\\nby imposing imperceptible changes to the inputs. However, these adversarial\\nexamples are most successful in white-box settings where the model and its\\nparameters are available. Finding adversarial examples that are transferable to\\nother models or developed in a black-box setting is significantly more\\ndifficult. In this paper, we propose the Direction-Aggregated adversarial\\nattacks that deliver transferable adversarial examples. Our method utilizes\\naggregated direction during the attack process for avoiding the generated\\nadversarial examples overfitting to the white-box model. Extensive experiments\\non ImageNet show that our proposed method improves the transferability of\\nadversarial examples significantly and outperforms state-of-the-art attacks,\\nespecially against adversarial robust models. The best averaged attack success\\nrates of our proposed method reaches 94.6\\\\% against three adversarial trained\\nmodels and 94.8\\\\% against five defense methods. It also reveals that current\\ndefense approaches do not prevent transferable adversarial attacks.', 'pdf_url': 'http://arxiv.org/pdf/2104.09172v2'}]}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "The paper \"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\" proposes a novel approach to improve the transferability of adversarial attacks, specifically focusing on Intermediate Level Attacks (ILA). The core idea is to enhance the diversity of reference images used in ILA by employing three data augmentation techniques: automated data augmentation, reverse adversarial update, and attack interpolation. The authors demonstrate that their proposed method, ILA-DA, outperforms existing state-of-the-art transfer-based attacks on both defended and undefended models.\n\nNow, I will create three reviewer agents with specialized expertise to evaluate this paper from different perspectives.\n\n*   **Agent 1: Adversarial Robustness Expert (ARE):** Focuses on the security implications, the effectiveness of the attack against defended models, and the novelty of the attack method in the adversarial robustness landscape.\n*   **Agent 2: Data Augmentation Specialist (DAS):** Evaluates the novelty and effectiveness of the proposed data augmentation techniques, their impact on transferability, and the justification for their specific design choices.\n*   **Agent 3: Deep Learning Generalization Analyst (DLA):** Assesses the theoretical grounding of the approach, the experimental design, and the generalizability of the findings to different architectures and datasets.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"gemini-2.5-flash-preview-05-20\": {\n      \"description\": \"Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math\",\n      \"create_expense_cost\": 0,\n      \"invoke_expense_cost\": 0.15,\n      \"output_expense_cost\": 0.6\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'gemini-2.5-flash-preview-05-20': {'description': 'Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math', 'create_expense_cost': 0, 'invoke_expense_cost': 0.15, 'output_expense_cost': 0.6}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gemini-2.5-flash-preview-05-20', 'system_prompt': \"You are an expert in adversarial robustness. Evaluate the paper's attack method, its effectiveness against defended models, and its novelty in the adversarial robustness landscape. Assess the security implications and potential impact of this research.\", 'description': 'Expert in adversarial robustness and security implications of deep learning models.', 'agent_name': 'ARE'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.8977626\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"system_prompt\": \"You are an expert in adversarial robustness. Evaluate the paper's attack method, its effectiveness against defended models, and its novelty in the adversarial robustness landscape. Assess the security implications and potential impact of this research.\",\n  \"description\": \"Expert in adversarial robustness and security implications of deep learning models.\",\n  \"agent_name\": \"ARE\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.8977626}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gemini-2.5-flash-preview-05-20', 'system_prompt': 'You are a specialist in data augmentation. Evaluate the novelty and effectiveness of the proposed data augmentation techniques, their impact on transferability, and the justification for their specific design choices. Provide constructive feedback on how the augmentation strategies could be further improved.', 'description': 'Specialist in data augmentation techniques for improving machine learning model performance.', 'agent_name': 'DAS'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.8959244\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"system_prompt\": \"You are a specialist in data augmentation. Evaluate the novelty and effectiveness of the proposed data augmentation techniques, their impact on transferability, and the justification for their specific design choices. Provide constructive feedback on how the augmentation strategies could be further improved.\",\n  \"description\": \"Specialist in data augmentation techniques for improving machine learning model performance.\",\n  \"agent_name\": \"DAS\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.8959244}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'Analyst specializing in deep learning generalization and experimental design.', 'agent_name': 'DLA', 'system_prompt': 'You are an analyst specializing in deep learning generalization. Assess the theoretical grounding of the approach, the experimental design, and the generalizability of the findings to different architectures and datasets. Critique the experimental methodology and suggest improvements for future research.', 'base_model': 'gemini-2.5-flash-preview-05-20'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.8940696\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"Analyst specializing in deep learning generalization and experimental design.\",\n  \"agent_name\": \"DLA\",\n  \"system_prompt\": \"You are an analyst specializing in deep learning generalization. Assess the theoretical grounding of the approach, the experimental design, and the generalizability of the findings to different architectures and datasets. Critique the experimental methodology and suggest improvements for future research.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.8940696}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Now that the three reviewer agents (ARE, DAS, and DLA) have been created, I will ask them to evaluate the paper.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'ARE', 'prompt': 'Evaluate the paper \"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\". Focus on the security implications, the effectiveness of the attack against defended models, and the novelty of the attack method in the adversarial robustness landscape.. Provide a weighted recommendation (Accept, Reject) with detailed justification and improvement pathways.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"As an expert in adversarial robustness, I've evaluated the proposed paper \\\"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\\\".\\n\\n---\\n\\n## Evaluation of ILA-DA\\n\\n**Core Idea:** The paper proposes ILA-DA, an attack method designed to improve the transferability of Intermediate Level Attacks (ILAs) by incorporating data augmentation techniques during the attack generation process. ILAs typically optimize perturbations to match or mismatch activation patterns in internal layers of a model, rather than just the final output logits. The goal is to create more robust, transferable adversarial examples.\\n\\n### 1. Effectiveness Against Defended Models\\n\\n**Mechanism of Effectiveness:**\\n*   **Intermediate Layer Target:** ILAs inherently aim to perturb features at a more fundamental level within the network. Many defenses primarily focus on regularizing or smoothing gradients at the *output* layer (e.g., PGD-based adversarial training aims to make the logits robust). By targeting intermediate layers, ILA-DA might exploit vulnerabilities in the feature extraction process itself, which might be less guarded by common adversarial training schemes.\\n*   **Data Augmentation for Transferability:** This is a well-established technique (e.g., Diversity Input Attacks (DI2-FGSM), Translation-Invariant Attacks (TIA), Scale-Invariant Attacks (SIA)). Applying input transformations (resizing, padding, rotation, etc.) during gradient calculation helps generalize the learned adversarial perturbation. The intuition is that by forcing the attack to be effective across various transformations of the input, the generated perturbation becomes less \\\"overfit\\\" to the source model's specific architecture and more generally transferable to other models, including defended ones.\\n*   **Combined Effect:** The combination is synergistic. ILAs aim for conceptually \\\"stronger\\\" perturbations (affecting fundamental features), and data augmentation makes these stronger perturbations *more broadly applicable*. This should, in principle, lead to better attack success rates in a black-box setting against models, especially those trained with standard adversarial training.\\n\\n**Likely Performance Against Defenses:**\\n*   **Standard Adversarial Training (e.g., PGD-AT, TRADES, MART):** ILA-DA is expected to show improved performance against these defenses in a black-box setting compared to traditional white-box or less transferable black-box attacks. Since these defenses primarily regularize output gradients, targeting intermediate features with augmented inputs can be more effective at finding \\\"blind spots.\\\"\\n*   **Gradient Masking/Obfuscated Gradients:** ILAs can sometimes be more robust to simple gradient masking defenses if the target layer's gradients are less affected, or if the chosen intermediate loss function implicitly bypasses the masking. The added data augmentation will further enhance this.\\n*   **Certified Defenses (e.g., CURE, Randomized Smoothing, Interval Bound Propagation):** It is less likely to significantly break certified defenses, as these provide theoretical guarantees against *any* perturbation within a certain L_p norm. However, it might find larger adversarial examples that still fall within the certified radius but are missed by weaker attacks.\\n*   **Defenses that explicitly target Feature Robustness (e.g., Feature Denoising, Deep Feature Suppression):** The effectiveness against such defenses would be a key test. If ILA-DA can still bypass defenses designed to robustify features, it would indicate a significant threat.\\n\\n### 2. Novelty in the Adversarial Robustness Landscape\\n\\n**Assessment:** The novelty of ILA-DA appears to be *incremental* rather than revolutionary, stemming from a combination of existing techniques.\\n\\n*   **Intermediate Level Attacks (ILAs):** Not new. Research on attacking intermediate layers (e.g., Feature Importance Attack (FIA), Nesterov-guided attacks, attention-based attacks, attacks targeting specific neurons or feature maps) has been ongoing for several years. The core idea of optimizing perturbations based on internal activations is well-established.\\n*   **Data Augmentation for Transferability:** This is also a widely adopted and highly effective strategy for improving the transferability of black-box adversarial examples. Techniques like DIM, SIM, TI-FGSM, and others have demonstrated that applying various transformations to the input during the attack generation process significantly improves transferability.\\n\\n**Core Novelty Claim:** The novelty lies in *combining* these two established ideas: using data augmentation specifically to improve the transferability of *intermediate-level attacks*. While the individual components are not new, their synergistic combination for this specific purpose (ILAs + DA for transferability) might be a novel exploration if it uncovers unique properties or yields significantly superior performance compared to simply applying DA to output-level attacks.\\n\\n**Critique on Novelty:** The paper needs to clearly articulate *why* this specific combination is more than just A + B. Does the data augmentation interact with intermediate layer targeting in a unique way? Does it unlock a different vulnerability than applying data augmentation to an output-level attack? Without such insights, it risks being perceived as an engineering improvement rather than a fundamental research contribution. For instance, if the data augmentation is applied in the exact same manner as in DIM/SIM-FGSM, simply to an ILA objective function, the novelty would be quite low.\\n\\n### 3. Security Implications and Potential Impact\\n\\n**Positive Implications (for researchers):**\\n*   **Stronger Benchmark:** ILA-DA could serve as a more challenging and realistic black-box benchmark for evaluating the robustness of new defense mechanisms. If a defense can withstand ILA-DA, it's likely more robust in real-world scenarios where attackers have limited model information.\\n*   **Guidance for Defense Development:** Understanding how ILA-DA exploits models can guide the development of new defenses that specifically target robustness in intermediate feature spaces, or make existing defenses more resilient to transferable attacks.\\n\\n**Negative Implications (for deployed systems):**\\n*   **Enhanced Black-Box Threat:** The primary negative implication is the increased threat of black-box attacks. If ILA-DA is highly transferable to unknown, deployed models, it significantly lowers the barrier for attackers to compromise AI systems, especially those in sensitive applications (e.g., autonomous driving, medical imaging, surveillance).\\n*   **Undermining Defenses:** If ILA-DA proves effective against various state-of-the-art defenses in black-box settings, it effectively undermines their perceived security, accelerating the \\\"adversarial arms race.\\\"\\n*   **Real-world Impact:** Systems relying on AI models for critical decisions could be fooled more easily, leading to safety hazards, privacy breaches, or financial losses.\\n\\n### Weighted Recommendation\\n\\n**Recommendation: Weak Accept**\\n\\n**Justification:**\\nThe paper addresses a critical problem in adversarial robustness: improving the transferability of black-box attacks, especially against defended models. The chosen approach, combining Intermediate Level Attacks with Data Augmentation, is empirically sound and likely to yield strong performance given the individual effectiveness of both components. This could lead to a more challenging benchmark for defenses, which is valuable for the field.\\n\\nHowever, the primary concern is **novelty**. While the combination of ILAs and data augmentation for transferability is explored, the paper needs to clearly demonstrate *why* this specific combination is fundamentally novel beyond a straightforward application of existing techniques. Without deeper theoretical insights into the unique synergy or a demonstrably revolutionary empirical jump that can't be explained by previous work, the contribution might lean more towards an engineering improvement.\\n\\n**Improvement Pathways:**\\n\\n1.  **Deepen Novelty Discussion:**\\n    *   **Beyond A+B:** Clearly articulate *why* applying data augmentation to an ILA is different or more effective than applying it to an output-level attack. Is there a unique interaction between the intermediate loss and the augmentation transformations?\\n    *   **New Augmentation Strategies:** Explore novel data augmentation strategies *specifically tailored* for intermediate-level attacks, rather than merely reusing existing DIM/SIM transformations.\\n    *   **Theoretical Insights:** Can any theoretical justifications or intuitions be provided for why this combination enhances transferability more effectively than existing methods?\\n\\n2.  **Richer Ablation Studies:**\\n    *   **Impact of Augmentation Type:** Systematically ablate different data augmentation techniques (e.g., random resizing, padding, rotation, color jittering) and their combinations to understand their individual contributions to transferability.\\n    *   **Target Layer Sensitivity:** Analyze the impact of choosing different intermediate layers for the ILA objective. Which layers are most susceptible to ILA-DA and why? Is there a pattern across different model architectures?\\n    *   **Loss Function Analysis:** If multiple ILA loss functions are explored, provide detailed analysis of their relative performance and the impact of data augmentation on each.\\n\\n3.  **Comprehensive Defense Evaluation:**\\n    *   **Beyond Standard AT:** Evaluate ILA-DA against a wider range of modern defenses, including certified defenses (even if only for small models), defenses that explicitly target feature robustness (e.g., feature denoisers, feature distillation), and defenses with more diverse robustness mechanisms.\\n    *   **Adaptive Attacks:** Discuss the potential for defenses to adapt to ILA-DA. How might a defense be specifically designed to thwart this attack?\\n\\n4.  **Computational Efficiency:**\\n    *   Discuss the computational cost of ILA-DA compared to other transferable attacks. ILAs can be more expensive than FGSM-variants, and data augmentation adds overhead. Providing a detailed analysis of runtime and memory requirements would be valuable.\\n\\nBy addressing these points, the paper can significantly strengthen its contribution, moving from an incremental improvement to a more impactful research piece.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.891354\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"ARE\",\n  \"prompt\": \"Evaluate the paper \\\"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\\\". Focus on the security implications, the effectiveness of the attack against defended models, and the novelty of the attack method in the adversarial robustness landscape.. Provide a weighted recommendation (Accept, Reject) with detailed justification and improvement pathways.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'As an expert in adversarial robustness, I\\'ve evaluated the proposed paper \"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\".\\n\\n---\\n\\n## Evaluation of ILA-DA\\n\\n**Core Idea:** The paper proposes ILA-DA, an attack method designed to improve the transferability of Intermediate Level Attacks (ILAs) by incorporating data augmentation techniques during the attack generation process. ILAs typically optimize perturbations to match or mismatch activation patterns in internal layers of a model, rather than just the final output logits. The goal is to create more robust, transferable adversarial examples.\\n\\n### 1. Effectiveness Against Defended Models\\n\\n**Mechanism of Effectiveness:**\\n*   **Intermediate Layer Target:** ILAs inherently aim to perturb features at a more fundamental level within the network. Many defenses primarily focus on regularizing or smoothing gradients at the *output* layer (e.g., PGD-based adversarial training aims to make the logits robust). By targeting intermediate layers, ILA-DA might exploit vulnerabilities in the feature extraction process itself, which might be less guarded by common adversarial training schemes.\\n*   **Data Augmentation for Transferability:** This is a well-established technique (e.g., Diversity Input Attacks (DI2-FGSM), Translation-Invariant Attacks (TIA), Scale-Invariant Attacks (SIA)). Applying input transformations (resizing, padding, rotation, etc.) during gradient calculation helps generalize the learned adversarial perturbation. The intuition is that by forcing the attack to be effective across various transformations of the input, the generated perturbation becomes less \"overfit\" to the source model\\'s specific architecture and more generally transferable to other models, including defended ones.\\n*   **Combined Effect:** The combination is synergistic. ILAs aim for conceptually \"stronger\" perturbations (affecting fundamental features), and data augmentation makes these stronger perturbations *more broadly applicable*. This should, in principle, lead to better attack success rates in a black-box setting against models, especially those trained with standard adversarial training.\\n\\n**Likely Performance Against Defenses:**\\n*   **Standard Adversarial Training (e.g., PGD-AT, TRADES, MART):** ILA-DA is expected to show improved performance against these defenses in a black-box setting compared to traditional white-box or less transferable black-box attacks. Since these defenses primarily regularize output gradients, targeting intermediate features with augmented inputs can be more effective at finding \"blind spots.\"\\n*   **Gradient Masking/Obfuscated Gradients:** ILAs can sometimes be more robust to simple gradient masking defenses if the target layer\\'s gradients are less affected, or if the chosen intermediate loss function implicitly bypasses the masking. The added data augmentation will further enhance this.\\n*   **Certified Defenses (e.g., CURE, Randomized Smoothing, Interval Bound Propagation):** It is less likely to significantly break certified defenses, as these provide theoretical guarantees against *any* perturbation within a certain L_p norm. However, it might find larger adversarial examples that still fall within the certified radius but are missed by weaker attacks.\\n*   **Defenses that explicitly target Feature Robustness (e.g., Feature Denoising, Deep Feature Suppression):** The effectiveness against such defenses would be a key test. If ILA-DA can still bypass defenses designed to robustify features, it would indicate a significant threat.\\n\\n### 2. Novelty in the Adversarial Robustness Landscape\\n\\n**Assessment:** The novelty of ILA-DA appears to be *incremental* rather than revolutionary, stemming from a combination of existing techniques.\\n\\n*   **Intermediate Level Attacks (ILAs):** Not new. Research on attacking intermediate layers (e.g., Feature Importance Attack (FIA), Nesterov-guided attacks, attention-based attacks, attacks targeting specific neurons or feature maps) has been ongoing for several years. The core idea of optimizing perturbations based on internal activations is well-established.\\n*   **Data Augmentation for Transferability:** This is also a widely adopted and highly effective strategy for improving the transferability of black-box adversarial examples. Techniques like DIM, SIM, TI-FGSM, and others have demonstrated that applying various transformations to the input during the attack generation process significantly improves transferability.\\n\\n**Core Novelty Claim:** The novelty lies in *combining* these two established ideas: using data augmentation specifically to improve the transferability of *intermediate-level attacks*. While the individual components are not new, their synergistic combination for this specific purpose (ILAs + DA for transferability) might be a novel exploration if it uncovers unique properties or yields significantly superior performance compared to simply applying DA to output-level attacks.\\n\\n**Critique on Novelty:** The paper needs to clearly articulate *why* this specific combination is more than just A + B. Does the data augmentation interact with intermediate layer targeting in a unique way? Does it unlock a different vulnerability than applying data augmentation to an output-level attack? Without such insights, it risks being perceived as an engineering improvement rather than a fundamental research contribution. For instance, if the data augmentation is applied in the exact same manner as in DIM/SIM-FGSM, simply to an ILA objective function, the novelty would be quite low.\\n\\n### 3. Security Implications and Potential Impact\\n\\n**Positive Implications (for researchers):**\\n*   **Stronger Benchmark:** ILA-DA could serve as a more challenging and realistic black-box benchmark for evaluating the robustness of new defense mechanisms. If a defense can withstand ILA-DA, it\\'s likely more robust in real-world scenarios where attackers have limited model information.\\n*   **Guidance for Defense Development:** Understanding how ILA-DA exploits models can guide the development of new defenses that specifically target robustness in intermediate feature spaces, or make existing defenses more resilient to transferable attacks.\\n\\n**Negative Implications (for deployed systems):**\\n*   **Enhanced Black-Box Threat:** The primary negative implication is the increased threat of black-box attacks. If ILA-DA is highly transferable to unknown, deployed models, it significantly lowers the barrier for attackers to compromise AI systems, especially those in sensitive applications (e.g., autonomous driving, medical imaging, surveillance).\\n*   **Undermining Defenses:** If ILA-DA proves effective against various state-of-the-art defenses in black-box settings, it effectively undermines their perceived security, accelerating the \"adversarial arms race.\"\\n*   **Real-world Impact:** Systems relying on AI models for critical decisions could be fooled more easily, leading to safety hazards, privacy breaches, or financial losses.\\n\\n### Weighted Recommendation\\n\\n**Recommendation: Weak Accept**\\n\\n**Justification:**\\nThe paper addresses a critical problem in adversarial robustness: improving the transferability of black-box attacks, especially against defended models. The chosen approach, combining Intermediate Level Attacks with Data Augmentation, is empirically sound and likely to yield strong performance given the individual effectiveness of both components. This could lead to a more challenging benchmark for defenses, which is valuable for the field.\\n\\nHowever, the primary concern is **novelty**. While the combination of ILAs and data augmentation for transferability is explored, the paper needs to clearly demonstrate *why* this specific combination is fundamentally novel beyond a straightforward application of existing techniques. Without deeper theoretical insights into the unique synergy or a demonstrably revolutionary empirical jump that can\\'t be explained by previous work, the contribution might lean more towards an engineering improvement.\\n\\n**Improvement Pathways:**\\n\\n1.  **Deepen Novelty Discussion:**\\n    *   **Beyond A+B:** Clearly articulate *why* applying data augmentation to an ILA is different or more effective than applying it to an output-level attack. Is there a unique interaction between the intermediate loss and the augmentation transformations?\\n    *   **New Augmentation Strategies:** Explore novel data augmentation strategies *specifically tailored* for intermediate-level attacks, rather than merely reusing existing DIM/SIM transformations.\\n    *   **Theoretical Insights:** Can any theoretical justifications or intuitions be provided for why this combination enhances transferability more effectively than existing methods?\\n\\n2.  **Richer Ablation Studies:**\\n    *   **Impact of Augmentation Type:** Systematically ablate different data augmentation techniques (e.g., random resizing, padding, rotation, color jittering) and their combinations to understand their individual contributions to transferability.\\n    *   **Target Layer Sensitivity:** Analyze the impact of choosing different intermediate layers for the ILA objective. Which layers are most susceptible to ILA-DA and why? Is there a pattern across different model architectures?\\n    *   **Loss Function Analysis:** If multiple ILA loss functions are explored, provide detailed analysis of their relative performance and the impact of data augmentation on each.\\n\\n3.  **Comprehensive Defense Evaluation:**\\n    *   **Beyond Standard AT:** Evaluate ILA-DA against a wider range of modern defenses, including certified defenses (even if only for small models), defenses that explicitly target feature robustness (e.g., feature denoisers, feature distillation), and defenses with more diverse robustness mechanisms.\\n    *   **Adaptive Attacks:** Discuss the potential for defenses to adapt to ILA-DA. How might a defense be specifically designed to thwart this attack?\\n\\n4.  **Computational Efficiency:**\\n    *   Discuss the computational cost of ILA-DA compared to other transferable attacks. ILAs can be more expensive than FGSM-variants, and data augmentation adds overhead. Providing a detailed analysis of runtime and memory requirements would be valuable.\\n\\nBy addressing these points, the paper can significantly strengthen its contribution, moving from an incremental improvement to a more impactful research piece.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.891354}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Evaluate the paper \"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\". Evaluate the novelty and effectiveness of the proposed data augmentation techniques, their impact on transferability, and the justification for their specific design choices. Provide constructive feedback on how the augmentation strategies could be further improved.', 'agent_name': 'DAS'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The paper \\\"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\\\" addresses a critical challenge in adversarial machine learning: the limited transferability of intermediate-level adversarial attacks (ILA). By proposing a data augmentation (DA) strategy, the authors aim to generate adversarial examples that are more robust and effective across different target models.\\n\\nLet's evaluate the proposed techniques based on your criteria:\\n\\n---\\n\\n### 1. Novelty of Proposed Data Augmentation Techniques\\n\\n*   **Core Idea Novelty:** The concept of applying data augmentation *specifically* to enhance the transferability of *intermediate-level attacks* is a notable contribution. While data augmentation is widely used in robust training and for improving the transferability of *input-level* attacks (e.g., diverse inputs, input transformations), its systematic application to regularize and generalize attacks at the *feature/gradient level* of intermediate layers is less explored. This shift in focus from input-space augmentation to *feature/gradient-space augmentation* for attack transferability is where the primary novelty lies.\\n*   **Specific Techniques (Hypothetical, as the paper details would be needed):**\\n    *   If the paper introduces novel *types* of augmentations (e.g., specific noise patterns or transformations tailored to feature activations or gradients) that haven't been seen in general DA literature or adversarial training, that would be highly novel.\\n    *   More likely, the techniques are adaptations or combinations of existing DA principles (like adding noise, applying transformations, or ensembling) but applied in a novel context (intermediate features/gradients during attack generation). For instance:\\n        *   **Feature Perturbation:** Adding random noise (Gaussian, uniform) directly to the intermediate feature maps or to the gradients used for attack generation. While noise is common, its targeted application here for ILA transferability is specific.\\n        *   **Feature Transformation:** Applying specific scaling, shifting, or even non-linear transformations to the features during the attack optimization.\\n        *   **Gradient Randomization/Smoothing:** Techniques to diversify the gradient information used, similar to some robust optimization methods.\\n    *   The novelty would stem from *how* these are designed to address the unique challenges of ILA transferability, rather than just general robustness.\\n\\n**Conclusion on Novelty:** The novelty primarily resides in the *application domain* and the *conceptual shift* to applying DA at the intermediate feature/gradient level for attack transferability, rather than necessarily inventing entirely new augmentation operations. However, the specific design and combination of these techniques for ILA can still be highly innovative.\\n\\n---\\n\\n### 2. Effectiveness of Proposed Data Augmentation Techniques\\n\\n*   **Empirical Success:** Based on the paper's title and common research trends, ILA-DA would be expected to demonstrate significant improvements in transferability rates compared to baseline ILA methods without DA. The effectiveness would be measured by:\\n    *   **Increased Attack Success Rate (ASR) on target models:** Showing that the augmented adversarial examples lead to misclassification on previously unseen models.\\n    *   **Higher Transferability Rate:** The percentage of adversarial examples generated on a source model that successfully attack one or more target models.\\n    *   **Robustness to Diverse Architectures:** Demonstrated effectiveness across a range of model architectures (e.g., ResNets, VGGs, DenseNets, MobileNets, ViTs), depths, and widths.\\n*   **Ablation Studies:** Effective papers typically include ablation studies to show the individual contribution of each DA component, proving that the proposed combination is synergistic and that each part plays a role in the overall effectiveness.\\n*   **Comparison to Baselines:** Superior performance against state-of-the-art ILA methods (and potentially other transferability-enhancing techniques like diverse inputs or ensemble attacks) would confirm its effectiveness.\\n*   **Scalability:** The techniques should ideally be effective without a prohibitive increase in the computational cost of generating adversarial examples.\\n\\n**Conclusion on Effectiveness:** If the paper's experimental results align with its claims, ILA-DA should prove effective in significantly boosting the transferability of intermediate-level attacks. The key is the *magnitude* of the improvement and its consistency across various evaluation scenarios.\\n\\n---\\n\\n### 3. Impact on Transferability\\n\\nThe impact of ILA-DA on transferability is direct and positive, stemming from the core design philosophy:\\n\\n*   **Regularization of Perturbation:** By introducing variation (via augmentation) during the attack generation process, the adversarial perturbation is forced to be less specific and brittle to the exact intermediate feature representations of the source model. It essentially regularizes the optimization process, making the resulting perturbation more generalizable.\\n*   **Mimicking Model Discrepancies:** The DA techniques (e.g., adding noise to features, transforming gradients) can be seen as simulating the inherent differences in feature extraction and representation learning across various neural network architectures. By exposing the attack generation process to these simulated variations, the resulting adversarial example becomes more robust to real-world model discrepancies.\\n*   **Focus on Universal Features:** The augmentation encourages the attack to exploit more \\\"universal\\\" or \\\"robust\\\" features that are common across different models, rather than relying on model-specific idiosyncrasies in intermediate layers. This makes the adversarial example less \\\"overfit\\\" to the source model.\\n*   **Enhanced Generalization:** In essence, ILA-DA improves the *generalization* of the adversarial examples across the model space, similar to how data augmentation improves the generalization of models in standard training.\\n\\n**Conclusion on Impact:** ILA-DA's impact is to turn intermediate-level attacks, which often suffer from low transferability due to their reliance on specific internal representations, into more robust and widely applicable threats.\\n\\n---\\n\\n### 4. Justification for Specific Design Choices\\n\\nThe justification for specific design choices within ILA-DA should ideally stem from an understanding of *why* ILA struggles with transferability and *how* different models' intermediate representations vary:\\n\\n*   **Why Intermediate-Level Attacks Lack Transferability:** ILA often target specific neurons or feature maps within a network. Different architectures (even trained on the same data) can learn vastly different intermediate representations, making an attack tuned to one model's internal structure ineffective against another. The justification for DA is to bridge this gap.\\n*   **Feature Perturbation/Noise:**\\n    *   **Justification:** Different models may extract features with slight variations in values, activations, or sparsity patterns. Adding noise to feature maps or gradients simulates this variability, forcing the attack to find perturbations that are effective despite these fluctuations. It makes the attack less sensitive to precise activation values and more robust to slight shifts in feature space.\\n    *   **Analogy:** Similar to how adding noise to input images makes a classifier robust to noisy inputs, adding noise to *features* makes the *attack* robust to variations in *features*.\\n*   **Feature Transformation (e.g., scaling, shifting, non-linearities):**\\n    *   **Justification:** Different network depths, widths, or activation functions can lead to feature maps with different scales or dynamic ranges. Applying transformations ensures the attack is not dependent on a specific feature magnitude or distribution. For instance, one model might learn a highly sparse representation, while another might learn a dense one; transformations can help bridge this.\\n*   **Gradient Randomization/Smoothing:**\\n    *   **Justification:** The gradient landscape for different models can vary significantly. Randomizing or smoothing the gradients used for optimizing the adversarial perturbation helps prevent overfitting to the source model's exact gradient profile, promoting a more generalized attack direction. It makes the attack less susceptible to the specific \\\"path\\\" dictated by one model's gradients.\\n*   **Hyperparameter Selection (e.g., intensity of augmentation):**\\n    *   **Justification:** This typically comes from empirical tuning and possibly sensitivity analysis. The chosen intensity should be sufficient to introduce meaningful variation without making the optimization problem intractable or rendering the attack ineffective against the source model itself.\\n\\n**Conclusion on Justification:** The justifications should align with addressing the fundamental problem of internal representation mismatch between neural networks. The proposed DA techniques are rational responses to making adversarial examples less dependent on model-specific internal features.\\n\\n---\\n\\n### 5. Constructive Feedback for Further Improvement\\n\\n1.  **Broader Evaluation & Real-World Scenarios:**\\n    *   **Transferability to Defended Models:** A crucial test is how well ILA-DA transfers to models that have undergone adversarial training or other robustification techniques. This would provide a more realistic assessment of its threat potential.\\n    *   **More Diverse Target Models:** Include non-standard architectures, models trained on different datasets (or subsets), quantized models, pruned models, and even Vision Transformers (ViTs) or MLP-Mixers, to see if the DA generalizes beyond CNNs.\\n    *   **Black-box Transferability:** While ILA is typically white-box during generation, exploring its efficacy in scenarios where the target model's internal structure is completely unknown (e.g., via query-based attacks, but leveraging the ILA-DA concept) would be valuable.\\n    *   **Computational Cost Analysis:** Quantify the overhead introduced by the DA techniques. While effectiveness is key, practical applications also consider the time and resources needed to generate these more transferable attacks.\\n\\n2.  **Deeper Understanding & Analysis:**\\n    *   **Visualization of Augmented Features/Gradients:** Provide qualitative analysis by visualizing the intermediate features or gradients with and without augmentation. How do the augmentations change the feature space or the gradient directions? This could offer insights into *why* it works.\\n    *   **Sensitivity Analysis of Hyperparameters:** A more thorough analysis of how the intensity and type of augmentation affect transferability, potentially providing guidelines for future work.\\n    *   **Feature Space Analysis:** Explore metrics to quantify the \\\"distance\\\" or \\\"similarity\\\" between intermediate feature spaces of different models and how ILA-DA helps bridge these gaps. For example, using CKA (Centered Kernel Alignment) or other similarity measures.\\n    *   **Targeted Augmentation:** Could the augmentations be *learned* or *adapted* based on the specific characteristics of the target model (if some prior knowledge is available)? For example, if target models are known to be sparse, augmentations could promote sparsity.\\n\\n3.  **Explore Alternative/Advanced Augmentations:**\\n    *   **Learned Augmentations:** Instead of hand-designed DA, could a meta-learning approach or an adversarial network learn the optimal augmentations that maximize transferability?\\n    *   **Mixup/CutMix in Feature Space:** Adapt concepts like Mixup or CutMix to intermediate feature representations, potentially creating \\\"virtual\\\" intermediate features that combine characteristics of different models.\\n    *   **Semantic-aware Augmentations:** Can augmentations be designed to target more \\\"semantic\\\" aspects of intermediate features, rather than just low-level noise? (e.g., transformations that mimic slight object deformations in feature space).\\n\\n4.  **Theoretical Justification:**\\n    *   While empirical results are strong, providing a more formal theoretical grounding for why these augmentations improve transferability (e.g., by regularizing the adversarial objective's Lipschitz constant, or connecting it to manifold learning) would significantly strengthen the paper.\\n\\n5.  **Implications for Defense:**\\n    *   While an attack paper, briefly discussing the implications for adversarial defense (e.g., \\\"Our findings suggest that defenses focused solely on input-level robustness might be insufficient, and more attention should be paid to intermediate layer robustness\\\") could add value.\\n\\nBy addressing these points, the paper could further solidify its contributions, provide deeper insights into the mechanisms of ILA transferability, and guide future research in both attack and defense strategies.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.8882324\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Evaluate the paper \\\"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\\\". Evaluate the novelty and effectiveness of the proposed data augmentation techniques, their impact on transferability, and the justification for their specific design choices. Provide constructive feedback on how the augmentation strategies could be further improved.\",\n  \"agent_name\": \"DAS\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The paper \"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\" addresses a critical challenge in adversarial machine learning: the limited transferability of intermediate-level adversarial attacks (ILA). By proposing a data augmentation (DA) strategy, the authors aim to generate adversarial examples that are more robust and effective across different target models.\\n\\nLet\\'s evaluate the proposed techniques based on your criteria:\\n\\n---\\n\\n### 1. Novelty of Proposed Data Augmentation Techniques\\n\\n*   **Core Idea Novelty:** The concept of applying data augmentation *specifically* to enhance the transferability of *intermediate-level attacks* is a notable contribution. While data augmentation is widely used in robust training and for improving the transferability of *input-level* attacks (e.g., diverse inputs, input transformations), its systematic application to regularize and generalize attacks at the *feature/gradient level* of intermediate layers is less explored. This shift in focus from input-space augmentation to *feature/gradient-space augmentation* for attack transferability is where the primary novelty lies.\\n*   **Specific Techniques (Hypothetical, as the paper details would be needed):**\\n    *   If the paper introduces novel *types* of augmentations (e.g., specific noise patterns or transformations tailored to feature activations or gradients) that haven\\'t been seen in general DA literature or adversarial training, that would be highly novel.\\n    *   More likely, the techniques are adaptations or combinations of existing DA principles (like adding noise, applying transformations, or ensembling) but applied in a novel context (intermediate features/gradients during attack generation). For instance:\\n        *   **Feature Perturbation:** Adding random noise (Gaussian, uniform) directly to the intermediate feature maps or to the gradients used for attack generation. While noise is common, its targeted application here for ILA transferability is specific.\\n        *   **Feature Transformation:** Applying specific scaling, shifting, or even non-linear transformations to the features during the attack optimization.\\n        *   **Gradient Randomization/Smoothing:** Techniques to diversify the gradient information used, similar to some robust optimization methods.\\n    *   The novelty would stem from *how* these are designed to address the unique challenges of ILA transferability, rather than just general robustness.\\n\\n**Conclusion on Novelty:** The novelty primarily resides in the *application domain* and the *conceptual shift* to applying DA at the intermediate feature/gradient level for attack transferability, rather than necessarily inventing entirely new augmentation operations. However, the specific design and combination of these techniques for ILA can still be highly innovative.\\n\\n---\\n\\n### 2. Effectiveness of Proposed Data Augmentation Techniques\\n\\n*   **Empirical Success:** Based on the paper\\'s title and common research trends, ILA-DA would be expected to demonstrate significant improvements in transferability rates compared to baseline ILA methods without DA. The effectiveness would be measured by:\\n    *   **Increased Attack Success Rate (ASR) on target models:** Showing that the augmented adversarial examples lead to misclassification on previously unseen models.\\n    *   **Higher Transferability Rate:** The percentage of adversarial examples generated on a source model that successfully attack one or more target models.\\n    *   **Robustness to Diverse Architectures:** Demonstrated effectiveness across a range of model architectures (e.g., ResNets, VGGs, DenseNets, MobileNets, ViTs), depths, and widths.\\n*   **Ablation Studies:** Effective papers typically include ablation studies to show the individual contribution of each DA component, proving that the proposed combination is synergistic and that each part plays a role in the overall effectiveness.\\n*   **Comparison to Baselines:** Superior performance against state-of-the-art ILA methods (and potentially other transferability-enhancing techniques like diverse inputs or ensemble attacks) would confirm its effectiveness.\\n*   **Scalability:** The techniques should ideally be effective without a prohibitive increase in the computational cost of generating adversarial examples.\\n\\n**Conclusion on Effectiveness:** If the paper\\'s experimental results align with its claims, ILA-DA should prove effective in significantly boosting the transferability of intermediate-level attacks. The key is the *magnitude* of the improvement and its consistency across various evaluation scenarios.\\n\\n---\\n\\n### 3. Impact on Transferability\\n\\nThe impact of ILA-DA on transferability is direct and positive, stemming from the core design philosophy:\\n\\n*   **Regularization of Perturbation:** By introducing variation (via augmentation) during the attack generation process, the adversarial perturbation is forced to be less specific and brittle to the exact intermediate feature representations of the source model. It essentially regularizes the optimization process, making the resulting perturbation more generalizable.\\n*   **Mimicking Model Discrepancies:** The DA techniques (e.g., adding noise to features, transforming gradients) can be seen as simulating the inherent differences in feature extraction and representation learning across various neural network architectures. By exposing the attack generation process to these simulated variations, the resulting adversarial example becomes more robust to real-world model discrepancies.\\n*   **Focus on Universal Features:** The augmentation encourages the attack to exploit more \"universal\" or \"robust\" features that are common across different models, rather than relying on model-specific idiosyncrasies in intermediate layers. This makes the adversarial example less \"overfit\" to the source model.\\n*   **Enhanced Generalization:** In essence, ILA-DA improves the *generalization* of the adversarial examples across the model space, similar to how data augmentation improves the generalization of models in standard training.\\n\\n**Conclusion on Impact:** ILA-DA\\'s impact is to turn intermediate-level attacks, which often suffer from low transferability due to their reliance on specific internal representations, into more robust and widely applicable threats.\\n\\n---\\n\\n### 4. Justification for Specific Design Choices\\n\\nThe justification for specific design choices within ILA-DA should ideally stem from an understanding of *why* ILA struggles with transferability and *how* different models\\' intermediate representations vary:\\n\\n*   **Why Intermediate-Level Attacks Lack Transferability:** ILA often target specific neurons or feature maps within a network. Different architectures (even trained on the same data) can learn vastly different intermediate representations, making an attack tuned to one model\\'s internal structure ineffective against another. The justification for DA is to bridge this gap.\\n*   **Feature Perturbation/Noise:**\\n    *   **Justification:** Different models may extract features with slight variations in values, activations, or sparsity patterns. Adding noise to feature maps or gradients simulates this variability, forcing the attack to find perturbations that are effective despite these fluctuations. It makes the attack less sensitive to precise activation values and more robust to slight shifts in feature space.\\n    *   **Analogy:** Similar to how adding noise to input images makes a classifier robust to noisy inputs, adding noise to *features* makes the *attack* robust to variations in *features*.\\n*   **Feature Transformation (e.g., scaling, shifting, non-linearities):**\\n    *   **Justification:** Different network depths, widths, or activation functions can lead to feature maps with different scales or dynamic ranges. Applying transformations ensures the attack is not dependent on a specific feature magnitude or distribution. For instance, one model might learn a highly sparse representation, while another might learn a dense one; transformations can help bridge this.\\n*   **Gradient Randomization/Smoothing:**\\n    *   **Justification:** The gradient landscape for different models can vary significantly. Randomizing or smoothing the gradients used for optimizing the adversarial perturbation helps prevent overfitting to the source model\\'s exact gradient profile, promoting a more generalized attack direction. It makes the attack less susceptible to the specific \"path\" dictated by one model\\'s gradients.\\n*   **Hyperparameter Selection (e.g., intensity of augmentation):**\\n    *   **Justification:** This typically comes from empirical tuning and possibly sensitivity analysis. The chosen intensity should be sufficient to introduce meaningful variation without making the optimization problem intractable or rendering the attack ineffective against the source model itself.\\n\\n**Conclusion on Justification:** The justifications should align with addressing the fundamental problem of internal representation mismatch between neural networks. The proposed DA techniques are rational responses to making adversarial examples less dependent on model-specific internal features.\\n\\n---\\n\\n### 5. Constructive Feedback for Further Improvement\\n\\n1.  **Broader Evaluation & Real-World Scenarios:**\\n    *   **Transferability to Defended Models:** A crucial test is how well ILA-DA transfers to models that have undergone adversarial training or other robustification techniques. This would provide a more realistic assessment of its threat potential.\\n    *   **More Diverse Target Models:** Include non-standard architectures, models trained on different datasets (or subsets), quantized models, pruned models, and even Vision Transformers (ViTs) or MLP-Mixers, to see if the DA generalizes beyond CNNs.\\n    *   **Black-box Transferability:** While ILA is typically white-box during generation, exploring its efficacy in scenarios where the target model\\'s internal structure is completely unknown (e.g., via query-based attacks, but leveraging the ILA-DA concept) would be valuable.\\n    *   **Computational Cost Analysis:** Quantify the overhead introduced by the DA techniques. While effectiveness is key, practical applications also consider the time and resources needed to generate these more transferable attacks.\\n\\n2.  **Deeper Understanding & Analysis:**\\n    *   **Visualization of Augmented Features/Gradients:** Provide qualitative analysis by visualizing the intermediate features or gradients with and without augmentation. How do the augmentations change the feature space or the gradient directions? This could offer insights into *why* it works.\\n    *   **Sensitivity Analysis of Hyperparameters:** A more thorough analysis of how the intensity and type of augmentation affect transferability, potentially providing guidelines for future work.\\n    *   **Feature Space Analysis:** Explore metrics to quantify the \"distance\" or \"similarity\" between intermediate feature spaces of different models and how ILA-DA helps bridge these gaps. For example, using CKA (Centered Kernel Alignment) or other similarity measures.\\n    *   **Targeted Augmentation:** Could the augmentations be *learned* or *adapted* based on the specific characteristics of the target model (if some prior knowledge is available)? For example, if target models are known to be sparse, augmentations could promote sparsity.\\n\\n3.  **Explore Alternative/Advanced Augmentations:**\\n    *   **Learned Augmentations:** Instead of hand-designed DA, could a meta-learning approach or an adversarial network learn the optimal augmentations that maximize transferability?\\n    *   **Mixup/CutMix in Feature Space:** Adapt concepts like Mixup or CutMix to intermediate feature representations, potentially creating \"virtual\" intermediate features that combine characteristics of different models.\\n    *   **Semantic-aware Augmentations:** Can augmentations be designed to target more \"semantic\" aspects of intermediate features, rather than just low-level noise? (e.g., transformations that mimic slight object deformations in feature space).\\n\\n4.  **Theoretical Justification:**\\n    *   While empirical results are strong, providing a more formal theoretical grounding for why these augmentations improve transferability (e.g., by regularizing the adversarial objective\\'s Lipschitz constant, or connecting it to manifold learning) would significantly strengthen the paper.\\n\\n5.  **Implications for Defense:**\\n    *   While an attack paper, briefly discussing the implications for adversarial defense (e.g., \"Our findings suggest that defenses focused solely on input-level robustness might be insufficient, and more attention should be paid to intermediate layer robustness\") could add value.\\n\\nBy addressing these points, the paper could further solidify its contributions, provide deeper insights into the mechanisms of ILA transferability, and guide future research in both attack and defense strategies.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.8882324}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'DLA', 'prompt': 'Evaluate the paper \"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\". Assess the theoretical grounding of the approach, the experimental design, and the generalizability of the findings to different architectures and datasets. Critique the experimental methodology and suggest improvements for future research.'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The paper \\\"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\\\" proposes a novel approach to enhance the transferability of Intermediate Level Attacks (ILAs) by applying data augmentation techniques to the intermediate features of the surrogate model. This is an interesting direction that aims to bridge the gap between the white-box nature of ILAs and their utility in black-box attack scenarios.\\n\\nHere's an assessment based on the requested criteria:\\n\\n---\\n\\n### 1. Theoretical Grounding of the Approach\\n\\nThe theoretical grounding of ILA-DA is largely intuitive and empirically driven, rather than deeply mathematical or rigorously proven.\\n\\n*   **Core Hypothesis:** The central hypothesis is that by augmenting the intermediate features (or the input to the intermediate layer) during the attack generation process, the resulting adversarial perturbations become less specific to the idiosyncrasies of the surrogate model's feature space and thus more generalizable and transferable to other target models.\\n*   **Analogy to Data Augmentation:** It draws a clear analogy to standard data augmentation (DA) in training, where DA applied to input data improves the generalization and robustness of models. Here, the idea is to apply DA to *features* to improve the *transferability* of the attack. The intuition is that just as DA on inputs helps a model learn more robust features, DA on features for an attack might help the attack perturb features in a more \\\"robust\\\" or \\\"universal\\\" direction across models.\\n*   **Intermediate Features vs. Input Features:** The key insight is targeting intermediate features. Intermediate features are generally more abstract, invariant, and less sensitive to small input perturbations compared to raw pixel values. The assumption is that these higher-level representations are more aligned across different (though similar) models, making them a better target for transferable attacks. By augmenting these, the attack might learn to target the \\\"core\\\" or \\\"shared\\\" semantics within the feature space, rather than highly model-specific artifacts.\\n*   **Weaknesses:**\\n    *   **Lack of Deep Mathematical Justification:** The paper doesn't delve into the mathematical properties of why augmenting intermediate features leads to better transferability. For example, is it smoothing the loss landscape in the feature space? Is it increasing the \\\"basin of attraction\\\" for adversarial examples? A more formal analysis of how feature-space DA interacts with model landscapes and transferability would strengthen the theoretical foundation.\\n    *   **Heuristic Nature:** While intuitive, the choice of specific data augmentation techniques (e.g., random noise, affine transformations) for *features* might be heuristic. The optimal DA strategies for feature spaces are not necessarily the same as those for pixel spaces.\\n    *   **Implicit Assumptions:** It implicitly assumes that adversarial examples generated by manipulating augmented intermediate features maintain their adversarial potency when propagated back to the input and then fed into a different target model.\\n\\nOverall, the theoretical grounding is based on a sound intuition extrapolated from the success of data augmentation in other contexts, combined with the understanding of intermediate features' properties. However, it could benefit from a more rigorous and less intuitive theoretical analysis.\\n\\n---\\n\\n### 2. Experimental Design\\n\\nThe experimental design appears to be standard for evaluating adversarial attack transferability, with some notable additions for ILA-DA.\\n\\n*   **Objective:** Clearly focused on demonstrating improved transferability of ILAs in black-box settings.\\n*   **Baselines:** Inclusion of strong baselines for adversarial transferability (e.g., standard ILA, MIM, DIM, TIM, PGD variants) is crucial for comparison. It's important that these baselines are implemented fairly and effectively.\\n*   **Models:**\\n    *   **Surrogate Models:** Common and diverse architectures (e.g., InceptionV3, ResNet-50) are typically used, which is appropriate.\\n    *   **Target Models:** A good set of target models should include various architectures (e.g., Inception, ResNet, VGG, DenseNet) and, importantly, adversarially trained models (e.g., Inc-Res-v2-Adv, ResNet-50-Adv) to assess robustness against stronger defenses.\\n*   **Datasets:** ImageNet is the de-facto standard for evaluating transferability due to its complexity and the availability of pre-trained models. Using a subset of ImageNet (e.g., 1000 images) is common for computational feasibility.\\n*   **Attack Parameters:** Standard parameters like L-infinity norm (epsilon), number of iterations, step size, and batch size are typical.\\n*   **Metrics:** Attack success rate (ASR) against target models is the primary metric, which is appropriate.\\n*   **Augmentation Strategy for Features:** This is the core novelty. The paper needs to detail:\\n    *   What specific data augmentation operations are applied to the intermediate features (e.g., adding Gaussian noise, dropout, feature scaling, random transformations in feature space).\\n    *   How the parameters of these augmentations are chosen and if they are tuned.\\n    *   Whether the augmentation is applied *during* the attack iteration (i.e., augmenting the feature before calculating the loss at each step) or in a pre-processing step.\\n*   **Ablation Studies:** Crucial for understanding the contribution of different components:\\n    *   Effect of different augmentation types.\\n    *   Effect of augmentation strength.\\n    *   Impact of choosing different intermediate layers.\\n    *   Comparison with applying DA to the input pixels instead of intermediate features.\\n*   **Cross-architecture and Cross-layer Evaluation:** Testing transferability from one surrogate model to multiple target models and investigating which intermediate layers are most effective for transfer.\\n\\n**Potential areas for improvement in experimental design:**\\n\\n*   **Hyperparameter Tuning:** A transparent and systematic approach to tuning hyperparameters (e.g., augmentation strength, layer choice) for ILA-DA is critical to ensure fair comparison and reproducibility. Was it exhaustive, or did it favor ILA-DA?\\n*   **Statistical Significance:** While ASRs are reported, evaluating the statistical significance of improvements (e.g., using confidence intervals or statistical tests) would strengthen the conclusions.\\n*   **Computational Cost:** Reporting the computational overhead of ILA-DA compared to standard ILAs is important, especially since applying DA to features might add complexity.\\n\\n---\\n\\n### 3. Generalizability of the Findings\\n\\nThe generalizability of the findings depends heavily on the scope of the experiments.\\n\\n*   **Architectures:**\\n    *   **Strengths:** Testing against a variety of CNN architectures (ResNet, Inception, VGG, DenseNet) is a good start. Including adversarially trained models is also crucial for real-world relevance.\\n    *   **Limitations:** The current landscape of deep learning includes architectures beyond traditional CNNs, such as Vision Transformers (ViTs), MLP-Mixers, or ConvNeXT. If the experiments are limited to only CNNs, the generalizability to these newer architectures, which have different inductive biases and feature representations, would be unclear. The effectiveness of feature-space DA might differ for attention-based models or models without explicit spatial hierarchies.\\n*   **Datasets:**\\n    *   **Strengths:** ImageNet is a robust dataset for transferability studies due to its diversity and complexity.\\n    *   **Limitations:** Limiting experiments to ImageNet means the findings might not generalize well to other domains (e.g., medical imaging, satellite imagery, tabular data) or smaller/simpler datasets (e.g., CIFAR-10, MNIST). The nature of \\\"intermediate features\\\" and what makes them transferable might be dataset-dependent. Different datasets might require different types or strengths of feature-space augmentation.\\n*   **Attack Settings:**\\n    *   **Norms:** Typically, L-infinity is tested. Exploring other norms (L1, L2) could broaden applicability.\\n    *   **Targeted vs. Non-Targeted:** Most transferability research focuses on non-targeted attacks. Generalizability to targeted attacks, which are often harder to transfer, would be a more challenging and valuable test.\\n*   **Data Augmentation Methods:** While the paper likely explores a few common DA techniques, the generalizability might be tied to those specific choices. Would other, more sophisticated, or learned augmentation strategies in feature space yield similar or better results?\\n\\nTo enhance generalizability, broader experiments across more diverse model architectures (including non-CNNs), multiple datasets, and various attack settings are necessary.\\n\\n---\\n\\n### 4. Critique of Experimental Methodology and Suggestions for Improvements\\n\\n**Critique:**\\n\\n1.  **Selection of Intermediate Layer:** The choice of which intermediate layer to attack is critical for ILAs. The paper should provide a clear rationale or empirical study for its chosen layer. Is it consistently optimal across different surrogate models or target models? Is it too early (less abstract) or too late (too specific)?\\n2.  **Specifics of Feature Augmentation:** The paper must clearly define *how* the data augmentation is applied to features.\\n    *   Is it applied to activations or gradients?\\n    *   Is it applied before or after activation functions?\\n    *   What are the specific DA operations (e.g., random Gaussian noise, scaling, channel shuffling, spatial transformations within feature maps)?\\n    *   How are the parameters for these operations chosen (e.g., noise standard deviation, scaling factor ranges)? Are they fixed, or adapted?\\n3.  **Computational Overhead:** Applying DA at each iteration within the attack loop could add significant computational cost. The paper should benchmark the time taken for ILA-DA versus standard ILAs to demonstrate its practicality.\\n4.  **Sensitivity to Hyperparameters:** How sensitive are the results to the chosen augmentation parameters (e.g., noise level, transformation magnitudes)? A small parameter change should not lead to a drastic drop in performance.\\n5.  **Ablation Study Depth:** While an ablation study is expected, ensure it covers:\\n    *   Individual contribution of each DA component.\\n    *   Comparison of feature-level DA vs. input-level DA for transferability.\\n    *   Effect of different intermediate layers.\\n6.  **Adversarially Trained Models as Surrogates:** If adversarially trained models are used as surrogate models for generating attacks, how does feature-space DA interact with their robustness? This is a challenging scenario often overlooked.\\n\\n**Suggestions for Future Research and Improvements:**\\n\\n1.  **Deeper Theoretical Analysis:**\\n    *   Investigate the geometric interpretation of feature-space DA on the loss landscape. Does it smooth the landscape, broaden local minima, or change the curvature in a way that promotes transferability?\\n    *   Explore connections to concepts like adversarial robustness (e.g., could feature-space DA during training be a defense?).\\n    *   Characterize the properties of intermediate features that make them more amenable to certain DA techniques for transferability.\\n2.  **Learned Feature Augmentation:** Instead of manually chosen DA techniques, could a neural network learn optimal augmentation strategies for intermediate features to maximize transferability? This could involve adversarial learning or reinforcement learning.\\n3.  **Beyond Standard CNNs:**\\n    *   Test ILA-DA against Vision Transformers (ViTs), ConvNeXT, and other emerging architectures to assess its universality.\\n    *   Explore how feature-space DA interacts with self-attention mechanisms or different tokenization strategies.\\n4.  **Multi-Model Ensembling and ILA-DA:** Combine ILA-DA with multi-model attack strategies (e.g., ensemble of surrogates). Does augmenting features from multiple models yield even higher transferability?\\n5.  **Targeted Attacks:** Extend the research to targeted attacks, which are significantly harder to transfer. This would be a more stringent test of the method's effectiveness.\\n6.  **Human Perceptibility vs. Transferability:** While not the primary focus, investigate if the improved transferability comes at the cost of increased perceptibility of the adversarial perturbation, or if it can maintain imperceptibility.\\n7.  **Defense Perspective:** Can the principles of ILA-DA be inverted? For example, could applying specific types of augmentation to intermediate features *during training* enhance a model's adversarial robustness? This would be a highly impactful extension.\\n8.  **Adaptive Layer Selection:** Could the model adaptively choose the \\\"best\\\" intermediate layer for augmentation based on the target model characteristics (if partially known) or an empirical heuristic?\\n9.  **Visualization:** Visualize the augmented intermediate features and their impact on the adversarial direction in the feature space. This could offer qualitative insights into \\\"why\\\" it works.\\n\\n---\\n\\n**Overall Assessment:**\\n\\n\\\"ILA-DA\\\" presents a promising and intuitively appealing concept for improving adversarial attack transferability. The idea of augmenting intermediate features is novel and leverages known strengths of data augmentation. The experimental design, while generally sound for this field, could be strengthened by more rigorous hyperparameter tuning, deeper ablation studies, and broader testing across newer architectures and diverse datasets. The main theoretical weakness lies in the lack of a deep mathematical explanation for *why* this specific form of augmentation works beyond an intuitive analogy. Future research should focus on solidifying this theoretical understanding and rigorously testing the method's limits across the evolving landscape of deep learning models and adversarial defenses.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.8847269\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"DLA\",\n  \"prompt\": \"Evaluate the paper \\\"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\\\". Assess the theoretical grounding of the approach, the experimental design, and the generalizability of the findings to different architectures and datasets. Critique the experimental methodology and suggest improvements for future research.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The paper \"ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation\" proposes a novel approach to enhance the transferability of Intermediate Level Attacks (ILAs) by applying data augmentation techniques to the intermediate features of the surrogate model. This is an interesting direction that aims to bridge the gap between the white-box nature of ILAs and their utility in black-box attack scenarios.\\n\\nHere\\'s an assessment based on the requested criteria:\\n\\n---\\n\\n### 1. Theoretical Grounding of the Approach\\n\\nThe theoretical grounding of ILA-DA is largely intuitive and empirically driven, rather than deeply mathematical or rigorously proven.\\n\\n*   **Core Hypothesis:** The central hypothesis is that by augmenting the intermediate features (or the input to the intermediate layer) during the attack generation process, the resulting adversarial perturbations become less specific to the idiosyncrasies of the surrogate model\\'s feature space and thus more generalizable and transferable to other target models.\\n*   **Analogy to Data Augmentation:** It draws a clear analogy to standard data augmentation (DA) in training, where DA applied to input data improves the generalization and robustness of models. Here, the idea is to apply DA to *features* to improve the *transferability* of the attack. The intuition is that just as DA on inputs helps a model learn more robust features, DA on features for an attack might help the attack perturb features in a more \"robust\" or \"universal\" direction across models.\\n*   **Intermediate Features vs. Input Features:** The key insight is targeting intermediate features. Intermediate features are generally more abstract, invariant, and less sensitive to small input perturbations compared to raw pixel values. The assumption is that these higher-level representations are more aligned across different (though similar) models, making them a better target for transferable attacks. By augmenting these, the attack might learn to target the \"core\" or \"shared\" semantics within the feature space, rather than highly model-specific artifacts.\\n*   **Weaknesses:**\\n    *   **Lack of Deep Mathematical Justification:** The paper doesn\\'t delve into the mathematical properties of why augmenting intermediate features leads to better transferability. For example, is it smoothing the loss landscape in the feature space? Is it increasing the \"basin of attraction\" for adversarial examples? A more formal analysis of how feature-space DA interacts with model landscapes and transferability would strengthen the theoretical foundation.\\n    *   **Heuristic Nature:** While intuitive, the choice of specific data augmentation techniques (e.g., random noise, affine transformations) for *features* might be heuristic. The optimal DA strategies for feature spaces are not necessarily the same as those for pixel spaces.\\n    *   **Implicit Assumptions:** It implicitly assumes that adversarial examples generated by manipulating augmented intermediate features maintain their adversarial potency when propagated back to the input and then fed into a different target model.\\n\\nOverall, the theoretical grounding is based on a sound intuition extrapolated from the success of data augmentation in other contexts, combined with the understanding of intermediate features\\' properties. However, it could benefit from a more rigorous and less intuitive theoretical analysis.\\n\\n---\\n\\n### 2. Experimental Design\\n\\nThe experimental design appears to be standard for evaluating adversarial attack transferability, with some notable additions for ILA-DA.\\n\\n*   **Objective:** Clearly focused on demonstrating improved transferability of ILAs in black-box settings.\\n*   **Baselines:** Inclusion of strong baselines for adversarial transferability (e.g., standard ILA, MIM, DIM, TIM, PGD variants) is crucial for comparison. It\\'s important that these baselines are implemented fairly and effectively.\\n*   **Models:**\\n    *   **Surrogate Models:** Common and diverse architectures (e.g., InceptionV3, ResNet-50) are typically used, which is appropriate.\\n    *   **Target Models:** A good set of target models should include various architectures (e.g., Inception, ResNet, VGG, DenseNet) and, importantly, adversarially trained models (e.g., Inc-Res-v2-Adv, ResNet-50-Adv) to assess robustness against stronger defenses.\\n*   **Datasets:** ImageNet is the de-facto standard for evaluating transferability due to its complexity and the availability of pre-trained models. Using a subset of ImageNet (e.g., 1000 images) is common for computational feasibility.\\n*   **Attack Parameters:** Standard parameters like L-infinity norm (epsilon), number of iterations, step size, and batch size are typical.\\n*   **Metrics:** Attack success rate (ASR) against target models is the primary metric, which is appropriate.\\n*   **Augmentation Strategy for Features:** This is the core novelty. The paper needs to detail:\\n    *   What specific data augmentation operations are applied to the intermediate features (e.g., adding Gaussian noise, dropout, feature scaling, random transformations in feature space).\\n    *   How the parameters of these augmentations are chosen and if they are tuned.\\n    *   Whether the augmentation is applied *during* the attack iteration (i.e., augmenting the feature before calculating the loss at each step) or in a pre-processing step.\\n*   **Ablation Studies:** Crucial for understanding the contribution of different components:\\n    *   Effect of different augmentation types.\\n    *   Effect of augmentation strength.\\n    *   Impact of choosing different intermediate layers.\\n    *   Comparison with applying DA to the input pixels instead of intermediate features.\\n*   **Cross-architecture and Cross-layer Evaluation:** Testing transferability from one surrogate model to multiple target models and investigating which intermediate layers are most effective for transfer.\\n\\n**Potential areas for improvement in experimental design:**\\n\\n*   **Hyperparameter Tuning:** A transparent and systematic approach to tuning hyperparameters (e.g., augmentation strength, layer choice) for ILA-DA is critical to ensure fair comparison and reproducibility. Was it exhaustive, or did it favor ILA-DA?\\n*   **Statistical Significance:** While ASRs are reported, evaluating the statistical significance of improvements (e.g., using confidence intervals or statistical tests) would strengthen the conclusions.\\n*   **Computational Cost:** Reporting the computational overhead of ILA-DA compared to standard ILAs is important, especially since applying DA to features might add complexity.\\n\\n---\\n\\n### 3. Generalizability of the Findings\\n\\nThe generalizability of the findings depends heavily on the scope of the experiments.\\n\\n*   **Architectures:**\\n    *   **Strengths:** Testing against a variety of CNN architectures (ResNet, Inception, VGG, DenseNet) is a good start. Including adversarially trained models is also crucial for real-world relevance.\\n    *   **Limitations:** The current landscape of deep learning includes architectures beyond traditional CNNs, such as Vision Transformers (ViTs), MLP-Mixers, or ConvNeXT. If the experiments are limited to only CNNs, the generalizability to these newer architectures, which have different inductive biases and feature representations, would be unclear. The effectiveness of feature-space DA might differ for attention-based models or models without explicit spatial hierarchies.\\n*   **Datasets:**\\n    *   **Strengths:** ImageNet is a robust dataset for transferability studies due to its diversity and complexity.\\n    *   **Limitations:** Limiting experiments to ImageNet means the findings might not generalize well to other domains (e.g., medical imaging, satellite imagery, tabular data) or smaller/simpler datasets (e.g., CIFAR-10, MNIST). The nature of \"intermediate features\" and what makes them transferable might be dataset-dependent. Different datasets might require different types or strengths of feature-space augmentation.\\n*   **Attack Settings:**\\n    *   **Norms:** Typically, L-infinity is tested. Exploring other norms (L1, L2) could broaden applicability.\\n    *   **Targeted vs. Non-Targeted:** Most transferability research focuses on non-targeted attacks. Generalizability to targeted attacks, which are often harder to transfer, would be a more challenging and valuable test.\\n*   **Data Augmentation Methods:** While the paper likely explores a few common DA techniques, the generalizability might be tied to those specific choices. Would other, more sophisticated, or learned augmentation strategies in feature space yield similar or better results?\\n\\nTo enhance generalizability, broader experiments across more diverse model architectures (including non-CNNs), multiple datasets, and various attack settings are necessary.\\n\\n---\\n\\n### 4. Critique of Experimental Methodology and Suggestions for Improvements\\n\\n**Critique:**\\n\\n1.  **Selection of Intermediate Layer:** The choice of which intermediate layer to attack is critical for ILAs. The paper should provide a clear rationale or empirical study for its chosen layer. Is it consistently optimal across different surrogate models or target models? Is it too early (less abstract) or too late (too specific)?\\n2.  **Specifics of Feature Augmentation:** The paper must clearly define *how* the data augmentation is applied to features.\\n    *   Is it applied to activations or gradients?\\n    *   Is it applied before or after activation functions?\\n    *   What are the specific DA operations (e.g., random Gaussian noise, scaling, channel shuffling, spatial transformations within feature maps)?\\n    *   How are the parameters for these operations chosen (e.g., noise standard deviation, scaling factor ranges)? Are they fixed, or adapted?\\n3.  **Computational Overhead:** Applying DA at each iteration within the attack loop could add significant computational cost. The paper should benchmark the time taken for ILA-DA versus standard ILAs to demonstrate its practicality.\\n4.  **Sensitivity to Hyperparameters:** How sensitive are the results to the chosen augmentation parameters (e.g., noise level, transformation magnitudes)? A small parameter change should not lead to a drastic drop in performance.\\n5.  **Ablation Study Depth:** While an ablation study is expected, ensure it covers:\\n    *   Individual contribution of each DA component.\\n    *   Comparison of feature-level DA vs. input-level DA for transferability.\\n    *   Effect of different intermediate layers.\\n6.  **Adversarially Trained Models as Surrogates:** If adversarially trained models are used as surrogate models for generating attacks, how does feature-space DA interact with their robustness? This is a challenging scenario often overlooked.\\n\\n**Suggestions for Future Research and Improvements:**\\n\\n1.  **Deeper Theoretical Analysis:**\\n    *   Investigate the geometric interpretation of feature-space DA on the loss landscape. Does it smooth the landscape, broaden local minima, or change the curvature in a way that promotes transferability?\\n    *   Explore connections to concepts like adversarial robustness (e.g., could feature-space DA during training be a defense?).\\n    *   Characterize the properties of intermediate features that make them more amenable to certain DA techniques for transferability.\\n2.  **Learned Feature Augmentation:** Instead of manually chosen DA techniques, could a neural network learn optimal augmentation strategies for intermediate features to maximize transferability? This could involve adversarial learning or reinforcement learning.\\n3.  **Beyond Standard CNNs:**\\n    *   Test ILA-DA against Vision Transformers (ViTs), ConvNeXT, and other emerging architectures to assess its universality.\\n    *   Explore how feature-space DA interacts with self-attention mechanisms or different tokenization strategies.\\n4.  **Multi-Model Ensembling and ILA-DA:** Combine ILA-DA with multi-model attack strategies (e.g., ensemble of surrogates). Does augmenting features from multiple models yield even higher transferability?\\n5.  **Targeted Attacks:** Extend the research to targeted attacks, which are significantly harder to transfer. This would be a more stringent test of the method\\'s effectiveness.\\n6.  **Human Perceptibility vs. Transferability:** While not the primary focus, investigate if the improved transferability comes at the cost of increased perceptibility of the adversarial perturbation, or if it can maintain imperceptibility.\\n7.  **Defense Perspective:** Can the principles of ILA-DA be inverted? For example, could applying specific types of augmentation to intermediate features *during training* enhance a model\\'s adversarial robustness? This would be a highly impactful extension.\\n8.  **Adaptive Layer Selection:** Could the model adaptively choose the \"best\" intermediate layer for augmentation based on the target model characteristics (if partially known) or an empirical heuristic?\\n9.  **Visualization:** Visualize the augmented intermediate features and their impact on the adversarial direction in the feature space. This could offer qualitative insights into \"why\" it works.\\n\\n---\\n\\n**Overall Assessment:**\\n\\n\"ILA-DA\" presents a promising and intuitively appealing concept for improving adversarial attack transferability. The idea of augmenting intermediate features is novel and leverages known strengths of data augmentation. The experimental design, while generally sound for this field, could be strengthened by more rigorous hyperparameter tuning, deeper ablation studies, and broader testing across newer architectures and diverse datasets. The main theoretical weakness lies in the lack of a deep mathematical explanation for *why* this specific form of augmentation works beyond an intuitive analogy. Future research should focus on solidifying this theoretical understanding and rigorously testing the method\\'s limits across the evolving landscape of deep learning models and adversarial defenses.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.8847269}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Based on the reviews from the three agents, here's a synthesis of their assessments:\n\n*   **ARE (Adversarial Robustness Expert):** Recommends a **Weak Accept**. Highlights the potential of ILA-DA as a stronger benchmark for defenses but expresses concerns about the incremental novelty. Suggests deepening the novelty discussion, richer ablation studies, and a more comprehensive defense evaluation.\n*   **DAS (Data Augmentation Specialist):** Provides a detailed evaluation of the novelty, effectiveness, and impact of the proposed data augmentation techniques. Offers constructive feedback for further improvement, including broader evaluation, deeper understanding, and exploration of alternative augmentations.\n*   **DLA (Deep Learning Generalization Analyst):** Assesses the theoretical grounding, experimental design, and generalizability of the findings. Critiques the experimental methodology and suggests improvements for future research, emphasizing the need for a deeper theoretical analysis and broader experiments.\n\n**Overall Recommendation:**\n\nThe paper presents a valuable contribution to the field of adversarial robustness by improving the transferability of intermediate-level attacks. The core idea of applying data augmentation to intermediate features is novel and intuitively sound. The experimental results demonstrate the effectiveness of the proposed method, ILA-DA, in outperforming existing state-of-the-art transfer-based attacks.\n\nHowever, the paper could be significantly strengthened by addressing the concerns raised by the reviewers, particularly regarding the theoretical grounding, novelty, and experimental design.\n\n**Specifically:**\n\n*   **Novelty:** The authors need to clearly articulate why the combination of ILAs and data augmentation is more than just a straightforward application of existing techniques. They should provide deeper theoretical insights into the unique synergy or demonstrate a revolutionary empirical jump.\n*   **Theoretical Grounding:** The paper would benefit from a more rigorous and less intuitive theoretical analysis of why augmenting intermediate features leads to better transferability.\n*   **Experimental Design:** The authors should provide a more transparent and systematic approach to hyperparameter tuning, evaluate the statistical significance of improvements, and report the computational overhead of ILA-DA.\n*   **Generalizability:** Broader experiments across more diverse model architectures (including non-CNNs), multiple datasets, and various attack settings are necessary to enhance the generalizability of the findings.\n\n**Final Decision:**\n\nGiven the potential value of the contribution and the areas for improvement, the final decision is:\n\n**FINAL DECISION: Reject**\n\n**Justification:** While the paper shows promise, the concerns about novelty and theoretical grounding are significant enough to warrant rejection. The authors should address these concerns in a future submission.\n"}], "ground_truth": "Accept: poster", "response_history": "Based on the reviews from the three agents, here's a synthesis of their assessments:\n\n*   **ARE (Adversarial Robustness Expert):** Recommends a **Weak Accept**. Highlights the potential of ILA-DA as a stronger benchmark for defenses but expresses concerns about the incremental novelty. Suggests deepening the novelty discussion, richer ablation studies, and a more comprehensive defense evaluation.\n*   **DAS (Data Augmentation Specialist):** Provides a detailed evaluation of the novelty, effectiveness, and impact of the proposed data augmentation techniques. Offers constructive feedback for further improvement, including broader evaluation, deeper understanding, and exploration of alternative augmentations.\n*   **DLA (Deep Learning Generalization Analyst):** Assesses the theoretical grounding, experimental design, and generalizability of the findings. Critiques the experimental methodology and suggests improvements for future research, emphasizing the need for a deeper theoretical analysis and broader experiments.\n\n**Overall Recommendation:**\n\nThe paper presents a valuable contribution to the field of adversarial robustness by improving the transferability of intermediate-level attacks. The core idea of applying data augmentation to intermediate features is novel and intuitively sound. The experimental results demonstrate the effectiveness of the proposed method, ILA-DA, in outperforming existing state-of-the-art transfer-based attacks.\n\nHowever, the paper could be significantly strengthened by addressing the concerns raised by the reviewers, particularly regarding the theoretical grounding, novelty, and experimental design.\n\n**Specifically:**\n\n*   **Novelty:** The authors need to clearly articulate why the combination of ILAs and data augmentation is more than just a straightforward application of existing techniques. They should provide deeper theoretical insights into the unique synergy or demonstrate a revolutionary empirical jump.\n*   **Theoretical Grounding:** The paper would benefit from a more rigorous and less intuitive theoretical analysis of why augmenting intermediate features leads to better transferability.\n*   **Experimental Design:** The authors should provide a more transparent and systematic approach to hyperparameter tuning, evaluate the statistical significance of improvements, and report the computational overhead of ILA-DA.\n*   **Generalizability:** Broader experiments across more diverse model architectures (including non-CNNs), multiple datasets, and various attack settings are necessary to enhance the generalizability of the findings.\n\n**Final Decision:**\n\nGiven the potential value of the contribution and the areas for improvement, the final decision is:\n\n**FINAL DECISION: Reject**\n\n**Justification:** While the paper shows promise, the concerns about novelty and theoretical grounding are significant enough to warrant rejection. The authors should address these concerns in a future submission.\n", "elapsed_time": 125.16416001319885, "reviewer_agents": {"ARE": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in adversarial robustness and security implications of deep learning models.", "system_prompt": "You are an expert in adversarial robustness. Evaluate the paper's attack method, its effectiveness against defended models, and its novelty in the adversarial robustness landscape. Assess the security implications and potential impact of this research.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "DAS": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Specialist in data augmentation techniques for improving machine learning model performance.", "system_prompt": "You are a specialist in data augmentation. Evaluate the novelty and effectiveness of the proposed data augmentation techniques, their impact on transferability, and the justification for their specific design choices. Provide constructive feedback on how the augmentation strategies could be further improved.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "DLA": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Analyst specializing in deep learning generalization and experimental design.", "system_prompt": "You are an analyst specializing in deep learning generalization. Assess the theoretical grounding of the approach, the experimental design, and the generalizability of the findings to different architectures and datasets. Critique the experimental methodology and suggest improvements for future research.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}}}
