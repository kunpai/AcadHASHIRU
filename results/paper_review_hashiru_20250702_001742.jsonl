{"paper_id": "Idusfje4-Wq", "prompt": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Diffusion Models for Causal Discovery via Topological Ordering\n\nNone: 1 INTRODUCTION Understanding the causal structure of a problem is important for areas such as economics, biology (Sachs et al., 2005) and healthcare (Sanchez et al., 2022), especially when reasoning about the effect of interventions. When interventional data from randomised trials are not available, causal discovery methods (Glymour et al., 2019) may be employed to discover the causal structure of a problem solely from observational data. Causal structure is typically modelled as a directed acyclic graph (DAG) G in which each node is associated with a random variable and each edge represents a causal mechanism i.e. how one variable influences another. However, learning such a model from data is NP-hard (Chickering, 1996). Traditional methods search the DAG space by testing for conditional independence between variables (Spirtes et al., 1993) or by optimising some goodness of fit measure (Chickering, 2002). Unfortunately, solv- ing the search problem with a greedy combinatorial optimisation method can be expensive and does not scale to high-dimensional problems. In line with previous work (Teyssier & Koller, 2005; Park & Klabjan, 2017; Bu\u0308hlmann et al., 2014; Solus et al., 2021; Wang et al., 2021; Rolland et al., 2022), we can speed up the combinatorial search problem over the space of DAGs by rephrasing it as a topological ordering task, ordering from leaf nodes to root nodes. The search space over DAGs with d nodes and (d2 \u2212 d)/2 possible edges is much larger than the space of permutations over d variables. Once a topological ordering of the nodes is found, the potential causal relations between later (cause) and earlier (effect) nodes can be pruned with a feature selection algorithm (e.g. Bu\u0308hlmann et al. (2014)) to yield a graph which is naturally directed and acyclic without further optimisation. Recently, Rolland et al. (2022) proposed the SCORE algorithm which uses the Hessian of the data log-likelihood, \u22072x log p(x) (i.e. the Jacobian of the score 1) for topological ordering. By verifying which elements of \u22072x log p(x)\u2019s diagonal are constant across all data points, leaf nodes can be iteratively identified and removed. The procedure from Rolland et al. (2022) scales linearly with the number of variables d but with polynomial complexity with the number of samples n i.e. O(dn3), due to the Stein gradient estimator (Li & Turner, 2018), which hinders its use for high-dimensional causal discovery. Here, we use neural networks (NN) trained with diffusion probabilistic modeling (DPM) for learning the score function \u2207x log p(x), inspired by Rolland et al. (2022). We propose a method to infer an additive residue which allows computing the score at each leaf-removal step without re-training the NN. In addition, the NN is trained over the entire dataset (n samples) but only a subsampled batch is used for finding leaf nodes.Thus, once the score model is learned, we can use it to order the graph with constant complexity on n, enabling causal discovery in high-dimensional settings. Interestingly, our algorithm does not require architectural constraints on the neural network, as in previous causal discovery methods based on neural networks (Lachapelle et al., 2020; Zheng et al., 2020; Yu et al., 2019; Ng et al., 2022). Our training procedure does not learn the causal mechanism directly, but the score of the data distribution. Contributions. In summary, we propose DiffAN, an identifiable algorithm leveraging a diffusion probabilistic model for topological ordering that enables causal discovery assuming an additive noise model: (i) To the best of our knowledge, we present the first causal discovery algorithm based on denoising diffusion training. The score estimated with the diffusion model is used to find leaf nodes iteratively; (ii) We prove (Section 2.2) that ANMs are identifiable from the variance of the log-likelihood\u2019s Hessian for a more general family of noise distribution, extending identifiability statements from Rolland et al. (2022); (iii) We show in Section 3 that when a leaf node is removed, the score of the new distribution can be estimated from the original score (before leaf removal) and its Jacobian. It allow us to propose an efficient algorithm for topological ordering which does not depend on re-training the score after each leaf removal iteration; (iv) We estimate the second-order derivatives (score\u2019s Jacobian or Hessian) of a data distribution using neural networks with diffusion training via backpropagation; (v) We show empirically that our method performs on a par with state-of-the-art causal discovery methods in small graph datasets but scales better to bigger datasets with more nodes and samples. 2 PRELIMINARIES:  2.1 PROBLEM DEFINITION: We consider the problem of discovering the causal structure between d variables, given a probability distribution p(x) from which a d-dimensional random vector x = (x1, . . . , xd) can be sampled. We assume that the true causal structure is described by a DAG G containing d nodes. Each node represents a random variable xi and edges represent the presence of causal relations between them. In other words, we can say that G defines a structural causal model (SCM) consisting of a collection of assignments xi := fi(Pa(xi), \u03f5i), where Pa(xi) are the parents of xi in G, and \u03f5i is a noise term independent of xi, also called exogenous noise. \u03f5i are i.i.d. from a smooth distribution p\u03f5. The SCM entails a unique distribution p(x) = \u220fd i=1 p(xi | Pa(xi)) over the variables x (Peters et al., 2017). The observational input data are X \u2208 Rn\u00d7d, where n is number of samples. The target output is an adjacency matrix A \u2208 Rd\u00d7d. 1Note that, throughout the paper, we use the term \u201cscore\u201d to denote the gradient of the data log-likelihood \u2207x log p(x); this is different to causal literature which uses \u201cscore\u201d to refer to the optimisation function in combinatorial search algorithms. The topological ordering (also called causal ordering or causal list) of a DAG G is defined as a non-unique permutation \u03c0 of d nodes such that a given node always appears first in the list than its descendants. Formally, \u03c0i < \u03c0j \u21d0\u21d2 j \u2208 DeG(xi) where DeG(xi) are the descendants of the ith node in G (Appendix B in Peters et al. (2017)). 2.2 NONLINEAR ADDITIVE NOISE MODELS: Learning a unique A from X with observational data requires additional assumptions. A common class of methods called additive noise models (ANM) (Shimizu et al., 2006; Hoyer et al., 2008; Peters et al., 2014; Bu\u0308hlmann et al., 2014) explores asymmetries in the data by imposing functional assumptions on the data generation process. In most cases, they assume that assignments take the form xi := fi(Pa(xi)) + \u03f5i with \u03f5i \u223c p\u03f5. Here we focus on the case described by Peters et al. (2014) where fi is nonlinear. We use the notation fi for fi(Pa(xi)) because the arguments of fi will always be Pa(xi) throughout this paper. We highlight that fi does not depend on i. Identifiability. We assume that the SCM follows an additive noise model (ANM) which is known to be identifiable from observational data (Hoyer et al., 2008; Peters et al., 2014). We also assume causal sufficiency, i.e. there are no hidden variables that are a common cause of at least two observed variables. In addition, corollary 33 from Peters et al. (2014) states that the true topological ordering of the DAG, as in our setting, is identifiable from a p(x) generated by an ANM without requiring causal minimality assumptions. Finding Leaves with the Score. Rolland et al. (2022) propose that the score of an ANM with distribution p(x) can be used to find leaves1. Before presenting how to find the leaves, we derive, following Lemma 2 in Rolland et al. (2022), an analytical expression for the score which can be written as \u2207xj log p(x) = \u2207xj log d\u220f i=1 p(xi | Pa(xi)) = \u2207xj d\u2211 i=1 log p(xi | Pa(xi)) = \u2207xj d\u2211 i=1 log p\u03f5 (xi \u2212 fi) \u25b7 Using \u03f5i = xi \u2212 fi = \u2202 log p\u03f5 (xj \u2212 fj) \u2202xj \u2212 \u2211 i\u2208Ch(xj) \u2202fi \u2202xj \u2202 log p\u03f5 (xi \u2212 fi) \u2202x . (1) Where Ch(xj) denotes the children of xj . Note that if xj does not have a child node, the second term on the last row disappears. In this case, the Jacobian of the score \u22072xj log p(x) (also denoted as the Hessian Hj,j(log p(x))2) becomes a constant. This observation provides a mechanism to identify leaf nodes in a graph because if the second-order derivative is constant, the variance across all data points should be zero. Lemma 1. Given a nonlinear ANM with a noise distribution p\u03f5, VarX [Hj,j(log p(x))] = 0 \u21d0\u21d2 j is a leaf & d log p\u03f5 dx2 = a, (2) where a is a constant and H(log p(x)) \u2208 Rd,d is the Hessian matrix of the log-density log p(x). Therefore, the leaves are only identifiable using the variance of the log-likelihood\u2019s Hessian for ANMs with noise distribution that have constant second-order derivatives. See proof in Appendix A.1. Note that Lemma 1 is a generalisation of Rolland et al. (2022) which only prove that Equation 2 can identify leaves in nonlinear ANMs with Gaussian noise. We derive a more general formulation which additionally requires the second-order derivative of the noise distribution to be constant. Indeed, the condition d log p \u03f5 dx2 = a is true for p \u03f5 following a Gaussian distribution which is consistent with Rolland et al. (2022). 1We refer to nodes without children in a DAG G as leaves. 2The subscripts i, j in Hi,j denote an element at the ith row and jth column. 2.3 DIFFUSION MODELS APPROXIMATE THE SCORE: The process of learning to denoise (Vincent, 2011) can approximate that of matching the score (Hyva\u0308rinen, 2005). A diffusion process gradually adds noise to a data distribution over time. Diffusion probabilistic models (DPMs) Sohl-Dickstein et al. (2015); Ho et al. (2020); Song et al. (2021) learn to reverse the diffusion process, starting with noise and recovering the data distribution. The diffusion process gradually adds Gaussian noise, with a time-dependent variance \u03b1t, to a sample x0 \u223c pdata(x) from the data distribution. Thus, the noisy variable xt, with t \u2208 [0, T ], is learned to correspond to versions of x0 perturbed by Gaussian noise following p (xt | x0) = N ( xt; \u221a \u03b1tx0, (1\u2212 \u03b1t) I ) , where \u03b1t := \u220ft j=0 (1\u2212 \u03b2j), \u03b2j is the variance scheduled between [\u03b2min, \u03b2max] and I is the identity matrix. DPMs (Ho et al., 2020) are learned with a weighted sum of denoising score matching objectives at different perturbation scales with \u03b8\u2217 = argmin \u03b8 Ex0,t,\u03f5 [ \u03bb(t) \u2225\u03f5\u03b8(xt, t)\u2212 \u03f5\u222522 ] , (3) where xt = \u221a \u03b1tx0 + \u221a 1\u2212 \u03b1t\u03f5, with x0 \u223c p(x) being a sample from the data distribution, t \u223c U (0, T ) and \u03f5 \u223c N (0, I) is the noise. \u03bb(t) is a loss weighting term following Ho et al. (2020). Throughout this paper, the important observation is that the trained model \u03f5\u03b8 approximates the score \u2207xj log p(x) of the data (Song & Ermon, 2019). \u03f5\u03b8 : Rd+1 \u2192 Rd is a neural network with learnable parameters \u03b8. The input is d+1 variables because we concatenate the t scalar variable with the input data. 3 THE DECIDUOUS SCORE: Discovering the complete topological ordering with the distribution\u2019s Hessian (Rolland et al., 2022) is done by finding the leaf node (Equation 2), appending the leaf node xl to the ordering list \u03c0 and removing the data column corresponding to xl from X before the next iteration d\u22121 times. Rolland et al. (2022) estimate the score\u2019s Jacobian (Hessian) at each iteration. Instead, we explore an alternative approach that does not require estimation of a new score after each leaf removal. In particular, we describe how to adjust the score of a distribution after each leaf removal, terming this a \u201cdeciduous score\u201d1. We obtain an analytical expression for the deciduous score and derive a way of computing it, based on the original score before leaf removal. In this section, we only consider that p(x) follows a distribution described by an ANM, we pose no additional assumptions over the noise distribution. Definition 1. Considering a DAG G which entails a distribution p(x) = \u220fd i=1 p(xi | Pa(xi)) \u2208 Rd. Let p(x\u2212l) = p(x) p(xl|Pa(xl)) \u2208 R d\u22121 be p(x) without the random variable corresponding to the leaf node xl. The deciduous score\u2207 log p(x\u2212l) \u2208 Rd\u22121 is the score of the distribution p(x\u2212l). Lemma 2. Given a ANM which entails a distribution p(x), we can use Equation 1 to find an analytical expression for an additive residue \u2206l between the distribution\u2019s score \u2207 log p(x) and its deciduous score \u2207 log p(x\u2212l) such that \u2206l = \u2207 log p(x)\u2212\u2207 log p(x\u2212l). (4) In particular, \u2206l is a vector {\u03b4j | \u2200j \u2208 [1, . . . , d] \\l} where the residue w.r.t. a node xj can be denoted as \u03b4j = \u2207xj log p(x)\u2212\u2207xj log p(x\u2212l) = \u2212 \u2202fi \u2202xj \u2202 log p\u03f5 (xi \u2212 fi) \u2202x . (5) If xj /\u2208 Pa(xl), \u03b4j = 0. Proof. Observing Equation 1, the score \u2207xj log p(x) only depends on the following random variables (i) Pa(xj), (ii) Ch(xj), and (iii) Pa(Ch(xj)). We consider xl to be a leaf node, therefore \u2207xj log p(x) only depends on xl if xj \u2208 Pa(xl). If xj \u2208 Pa(xl), the only term depending on \u2207xj log p(x) dependent on xl is one of the terms inside the summation. 1An analogy to deciduous trees which seasonally shed leaves during autumn. However, we wish to estimate the deciduous score\u2207 log p(x\u2212l) without direct access to the function fl, to its derivative, nor to the distribution p\u03f5. Therefore, we now derive an expression for \u2206l using solely the score and the Hessian of log p(x). Theorem 1. Consider an ANM of distribution p(x) with score \u2207 log p(x) and the score\u2019s Jacobian H(log p(x)). The additive residue \u2206l necessary for computing the deciduous score (as in Proposition 2) can be estimated with \u2206l = Hl(log p(x)) \u00b7 \u2207xl log p(x) Hl,l(log p(x)) . (6) See proof in Appendix A.2. 4 CAUSAL DISCOVERY WITH DIFFUSION MODELS: DPMs approximate the score of the data distribution (Song & Ermon, 2019). In this section, we explore how to use DPMs to perform leaf discovery and compute the deciduous score, based on Theorem 1, for iteratively finding and removing leaf nodes without re-training the score. 4.1 APPROXIMATING THE SCORE\u2019S JACOBIAN VIA DIFFUSION TRAINING: The score\u2019s Jacobian can be approximated by learning the score \u03f5\u03b8 with denoising diffusion training of neural networks and back-propagating (Rumelhart et al., 1986)1 from the output to the input variables. It can be written, for an input data point x \u2208 Rd, as Hi,j log p(x) \u2248 \u2207i,j\u03f5\u03b8(x, t), (7) where \u2207i,j\u03f5\u03b8(x, t) means the ith output of \u03f5\u03b8 is backpropagated to the jth input. The diagonal of the Hessian in Equation 7 can, then, be used for finding leaf nodes as in Equation 2. In a two variable setting, it is sufficient for causal discovery to (i) train a diffusion model (Equation 3); (ii) approximate the score\u2019s Jacobian via backpropagation (Equation 7); (iii) compute variance of 1The Jacobian of a neural network can be efficiently computed with auto-differentiation libraries such as functorch (Horace He, 2021). the diagonal across all data points; (iv) identify the variable with lowest variance as effect (Equation 2). We illustrate in Appendix C the Hessian of a two variable SCM computed with a diffusion model. 4.2 TOPOLOGICAL ORDERING: When a DAG contains more than two nodes, the process of finding leaf nodes (i.e. the topological order) needs to be done iteratively. The naive (greedy) approach would be to remove the leaf node from the dataset, recompute the score, and compute the variance of the new distribution\u2019s Hessian to identify the next leaf node (Rolland et al., 2022). Since we employ diffusion models to estimate the score, this equates to re-training the model each time after a leaf is removed. We hence propose a method to compute the deciduous score \u2207 log p(x\u2212l) using Theorem 1 to remove leaves from the initial score without re-training the neural network. In particular, assuming that a leaf xl is found, the residue \u2206l can be approximated1 with \u2206l(x, t) \u2248 \u2207l\u03f5\u03b8(x, t) \u00b7 \u03f5\u03b8(x, t)l \u2207l,l\u03f5\u03b8(x, t) (8) where \u03f5\u03b8(x, t)l is output corresponding to the leaf node. Note that the term \u2207l\u03f5\u03b8(x, t) is a vector of size d and the other term is a scalar. During topological ordering, we compute \u2206\u03c0 , which is the summation of \u2206l over all leaves already discovered and appended to \u03c0. Naturally, we only compute \u2206l w.r.t. nodes xj /\u2208 \u03c0 because xj \u2208 \u03c0 have already been ordered and are not taken into account anymore. In practice, we observe that training \u03f5\u03b8 on X but using a subsample B \u2208 Rk\u00d7d of size k randomly sampled from X increases speed without compromising performance (see Section 4.3). In addition, we observe that the absolute value of the residue \u03b4l decreases if the values of xl are set to zero once the leaf node is discovered. Therefore, we apply a mask M\u03c0 \u2208 {0, 1}k\u00d7d over leaves discovered in the previous iterations and compute only the Jacobian of the outputs corresponding to x\u2212l. M\u03c0 is updated after each iteration based on the ordered nodes \u03c0. We then find a leaf node according to leaf = argmin xi\u2208x VarB [\u2207x (score(M\u03c0 \u2299B, t))] , (9) where \u03f5\u03b8 is a DPM trained with Equation 3. See Appendix E.2 for the choice of t. This topological ordering procedure is formally described in Algorithm 1, score(\u2212\u03c0) means that we only consider the outputs for nodes xj /\u2208 \u03c0 Algorithm 1: Topological Ordering with DiffAN Input: X \u2208 Rn\u00d7d, trained diffusion model \u03f5\u03b8, ordering batch size k \u03c0 = [], \u2206\u03c0 = 0k\u00d7d, M\u03c0 = 1k\u00d7d, score = \u03f5\u03b8 while \u2225\u03c0\u2225 =\u0338 d do B k\u2190X // Randomly sample a batch of k elements B \u2190 B \u25e6M\u03c0 // Mask removed leaves \u2206\u03c0 = Get\u2206\u03c0(score,B) // Sum of Equation 8 over \u03c0 score = score(\u2212\u03c0) + \u2206\u03c0 // Update score with residue leaf = GetLeaf(score,B) // Equation 9 \u03c0 = [leaf, \u03c0] // Append leaf to ordered list M:,leaf = 0 // Set discovered leaf to zero end Output: Topological order \u03c0 4.3 COMPUTATIONAL COMPLEXITY AND PRACTICAL CONSIDERATIONS: We now study the complexity of topological ordering with DiffAN w.r.t. the number of samples n and number of variables d in a dataset. In addition, we discuss what are the complexities of a greedy version as well as approximation which only utilises masking. 1The diffusion model itself is an approximation of the score, therefore its gradients are approximations of the score derivatives. Complexity on n. Our method separates learning the score \u03f5\u03b8 from computing the variance of the Hessian\u2019s diagonal across data points, in contrast to Rolland et al. (2022). We use all n samples in X for learning the score function with diffusion training (Equation 3). It does not involve expensive constrained optimisation techniques1 and we train the model for a fixed number of epochs (which is linear with n) or until reaching the early stopping criteria. We use a MLP that grows in width with d but it does not significantly affect complexity. Therefore, we consider training to be O(n). Moreover, Algorithm 1 is computed over a batch B with size k < n instead of the entire dataset X , as described in Equation 9. Note that the number of samples k in B can be arbitrarily small and constant for different datasets. In Section 5.2, we verify that the accuracy of causal discovery initially improves as k is increased but eventually tapers off. Complexity on d. Once \u03f5\u03b8 is trained, a topological ordering can be obtained by running\u2207x\u03f5\u03b8(x, t) d times. Moreover, computing the Jacobian of the score requires back-propagating the gradients d\u2212 i times, where i is the number of nodes already ordered in a given iteration. Finally, computing the deciduous score\u2019s residue (Equation 8) means computing gradient of the i nodes. Resulting in a complexity of O(d \u00b7 (d \u2212 i) \u00b7 i) with i varying from 0 to d which can be described by O(d3). The final topological ordering complexity is therefore O(n+ d3). DiffAN Greedy. A greedy version of the algorithm re-trains the \u03f5\u03b8 after each leaf removal iteration. In this case, the deciduous score is not computed, decreasing the complexity w.r.t. d but increasing w.r.t. n. DiffAN greedy has O(nd2) ordering complexity. DiffAN Masking. We verify empirically that the masking procedure described in Section 4.2 can significantly reduce the deciduous score\u2019s residue absolute value while maintaining causal discovery capabilities. In DiffAN Masking, we do not re-train the \u03f5\u03b8 nor compute the deciduous score. This ordering algorithm is an approximation but has shown to work well in practice while showing remarkable scalability. DiffAN Masking has O(n+ d2) ordering complexity. 5 EXPERIMENTS: In our experiments, we train a NN with a DPM objective to perform topological ordering and follow this with a pruning post-processing step (Bu\u0308hlmann et al., 2014). The performance is evaluated on synthetic and real data and compared to state-of-the-art causal discovery methods from observational data which are either ordering-based or gradient-based methods, NN architecture. We use a 4- layer multilayer perceptron (MLP) with LeakyReLU and layer normalisation. Metrics. We use the structural Hamming distance (SHD), Structural Intervention Distance (SID) (Peters & Bu\u0308hlmann, 2015), Order Divergence (Rolland et al., 2022) and run time in seconds. See Appendix D.3 for details of each metric. Baselines. We use CAM (Bu\u0308hlmann et al., 2014), GranDAG (Lachapelle et al., 2020) and SCORE (Rolland et al., 2022). We apply the pruning procedure of Bu\u0308hlmann et al. (2014) to all methods. See detailed results in Appendix D. 5.1 SYNTHETIC DATA: In this experiment, we consider causal relationships with fi being a function sampled from a Gaussian Process (GP) with radial basis function kernel of bandwidth one. We generate data from additive noise models which follow a Gaussian, Exponential or Laplace distributions with noise scales in the intervals {[0.4, 0.8], [0.8, 1.2], [1, 1]}, which are known to be identifiable (Peters et al., 2014). The causal graph is generated using the Erdo\u0308s-Re\u0301nyi (ER) (Erdo\u030bs et al., 1960) and Scale Free (SF) (Bolloba\u0301s et al., 2003) models. For a fixed number of nodes d, we vary the sparsity of the sampled graph by setting the average number of edges to be either d or 5d. We use the notation [d][graph type][sparsity] for indicating experiments over different synthetic datasets. We show that DiffAN performs on par with baselines while being extremely fast. We also explore the role of overfitting in Appendix E.1, the difference between DiffAN with masking only and the greedy version in Appendix E.3, how we choose t during ordering in Appendix E.2 and we give results stratified by experiment in Appendix E.4. 1Such as the Augmented Lagrangian method (Zheng et al., 2018; Lachapelle et al., 2020). 5.2 SCALING UP WITH DIFFAN MASKING: We now verify how DiffAN scales to bigger datasets, in terms of the number of samples n. Here, we use DiffAN masking because computing the residue with DiffAN would be too expensive for very big d. We evaluate only the topological ordering, ignoring the final pruning step. Therefore, the performance will be measured solely with the Order Divergence metric. We evaluate how DiffAN compares to SCORE (Rolland et al., 2022), the previous state-of-the-art, in terms of run time (in seconds) and and the performance (order divergence) over a dataset with d = 500 and different sample sizes n \u2208 102, . . . , 105. As illustrated in Figure 1 and Figure 4 (left), SCORE is faster for a small sample size but as the size of the dataset increases, DiffAN becomes the more tractable option. Ordering batch size. An important aspect of our method, discussed in Section 4.3, that allows scalability in terms of n is the separation between learning the score function \u03f5\u03b8 and computing the Hessian variance across a batch of size k, with k < n. Therefore, we show empirically, as illustrated in Figure 4 (right), that decreasing k does not strongly impact performance for datasets with d \u2208 10, 20, 50. 5.3 REAL DATA: We consider two real datasets: (i) Sachs: A protein signaling network based on expression levels of proteins and phospholipids (Sachs et al., 2005). We consider only the observational data (n = 853 samples) since our method targets discovery of causal mechanisms when only observational data is available. The ground truth causal graph given by Sachs et al. (2005) has 11 nodes and 17 edges. (ii) SynTReN: We also evaluate the models on a pseudo-real dataset sampled from SynTReN generator (Van den Bulcke et al., 2006). Results, in Table 1, show that our method is competitive against other state-of-the-art causal discovery baselines on real datasets. 6 RELATED WORKS: The observation that a causal DAG can be partially represented with a topological ordering goes back to Verma & Pearl (1990). Searching the topological ordering space instead of searching over the space of DAGs has been done with greedy Markov Chain Monte Carlo (MCMC) (Friedman & Koller, 2003), greedy hill-climbing search (Teyssier & Koller, 2005), arc search (Park & Klabjan, 2017), restricted maximum likelihood estimators (Bu\u0308hlmann et al., 2014), sparsest permutation (Raskutti & Uhler, 2018; Lam et al., 2022; Solus et al., 2021), and reinforcement learning (Wang et al., 2021). In linear additive models, Ghoshal & Honorio (2018); Chen et al. (2019) propose an approach, under some assumptions on the noise variances, to discover the causal graph by sequentially identifying leaves based on an estimation of the precision matrix. 7 CONCLUSION: We have presented the first method using diffusion models for causal discovery. Since DPMs approximate the score of the data distribution, they can be used to compute the log-likelihood\u2019s Hessian by backpropagating each element of the output with respect to each element of the input. Thus, DPMs can be used for discovering leaf nodes by identifying elements in the Hessian\u2019s diagonal with a variance of zero across data samples. Applying the leaf finding algorithm d (number of nodes) times, one can discover the topological ordering of the true causal graph. At each iteration, the number of nodes decreases. We avoid re-training the score model by adjusting the score to remove the contribution of the leaf most recently removed. Our empirical results show that neural networks in this setting can be efficiently used for topological ordering in high-dimensional graphs (up to 500 nodes) and with large datasets (up to 105 samples). Future Work. Denoising diffusion is an efficient way of learning the score of a distribution and computing its Jacobian. However, the method for computing the deciduous score developed in Section 3 can be used with other techniques where differentiating the score to obtain the Hessian matrix is a faster option than learning a new score, e.g. sliced score matching (Song et al., 2020) and approximate backpropagation (Kingma & Cun, 2010). In addition, rephrasing the causal discovery problem as topological ordering is not enough for learning the complete causal graph; spurious edges need to be pruned as a post-processing step. Therefore, future work could more comprehensively explore which feature selection methods are best suited for this task. Moreover, DPMs are generative models. Generative diffusion models in the context of causal estimation have been explored by Sanchez & Tsaftaris (2022). In this work, we have not explored the generative aspect, we have solely used DPMs as an approximation of the score. A PROOFS: We re-write Equation 1 here for improved readability: \u2207xj log p(x) = \u2202 log p\u03f5 (xj \u2212 fj) \u2202xj \u2212 \u2211 i\u2208Ch(xj) \u2202fi \u2202xj \u2202 log p\u03f5 (xi \u2212 fi) \u2202x . (10) A.1 PROOF LEMMA 1: Proof. We start by showing the \u201c\u21d0\u201d direction by deriving Equation 1 w.r.t. xj . If xj is a leaf, only the first term of the equation is present, then taking its derivative results in Hl,l(log p(x)) = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x2 \u00b7 dfl dxl = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x2 . (11) Therefore, only if j is a leaf and d log p \u03f5 dx2 = a, VarX [Hl,l(log p(x))] = 0. The remaining of the proof follows Rolland et al. (2022) (which was done for a Gaussian noise only), we prove by contradiction that\u21d2 is also true. In particular, if we consider that xj is not a leaf and \u2207xj log p(x) = c, with c being a constant, we can write \u2207xj log p(x) = cxj + g(x\u2212j). (12) Replacing Equation 12 in to Equation 1, we have cxj + g(x\u2212j) = \u2202 log p\u03f5 (xj \u2212 fj) \u2202x \u2212 \u2211 xi\u2208Ch(xj) \u2202fi \u2202xj \u2202 log p\u03f5 (xi \u2212 fi) \u2202x . (13) Let xc \u2208 Ch(xj) such that xc \u0338\u2208 Pa(Ch(xj)). xc always exist since xj is not a leaf, and it suffices to pick a child of xc appearing at last position in some topological order. If we isolate the terms depending on xc on the RHS of Equation 13, we have cxj + \u2202 log p\u03f5 (xj \u2212 fj) \u2202x \u2212 \u2211 xi\u2208Ch(xj),xi \u0338=xc \u2202fi \u2202xj \u2202 log p\u03f5 (xi \u2212 fi) \u2202x = \u2202fc \u2202xj \u2202 log p\u03f5 (xc \u2212 fc) \u2202x \u2212 g(x\u2212j). (14) Deriving both sides w.r.t. xc, since the LHS of Equation 14 does not depend on xc, we can write \u2202 \u2202xc ( \u2202fc \u2202xj \u2202 log p\u03f5 (xc \u2212 fc) \u2202x \u2212 g(x\u2212j) ) = 0 \u21d2 \u2202fc \u2202xj : a \u2202 log p\u03f5 (xc \u2212 fc) \u2202x2 = \u2202g(x\u2212j) \u2202xc (15) Since g does not depend on xj , \u2202fc\u2202xj does not depend on xj neither, implying that fc is linear in xj , contradicting the non-linearity assumption. A.2 PROOF THEOREM 1: Proof. Using Equation 1, we will derive expressions for each of the elements in Equation 6 and show that it is equivalent to \u03b4l in Equation 5. First, note that the score of a leaf node xl can be denoted as: \u2207xl log p(x) = \u2202 log p\u03f5 (xj \u2212 fj) \u2202xj = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x \u00b7 d (xl \u2212 fl) dxl = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x (16) Second, replacing Equation 16 into each element of Hl(log p(x)) \u2208 Rd, we can write Hl,j(log p(x)) = \u2202 \u2202xj [\u2207xl log p(x)] = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x2 \u00b7 d (xl \u2212 fl) dxj = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x2 \u00b7 dfl dxj . (17) If j = l in Equation 17, we have Hl,l(log p(x)) = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x2 \u00b7 dfl dxl = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x2 . (18) Finally, replacing Equations 16, 17 and 11 into the Equation 5 for a single node xj , if j \u0338= l, we have \u03b4j = Hl,j(log p(x)) \u00b7 \u2207xl log p(x) Hl,l(log p(x)) = \u2202 log p\u03f5(xl\u2212fl) \u2202x2 \u00b7 dfl dxj \u00b7 \u2202 log p \u03f5(xl\u2212fl) \u2202x \u2202 log p\u03f5(xl\u2212fl) \u2202x2 = dfl dxj \u00b7 \u2202 log p \u03f5 (xl \u2212 fl) \u2202x . (19) The last line in Equation 19 is the same as in Equation 5 from Lemma 2, proving that \u03b4j can be written using the first and second order derivative of the log-likelihood. B SCORE OF NONLINEAR ANM WITH GAUSSIAN NOISE: A SCM entails a distribution p(x) = d\u220f i=1 p(xi | Pa(xi)), (20) over the variables x (Peters et al., 2017) By assuming that the noise variables \u03f5i \u223c N (0, \u03c32i ) and inserting the ANM function, Equation 20 can be written as log p(x) = \u22121 2 d\u2211 i=1 ( xi \u2212 fi(Pa(xi)) \u03c3i )2 \u2212 1 2 d\u2211 i=1 log(2\u03c0\u03c32i ). (21) The score of p(x) can hence be written as \u2207xj log p(x) = \u2212 xj \u2212 fj(Pa(xj)) \u03c32j + \u2211 i\u2208children(j) \u2202fi \u2202xj (Pa(xi)) xi \u2212 fi(Pa(xi)) \u03c32i . (22) C VISUALISATION OF THE SCORE\u2019S JACOBIAN FOR TWO VARIABLES Considering a two variables problem where the causal mechanisms are B = f\u03c9(A)+\u03f5B and A = \u03f5A with \u03f5A, \u03f5B \u223c N (0, 1) and f\u03c9 being a two-layer MLP with randomly initialised weights. Note that the variance of \u2202 2 log p(A,B) \u2202B2 , while not 0 as predicted by Equation 2, is smaller than \u22022 log p(A,B) \u2202A2 allowing discovery of the true causal direction. D EXPERIMENTS DETAILS:  D.1 HYPERPARAMETERS OF DPM TRAINING: We now describe the hyperparameters for the diffusion training. We use number of time steps T = 100, \u03b2t is a linearly scheduled between \u03b2min = 0.0001 and \u03b2max = 0.02. The model is trained Layer Hyperparameters: according to Equation 3 which follows Ho et al. (2020). During sampling, t is sampled from a Uniform distribution. D.2 NEURAL ARCHITECTURE: The neural network follows a simple MLP with 5 Linear layers, LeakyReLU activation function, Layer Normalization and Dropout in the first layer. The full architecture is detailed in Table 2. D.3 METRICS: For each method, we compute the SHD. Structural Hamming distance between the output and the true causal graph, which counts the number of missing, falsely detected, or reversed edges. SID. Structural Intervention Distance is based on a graphical criterion only and quantifies the closeness between two DAGs in terms of their corresponding causal inference statements(Peters & Bu\u0308hlmann, 2015). Order Divergence. Rolland et al. (2022) propose this quantity for measuring how well the topological order is estimated. For an ordering \u03c0, and a target adjacency matrix A, we define the topological order divergence Dtop(\u03c0,A) as Dtop(\u03c0,A) = d\u2211 i=1 \u2211 j:\u03c0i>\u03c0j Aij . (23) If \u03c0 is a correct topological order for A, then Dtop(\u03c0,A) = 0. Otherwise, Dtop(\u03c0,A) counts the number of edges that cannot be recovered due to the choice of topological order. Therefore, it provides a lower bound on the SHD of the final algorithm (irrespective of the pruning method). E OTHER RESULTS:  E.1 OVERFITTING: The data used for topological ordering (inference) is a subset of the training data. Therefore, it is not obvious if overfitting would be an issue with our algorithm. Therefore, we run an experiment where we fix the number of epochs to 2000 considered high for a set of runs and use early stopping for another set in order to verify if overfitting is an issue. On average across all 20 nodes datasets, the early stopping strategy output an ordering diverge of 9.5 whilst overfitting is at 11.1 showing that the method does not benefit from overfitting. E.2 OPTIMAL t FOR SCORE ESTIMATION As noted by Vincent (2011), the best approximation of the score by a learned denoising function is when the training signal-to-noise (SNR) ratio is low. In diffusion model training, t = 0 corresponds to the coefficient with lowest SNR. However, we found empirically that the best score estimate varies somehow randomly across different values of t. Therefore, we run the the leaf finding function (Equation 9) N times for values of t evenly spaced in the [0, T ] interval and choose the best leaf based on majority vote. We show in Figure 6 that majority voting is a better approach than choosing a constant value for t. E.3 ABLATIONS OF DIFFAN MASKING AND GREEDY: We now verify how DiffAN masking and DiffAN greedy compare against the original version detailed in the main text which computes the deciduous score. Here, we use the same datasets decribed in Section 5.1 which comprises 4 (20ER1, 20ER5, 20SF1, 20SF5) synthetic dataset types with 27 variations over seeds, noise type and noise scale. We observe, in Figure 7, that the greedy version performs the best but it is the slowest, as seen in Figure 8. DiffAN masking E.4 DETAILED RESULTS: We present the numerical results for the violinplots in Section 5.1 in Tables 3 and 4. The results are presented in meanstd with statistics acquired over experiments with 3 seeds.", "agent_review": [{"role": "user", "content": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Diffusion Models for Causal Discovery via Topological Ordering\n\nNone: 1 INTRODUCTION Understanding the causal structure of a problem is important for areas such as economics, biology (Sachs et al., 2005) and healthcare (Sanchez et al., 2022), especially when reasoning about the effect of interventions. When interventional data from randomised trials are not available, causal discovery methods (Glymour et al., 2019) may be employed to discover the causal structure of a problem solely from observational data. Causal structure is typically modelled as a directed acyclic graph (DAG) G in which each node is associated with a random variable and each edge represents a causal mechanism i.e. how one variable influences another. However, learning such a model from data is NP-hard (Chickering, 1996). Traditional methods search the DAG space by testing for conditional independence between variables (Spirtes et al., 1993) or by optimising some goodness of fit measure (Chickering, 2002). Unfortunately, solv- ing the search problem with a greedy combinatorial optimisation method can be expensive and does not scale to high-dimensional problems. In line with previous work (Teyssier & Koller, 2005; Park & Klabjan, 2017; Bu\u0308hlmann et al., 2014; Solus et al., 2021; Wang et al., 2021; Rolland et al., 2022), we can speed up the combinatorial search problem over the space of DAGs by rephrasing it as a topological ordering task, ordering from leaf nodes to root nodes. The search space over DAGs with d nodes and (d2 \u2212 d)/2 possible edges is much larger than the space of permutations over d variables. Once a topological ordering of the nodes is found, the potential causal relations between later (cause) and earlier (effect) nodes can be pruned with a feature selection algorithm (e.g. Bu\u0308hlmann et al. (2014)) to yield a graph which is naturally directed and acyclic without further optimisation. Recently, Rolland et al. (2022) proposed the SCORE algorithm which uses the Hessian of the data log-likelihood, \u22072x log p(x) (i.e. the Jacobian of the score 1) for topological ordering. By verifying which elements of \u22072x log p(x)\u2019s diagonal are constant across all data points, leaf nodes can be iteratively identified and removed. The procedure from Rolland et al. (2022) scales linearly with the number of variables d but with polynomial complexity with the number of samples n i.e. O(dn3), due to the Stein gradient estimator (Li & Turner, 2018), which hinders its use for high-dimensional causal discovery. Here, we use neural networks (NN) trained with diffusion probabilistic modeling (DPM) for learning the score function \u2207x log p(x), inspired by Rolland et al. (2022). We propose a method to infer an additive residue which allows computing the score at each leaf-removal step without re-training the NN. In addition, the NN is trained over the entire dataset (n samples) but only a subsampled batch is used for finding leaf nodes.Thus, once the score model is learned, we can use it to order the graph with constant complexity on n, enabling causal discovery in high-dimensional settings. Interestingly, our algorithm does not require architectural constraints on the neural network, as in previous causal discovery methods based on neural networks (Lachapelle et al., 2020; Zheng et al., 2020; Yu et al., 2019; Ng et al., 2022). Our training procedure does not learn the causal mechanism directly, but the score of the data distribution. Contributions. In summary, we propose DiffAN, an identifiable algorithm leveraging a diffusion probabilistic model for topological ordering that enables causal discovery assuming an additive noise model: (i) To the best of our knowledge, we present the first causal discovery algorithm based on denoising diffusion training. The score estimated with the diffusion model is used to find leaf nodes iteratively; (ii) We prove (Section 2.2) that ANMs are identifiable from the variance of the log-likelihood\u2019s Hessian for a more general family of noise distribution, extending identifiability statements from Rolland et al. (2022); (iii) We show in Section 3 that when a leaf node is removed, the score of the new distribution can be estimated from the original score (before leaf removal) and its Jacobian. It allow us to propose an efficient algorithm for topological ordering which does not depend on re-training the score after each leaf removal iteration; (iv) We estimate the second-order derivatives (score\u2019s Jacobian or Hessian) of a data distribution using neural networks with diffusion training via backpropagation; (v) We show empirically that our method performs on a par with state-of-the-art causal discovery methods in small graph datasets but scales better to bigger datasets with more nodes and samples. 2 PRELIMINARIES:  2.1 PROBLEM DEFINITION: We consider the problem of discovering the causal structure between d variables, given a probability distribution p(x) from which a d-dimensional random vector x = (x1, . . . , xd) can be sampled. We assume that the true causal structure is described by a DAG G containing d nodes. Each node represents a random variable xi and edges represent the presence of causal relations between them. In other words, we can say that G defines a structural causal model (SCM) consisting of a collection of assignments xi := fi(Pa(xi), \u03f5i), where Pa(xi) are the parents of xi in G, and \u03f5i is a noise term independent of xi, also called exogenous noise. \u03f5i are i.i.d. from a smooth distribution p\u03f5. The SCM entails a unique distribution p(x) = \u220fd i=1 p(xi | Pa(xi)) over the variables x (Peters et al., 2017). The observational input data are X \u2208 Rn\u00d7d, where n is number of samples. The target output is an adjacency matrix A \u2208 Rd\u00d7d. 1Note that, throughout the paper, we use the term \u201cscore\u201d to denote the gradient of the data log-likelihood \u2207x log p(x); this is different to causal literature which uses \u201cscore\u201d to refer to the optimisation function in combinatorial search algorithms. The topological ordering (also called causal ordering or causal list) of a DAG G is defined as a non-unique permutation \u03c0 of d nodes such that a given node always appears first in the list than its descendants. Formally, \u03c0i < \u03c0j \u21d0\u21d2 j \u2208 DeG(xi) where DeG(xi) are the descendants of the ith node in G (Appendix B in Peters et al. (2017)). 2.2 NONLINEAR ADDITIVE NOISE MODELS: Learning a unique A from X with observational data requires additional assumptions. A common class of methods called additive noise models (ANM) (Shimizu et al., 2006; Hoyer et al., 2008; Peters et al., 2014; Bu\u0308hlmann et al., 2014) explores asymmetries in the data by imposing functional assumptions on the data generation process. In most cases, they assume that assignments take the form xi := fi(Pa(xi)) + \u03f5i with \u03f5i \u223c p\u03f5. Here we focus on the case described by Peters et al. (2014) where fi is nonlinear. We use the notation fi for fi(Pa(xi)) because the arguments of fi will always be Pa(xi) throughout this paper. We highlight that fi does not depend on i. Identifiability. We assume that the SCM follows an additive noise model (ANM) which is known to be identifiable from observational data (Hoyer et al., 2008; Peters et al., 2014). We also assume causal sufficiency, i.e. there are no hidden variables that are a common cause of at least two observed variables. In addition, corollary 33 from Peters et al. (2014) states that the true topological ordering of the DAG, as in our setting, is identifiable from a p(x) generated by an ANM without requiring causal minimality assumptions. Finding Leaves with the Score. Rolland et al. (2022) propose that the score of an ANM with distribution p(x) can be used to find leaves1. Before presenting how to find the leaves, we derive, following Lemma 2 in Rolland et al. (2022), an analytical expression for the score which can be written as \u2207xj log p(x) = \u2207xj log d\u220f i=1 p(xi | Pa(xi)) = \u2207xj d\u2211 i=1 log p(xi | Pa(xi)) = \u2207xj d\u2211 i=1 log p\u03f5 (xi \u2212 fi) \u25b7 Using \u03f5i = xi \u2212 fi = \u2202 log p\u03f5 (xj \u2212 fj) \u2202xj \u2212 \u2211 i\u2208Ch(xj) \u2202fi \u2202xj \u2202 log p\u03f5 (xi \u2212 fi) \u2202x . (1) Where Ch(xj) denotes the children of xj . Note that if xj does not have a child node, the second term on the last row disappears. In this case, the Jacobian of the score \u22072xj log p(x) (also denoted as the Hessian Hj,j(log p(x))2) becomes a constant. This observation provides a mechanism to identify leaf nodes in a graph because if the second-order derivative is constant, the variance across all data points should be zero. Lemma 1. Given a nonlinear ANM with a noise distribution p\u03f5, VarX [Hj,j(log p(x))] = 0 \u21d0\u21d2 j is a leaf & d log p\u03f5 dx2 = a, (2) where a is a constant and H(log p(x)) \u2208 Rd,d is the Hessian matrix of the log-density log p(x). Therefore, the leaves are only identifiable using the variance of the log-likelihood\u2019s Hessian for ANMs with noise distribution that have constant second-order derivatives. See proof in Appendix A.1. Note that Lemma 1 is a generalisation of Rolland et al. (2022) which only prove that Equation 2 can identify leaves in nonlinear ANMs with Gaussian noise. We derive a more general formulation which additionally requires the second-order derivative of the noise distribution to be constant. Indeed, the condition d log p \u03f5 dx2 = a is true for p \u03f5 following a Gaussian distribution which is consistent with Rolland et al. (2022). 1We refer to nodes without children in a DAG G as leaves. 2The subscripts i, j in Hi,j denote an element at the ith row and jth column. 2.3 DIFFUSION MODELS APPROXIMATE THE SCORE: The process of learning to denoise (Vincent, 2011) can approximate that of matching the score (Hyva\u0308rinen, 2005). A diffusion process gradually adds noise to a data distribution over time. Diffusion probabilistic models (DPMs) Sohl-Dickstein et al. (2015); Ho et al. (2020); Song et al. (2021) learn to reverse the diffusion process, starting with noise and recovering the data distribution. The diffusion process gradually adds Gaussian noise, with a time-dependent variance \u03b1t, to a sample x0 \u223c pdata(x) from the data distribution. Thus, the noisy variable xt, with t \u2208 [0, T ], is learned to correspond to versions of x0 perturbed by Gaussian noise following p (xt | x0) = N ( xt; \u221a \u03b1tx0, (1\u2212 \u03b1t) I ) , where \u03b1t := \u220ft j=0 (1\u2212 \u03b2j), \u03b2j is the variance scheduled between [\u03b2min, \u03b2max] and I is the identity matrix. DPMs (Ho et al., 2020) are learned with a weighted sum of denoising score matching objectives at different perturbation scales with \u03b8\u2217 = argmin \u03b8 Ex0,t,\u03f5 [ \u03bb(t) \u2225\u03f5\u03b8(xt, t)\u2212 \u03f5\u222522 ] , (3) where xt = \u221a \u03b1tx0 + \u221a 1\u2212 \u03b1t\u03f5, with x0 \u223c p(x) being a sample from the data distribution, t \u223c U (0, T ) and \u03f5 \u223c N (0, I) is the noise. \u03bb(t) is a loss weighting term following Ho et al. (2020). Throughout this paper, the important observation is that the trained model \u03f5\u03b8 approximates the score \u2207xj log p(x) of the data (Song & Ermon, 2019). \u03f5\u03b8 : Rd+1 \u2192 Rd is a neural network with learnable parameters \u03b8. The input is d+1 variables because we concatenate the t scalar variable with the input data. 3 THE DECIDUOUS SCORE: Discovering the complete topological ordering with the distribution\u2019s Hessian (Rolland et al., 2022) is done by finding the leaf node (Equation 2), appending the leaf node xl to the ordering list \u03c0 and removing the data column corresponding to xl from X before the next iteration d\u22121 times. Rolland et al. (2022) estimate the score\u2019s Jacobian (Hessian) at each iteration. Instead, we explore an alternative approach that does not require estimation of a new score after each leaf removal. In particular, we describe how to adjust the score of a distribution after each leaf removal, terming this a \u201cdeciduous score\u201d1. We obtain an analytical expression for the deciduous score and derive a way of computing it, based on the original score before leaf removal. In this section, we only consider that p(x) follows a distribution described by an ANM, we pose no additional assumptions over the noise distribution. Definition 1. Considering a DAG G which entails a distribution p(x) = \u220fd i=1 p(xi | Pa(xi)) \u2208 Rd. Let p(x\u2212l) = p(x) p(xl|Pa(xl)) \u2208 R d\u22121 be p(x) without the random variable corresponding to the leaf node xl. The deciduous score\u2207 log p(x\u2212l) \u2208 Rd\u22121 is the score of the distribution p(x\u2212l). Lemma 2. Given a ANM which entails a distribution p(x), we can use Equation 1 to find an analytical expression for an additive residue \u2206l between the distribution\u2019s score \u2207 log p(x) and its deciduous score \u2207 log p(x\u2212l) such that \u2206l = \u2207 log p(x)\u2212\u2207 log p(x\u2212l). (4) In particular, \u2206l is a vector {\u03b4j | \u2200j \u2208 [1, . . . , d] \\l} where the residue w.r.t. a node xj can be denoted as \u03b4j = \u2207xj log p(x)\u2212\u2207xj log p(x\u2212l) = \u2212 \u2202fi \u2202xj \u2202 log p\u03f5 (xi \u2212 fi) \u2202x . (5) If xj /\u2208 Pa(xl), \u03b4j = 0. Proof. Observing Equation 1, the score \u2207xj log p(x) only depends on the following random variables (i) Pa(xj), (ii) Ch(xj), and (iii) Pa(Ch(xj)). We consider xl to be a leaf node, therefore \u2207xj log p(x) only depends on xl if xj \u2208 Pa(xl). If xj \u2208 Pa(xl), the only term depending on \u2207xj log p(x) dependent on xl is one of the terms inside the summation. 1An analogy to deciduous trees which seasonally shed leaves during autumn. However, we wish to estimate the deciduous score\u2207 log p(x\u2212l) without direct access to the function fl, to its derivative, nor to the distribution p\u03f5. Therefore, we now derive an expression for \u2206l using solely the score and the Hessian of log p(x). Theorem 1. Consider an ANM of distribution p(x) with score \u2207 log p(x) and the score\u2019s Jacobian H(log p(x)). The additive residue \u2206l necessary for computing the deciduous score (as in Proposition 2) can be estimated with \u2206l = Hl(log p(x)) \u00b7 \u2207xl log p(x) Hl,l(log p(x)) . (6) See proof in Appendix A.2. 4 CAUSAL DISCOVERY WITH DIFFUSION MODELS: DPMs approximate the score of the data distribution (Song & Ermon, 2019). In this section, we explore how to use DPMs to perform leaf discovery and compute the deciduous score, based on Theorem 1, for iteratively finding and removing leaf nodes without re-training the score. 4.1 APPROXIMATING THE SCORE\u2019S JACOBIAN VIA DIFFUSION TRAINING: The score\u2019s Jacobian can be approximated by learning the score \u03f5\u03b8 with denoising diffusion training of neural networks and back-propagating (Rumelhart et al., 1986)1 from the output to the input variables. It can be written, for an input data point x \u2208 Rd, as Hi,j log p(x) \u2248 \u2207i,j\u03f5\u03b8(x, t), (7) where \u2207i,j\u03f5\u03b8(x, t) means the ith output of \u03f5\u03b8 is backpropagated to the jth input. The diagonal of the Hessian in Equation 7 can, then, be used for finding leaf nodes as in Equation 2. In a two variable setting, it is sufficient for causal discovery to (i) train a diffusion model (Equation 3); (ii) approximate the score\u2019s Jacobian via backpropagation (Equation 7); (iii) compute variance of 1The Jacobian of a neural network can be efficiently computed with auto-differentiation libraries such as functorch (Horace He, 2021). the diagonal across all data points; (iv) identify the variable with lowest variance as effect (Equation 2). We illustrate in Appendix C the Hessian of a two variable SCM computed with a diffusion model. 4.2 TOPOLOGICAL ORDERING: When a DAG contains more than two nodes, the process of finding leaf nodes (i.e. the topological order) needs to be done iteratively. The naive (greedy) approach would be to remove the leaf node from the dataset, recompute the score, and compute the variance of the new distribution\u2019s Hessian to identify the next leaf node (Rolland et al., 2022). Since we employ diffusion models to estimate the score, this equates to re-training the model each time after a leaf is removed. We hence propose a method to compute the deciduous score \u2207 log p(x\u2212l) using Theorem 1 to remove leaves from the initial score without re-training the neural network. In particular, assuming that a leaf xl is found, the residue \u2206l can be approximated1 with \u2206l(x, t) \u2248 \u2207l\u03f5\u03b8(x, t) \u00b7 \u03f5\u03b8(x, t)l \u2207l,l\u03f5\u03b8(x, t) (8) where \u03f5\u03b8(x, t)l is output corresponding to the leaf node. Note that the term \u2207l\u03f5\u03b8(x, t) is a vector of size d and the other term is a scalar. During topological ordering, we compute \u2206\u03c0 , which is the summation of \u2206l over all leaves already discovered and appended to \u03c0. Naturally, we only compute \u2206l w.r.t. nodes xj /\u2208 \u03c0 because xj \u2208 \u03c0 have already been ordered and are not taken into account anymore. In practice, we observe that training \u03f5\u03b8 on X but using a subsample B \u2208 Rk\u00d7d of size k randomly sampled from X increases speed without compromising performance (see Section 4.3). In addition, we observe that the absolute value of the residue \u03b4l decreases if the values of xl are set to zero once the leaf node is discovered. Therefore, we apply a mask M\u03c0 \u2208 {0, 1}k\u00d7d over leaves discovered in the previous iterations and compute only the Jacobian of the outputs corresponding to x\u2212l. M\u03c0 is updated after each iteration based on the ordered nodes \u03c0. We then find a leaf node according to leaf = argmin xi\u2208x VarB [\u2207x (score(M\u03c0 \u2299B, t))] , (9) where \u03f5\u03b8 is a DPM trained with Equation 3. See Appendix E.2 for the choice of t. This topological ordering procedure is formally described in Algorithm 1, score(\u2212\u03c0) means that we only consider the outputs for nodes xj /\u2208 \u03c0 Algorithm 1: Topological Ordering with DiffAN Input: X \u2208 Rn\u00d7d, trained diffusion model \u03f5\u03b8, ordering batch size k \u03c0 = [], \u2206\u03c0 = 0k\u00d7d, M\u03c0 = 1k\u00d7d, score = \u03f5\u03b8 while \u2225\u03c0\u2225 =\u0338 d do B k\u2190X // Randomly sample a batch of k elements B \u2190 B \u25e6M\u03c0 // Mask removed leaves \u2206\u03c0 = Get\u2206\u03c0(score,B) // Sum of Equation 8 over \u03c0 score = score(\u2212\u03c0) + \u2206\u03c0 // Update score with residue leaf = GetLeaf(score,B) // Equation 9 \u03c0 = [leaf, \u03c0] // Append leaf to ordered list M:,leaf = 0 // Set discovered leaf to zero end Output: Topological order \u03c0 4.3 COMPUTATIONAL COMPLEXITY AND PRACTICAL CONSIDERATIONS: We now study the complexity of topological ordering with DiffAN w.r.t. the number of samples n and number of variables d in a dataset. In addition, we discuss what are the complexities of a greedy version as well as approximation which only utilises masking. 1The diffusion model itself is an approximation of the score, therefore its gradients are approximations of the score derivatives. Complexity on n. Our method separates learning the score \u03f5\u03b8 from computing the variance of the Hessian\u2019s diagonal across data points, in contrast to Rolland et al. (2022). We use all n samples in X for learning the score function with diffusion training (Equation 3). It does not involve expensive constrained optimisation techniques1 and we train the model for a fixed number of epochs (which is linear with n) or until reaching the early stopping criteria. We use a MLP that grows in width with d but it does not significantly affect complexity. Therefore, we consider training to be O(n). Moreover, Algorithm 1 is computed over a batch B with size k < n instead of the entire dataset X , as described in Equation 9. Note that the number of samples k in B can be arbitrarily small and constant for different datasets. In Section 5.2, we verify that the accuracy of causal discovery initially improves as k is increased but eventually tapers off. Complexity on d. Once \u03f5\u03b8 is trained, a topological ordering can be obtained by running\u2207x\u03f5\u03b8(x, t) d times. Moreover, computing the Jacobian of the score requires back-propagating the gradients d\u2212 i times, where i is the number of nodes already ordered in a given iteration. Finally, computing the deciduous score\u2019s residue (Equation 8) means computing gradient of the i nodes. Resulting in a complexity of O(d \u00b7 (d \u2212 i) \u00b7 i) with i varying from 0 to d which can be described by O(d3). The final topological ordering complexity is therefore O(n+ d3). DiffAN Greedy. A greedy version of the algorithm re-trains the \u03f5\u03b8 after each leaf removal iteration. In this case, the deciduous score is not computed, decreasing the complexity w.r.t. d but increasing w.r.t. n. DiffAN greedy has O(nd2) ordering complexity. DiffAN Masking. We verify empirically that the masking procedure described in Section 4.2 can significantly reduce the deciduous score\u2019s residue absolute value while maintaining causal discovery capabilities. In DiffAN Masking, we do not re-train the \u03f5\u03b8 nor compute the deciduous score. This ordering algorithm is an approximation but has shown to work well in practice while showing remarkable scalability. DiffAN Masking has O(n+ d2) ordering complexity. 5 EXPERIMENTS: In our experiments, we train a NN with a DPM objective to perform topological ordering and follow this with a pruning post-processing step (Bu\u0308hlmann et al., 2014). The performance is evaluated on synthetic and real data and compared to state-of-the-art causal discovery methods from observational data which are either ordering-based or gradient-based methods, NN architecture. We use a 4- layer multilayer perceptron (MLP) with LeakyReLU and layer normalisation. Metrics. We use the structural Hamming distance (SHD), Structural Intervention Distance (SID) (Peters & Bu\u0308hlmann, 2015), Order Divergence (Rolland et al., 2022) and run time in seconds. See Appendix D.3 for details of each metric. Baselines. We use CAM (Bu\u0308hlmann et al., 2014), GranDAG (Lachapelle et al., 2020) and SCORE (Rolland et al., 2022). We apply the pruning procedure of Bu\u0308hlmann et al. (2014) to all methods. See detailed results in Appendix D. 5.1 SYNTHETIC DATA: In this experiment, we consider causal relationships with fi being a function sampled from a Gaussian Process (GP) with radial basis function kernel of bandwidth one. We generate data from additive noise models which follow a Gaussian, Exponential or Laplace distributions with noise scales in the intervals {[0.4, 0.8], [0.8, 1.2], [1, 1]}, which are known to be identifiable (Peters et al., 2014). The causal graph is generated using the Erdo\u0308s-Re\u0301nyi (ER) (Erdo\u030bs et al., 1960) and Scale Free (SF) (Bolloba\u0301s et al., 2003) models. For a fixed number of nodes d, we vary the sparsity of the sampled graph by setting the average number of edges to be either d or 5d. We use the notation [d][graph type][sparsity] for indicating experiments over different synthetic datasets. We show that DiffAN performs on par with baselines while being extremely fast. We also explore the role of overfitting in Appendix E.1, the difference between DiffAN with masking only and the greedy version in Appendix E.3, how we choose t during ordering in Appendix E.2 and we give results stratified by experiment in Appendix E.4. 1Such as the Augmented Lagrangian method (Zheng et al., 2018; Lachapelle et al., 2020). 5.2 SCALING UP WITH DIFFAN MASKING: We now verify how DiffAN scales to bigger datasets, in terms of the number of samples n. Here, we use DiffAN masking because computing the residue with DiffAN would be too expensive for very big d. We evaluate only the topological ordering, ignoring the final pruning step. Therefore, the performance will be measured solely with the Order Divergence metric. We evaluate how DiffAN compares to SCORE (Rolland et al., 2022), the previous state-of-the-art, in terms of run time (in seconds) and and the performance (order divergence) over a dataset with d = 500 and different sample sizes n \u2208 102, . . . , 105. As illustrated in Figure 1 and Figure 4 (left), SCORE is faster for a small sample size but as the size of the dataset increases, DiffAN becomes the more tractable option. Ordering batch size. An important aspect of our method, discussed in Section 4.3, that allows scalability in terms of n is the separation between learning the score function \u03f5\u03b8 and computing the Hessian variance across a batch of size k, with k < n. Therefore, we show empirically, as illustrated in Figure 4 (right), that decreasing k does not strongly impact performance for datasets with d \u2208 10, 20, 50. 5.3 REAL DATA: We consider two real datasets: (i) Sachs: A protein signaling network based on expression levels of proteins and phospholipids (Sachs et al., 2005). We consider only the observational data (n = 853 samples) since our method targets discovery of causal mechanisms when only observational data is available. The ground truth causal graph given by Sachs et al. (2005) has 11 nodes and 17 edges. (ii) SynTReN: We also evaluate the models on a pseudo-real dataset sampled from SynTReN generator (Van den Bulcke et al., 2006). Results, in Table 1, show that our method is competitive against other state-of-the-art causal discovery baselines on real datasets. 6 RELATED WORKS: The observation that a causal DAG can be partially represented with a topological ordering goes back to Verma & Pearl (1990). Searching the topological ordering space instead of searching over the space of DAGs has been done with greedy Markov Chain Monte Carlo (MCMC) (Friedman & Koller, 2003), greedy hill-climbing search (Teyssier & Koller, 2005), arc search (Park & Klabjan, 2017), restricted maximum likelihood estimators (Bu\u0308hlmann et al., 2014), sparsest permutation (Raskutti & Uhler, 2018; Lam et al., 2022; Solus et al., 2021), and reinforcement learning (Wang et al., 2021). In linear additive models, Ghoshal & Honorio (2018); Chen et al. (2019) propose an approach, under some assumptions on the noise variances, to discover the causal graph by sequentially identifying leaves based on an estimation of the precision matrix. 7 CONCLUSION: We have presented the first method using diffusion models for causal discovery. Since DPMs approximate the score of the data distribution, they can be used to compute the log-likelihood\u2019s Hessian by backpropagating each element of the output with respect to each element of the input. Thus, DPMs can be used for discovering leaf nodes by identifying elements in the Hessian\u2019s diagonal with a variance of zero across data samples. Applying the leaf finding algorithm d (number of nodes) times, one can discover the topological ordering of the true causal graph. At each iteration, the number of nodes decreases. We avoid re-training the score model by adjusting the score to remove the contribution of the leaf most recently removed. Our empirical results show that neural networks in this setting can be efficiently used for topological ordering in high-dimensional graphs (up to 500 nodes) and with large datasets (up to 105 samples). Future Work. Denoising diffusion is an efficient way of learning the score of a distribution and computing its Jacobian. However, the method for computing the deciduous score developed in Section 3 can be used with other techniques where differentiating the score to obtain the Hessian matrix is a faster option than learning a new score, e.g. sliced score matching (Song et al., 2020) and approximate backpropagation (Kingma & Cun, 2010). In addition, rephrasing the causal discovery problem as topological ordering is not enough for learning the complete causal graph; spurious edges need to be pruned as a post-processing step. Therefore, future work could more comprehensively explore which feature selection methods are best suited for this task. Moreover, DPMs are generative models. Generative diffusion models in the context of causal estimation have been explored by Sanchez & Tsaftaris (2022). In this work, we have not explored the generative aspect, we have solely used DPMs as an approximation of the score. A PROOFS: We re-write Equation 1 here for improved readability: \u2207xj log p(x) = \u2202 log p\u03f5 (xj \u2212 fj) \u2202xj \u2212 \u2211 i\u2208Ch(xj) \u2202fi \u2202xj \u2202 log p\u03f5 (xi \u2212 fi) \u2202x . (10) A.1 PROOF LEMMA 1: Proof. We start by showing the \u201c\u21d0\u201d direction by deriving Equation 1 w.r.t. xj . If xj is a leaf, only the first term of the equation is present, then taking its derivative results in Hl,l(log p(x)) = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x2 \u00b7 dfl dxl = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x2 . (11) Therefore, only if j is a leaf and d log p \u03f5 dx2 = a, VarX [Hl,l(log p(x))] = 0. The remaining of the proof follows Rolland et al. (2022) (which was done for a Gaussian noise only), we prove by contradiction that\u21d2 is also true. In particular, if we consider that xj is not a leaf and \u2207xj log p(x) = c, with c being a constant, we can write \u2207xj log p(x) = cxj + g(x\u2212j). (12) Replacing Equation 12 in to Equation 1, we have cxj + g(x\u2212j) = \u2202 log p\u03f5 (xj \u2212 fj) \u2202x \u2212 \u2211 xi\u2208Ch(xj) \u2202fi \u2202xj \u2202 log p\u03f5 (xi \u2212 fi) \u2202x . (13) Let xc \u2208 Ch(xj) such that xc \u0338\u2208 Pa(Ch(xj)). xc always exist since xj is not a leaf, and it suffices to pick a child of xc appearing at last position in some topological order. If we isolate the terms depending on xc on the RHS of Equation 13, we have cxj + \u2202 log p\u03f5 (xj \u2212 fj) \u2202x \u2212 \u2211 xi\u2208Ch(xj),xi \u0338=xc \u2202fi \u2202xj \u2202 log p\u03f5 (xi \u2212 fi) \u2202x = \u2202fc \u2202xj \u2202 log p\u03f5 (xc \u2212 fc) \u2202x \u2212 g(x\u2212j). (14) Deriving both sides w.r.t. xc, since the LHS of Equation 14 does not depend on xc, we can write \u2202 \u2202xc ( \u2202fc \u2202xj \u2202 log p\u03f5 (xc \u2212 fc) \u2202x \u2212 g(x\u2212j) ) = 0 \u21d2 \u2202fc \u2202xj : a \u2202 log p\u03f5 (xc \u2212 fc) \u2202x2 = \u2202g(x\u2212j) \u2202xc (15) Since g does not depend on xj , \u2202fc\u2202xj does not depend on xj neither, implying that fc is linear in xj , contradicting the non-linearity assumption. A.2 PROOF THEOREM 1: Proof. Using Equation 1, we will derive expressions for each of the elements in Equation 6 and show that it is equivalent to \u03b4l in Equation 5. First, note that the score of a leaf node xl can be denoted as: \u2207xl log p(x) = \u2202 log p\u03f5 (xj \u2212 fj) \u2202xj = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x \u00b7 d (xl \u2212 fl) dxl = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x (16) Second, replacing Equation 16 into each element of Hl(log p(x)) \u2208 Rd, we can write Hl,j(log p(x)) = \u2202 \u2202xj [\u2207xl log p(x)] = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x2 \u00b7 d (xl \u2212 fl) dxj = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x2 \u00b7 dfl dxj . (17) If j = l in Equation 17, we have Hl,l(log p(x)) = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x2 \u00b7 dfl dxl = \u2202 log p\u03f5 (xl \u2212 fl) \u2202x2 . (18) Finally, replacing Equations 16, 17 and 11 into the Equation 5 for a single node xj , if j \u0338= l, we have \u03b4j = Hl,j(log p(x)) \u00b7 \u2207xl log p(x) Hl,l(log p(x)) = \u2202 log p\u03f5(xl\u2212fl) \u2202x2 \u00b7 dfl dxj \u00b7 \u2202 log p \u03f5(xl\u2212fl) \u2202x \u2202 log p\u03f5(xl\u2212fl) \u2202x2 = dfl dxj \u00b7 \u2202 log p \u03f5 (xl \u2212 fl) \u2202x . (19) The last line in Equation 19 is the same as in Equation 5 from Lemma 2, proving that \u03b4j can be written using the first and second order derivative of the log-likelihood. B SCORE OF NONLINEAR ANM WITH GAUSSIAN NOISE: A SCM entails a distribution p(x) = d\u220f i=1 p(xi | Pa(xi)), (20) over the variables x (Peters et al., 2017) By assuming that the noise variables \u03f5i \u223c N (0, \u03c32i ) and inserting the ANM function, Equation 20 can be written as log p(x) = \u22121 2 d\u2211 i=1 ( xi \u2212 fi(Pa(xi)) \u03c3i )2 \u2212 1 2 d\u2211 i=1 log(2\u03c0\u03c32i ). (21) The score of p(x) can hence be written as \u2207xj log p(x) = \u2212 xj \u2212 fj(Pa(xj)) \u03c32j + \u2211 i\u2208children(j) \u2202fi \u2202xj (Pa(xi)) xi \u2212 fi(Pa(xi)) \u03c32i . (22) C VISUALISATION OF THE SCORE\u2019S JACOBIAN FOR TWO VARIABLES Considering a two variables problem where the causal mechanisms are B = f\u03c9(A)+\u03f5B and A = \u03f5A with \u03f5A, \u03f5B \u223c N (0, 1) and f\u03c9 being a two-layer MLP with randomly initialised weights. Note that the variance of \u2202 2 log p(A,B) \u2202B2 , while not 0 as predicted by Equation 2, is smaller than \u22022 log p(A,B) \u2202A2 allowing discovery of the true causal direction. D EXPERIMENTS DETAILS:  D.1 HYPERPARAMETERS OF DPM TRAINING: We now describe the hyperparameters for the diffusion training. We use number of time steps T = 100, \u03b2t is a linearly scheduled between \u03b2min = 0.0001 and \u03b2max = 0.02. The model is trained Layer Hyperparameters: according to Equation 3 which follows Ho et al. (2020). During sampling, t is sampled from a Uniform distribution. D.2 NEURAL ARCHITECTURE: The neural network follows a simple MLP with 5 Linear layers, LeakyReLU activation function, Layer Normalization and Dropout in the first layer. The full architecture is detailed in Table 2. D.3 METRICS: For each method, we compute the SHD. Structural Hamming distance between the output and the true causal graph, which counts the number of missing, falsely detected, or reversed edges. SID. Structural Intervention Distance is based on a graphical criterion only and quantifies the closeness between two DAGs in terms of their corresponding causal inference statements(Peters & Bu\u0308hlmann, 2015). Order Divergence. Rolland et al. (2022) propose this quantity for measuring how well the topological order is estimated. For an ordering \u03c0, and a target adjacency matrix A, we define the topological order divergence Dtop(\u03c0,A) as Dtop(\u03c0,A) = d\u2211 i=1 \u2211 j:\u03c0i>\u03c0j Aij . (23) If \u03c0 is a correct topological order for A, then Dtop(\u03c0,A) = 0. Otherwise, Dtop(\u03c0,A) counts the number of edges that cannot be recovered due to the choice of topological order. Therefore, it provides a lower bound on the SHD of the final algorithm (irrespective of the pruning method). E OTHER RESULTS:  E.1 OVERFITTING: The data used for topological ordering (inference) is a subset of the training data. Therefore, it is not obvious if overfitting would be an issue with our algorithm. Therefore, we run an experiment where we fix the number of epochs to 2000 considered high for a set of runs and use early stopping for another set in order to verify if overfitting is an issue. On average across all 20 nodes datasets, the early stopping strategy output an ordering diverge of 9.5 whilst overfitting is at 11.1 showing that the method does not benefit from overfitting. E.2 OPTIMAL t FOR SCORE ESTIMATION As noted by Vincent (2011), the best approximation of the score by a learned denoising function is when the training signal-to-noise (SNR) ratio is low. In diffusion model training, t = 0 corresponds to the coefficient with lowest SNR. However, we found empirically that the best score estimate varies somehow randomly across different values of t. Therefore, we run the the leaf finding function (Equation 9) N times for values of t evenly spaced in the [0, T ] interval and choose the best leaf based on majority vote. We show in Figure 6 that majority voting is a better approach than choosing a constant value for t. E.3 ABLATIONS OF DIFFAN MASKING AND GREEDY: We now verify how DiffAN masking and DiffAN greedy compare against the original version detailed in the main text which computes the deciduous score. Here, we use the same datasets decribed in Section 5.1 which comprises 4 (20ER1, 20ER5, 20SF1, 20SF5) synthetic dataset types with 27 variations over seeds, noise type and noise scale. We observe, in Figure 7, that the greedy version performs the best but it is the slowest, as seen in Figure 8. DiffAN masking E.4 DETAILED RESULTS: We present the numerical results for the violinplots in Section 5.1 in Tables 3 and 4. The results are presented in meanstd with statistics acquired over experiments with 3 seeds."}, {"role": "assistant", "content": "Error generating response: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}", "metadata": {"title": "Error generating response", "id": 0, "status": "done"}}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "**Step 5: Collaborative Review Discussion and Consensus Building**\n\nThe agents have provided detailed assessments. I will now analyze their feedback and facilitate a discussion to reach a consensus.\n\n*   **CausalDiscoveryExpert:** Highlights the novelty of the ML application but emphasizes the need for explicit identifiability assumptions and a theoretical link to causal principles.\n*   **DiffusionModelTechnician:** Provides a thorough technical evaluation of the diffusion model implementation and suggests improvements for the variance schedule, sampling strategies, and the deciduous score approximation.\n*   **ScalabilityAnalyst:** Offers a comprehensive analysis of the computational complexity and empirical evidence for scalability, emphasizing the importance of sparsity and data quality.\n\nThe main point of contention is the identifiability assumptions. The CausalDiscoveryExpert argues that the paper needs to explicitly state and justify the assumptions that allow the diffusion model to identify causal direction. The DiffusionModelTechnician focuses on the technical validity of the diffusion model and suggests improvements to the implementation. The ScalabilityAnalyst provides a broader perspective on the challenges of scaling causal discovery methods.\n\nI will now try to get the CausalDiscoveryExpert to provide a more concrete assessment, given the other agent's feedback.\n"}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "Okay, I will synthesize the reviews and provide a final decision.\n\n**Step 7: Comprehensive Review Synthesis and Balanced Recommendation**\n\nThe reviewers have raised several important points. The core strength of the paper lies in its innovative application of diffusion models to causal discovery, offering a potentially scalable alternative to existing methods. The theoretical extension of identifiability conditions for ANMs is also a valuable contribution. However, the reviewers also identified weaknesses in the theoretical grounding, specifically regarding the identifiability assumptions and the practical implementation of the deciduous score.\n\nHere's a summary of the key concerns:\n\n*   **Identifiability:** The paper needs to clearly state and justify the identifiability assumptions required for causal discovery with diffusion models. While the paper extends the identifiability results from Rolland et al. (2022), it needs to explicitly address how these assumptions translate to the diffusion model setting.\n*   **Deciduous Score Approximation:** The approximation of the deciduous score (Theorem 1 and Equation 8) needs further justification and empirical validation. The reviewers questioned the accuracy and stability of this approximation, especially in high-dimensional settings.\n*   **Experimental Validation:** While the paper demonstrates promising results on synthetic and real-world datasets, the experimental evaluation could be strengthened by including more diverse datasets, comparing against a wider range of baselines, and providing more detailed ablation studies.\n\nOverall, the paper presents a novel and potentially impactful approach to causal discovery. However, the identified weaknesses need to be addressed before the paper can be considered for acceptance.\n\n**Step 8: Review Finalization and Resource Cleanup**\n\nBased on the reviewers' assessments and the synthesis above, the final recommendation is:\n\nFINAL DECISION: Reject\n"}], "ground_truth": "Accept: poster", "response_history": "Okay, I will synthesize the reviews and provide a final decision.\n\n**Step 7: Comprehensive Review Synthesis and Balanced Recommendation**\n\nThe reviewers have raised several important points. The core strength of the paper lies in its innovative application of diffusion models to causal discovery, offering a potentially scalable alternative to existing methods. The theoretical extension of identifiability conditions for ANMs is also a valuable contribution. However, the reviewers also identified weaknesses in the theoretical grounding, specifically regarding the identifiability assumptions and the practical implementation of the deciduous score.\n\nHere's a summary of the key concerns:\n\n*   **Identifiability:** The paper needs to clearly state and justify the identifiability assumptions required for causal discovery with diffusion models. While the paper extends the identifiability results from Rolland et al. (2022), it needs to explicitly address how these assumptions translate to the diffusion model setting.\n*   **Deciduous Score Approximation:** The approximation of the deciduous score (Theorem 1 and Equation 8) needs further justification and empirical validation. The reviewers questioned the accuracy and stability of this approximation, especially in high-dimensional settings.\n*   **Experimental Validation:** While the paper demonstrates promising results on synthetic and real-world datasets, the experimental evaluation could be strengthened by including more diverse datasets, comparing against a wider range of baselines, and providing more detailed ablation studies.\n\nOverall, the paper presents a novel and potentially impactful approach to causal discovery. However, the identified weaknesses need to be addressed before the paper can be considered for acceptance.\n\n**Step 8: Review Finalization and Resource Cleanup**\n\nBased on the reviewers' assessments and the synthesis above, the final recommendation is:\n\nFINAL DECISION: Reject\n", "elapsed_time": 305.18769001960754, "reviewer_agents": {"CausalDiscoveryExpert": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in causal discovery algorithms and identifiability assumptions.", "system_prompt": "You are an expert in causal discovery. You will evaluate the novelty, significance, and theoretical contributions of the proposed method in the context of existing causal discovery algorithms. Assess the identifiability assumptions and the algorithm's ability to handle different types of causal relationships and noise distributions.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "DiffusionModelSpecialist": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Specialist in diffusion models and score estimation techniques.", "system_prompt": "You are a specialist in diffusion models. You will evaluate the technical aspects of using diffusion models for score estimation and topological ordering. Analyze the correctness of the proposed method for computing the deciduous score and the validity of the approximations made in the algorithm.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "ExperimentalValidationAnalyst": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Analyst for experimental validation and comparison with state-of-the-art methods.", "system_prompt": "You are an expert in experimental validation. You will assess the experimental design, results, and comparisons with state-of-the-art methods. Focus on the scalability of the algorithm, the choice of datasets, and the appropriateness of the evaluation metrics.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "DiffusionModelTechnician": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Technician specializing in diffusion models, score estimation, and deciduous score approximation.", "system_prompt": "You are a technician specializing in diffusion models. Evaluate the technical details of the diffusion model implementation, the score estimation, and the deciduous score approximation. Assess the validity of the approximations and provide suggestions for improvement.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "ScalabilityAnalyst": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Analyst focusing on the scalability of causal discovery methods.", "system_prompt": "You are an analyst focusing on the scalability of causal discovery methods. Assess the computational complexity analysis and the empirical evidence for scalability to high-dimensional datasets. Evaluate the practical considerations and provide insights on the limitations and potential for real-world applications.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}}}
