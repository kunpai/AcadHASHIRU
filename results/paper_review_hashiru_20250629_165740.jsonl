{"paper_id": "gcD2UtCGMc2", "prompt": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Replicable Bandits\n\n1 INTRODUCTION: In order for scientific findings to be valid and reliable, the experimental process must be repeatable, and must provide coherent results and conclusions across these repetitions. In fact, lack of reproducibility has been a major issue in many scientific areas; a 2016 survey that appeared in Nature (Baker, 2016a) revealed that more than 70% of researchers failed in their attempt to reproduce another researcher\u2019s experiments. What is even more concerning is that over 50% of them failed to reproduce their own findings. Similar concerns have been raised by the machine learning community, e.g., the ICLR 2019 Reproducibility Challenge (Pineau et al., 2019) and NeurIPS 2019 Reproducibility Program (Pineau et al., 2021), due to the to the exponential increase in the number of publications and the reliability of the findings. The aforementioned empirical evidence has recently led to theoretical studies and rigorous definitions of reproducibility. In particular, the works of Impagliazzo et al. (2022) and Ahn et al. (2022) considered reproducibility as an algorithmic property through the lens of (offline) learning and convex optimization, respectively. In a similar vein, in the current work, we introduce the notion of reproducibility in the context of interactive learning and decision making. In particular, we study reproducible policy design for the fundamental setting of stochastic bandits. A multi-armed bandit (MAB) is a one-player game that is played over T rounds where there is a set of different arms/actions A of size |A| = K (in the more general case of linear bandits, we can consider even an infinite number of arms). In each round t = 1, 2, . . . , T , the player pulls an arm at \u2208 A and receives a corresponding reward rt. In the stochastic setting, the rewards of each arm are sampled in each round independently, from some fixed but unknown, distribution supported on [0, 1]. Crucially, each arm has a potentially different reward distribution, but the distribution of each arm is fixed over time. A bandit algorithm A at every round t takes as input the sequence of arm-reward pairs that it has seen so far, i.e., (a1, r1), . . . , (at\u22121, rt\u22121), then uses (potentially) some internal randomness \u03be to pull an arm at \u2208 A and, finally, observes the associated reward rt \u223c Dat . We propose the following natural notion of a reproducible bandit algorithm, which is inspired by the definition of Impagliazzo et al. (2022). Intuitively, a bandit algorithm is reproducible if two distinct executions of the algorithm, with internal randomness fixed between both runs, but with independent reward realizations, give the exact same sequence of played arms, with high probability. More formally, we have the following definition. Definition 1 (Reproducible Bandit Algorithm). Let \u03c1 \u2208 [0, 1]. We call a bandit algorithm A \u03c1reproducible in the stochastic setting if for any distribution Daj over [0, 1] of the rewards of the j-th arm aj \u2208 A, and for any two executions of A, where the internal randomness \u03be is shared across the executions, it holds that Pr \u03be,r(1),r(2) [( a (1) 1 , . . . , a (1) T ) = ( a (2) 1 , . . . , a (2) T )] \u2265 1\u2212 \u03c1 . Here, a(i)t = A(a (i) 1 , r (i) 1 , ..., a (i) t\u22121, r (i) t\u22121; \u03be) is the t-th action taken by the algorithm A in execution i \u2208 {1, 2}. The reason why we allow for some fixed internal randomness is that the algorithm designer has control over it, e.g., they can use the same seed for their (pseudo)random generator between two executions. Clearly, naively designing a reproducible bandit algorithm is not quite challenging. For instance, an algorithm that always pulls the same arm or an algorithm that plays the arms in a particular random sequence determined by the shared random seed \u03be are both reproducible. The caveat is that the performance of these algorithms in terms of expected regret will be quite poor. In this work, we aim to design bandit algorithms which are reproducible and enjoy small expected regret. In the stochastic setting, the (expected) regret after T rounds is defined as E[RT ] = T max a\u2208A \u00b5a \u2212E [ T\u2211 t=1 \u00b5at ] , where \u00b5a = Er\u223cDa [r] is the mean reward for arm a \u2208 A. In a similar manner, we can define the regret in the more general setting of linear bandits (see, Section 5) Hence, the overarching question in this work is the following: Is it possible to design reproducible bandit algorithms with small expected regret?: At a first glance, one might think that this is not possible, since it looks like reproducibility contradicts the exploratory behavior that a bandit algorithm should possess. However, our main results answer this question in the affirmative and can be summarized in Table 1. 1.1 RELATED WORK: Reproducibility In this work, we introduce the notion of reproducibility in the context of interactive learning and, in particular, in the fundamental setting of stochastic bandits. Close to our work, the notion of a reproducible algorithm in the context of learning was proposed by Impagliazzo et al. (2022), where it is shown how any statistical query algorithm can be made reproducible with a moderate increase in its sample complexity. Using this result, they provide reproducible algorithms for finding approximate heavy-hitters, medians, and the learning of half-spaces. Reproducibility has been also considered in the context of optimization by Ahn et al. (2022). We mention that in Ahn et al. (2022) the notion of a reproducible algorithm is different from our work and that of Impagliazzo et al. (2022), in the sense that the outputs of two different executions of the algorithm do not need to be exactly the same. From a more application-oriented perspective, Shamir & Lin (2022) study irreproducibility in recommendation systems and propose the use of smooth activations (instead of ReLUs) to improve recommendation reproducibility. In general, the reproducibility crisis is reported in various scientific disciplines (Ioannidis, 2005; McNutt, 2014; Baker, 2016b; Goodman et al., 2016; Lucic et al., 2018; Henderson et al., 2018). For more details we refer to the report of the NeurIPS 2019 Reproducibility Program (Pineau et al., 2021) and the ICLR 2019 Reproducibility Challenge (Pineau et al., 2019). Bandit Algorithms Stochastic multi-armed bandits for the general setting without structure have been studied extensively (Slivkins, 2019; Lattimore & Szepesva\u0301ri, 2020; Bubeck et al., 2012b; Auer et al., 2002; Cesa-Bianchi & Fischer, 1998; Kaufmann et al., 2012a; Audibert et al., 2010; Agrawal & Goyal, 2012; Kaufmann et al., 2012b). In this setting, the optimum regret achievable is O ( log(T ) \u2211 i:\u2206i>0 \u2206\u22121 ) ; this is achieved, e.g., by the upper confidence bound (UCB) algorithm of Auer et al. (2002). The setting of d-dimensional linear stochastic bandits is also well-explored (Dani et al., 2008; Abbasi-Yadkori et al., 2011) under the well-specified linear reward model, achieving (near) optimal problem-independent regret of O(d \u221a T log(T )) (Lattimore & Szepesva\u0301ri, 2020). Note that the best-known lower bound is \u2126(d \u221a T ) (Dani et al., 2008) and that the number of arms can, in principle, be unbounded. For a finite number of arms K, the best known upper bound is O(d \u221a T log(K)) (Bubeck et al., 2012a). Our work focuses on the design of reproducible bandit algorithms and we hence consider only stochastic environments. In general, there is also extensive work in adversarial bandits and we refer the interested reader to Lattimore & Szepesva\u0301ri (2020). Batched Bandits While sequential bandit problems have been studied for almost a century, there is much interest in the batched setting too. In many settings, like medical trials, one has to take a lot of actions in parallel and observe their rewards later. The works of Auer & Ortner (2010) and CesaBianchi et al. (2013) provided sequential bandit algorithms which can easily work in the batched setting. The works of Gao et al. (2019) and Esfandiari et al. (2021) are focusing exclusively on the batched setting. Our work on reproducible bandits builds upon some of the techniques from these two lines of work. 2 STOCHASTIC BANDITS AND REPRODUCIBILITY: In this section, we first highlight the main challenges in order to guarantee reproducibility and then discuss how the results of Impagliazzo et al. (2022) can be applied in our setting. 2.1 WARM-UP I: NAIVE REPRODUCIBILITY AND CHALLENGES: Let us consider the stochastic two-arm setting (K = 2) and a bandit algorithm A with two independent executions, A1 and A2. The algorithm Ai plays the sequence 1, 2, 1, 2, . . . until some, potentially random, round Ti \u2208 N after which one of the two arms is eliminated and, from that point, the algorithm picks the winning arm ji \u2208 {1, 2}. The algorithm A is \u03c1-reproducible if and only if T1 = T2 and j1 = j2 with probability 1\u2212 \u03c1. Assume that |\u00b51 \u2212 \u00b52| = \u2206 where \u00b5i is the mean of the distribution of the i-th arm. If we assume that \u2206 is known, then we can run the algorithm for T1 = T2 = C\u22062 log(1/\u03c1) for some universal constant C > 0 and obtain that, with probability 1 \u2212 \u03c1, it will hold that \u00b5\u0302(j)1 \u2248 \u00b51 and \u00b5\u0302 (j) 2 \u2248 \u00b52 for j \u2208 {1, 2}, where \u00b5\u0302(j)i is the estimation of arm\u2019s i mean during execution j. Hence, knowing \u2206 implies that the stopping criterion of the algorithm A is deterministic and that, with high probability, the winning arm will be detected at time T1 = T2. This will make the algorithm \u03c1-reproducible. Observe that when K = 2, the only obstacle to reproducibility is that the algorithm should decide at the same time to select the winning arm and the selection must be the same in the two execution threads. In the presence of multiple arms, there exists the additional constraint that the above conditions must be satisfied during, potentially, multiple arm eliminations. Hence, the two questions arising from the above discussion are (i) how to modify the above approach when \u2206 is unknown and (ii) how to deal with K > 2 arms. A potential solution to the second question (on handling K > 2 arms) is the Execute-Then-Commit (ETC) strategy. Consider the stochastic K-arm bandit setting. For any \u03c1 \u2208 (0, 1), the ETC algorithm with known \u2206 = mini \u2206i and horizon T that uses m = 4\u22062 log(1/\u03c1) deterministic exploration phases before commitment is \u03c1-reproducible. The intuition is exactly the same as in the K = 2 case. The caveats of this approach are that it assumes that \u2206 is known and that the obtained regret is quite unsatisfying. In particular, it achieves regret bounded by m \u2211 i\u2208[K] \u2206i + \u03c1 \u00b7 (T \u2212mK) \u2211 i\u2208[k] \u2206i. Next, we discuss how to improve the regret bound without knowing the gaps \u2206i. Before designing new algorithms, we will inspect the guarantees that can be obtained by combining ideas from previous results in the bandits literature and the recent work in reproducible learning of Impagliazzo et al. (2022). 2.2 WARM-UP II: BANDIT ALGORITHMS AND REPRODUCIBLE MEAN ESTIMATION: First, we remark that we work in the stochastic setting and the distributions of the rewards of the two arms are subgaussian. Thus, the problem of estimating their mean is an instance of a statistical query for which we can use the algorithm of Impagliazzo et al. (2022) to get a reproducible mean estimator for the distributions of the rewards of the arms. Proposition 2 (Reproducible Mean Estimation (Impagliazzo et al., 2022)). Let \u03c4, \u03b4, \u03c1 \u2208 [0, 1]. There exists a \u03c1-reproducible algorithm ReprMeanEstimation that draws \u2126 ( log(1/\u03b4) \u03c42(\u03c1\u2212\u03b4)2 ) samples from a distribution with mean \u00b5 and computes an estimate \u00b5\u0302 that satisfies |\u00b5\u0302 \u2212 \u00b5| \u2264 \u03c4 with probability at least 1\u2212 \u03b4. Notice that we are working in the regime where \u03b4 \u226a \u03c1, so the sample complexity is \u2126 ( log(1/\u03b4) \u03c42\u03c12 ) . The straightforward approach is to try to use an optimal multi-armed algorithm for the stochastic setting, such as UCB or arm-elimination (Even-Dar et al., 2006), combined with the reproducible mean estimator. However, it is not hard to see that this approach does not give meaningful results: if we want to achieve reproducibility \u03c1 we need to call the reproducible mean estimator routine with parameter \u03c1/(KT ), due to the union bound that we need to take. This means that we need to pull every arm at least K2T 2 times, so the regret guarantee becomes vacuous. This gives us the first key insight to tackle the problem: we need to reduce the number of calls to the mean estimator. Hence, we will draw inspiration from the line of work in stochastic batched bandits (Gao et al., 2019; Esfandiari et al., 2021) to derive reproducible bandit algorithms. 3 REPRODUCIBLE MEAN ESTIMATION FOR BATCHED BANDITS: As a first step, we would like to show how one could combine the existing reproducible algorithms of Impagliazzo et al. (2022) with the batched bandits approach of Esfandiari et al. (2021) to get some preliminary non-trivial results. We build an algorithm for the K-arm setting, where the gaps \u2206j are unknown to the learner. Let \u03b4 be the confidence parameter of the arm elimination algorithm and \u03c1 be the reproducibility guarantee we want to achieve. Our approach is the following: let us, deterministically, split the time interval into sub-intervals of increasing length. We treat each subinterval as a batch of samples where we pull each active arm the same number of times and use the reproducible mean estimation algorithm to, empirically, compute the true mean. At the end of each batch, we decide to eliminate some arm j using the standard UCB estimate. Crucially, if we condition on the event that all the calls to the reproducible mean estimator return the same number, then the algorithm we propose is reproducible. Theorem 3. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm (presented in Algorithm 1) for the stochastic bandit problem with K arms and gaps (\u2206j)j\u2208[K] whose expected regret is E[RT ] \u2264 C \u00b7 K2 log2(T ) \u03c12 \u2211 j:\u2206j>0 ( \u2206j + log(KT log(T )) \u2206j ) , where C > 0 is an absolute numerical constant, and its running time is polynomial in K,T and 1/\u03c1. Algorithm 1 Mean-Estimation Based Reproducible Algorithm for Stochastic MAB (Theorem 3) 1: Input: time horizon T, number of arms K, reproducibility \u03c1 2: Initialization: B \u2190 log(T ), q \u2190 T 1/B , c0 \u2190 0, A \u2190 [K], r \u2190 T , \u00b5\u0302a \u2190 0,\u2200a \u2208 A 3: for i = 1 to B \u2212 1 do 4: if \u230aqi\u230b \u00b7 |A| > r then 5: break 6: ci = ci\u22121 + \u230aqi\u230b 7: Pull every arm a \u2208 A for \u230aqi\u230b times 8: for a \u2208 A do 9: \u00b5\u0302a \u2190 ReprMeanEstimation(\u03b4 = 1/(2KTB), \u03c4 = 1, \u221a log(2KTB)/ci, \u03c1 \u2032 = \u03c1/(KB)) \u25b7 Proposition 2 10: r \u2190 r \u2212 |A| \u00b7 \u230aqi\u230b 11: for a \u2208 A do 12: if \u00b5\u0302a < maxa\u2208A \u00b5\u0302a \u2212 2\u03c4 then 13: Remove a from A 14: In the last batch play argmaxa\u2208A \u00b5\u0302a The above result, whose proof can be found in Appendix A, states that, by combining the tools from Impagliazzo et al. (2022) and Esfandiari et al. (2021), we can design a reproducible bandit algorithm with (instance-dependent) expected regret O(K2 log3(T )/\u03c12). Notice that the regret guarantee has an extra K2 log2(T )/\u03c12 factor compared to its non-reproducible counterpart in Esfandiari et al. (2021) (Theorem 5.1). This is because, due to a union bound over the rounds and the arms, we need to call the reproducible mean estimator with parameter \u03c1/(K log(T )). In the next section, we show how to get rid of the log2(T ) by designing a new algorithm. 4 IMPROVED ALGORITHMS FOR REPRODUCIBLE STOCHASTIC BANDITS: While the previous result provides a non-trivial regret bound, it is not optimal with respect to the time horizon T . In this section, we show to improve it by designing a new algorithm, presented in Algorithm 2, which satisfies the guarantees of Theorem 4 and, essentially, decreases the dependence on the time horizon T from log3(T ) to log(T ). Our main result for reproducible stochastic multiarmed bandits with K arms follows. Theorem 4. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm (presented in Algorithm 2) for the stochastic bandit problem with K arms and gaps (\u2206j)j\u2208[K] whose expected regret is E[RT ] \u2264 C \u00b7 K2 \u03c12 \u2211 j:\u2206j>0 ( \u2206j + log(KT log(T )) \u2206j ) , where C > 0 is an absolute numerical constant, and its running time is polynomial in K,T and 1/\u03c1. Note that, compared to the non-reproducible setting, we incur an extra factor of K2/\u03c12 in the regret. The proof can be found in Appendix B. Let us now describe how Algorithm 2 works. We decompose the time horizon into B = log(T ) batches. Without the reproducibility constraint, one could draw qi samples in batch i from each arm and estimate the mean reward. With the reproducibility constraint, we have to boost this: in each batch i, we pull each active arm O(\u03b2qi) times, for some q to be determined, where \u03b2 = O(K2/\u03c12) is the reproducibility blow-up. Using these samples, we compute the empirical mean \u00b5\u0302(i)\u03b1 for any active arm \u03b1. Note that U\u0303i in Algorithm 2 corresponds to the size of the actual confidence interval of the estimation and Ui corresponds to the confidence interval of an algorithm that does not use the \u03b2-blow-up in the number of samples. The novelty of our approach comes from the choice of the interval around the mean of the maximum arm: we pick a threshold uniformly at random from an interval of size Ui/2 around the maximum mean. Then, the algorithm checks whether \u00b5\u0302(i)a + U\u0303i < max \u00b5\u0302 (i) a\u2032 \u2212 U i, where max runs over the active arms a\u2032 in batch i, and eliminates arms accordingly. To prove the result we show that there are three regions that some arm j can be in relative to the confidence interval of the best arm in batch i (cf. Appendix B). If Algorithm 2 Reproducible Algorithm for Stochastic Multi-Armed Bandits (Theorem 4) 1: Input: time horizon T, number of arms K, reproducibility \u03c1 2: Initialization: B \u2190 log(T ), q \u2190 T 1/B , c0 \u2190 0, A0 \u2190 [K], r \u2190 T , \u00b5\u0302a \u2190 0,\u2200a \u2208 A0 3: \u03b2 \u2190 \u230amax{K2/\u03c12, 2304}\u230b 4: for i = 1 to B \u2212 1 do 5: if \u03b2\u230aqi\u230b \u00b7 |Ai| > r then 6: break 7: Ai \u2190 Ai\u22121 8: for a \u2208 Ai do 9: Pull arm a for \u03b2\u230aqi\u230b times 10: Compute the empirical mean \u00b5\u0302(i)\u03b1 11: ci \u2190 ci\u22121 + \u230aqi\u230b 12: c\u0303i \u2190 \u03b2ci 13: U\u0303i \u2190 \u221a 2 ln(2KTB)/c\u0303i 14: Ui \u2190 \u221a 2 ln(2KTB)/ci 15: U i \u2190 Uni[Ui/2, Ui] 16: r \u2190 r \u2212 \u03b2 \u00b7 |Ai| \u00b7 \u230aqi\u230b 17: for a \u2208 Ai do 18: if \u00b5\u0302(i)a + U\u0303i < maxa\u2208Ai \u00b5\u0302 (i) a \u2212 U i then 19: Remove a from Ai 20: In the last batch play argmaxa\u2208AB\u22121 \u00b5\u0302 (B\u22121) a it lies in two of these regions, then the decision of whether to keep it or discard it is the same in both executions of the algorithm. However, if it is in the third region, the decision could be different between parallel executions, and since it relies on some external and unknown randomness, it is not clear how to reason about it. To overcome this issue, we use the random threshold to argue about the probability that the decision between two executions differs. The crucial observation that allows us to get rid of the extra log2(T ) factor is that there are correlations between consecutive batches: we prove that if some arm j lies in this \u201cbad\u201d region in some batch i, then it will be outside this region after a constant number of batches. 5 REPRODUCIBLE STOCHASTIC LINEAR BANDITS: We now investigate reproducibility in the more general setting of stochastic linear bandits. In this setting, each arm is a vector a \u2208 Rd belonging to some action set A \u2286 Rd, and there is a parameter \u03b8\u22c6 \u2208 Rd unknown to the player. In round t, the player chooses some action at \u2208 A and receives a reward rt = \u27e8\u03b8\u22c6, at\u27e9 + \u03b7t, where \u03b7t is a zero-mean 1-subgaussian random variable independent of any other source of randomness. This means that E[\u03b7t] = 0 and satisfies E[exp(\u03bb\u03b7t)] \u2264 exp(\u03bb2/2) for any \u03bb \u2208 R. For normalization purposes, it is standard to assume that \u2225\u03b8\u22c6\u22252 \u2264 1 and supa\u2208A \u2225a\u22252 \u2264 1. In the linear setting, the expected regret after T pulls a1, . . . , aT can be written as E[RT ] = T sup a\u2208A \u27e8\u03b8\u22c6, a\u27e9 \u2212E [ T\u2211 t=1 \u27e8\u03b8\u22c6, at\u27e9 ] . In Section 5.1 we provide results for the finite action space case, i.e., when |A| = K. Next, in Section 5.2, we study reproducible linear bandit algorithms when dealing with infinite action spaces. In the following, we work in the regime where T \u226b d. We underline that our approach leverages connections of stochastic linear bandits with G-optimal experiment design, core sets constructions, and least-squares estimators. Roughly speaking, the goal of G-optimal design is to find a (small) subset of arms A\u2032, which is called the core set, and define a distribution \u03c0 over them with the following property: for any \u03b5 > 0, \u03b4 > 0 pulling only these arms for an appropriate number of times and computing the least-squares estimate \u03b8\u0302 guarantees that supa\u2208A\u27e8a, \u03b8\u2217\u2212 \u03b8\u0302\u27e9 \u2264 \u03b5, with probability 1\u2212\u03b4. For an extensive discussion, we refer to Chapters 21 and 22 of Lattimore & Szepesva\u0301ri (2020). 5.1 FINITE ACTION SET: We first introduce a lemma that allows us to reduce the size of the action set that our algorithm has to search over. Lemma 5 (See Chapters 21 and 22 in Lattimore & Szepesva\u0301ri (2020)). For any finite action set A that spans Rd and any \u03b4, \u03b5 > 0, there exists an algorithm that, in time polynomial in d, computes a multi-set of \u0398(d log(1/\u03b4)/\u03b52+d log log d) actions (possibly with repetitions) such that (i) they span Rd and (ii) if we perform these actions in a batched stochastic d-dimensional linear bandits setting with true parameter \u03b8\u22c6 \u2208 Rd and let \u03b8\u0302 be the least-squares estimate for \u03b8\u22c6, then, for any a \u2208 A, with probability at least 1\u2212 \u03b4, we have \u2223\u2223\u2223\u2329a, \u03b8\u22c6 \u2212 \u03b8\u0302\u232a\u2223\u2223\u2223 \u2264 \u03b5. Essentially, the multi-set in Lemma 5 is obtained using an approximate G-optimal design algorithm. Thus, it is crucial to check whether this can be done in a reproducible manner. Recall that the above set of distinct actions is called the core set and is the solution of an (approximate) G-optimal design problem. To be more specific, consider a distribution \u03c0 : A \u2192 [0, 1] and define V (\u03c0) =\u2211 a\u2208A \u03c0(a)aa \u22a4 \u2208 Rd\u00d7d and g(\u03c0) = supa\u2208A \u2225a\u22252V (\u03c0)\u22121 . The distribution \u03c0 is called a design and the goal of G-optimal design is to find a design that minimizes g. Since the number of actions is finite, this problem reduces to an optimization problem which can be solved efficiently using standard optimization methods (e.g., the Frank-Wolfe method). Since the initialization is the same, the algorithm that finds the optimal (or an approximately optimal) design is reproducible under the assumption that the gradients and the projections do not have numerical errors. This perspective is orthogonal to the work of Ahn et al. (2022), that defines reproducibility from a different viewpoint. Algorithm 3 Reproducible Algorithm for Stochastic Linear Bandits (Theorem 6) 1: Input: number of arms K, time horizon T, reproducibility \u03c1 2: Initialization: B \u2190 log(T ), q \u2190 (T/c)1/B , A \u2190 [K], r \u2190 T 3: \u03b2 \u2190 \u230amax{K2/\u03c12, 2304}\u230b 4: for i = 1 to B \u2212 1 do 5: \u03b5\u0303i = \u221a d log(KT 2)/(\u03b2qi) 6: \u03b5i = \u221a d log(KT 2)/qi 7: ni = 10d log(KT 2)/\u03b52i 8: a1, . . . , ani \u2190 multi-set given by Lemma 5 with parameters \u03b4 = 1/(KT 2) and \u03b5 = \u03b5\u0303i 9: if ni > r then 10: break 11: Pull every arm a1, . . . , ani and receive rewards r1, . . . , rni 12: Compute the LSE \u03b8\u0302i \u2190 (\u2211ni j=1 aja T j )\u22121 (\u2211ni j=1 ajrj ) 13: \u03b5i \u2190 Uni[\u03b5i/2, \u03b5i]:  14: r \u2190 r \u2212 ni: 15: for a \u2208 A do 16: if \u27e8a, \u03b8\u0302i\u27e9+ \u03b5\u0303i < maxa\u2208A\u27e8a, \u03b8\u0302i\u27e9 \u2212 \u03b5i then 17: Remove a from A 18: In the last batch play argmaxa\u2208A\u27e8a, \u03b8\u0302B\u22121\u27e9 In our batched bandit algorithm (Algorithm 3), the multi-set of arms a1, . . . , ani computed in each batch is obtained via a deterministic algorithm with runtime poly(K, d), where |A| = K. Hence, the multi-set will be the same in two different executions of the algorithm. On the other hand, the LSE will not be since it depends on the stochastic rewards. We apply the techniques that we developed in the reproducible stochastic MAB setting in order to design our algorithm. Our main result for reproducible d-dimensional stochastic linear bandits with K arms follows. For the proof, we refer to Appendix C. Theorem 6. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm for the stochastic d-dimensional linear bandit problem with K arms whose expected regret is E[RT ] \u2264 C \u00b7 K2 \u03c12 \u221a dT log(KT ) , where C > 0 is an absolute numerical constant, and its running time is polynomial in d,K, T and 1/\u03c1. Note that the best known non-reproducible algorithm achieves an upper bound of O\u0303( \u221a dT log(K)) and, hence, our algorithm incurs a reproducibility overhead of order K2/\u03c12. The intuition behind the proof is similar to the multi-armed bandit setting in Section 4. 5.2 INFINITE ACTION SET: Let us proceed to the setting where the action set A is unbounded. Unfortunately, even when d = 1, we cannot directly get an algorithm that has satisfactory regret guarantees by discretizing the space and using Algorithm 3. The approach of Esfandiari et al. (2021) is to discretize the action space and use an 1/T -net to cover it, i.e. a set A\u2032 \u2286 A such that for all a \u2208 A there exists some a\u2032 \u2208 A\u2032 with ||a \u2212 a\u2032||2 \u2264 1/T . It is known that there exists such a net of size at most (3T )d (Vershynin, 2018, Corollary 4.2.13). Then, they apply the algorithm for the finite arms setting, increasing their regret guarantee by a factor of \u221a d. However, our reproducible algorithm for this setting contains an additional factor of K2 in the regret bound. Thus, even when d = 1, our regret guarantee is greater than T, so the bound is vacuous. One way to fix this issue and get a sublinear regret guarantee is to use a smaller net. We use a 1/T 1/(4d+2)\u2212net that has size at most (3T ) d 4d+2 and this yields an expected regret of order O(T 4d+1/(4d+2) \u221a d log(T )/\u03c12). For further details, we refer to Appendix D. Even though the regret guarantee we managed to get using the smaller net of Appendix D is sublinear in T , it is not a satisfactory bound. The next step is to provide an algorithm for the infinite action setting using a reproducible LSE subroutine combined with the batching approach of Esfandiari et al. (2021). We will make use of the next lemma. Lemma 7 (Section 21.2 Note 3 of Lattimore & Szepesva\u0301ri (2020)). There exists a deterministic algorithm that, given an action space A \u2286 Rd, computes a 2-approximate G-optimal design \u03c0 with a core set of size O(d log log(d)). We additionally prove the next useful lemma, which, essentially, states that we can assume without loss of generality that every arm in the support of \u03c0 has mass at least \u2126(1/(d log(d))). We refer to Appendix F.1 for the proof. Lemma 8 (Effective Support). Let \u03c0 be the distribution that corresponds to the 2-approximate optimal G-design of Lemma 7 with input A. Assume that \u03c0(a) \u2264 c/(d log(d)), where c > 0 is some absolute numerical constant, for some arm a in the core set. Then, we can construct a distribution \u03c0\u0302 such that, for any arm a in the core set, \u03c0\u0302(a) \u2265 C/(d log(d)), where C > 0 is an absolute constant, so that it holds sup a\u2032\u2208A \u2225a\u2032\u22252V (\u03c0\u0302)\u22121 \u2264 4d . The upcoming lemma is a reproducible algorithm for the least-squares estimator and, essentially, builds upon Lemma 7 and Lemma 8. Its proof can be found at Appendix F.2. Lemma 9 (Reproducible LSE). Let \u03c1, \u03b5 \u2208 (0, 1] and 0 < \u03b4 \u2264 min{\u03c1, 1/d}1. Consider an environment of d-dimensional stochastic linear bandits with infinite action space A. Assume that \u03c0 is a 4-approximate optimal design with associated core set C as computed by Lemma 7 with input A. There exists a \u03c1-reproducible algorithm that pulls each arm a \u2208 C a total of \u2126 ( d4 log(d/\u03b4) log2 log(d) log log log(d) \u03b52\u03c12 ) times and outputs an estimate \u03b8SQ that satisfies supa\u2208A |\u27e8a, \u03b8SQ \u2212 \u03b8\u22c6\u27e9| \u2264 \u03b5 , with probability at least 1\u2212 \u03b4. The main result for the infinite actions\u2019 case, obtained by Algorithm 4, follows. Its proof can be found at Appendix E. 1We can handle the case of 0 < \u03b4 \u2264 d by paying an extra log d factor in the sample complexity. Algorithm 4 Reproducible LSE Algorithm for Stochastic Infinite Action Set (Theorem 10) 1: Input: time horizon T, action set A \u2286 Rd, reproducibility \u03c1 2: A\u2032 \u2190 1/T -net of A 3: Initialization: r \u2190 T,B \u2190 log(T ), q \u2190 (T/c)1/B 4: for i = 1 to B \u2212 1 do 5: qi denotes the number of pulls of all arms before the reproducibuility blow-up 6: \u03b5i = c \u00b7 d \u221a log(T )/qi 7: The blow-up is Mi = qi \u00b7 d3 log(d) log2 log(d) log log log(d) log2(T )/\u03c12 8: a1, . . . , a|Ci| \u2190 core set Ci of the design given by Lemma 7 with parameter A\u2032 9: if \u2308Mi\u2309 > r then 10: break 11: Pull every arm aj for Ni = \u2308Mi\u2309/|Ci| rounds and receive rewards r(j)1 , ..., r (j) Ni for j \u2208 [|Ci|] 12: Si = {(aj , r(j)t ) : t \u2208 [Ni], j \u2208 [|Ci|]} 13: \u03b8\u0302i \u2190 ReproducibleLSE(Si, \u03c1\u2032 = \u03c1/(dB), \u03b4 = 1/(2|A\u2032|T 2), \u03c4 = min{\u03b5i, 1}) 14: r \u2190 r \u2212 \u2308Mi\u2309 15: for a \u2208 A\u2032 do 16: if \u27e8a, \u03b8\u0302i\u27e9 < maxa\u2208A\u2032\u27e8a, \u03b8\u0302i\u27e9 \u2212 2\u03b5i then 17: Remove a from A\u2032 18: In the last batch play argmaxa\u2208A\u2032\u27e8a, \u03b8\u0302B\u22121\u27e9 19: 20: ReproducibleLSE(S, \u03c1, \u03b4, \u03c4) 21: for a \u2208 C do 22: v(a)\u2190 ReproducibleSQ(\u03d5 : x \u2208 R 7\u2192 x \u2208 R, S, \u03c1, \u03b4, \u03c4) \u25b7 Impagliazzo et al. (2022) 23: return ( \u2211 j\u2208|S| aja \u22a4 j ) \u22121 \u00b7 ( \u2211 a\u2208C a na v(a)) Theorem 10. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm (Algorithm 4) for the stochastic d-dimensional linear bandit problem with infinite action set whose expected regret is E[RT ] \u2264 C \u00b7 d4 log(d) log2 log(d) log log log(d) \u03c12 \u221a T log3/2(T ) , where C > 0 is an absolute numerical constant, and its running time is polynomial in T d and 1/\u03c1. Our algorithm for the infinite arm linear bandit case enjoys an expected regret of order O\u0303(poly(d) \u221a T ). We underline that the dependence of the regret on the time horizon is (almost) optimal, and we incur an extra d3 factor in the regret guarantee compared to the non-reproducible algorithm of Esfandiari et al. (2021). We now comment on the time complexity of our algorithm. Remark 11. The current implementation of our algorithm requires time exponential in d. However, for a general convex setA, given access to a separation oracle for it and an oracle that computes an (approximate) G-optimal design, we can execute it in polynomial time and with polynomially many calls to the oracle. Notably, when A is a polytope such oracles exist. We underline that computational complexity issues also arise in the traditional setting of linear bandits with an infinite number of arms and the computational overhead that the reproducibility requirement adds is minimal. For further details, we refer to Appendix G. 6 CONCLUSION AND FUTURE DIRECTIONS: In this paper, we have provided a formal notion of reproducibility for stochastic bandits and we have developed algorithms for the multi-armed bandit and the linear bandit settings that satisfy this notion and enjoy a small regret decay compared to their non-reproducible counterparts. An immediate future direction would be to find the optimal dependence on the number of arms K and the dimension d. Notice that the dependence on \u03c1 is optimal, and this follows from the lower bound in Impagliazzo et al. (2022). Our ideas can be applied to more complicated settings, like misspecified linear bandits (Ghosh et al., 2017), and give similar results. We hope and believe that our paper will inspire future works in reproducible interactive learning algorithms. A THE PROOF OF THEOREM 3: Theorem. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm (Algorithm 1) for the stochastic bandit problem with K arms and gaps (\u2206j)j\u2208[K] whose expected regret is E[RT ] \u2264 C \u00b7 K2 log2(T ) \u03c12 \u2211 j:\u2206j>0 ( \u2206j + log(2KT log(T )) \u2206j ) , where C > 0 is an absolute numerical constant, and its running time is polynomial in K,T and 1/\u03c1. Proof. First, we claim that the algorithm is \u03c1-reproducible: since the elimination decisions are taken in the same iterates and are based solely on the mean estimations, the reproducibility of the algorithm of Proposition 2 implies the reproducibility of the whole algorithm. In particular, Pr[(a1, ..., aT ) \u0338= (a\u20321, ..., a\u2032T )] = Pr[\u2203i \u2208 [B],\u2203j \u2208 [K] : \u00b5\u0302 (i) j was not reproducible] \u2264 \u03c1 . During each batch i, we draw for any active arm \u230aqi\u230b fresh samples for a total of ci samples and use the reproducible mean estimation algorithm to estimate its mean. For an active arm, at the end of some batch i \u2208 [B], we say that its estimation is \u201ccorrect\u201d if the estimation of its mean is within\u221a log(2KTB)/ci from the true mean. Using Proposition 2, the estimation of any active arm at the end of any batch (except possibly the last batch) is correct with probability at least 1\u2212 1/(2KTB) and so, by the union bound, the probability that the estimation is incorrect for some arm at the end of some batch is bounded by 1/T . We remark that when \u03b4 < \u03c1, the sample complexity of Proposition 2 reduces to O(log(1/\u03b4)/(\u03c42\u03c12)). Let E denote the event that our estimates are correct. The total expected regret can be bounded as E[RT ] \u2264 T \u00b7 1/T +E[RT |E ] . It suffices to bound the second term of the RHS and hence we can assume that each gap is correctly estimated within an additive factor of \u221a log(2KTB)/ci after batch i. First, due to the elimination condition, we get that the best arm is never eliminated. Next, we have that E[RT |E ] = \u2211 j:\u2206j>0 \u2206j E[Tj |E ] , where Tj is the total number of pulls of arm j. Fix a sub-optimal arm j and assume that i + 1 was the last batch it was active. Since this arm is not eliminated at the end of batch i, and the estimations are correct, we have that \u2206j \u2264 \u221a log(2KTB)/ci , and so ci \u2264 log(2KTB)/\u22062j . Hence, the number of pulls to get the desired bound due to Proposition 2 is (since we need to pull an arm ci/\u03c121 times in order to get an estimate at distance\u221a log(1/\u03b4)/c2i with probability 1\u2212 \u03b4 in a \u03c11-reproducible manner when \u03b4 < \u03c11) Tj \u2264 ci+1/\u03c121 = q/\u03c121(1 + ci) \u2264 q/\u03c121 \u00b7 (1 + log(2KTB)/\u22062j ) . This implies that the total regret is bounded by E[RT ] \u2264 1 + q/\u03c121 \u00b7 \u2211 j:\u2206j>0 ( \u2206j + log(2KTB) \u2206j ) . We finally set q = T 1/B and B = log(T ). Moreover, we have that \u03c11 = \u03c1/(KB). These yield E[RT ] \u2264 K2 log2(T ) \u03c12 \u2211 j:\u2206j>0 ( \u2206j + log(2KT log(T )) \u2206j ) . This completes the proof. B THE PROOF OF THEOREM 4: Theorem. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm (Algorithm 2) for the stochastic bandit problem with K arms and gaps (\u2206j)j\u2208[K] whose expected regret is E[RT ] \u2264 C \u00b7 K2 \u03c12 \u2211 j:\u2206j>0 (\u2206j + log(KT log(T ))/\u2206j) , for some absolute numerical constant C > 0, and its running time is polynomial in K,T and 1/\u03c1. For intuition, we begin with a non tight analysis which however provides the main ideas behind the actual proof. Non Tight Analysis Assume that the environment has K arms with unknown means \u00b5i and let T be the number of rounds. Consider B to the total number of batches and \u03b2 > 1. We set q = T 1/B . In each batch i \u2208 [B], we pull each arm \u03b2\u230a qi\u230b times. Hence, after the i-th batch, we will have drawn c\u0303i = \u2211 1\u2264j\u2264i \u03b2\u230aqj\u230b independent and identically distributed samples from each arm. Let us also set ci = \u2211 1\u2264j\u2264i\u230aqj\u230b. Let us fix i \u2208 [B]. Using Hoeffding\u2019s bound for subgaussian concentration, the length of the confidence bound for arm j \u2208 [K] that guarantees 1 \u2212 \u03b4 probability of success (in the sense that the empirical estimate \u00b5\u0302j will be close to the true \u00b5j) is equal to U\u0303i = \u221a 2 log(1/\u03b4)/c\u0303i , when the estimator uses c\u0303i samples. Also, let Ui = \u221a 2 log(1/\u03b4)/ci . Assume that the active arms at the batch iteration i lie in the set Ai. Consider the estimates {\u00b5\u0302(i)j }i\u2208[B],j\u2208Ai , where \u00b5\u0302 (i) j is the empirical mean of arm j using c\u0303i samples. We will eliminate an arm j at the end of the batch iteration i if \u00b5\u0302 (i) j + U\u0303i \u2264 max t\u2208Ai \u00b5\u0302 (i) t \u2212 U i , where U i \u223c Uni[Ui/2, Ui]. For the remaining of the proof, we condition on the event E that for every arm j \u2208 [K] and every batch i \u2208 [B] the true mean is within U\u0303i from the empirical one. We first argue about the reproducibility of our algorithm. Consider a fixed round i (end of i-th batch) and a fixed arm j. Let i\u22c6 be the optimal empirical arm after the i-th batch. Let \u00b5\u0302(i) \u2032 j , \u00b5\u0302 (i)\u2032 i\u22c6 the empirical estimates of arms j, i \u22c6 after the i-th batch, under some other execution of the algorithm. We condition on the event E \u2032 for the other execution as well. Notice that |\u00b5\u0302(i) \u2032 j \u2212 \u00b5\u0302 (i) j | \u2264 2U\u0303i, |\u00b5\u0302 (i)\u2032 i\u22c6 \u2212 \u00b5\u0302 (i) i\u22c6 | \u2264 2U\u0303i. Notice that, since the randomness of U i is shared, if \u00b5\u0302 (i) j + U\u0303i \u2265 \u00b5\u0302 (i) i\u22c6 \u2212 U i + 4U\u0303i, then the arm j will not be eliminated after the i-th batch in some other execution of the algorithm as well. Similarly, if \u00b5\u0302(i)j + U\u0303i < \u00b5\u0302 (i) i\u22c6 \u2212U i \u2212 4U\u0303i the the arm j will get eliminated after the i-th batch in some other execution of the algorithm as well. In particular, this means that if \u00b5\u0302(i)j \u2212 2U\u0303i > \u00b5\u0302 (i) i\u22c6 + U\u0303i \u2212 Ui/2 then the arm j will not get eliminated in some other execution of the algorithm and if \u00b5\u0302(i)j + 5U\u0303i < \u00b5\u0302 (i) i\u22c6 \u2212 Ui then the arm j will also get eliminated in some other execution of the algorithm with probability 1 under the event E \u2229 E \u2032. We call the above two cases good since they preserve reproducibility. Thus, it suffices to bound the probability that the decision about arm j will be different between the two executions when we are in neither of these cases. Then, the worst case bound due to the mass of the uniform probability measure is 16 \u221a 2 log(1/\u03b4)/c\u0303i\u221a 2 log(1/\u03b4)/ci . This implies that the probability mass of the bad event is at most 16 \u221a ci/c\u0303i = 16 \u221a 1/\u03b2. A union bound over all arms and batches yields that the probability that two distinct executions differ in at least one pull is Pr[(a1, . . . , aT ) \u0338= (a\u20321, . . . , a\u2032T )] \u2264 16KB \u221a 1/\u03b2 + 2\u03b4 , and since \u03b4 \u2264 \u03c1 it suffices to pick \u03b2 = 768K2B2/\u03c12. We now focus on the regret of our algorithm. Let us set \u03b4 = 1/(KTB). Fix a sub-optimal arm j and assume that batch i+ 1 was the last batch that is was active. We obtain that the total number of pulls of this arm is Tj \u2264 c\u0303i+1 \u2264 \u03b2q(1 + ci) \u2264 \u03b2q(1 + 8 log(1/\u03b4)/\u22062j ) From the reproducibility analysis, it suffices to take \u03b2 of order K2 log2(T )/\u03c12 and so E[RT ] \u2264 T \u00b71/T+E[RT |E ] = 1+ \u2211 j:\u2206j>0 \u2206j E[Tj |E ] \u2264 C \u00b7K2 log2(T ) \u03c12 \u2211 j:\u2206j>0 ( \u2206j + log(KT log(T )) \u2206j ) , for some absolute constant C > 0. Notice that the above analysis which uses a naive union bound does not yield the desired regret bound. We next provide a more tight analysis of the same algorithm that achieves the regret bound of Theorem 4. Improved Analysis (The Proof of Theorem 4) In the previous analysis, we used a union bound over all arms and all batches in order to control the probability of the bad event. However, we can obtain an improved regret bound as follows. Fix a sub-optimal arm i \u2208 [K] and let t be the first round that it appears in the bad event. We claim that after a constant number of rounds, this arm will be eliminated. This will shave the O(log2(T )) factor from the regret bound. Essentially, as indicated in the previous proof, the bad event corresponds to the case where the randomness of the cut-off threshold U can influence the decision of whether the algorithm eliminates an arm or not. The intuition is that during the rounds t and t+1, given that the two intervals intersected at round t, we know that the probability that they intersect again is quite small since the interval of the optimal mean is moving upwards, the interval of the sub-optimal mean is concentrating around the guess and the two estimations have been moved by at most a constant times the interval\u2019s length. Since the bad event occurs at round t, we know that \u00b5\u0302 (t) j \u2208 [ \u00b5\u0302 (t) t\u22c6 \u2212 Ut \u2212 5U\u0303t, \u00b5\u0302 (t) t\u22c6 \u2212 Ut/2 + 3U\u0303t ] . In the above \u00b5\u0302tt\u22c6 is the estimate of the optimal mean at round t whose index is denoted by t \u22c6. Now assume that the bad event for arm j also occurs at round t+ k. Then, we have that \u00b5\u0302 (t+k) j \u2208 [ \u00b5\u0302 (t+k) (t+k)\u22c6 \u2212 Ut+k \u2212 5U\u0303t+k, \u00b5\u0302 (t+k) (t+k)\u22c6 \u2212 Ut+k/2 + 3U\u0303t+k ] . First, notice that since the concentration inequality under event E holds for rounds t, t+ k we have that \u00b5\u0302(t+k)j \u2264 \u00b5\u0302 (t) j + U\u0303t + U\u0303t+k. Thus, combining it with the above inequalities gives us \u00b5\u0302 (t+k) (t+k)\u22c6 \u2212 Ut+k \u2212 5U\u0303t+k \u2264 \u00b5\u0302 (t+k) j \u2264 \u00b5\u0302 (t) j + U\u0303t + U\u0303t+k \u2264 \u00b5\u0302 (t) t\u22c6 \u2212 Ut/2 + 4U\u0303t + U\u0303t+k. We now compare \u00b5\u0302(t)t\u22c6 , \u00b5\u0302 (t+k) (t+k)\u22c6 . Let o denote the optimal arm. We have that \u00b5\u0302 (t+k) (t+k)\u22c6 \u2265 \u00b5\u0302 (t+k) o \u2265 \u00b5o \u2212 U\u0303t+k \u2265 \u00b5t\u22c6 \u2212 U\u0303t+k \u2265 \u00b5\u0302 (t) t\u22c6 \u2212 U\u0303t \u2212 U\u0303t+k. This gives us that \u00b5\u0302 (t) t\u22c6 \u2212 Ut+k \u2212 6U\u0303t+k \u2212 U\u0303t \u2264 \u00b5\u0302 (t+k) (t+k)\u22c6 \u2212 Ut+k \u2212 5U\u0303t+k. Thus, we have established that \u00b5\u0302 (t) t\u22c6 \u2212 Ut+k \u2212 6U\u0303t+k \u2212 U\u0303t \u2264 \u00b5\u0302 (t) t\u22c6 \u2212 Ut/2 + 4U\u0303t + U\u0303t+k =\u21d2 Ut+k \u2265 Ut/2\u2212 7U\u0303t+k \u2212 5U\u0303t \u2265 Ut/2\u2212 12U\u0303t. Since \u03b2 \u2265 2304, we get that 12U\u0303t \u2264 Ut/4. Thus, we get that Ut+k \u2265 Ut/4. Notice that Ut+k Ut = \u221a ct ct+k , thus it immediately follows that ct ct+k \u2265 1 16 =\u21d2 q t+1 \u2212 1 qt+k+1 \u2212 1 \u2265 1 16 =\u21d2 16 ( 1\u2212 1 qt+1 ) \u2265 qk \u2212 1 qt+1 =\u21d2 qk \u2264 16 + 1 qt+1 \u2264 17 =\u21d2 k log q \u2264 log 17 =\u21d2 k \u2264 5, when we pick B = log(T ) batches. Thus, for every arm the bad event can happen at most 6 times, by taking a union bound over the K arms we see that the probability that our algorithm is not reproducible is at most O(K \u221a 1/\u03b2), so picking \u03b2 = \u0398(K2/\u03c12) suffices to get the result. C THE PROOF OF THEOREM 6: Theorem. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm for the stochastic ddimensional linear bandit problem with K arms whose expected regret is E[RT ] \u2264 O ( K2 \u03c12 \u221a dT log(KT ) ) , and its running time is polynomial in d,K, T and 1/\u03c1. Proof. Let c, C be the constants hidden in Lemma 5, i.e., the size of the multi-set is in [cd log(1/\u03b4)/\u03b52, Cd log(1/\u03b4)/\u03b52]. We know that the size of each batch ni \u2208 [cqi, Cqi] (see Lemma 5), so by the end of the B \u2212 1 batch we will have less than nB pulls left. Hence, the number of batches is at most B. We first define the event E that the estimates of all arms after the end of each batch are accurate, i.e., for every active arm a at the beginning of the i-th batch, at the end of the batch we have that\u2223\u2223\u2223\u2329a, \u03b8\u0302i \u2212 \u03b8\u22c6\u232a\u2223\u2223\u2223 \u2264 \u03b5\u0303i. Since \u03b4 = 1/(KT 2) and there are at most T batches and K active arms in each batch, a simple union bound shows that E happens with probability at least 1 \u2212 1/T. We condition on the event E throughout the rest of the proof. We now argue about the regret bound of our algorithm. We first show that any optimal arm a\u2217 will not get eliminated. Indeed, consider any sub-optimal arm a \u2208 [K] and any batch i \u2208 [B]. Under the event E we have that \u27e8a, \u03b8\u0302i\u27e9 \u2212 \u27e8a\u2217, \u03b8\u0302i\u27e9 \u2264 (\u27e8a, \u03b8\u2217\u27e9+ \u03b5\u0303i)\u2212 (\u27e8a\u2217, \u03b8\u2217\u27e9 \u2212 \u03b5\u0303i) < 2\u03b5\u0303i < \u03b5i + \u03b5i. Next, we need to bound the number of times we pull some fixed suboptimal arm a \u2208 [K]. We let \u2206 = \u27e8a\u2217 \u2212 a, \u03b8\u2217\u27e9 denote the gap and we let i be the smallest integer such that \u03b5i < \u2206/4. We claim that this arm will get eliminated by the end of batch i. Indeed, \u27e8a\u2217, \u03b8\u0302i\u27e9 \u2212 \u27e8a, \u03b8\u0302i\u27e9 \u2265 (\u27e8a\u2217, \u03b8\u0302i\u27e9 \u2212 \u03b5\u0303i)\u2212 (\u27e8a, \u03b8\u0302i\u27e9+ \u03b5\u0303i) = \u2206\u2212 2\u03b5\u0303i > 4\u03b5i \u2212 2\u03b5\u0303i > \u03b5\u0303i + \u03b5i. This shows that during any batch i, all the active arms have gap at most 4\u03b5i\u22121. Thus, the regret of the algorithm conditioned on the event E is at most B\u2211 i=1 4ni\u03b5i\u22121 \u2264 4\u03b2C B\u2211 i=1 qi \u221a d log(KT 2)/qi\u22121 \u2264 6\u03b2Cq \u221a d log(KT ) B\u22121\u2211 i=0 qi/2 \u2264 O ( \u03b2qB/2+1 \u221a d log(KT ) ) = O ( K2 \u03c12 qB/2+1 \u221a d log(KT ) ) = O ( K2 \u03c12 q \u221a dT log(KT ) ) . Thus, the overall regret is bounded by \u03b4 \u00b7 T + (1 \u2212 \u03b4) \u00b7 O ( K2 \u03c12 q \u221a dT log(KT ) ) = O ( K2 \u03c12 q \u221a dT log(KT ) ) . We now argue about the reproducibility of our algorithm. The analysis follows in a similar fashion as in Theorem 4. Let \u03b8\u0302i, \u03b8\u0302\u2032i be the LSE after the i-th batch, under two different executions of the algorithm and assume that the set of active arms. We condition on the event E \u2032 for the other execution as well. Assume that the set of active arms is the same under both executions at the beginning of batch i. Notice that since the set that is guaranteed by Lemma 5 is computed by a deterministic algorithm, both executions will pull the same arms in batch i. Consider a suboptimal arm a and let ai\u2217 = argmaxa\u2208A\u27e8\u03b8\u0302i, a\u27e9, a\u2032i\u2217 = argmaxa\u2208A\u27e8\u03b8\u0302\u2032i, a\u27e9. Under the event E \u2229 E \u2032 we have that |\u27e8a, \u03b8\u0302i \u2212 \u03b8\u0302\u2032i\u27e9| \u2264 2\u03b5\u0303i, |\u27e8ai\u2217 , \u03b8\u0302i \u2212 \u03b8\u0302\u2032i\u27e9| \u2264 2\u03b5\u0303i, and |\u27e8a\u2032i\u2217 , \u03b8\u0302\u2032i\u27e9 \u2212 \u27e8ai\u2217 , \u03b8\u0302i\u27e9| \u2264 2\u03b5\u0303i. Notice that, since the randomness of \u03b5i is shared, if \u27e8a, \u03b8\u0302i\u27e9 + \u03b5\u0303i \u2265 \u27e8ai\u2217 , \u03b8\u0302i\u27e9 \u2212 \u03b5i + 4\u03b5\u0303i, then the arm a will not be eliminated after the i-th batch in some other execution of the algorithm as well. Similarly, if \u27e8a, \u03b8\u0302i\u27e9+ \u03b5\u0303i < \u27e8ai\u2217 , \u03b8\u0302i\u27e9\u2212 \u03b5i\u2212 4\u03b5\u0303i the the arm a will get eliminated after the i-th batch in some other execution of the algorithm as well. In particular, this means that if \u27e8a, \u03b8\u0302i\u27e9\u22122\u03b5\u0303i > \u27e8ai\u2217 , \u03b8\u0302i\u27e9+\u03b5\u0303i\u2212\u03b5i/2 then the arm a will not get eliminated in some other execution of the algorithm and if \u27e8a, \u03b8\u0302i\u27e9+5\u03b5\u0303i < \u27e8ai\u2217 , \u03b8\u0302i\u27e9 \u2212 \u03b5i then the arm j will also get eliminated in some other execution of the algorithm with probability 1 under the event E \u2229E \u2032. Thus, it suffices to bound the probability that the decision about arm j will be different between the two executions when we are in neither of these cases. Then, the worst case bound due to the mass of the uniform probability measure is 16 \u221a d log(1/\u03b4)/c\u0303i\u221a d log(1/\u03b4)/ci . This implies that the probability mass of the bad event is at most 16 \u221a ci/c\u0303i = 16 \u221a 1/\u03b2. A naive union bound would require us to pick \u03b2 = \u0398(K2 log2 T/\u03c12). We next show to avoid the log2 T factor. Fix a sub-optimal arm a \u2208 [K] and let t be the first round that it appears in the bad event. Since the bad event occurs at round t, we know that \u27e8a, \u03b8\u0302t\u27e9 \u2208 [ \u27e8at\u2217 , \u03b8\u0302t\u27e9 \u2212 \u03b5t \u2212 5\u03b5\u0303t, \u27e8at\u2217 , \u03b8\u0302t\u27e9 \u2212 \u03b5t/2 + 3\u03b5\u0303t ] . In the above, at\u2217 is the optimal arm at round t w.r.t. the LSE. Now assume that the bad event for arm a also occurs at round t+ k. Then, we have that \u27e8a, \u03b8\u0302t+k\u27e9 \u2208 [ \u27e8a(t+k)\u2217 , \u03b8\u0302t+k\u27e9 \u2212 \u03b5t+k \u2212 5\u03b5\u0303t+k, \u27e8a(t+k)\u2217 , \u03b8\u0302t+k\u27e9 \u2212 \u03b5t/2 + 3\u03b5\u0303t+k ] . First, notice that since the concentration inequality under event E holds for rounds t, t+ k we have that \u27e8a, \u03b8\u0302t+k\u27e9 \u2264 \u27e8a, \u03b8\u0302t\u27e9+ \u03b5\u0303t + \u03b5\u0303t+k. Thus, combining it with the above inequalities gives us \u27e8a(t+k)\u2217 , \u03b8\u0302t+k\u27e9\u2212 \u03b5t+k \u2212 5\u03b5\u0303t+k \u2264 \u27e8a, \u03b8\u0302t+k\u27e9 \u2264 \u27e8a, \u03b8\u0302t\u27e9+ \u03b5\u0303t + \u03b5\u0303t+k \u2264 \u27e8at\u2217 , \u03b8\u0302t\u27e9\u2212 \u03b5t/2+ 4\u03b5\u0303t + \u03b5\u0303t+k. We now compare \u27e8at\u2217 , \u03b8\u0302t\u27e9, \u27e8a(t+k)\u2217 , \u03b8\u0302t+k\u27e9. Let a\u2217 denote the optimal arm. We have that \u27e8a(t+k)\u2217 , \u03b8\u0302t+k\u27e9 \u2265 \u27e8a\u2217, \u03b8\u0302t+k\u27e9 \u2265 \u27e8a\u2217, \u03b8\u2217\u27e9 \u2212 \u03b5\u0303t+k \u2265 \u27e8at\u2217 , \u03b8\u2217\u27e9 \u2212 \u03b5\u0303t+k \u2265 \u27e8at\u2217 , \u03b8\u0302t\u27e9 \u2212 \u03b5\u0303t+k \u2212 \u03b5\u0303t. This gives us that \u27e8at\u2217 , \u03b8\u0302t\u27e9 \u2212 \u03b5t+k \u2212 6\u03b5\u0303t+k \u2212 \u03b5\u0303t \u2264 \u27e8a(t+k)\u2217 , \u03b8\u0302t+k\u27e9 \u2212 \u03b5t+k \u2212 5\u03b5\u0303t+k. Thus, we have established that \u27e8at\u2217 , \u03b8\u0302t\u27e9 \u2212 \u03b5t+k \u2212 6\u03b5\u0303t+k \u2212 \u03b5\u0303t \u2264 \u27e8at\u2217 , \u03b8\u0302t\u27e9 \u2212 \u03b5t/2 + 4\u03b5\u0303t + \u03b5\u0303t+k =\u21d2 \u03b5t+k \u2265 \u03b5t/2\u2212 7\u03b5\u0303t+k \u2212 5\u03b5\u0303t \u2265 \u03b5t/2\u2212 12\u03b5\u0303t. Since \u03b2 \u2265 2304, we get that 12\u03b5\u0303t \u2264 \u03b5t/4. Thus, we get that \u03b5t+k \u2265 \u03b5t/4. Notice that \u03b5t+k \u03b5t = \u221a qt qt+k , thus it immediately follows that qt qt+k \u2265 1 16 =\u21d2 qk \u2264 16 =\u21d2 k log q \u2264 log 16 =\u21d2 k \u2264 4, when we pick B = log(T ) batches. Thus, for every arm the bad event can happen at most 5 times, by taking a union bound over the K arms we see that the probability that our algorithm is not reproducible is at most O(K \u221a 1/\u03b2), so picking \u03b2 = \u0398(K2/\u03c12) suffices to get the result. D NAIVE APPLICATION OF ALGORITHM 3 WITH INFINITE ACTION SPACE: We use a 1/T 1/(4d+2)\u2212net that has size at most (3T ) d 4d+2 . Let A\u2032 be the new set of arms. We then run Algorithm 3 using A\u2032. This gives us the following result, that is proved right after. Corollary 12. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There is a \u03c1-reproducible algorithm for the stochastic ddimensional linear bandit problem with infinite arms whose expected regret is at most E[RT ] \u2264 C \u00b7 T 4d+1 4d+2 \u03c12 \u221a d log(T ) , where C > 0 is an absolute numerical constant. Proof. Since K \u2264 (3T ) d 4d+2 , we have that T sup a\u2208A\u2032 \u27e8a, \u03b8\u2217\u27e9 \u2212E [ T\u2211 i=1 \u27e8at, \u03b8\u2217\u27e9 ] \u2264 O ( (3T ) 2d 4d+2 \u03c12 \u221a dT log ( T (3T ) d 4d+2 )) = O ( T 4d+1 4d+2 \u03c12 \u221a d log(T ) ) Comparing to the best arm in A, we have that: T sup a\u2208A \u27e8a, \u03b8\u2217\u27e9 \u2212E [ T\u2211 i=1 \u27e8at, \u03b8\u2217\u27e9 ] = ( T sup a\u2208A \u27e8a, \u03b8\u2217\u27e9 \u2212 T sup a\u2208A\u2032 \u27e8a, \u03b8\u2217\u27e9 ) + ( T sup a\u2208A\u2032 \u27e8a, \u03b8\u2217\u27e9 \u2212E [ T\u2211 i=1 \u27e8at, \u03b8\u2217\u27e9 ]) Our choice of the 1/T 1/(4d+2)-net implies that for every a \u2208 A there exists some a\u2032 \u2208 A\u2032 such that ||a \u2212 a\u2032||2 \u2264 1/T 1/(4d+2). Thus, supa\u2208A\u27e8a, \u03b8\u2217\u27e9 \u2212 supa\u2032\u2208A\u2032\u27e8a\u2032, \u03b8\u2217\u27e9 \u2264 ||a \u2212 a\u2032||2||\u03b8\u2217||2 \u2264 1/T 1/(4d+2). Thus, the total regret is at most T \u00b7 1/T 1/(4d+2) +O ( T 4d+1 4d+2 \u03c12 \u221a d log(T ) ) = O ( T 4d+1 4d+2 \u03c12 \u221a d log(T ) ) . E THE PROOF OF THEOREM 10: Theorem. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm (Algorithm 4) for the stochastic d-dimensional linear bandit problem with infinite action set whose expected regret is E[RT ] \u2264 O ( d4 log(d) log2 log(d) log log log(d) \u03c12 \u221a T log3/2(T ) ) , and its running time is polynomial in T d and 1/\u03c1. Proof. First, the algorithm is \u03c1-reproducible since in each batch we use a reproducible LSE subroutine with parameter \u03c1\u2032 = \u03c1/B. This implies that Pr[(a1, ..., aT ) \u0338= (a\u20321, ..., a\u2032T )] = Pr[\u2203i \u2208 [B] : \u03b8\u0302i was not reproducible] \u2264 \u03c1 . Let us fix a batch iteration i \u2208 [B \u2212 1]. Set Ci be the core set computed by Lemma 7. The algorithm first pulls ni = Cd4 log(d/\u03b4) log2 log(d) log log log(d) \u03b52i\u03c1 \u20322 times each one of the arms of the i-th core set Ci, as indicated by Lemma 9 and computes the LSE \u03b8\u0302i in a reproducible way using the algorithm of Lemma 9. Let E be the event that over all batches the estimations are correct. We pick \u03b4 = 1/(2|A\u2032|T 2) so that this good event does hold with probability at least 1\u2212 1/T . Our goal is to control the expected regret which can be written as E[RT ] = T sup a\u2208A \u27e8a, \u03b8\u22c6\u27e9 \u2212E T\u2211 t=1 \u27e8at, \u03b8\u22c6\u27e9 . We have that T sup a\u2208A \u27e8a, \u03b8\u22c6\u27e9 \u2212 T sup a\u2032\u2208A\u2032 \u27e8a\u2032, \u03b8\u22c6\u27e9 \u2264 1 , since A\u2032 is a deterministic 1/T -net of A. Also, let us set the expected regret of the bounded action sub-problem as E[R\u2032T ] = T sup a\u2032\u2208A\u2032 \u27e8a\u2032, \u03b8\u22c6\u27e9 \u2212E T\u2211 t=1 \u27e8at, \u03b8\u22c6\u27e9 . We can now employ the analysis of the finite arm case. During batch i, any active arm has gap at most 4\u03b5i\u22121, so the instantaneous regret in any round is not more than 4\u03b5i\u22121. The expected regret conditional on the good event E is upper bounded by E[R\u2032T |E ] \u2264 B\u2211 i=1 4Mi\u03b5i\u22121 , where Mi is the total number of pulls in batch i (using the reproducibility blow-up) and \u03b5i\u22121 is the error one would achieve by drawing qi samples (ignoring the blow-up). Then, for some absolute constant C > 0, we have that E[R\u2032T |E ] \u2264 B\u2211 i=1 4 ( qi d3 log(d) log2 log(d) log log log(d) log2 T \u03c12 ) \u00b7 \u221a d2 log(T )/qi\u22121 , which yields that E[R\u2032T |E ] \u2264 C d4 log(d) log2 log(d) log log log(d) log(T ) \u221a log(T ) \u03c12 \u00b7 S , where we set S := B\u2211 i=1 qi q(i\u22121)/2 = q1/2 B\u2211 i=1 qi/2 = q(1+B)/2 . We pick B = log(T ) and get that, if q = T 1/B then S = \u0398( \u221a T ). We remark that this choice of q is valid since B\u2211 i=1 qi = qB+1 \u2212 q q \u2212 1 = \u0398(qB)\u2212 1 \u2265 T\u03c1 2 d3 log(d) log2 log(d) log log log(d) . Hence, we have that E[R\u2032T |E ] \u2264 O ( d4 log(d) log2 log(d) log log log(d) \u03c12 \u221a T log3/2(T ) ) . Note that when E does not hold, we can bound the expected regret by 1/T \u00b7 T = 1. This implies that the overall regret E[RT ] \u2264 2 + E[R\u2032T |E ] and so it satisfies the desired bound and the proof is complete. F DEFERRED LEMMATA: F.1 THE PROOF OF LEMMA 8 Proof. Consider the distribution \u03c0 that is a 2-approximation to the optimal G-design and has support |C| = O(d log log d). Let C\u2032 be the set of arms in the support such that \u03c0(a) \u2264 c/d log d. We consider \u03c0\u0303 = (1 \u2212 x)\u03c0 + xa, where a \u2208 C\u2032 and x will be specified later. Consider now the matrix V (\u03c0\u0303). Using the Sherman-Morrison formula, we have that V (\u03c0\u0303)\u22121 = 1 1\u2212 x V (\u03c0)\u22121 \u2212 xV (\u03c0) \u22121aa\u22a4V (\u03c0)\u22121 (1\u2212 x)2 ( 1 + 11\u2212x ||a|| 2 V (\u03c0)\u22121 ) = 1 1\u2212 x ( V (\u03c0)\u22121 \u2212 xV (\u03c0) \u22121aa\u22a4V (\u03c0)\u22121 1\u2212 x+ ||a||2V (\u03c0)\u22121 ) . Consider any arm a\u2032. Then, ||a\u2032||2V (\u03c0\u0303)\u22121 = 1 1\u2212 x ||a||2V (\u03c0)\u22121 \u2212 x 1\u2212 x \u00b7 (a \u22a4V (\u03c0)\u22121a\u2032)2 1\u2212 x+ ||a||2V (\u03c0)\u22121 \u2264 1 1\u2212 x ||a||2V (\u03c0)\u22121 . Note that we apply this transformation at most O(d log log d) times. Let \u03c0\u0302 be the distribution we end up with. We see that ||a\u2032||2V (\u03c0\u0302)\u22121 \u2264 ( 1 1\u2212 x )cd log log d ||a||2V (\u03c0)\u22121 \u2264 2 ( 1 1\u2212 x )cd log log d d. Notice that there is a constant c\u2032 such that when x = c\u2032/d log d we have that ( 1 1\u2212x )cd log log d \u2264 2. Moreover, notice that the mass of every arm is at least x(1 \u2212 x)|C| \u2265 x \u2212 |C|x2 = c\u2032/(d log(d)) \u2212 c\u2032\u2032d log log d/(d2 log2(d)) \u2265 c/(d log(d)), for some absolute numerical constant c > 0. This concludes the claim. F.2 THE PROOF OF LEMMA 9 Proof. The proof works when we can treat \u2126(\u2308d log(1/\u03b4)\u03c0(a)/\u03b52\u2309) as \u2126(d log(1/\u03b4)\u03c0(a)/\u03b52), i.e., as long as \u03c0(a) = \u2126(\u03b52/d log(1/\u03b4)). In the regime we are in, this point is handled thanks to Lemma 8. Combining the following proof with Lemma 8, we can obtain the desired result. We underline that we work in the fixed design setting: the arms ai are deterministically chosen independently of the rewards ri. Assume that the core set of Lemma 7 is the set C. Fix the multi-set S = {(ai, ri) : i \u2208 [M ]}, where each arm a lies in the core set and is pulled na = \u0398(d\u03c0(a) log(1/\u03b4)/\u03b5 2) times. Hence, we have that M = \u2211 a\u2208C na = \u0398 ( d log(1/\u03b4)/\u03b52) ) . Let also V = \u2211 i\u2208[M ] aia \u22a4 i . The least-squares estimator can be written as \u03b8 (\u03b5) LSE = V \u22121 \u2211 i\u2208[M ] airi = V \u22121 \u2211 a\u2208C a \u2211 i\u2208[na] ri(a) , where each a lies in the core set (deterministically) and ri(a) is the i-th reward generated independently by the linear regression process \u27e8\u03b8\u22c6, a\u27e9+\u03be, where \u03be is a fresh zero mean sub-gaussian random variable. Our goal is to reproducibly estimate the value \u2211 i\u2208[na] ri(a) for any a. This is sufficient since two independent executions of the algorithm share the set C and na for any a. Note that the above sum is a random variable. In the following, we condition on the high-probability event that the average reward of the arm a is \u03b5-close to the expected one, i.e., the value \u27e8\u03b8\u22c6, a\u27e9. This happens with probability at least 1 \u2212 \u03b4/(2|C|), given \u2126\u0303(d\u03c0(a) log(d/\u03b4)/\u03b52) samples from arm a \u2208 C. In order to guarantee reproducibility, we will apply a result from Impagliazzo et al. (2022). Since we will union bound over all arms in the core set and |C| = O(d log log(d)) (via Lemma 7), we will make use of a (\u03c1/|C|)-reproducible algorithm that gives an estimate v(a) \u2208 R such that |\u27e8\u03b8\u22c6, a\u27e9 \u2212 v(a)| \u2264 \u03c4 , with probability at least 1\u2212 \u03b4/(2|C|). For \u03b4 < \u03c1, the algorithm uses Sa = \u2126 ( d2 log(d/\u03b4) log2 log(d) log log log(d)/(\u03c12\u03c42) ) many samples from the linear regression with fixed arm a \u2208 C. Since we have conditioned on the randomness of ri(a) for any i, we get\u2223\u2223\u2223\u2223\u2223\u2223 1na \u2211 i\u2208[na] ri(a)\u2212 v(a) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b5+ \u03c4 , with probability at least 1 \u2212 \u03b4/|C|. Hence, by repeating this approach for all arms in the core set, we set \u03b8SQ = V \u22121 \u2211 a\u2208C a na v(a). Let us condition on the randomness of the estimate \u03b8 (\u03b5) LSE. We have that sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8\u22c6\u27e9| \u2264 sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8(\u03b5)LSE\u27e9|+ sup a\u2032\u2208A |\u27e8a\u2032, \u03b8(\u03b5)LSE \u2212 \u03b8 \u22c6\u27e9| . Note that the second term is \u03b5 with probability at least 1\u2212 \u03b4 via Lemma 5. Our next goal is to tune the accuracy \u03c4 \u2208 (0, 1) so that the first term yields another \u03b5 error. For the first term, we have that sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8(\u03b5)LSE\u27e9| \u2264 sup a\u2032\u2208A \u2223\u2223\u2223\u2223\u2223\u27e8a\u2032, V \u22121\u2211 a\u2208C a na (\u03b5+ \u03c4)\u27e9 \u2223\u2223\u2223\u2223\u2223 Note that V = Cd log(d/\u03b4)\u03b52 \u2211 a\u2208C \u03c0(a)aa \u22a4 and so V \u22121 = \u03b5 2 Cd log(d/\u03b4)V (\u03c0) \u22121, for some absolute constant C > 0. This implies that sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8(\u03b5)LSE\u27e9| \u2264 (\u03b5+ \u03c4) sup a\u2032\u2208A \u2223\u2223\u2223\u2223\u2223 \u2329 a\u2032, \u03b52 Cd log(d/\u03b4) V (\u03c0)\u22121 \u2211 a\u2208C Cd log(d/\u03b4)\u03c0(a) \u03b52 a \u232a\u2223\u2223\u2223\u2223\u2223 . Hence, we get that sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8(\u03b5)LSE\u27e9| \u2264 (\u03b5+ \u03c4) sup a\u2032\u2208A \u2223\u2223\u2223\u2223\u2223 \u2329 a\u2032, V (\u03c0)\u22121 \u2211 a\u2208C \u03c0(a)a \u232a\u2223\u2223\u2223\u2223\u2223 . Consider a fixed arm a\u2032 \u2208 A. Then,\u2223\u2223\u2223\u2223\u2223 \u2329 a\u2032, V (\u03c0)\u22121 \u2211 a\u2208C \u03c0(a)a \u232a\u2223\u2223\u2223\u2223\u2223 \u2264\u2211 a\u2208C \u03c0(a) \u2223\u2223\u27e8a\u2032, V (\u03c0)\u22121a\u27e9\u2223\u2223 \u2264 \u2211 a\u2208C \u03c0(a) ( 1 + \u2223\u2223\u27e8a\u2032, V (\u03c0)\u22121a\u27e9\u2223\u22232) = 1 + \u2211 a\u2208C \u03c0(a) \u2223\u2223\u27e8a\u2032, V (\u03c0)\u22121a\u27e9\u2223\u22232 = 1 + ||a\u2032||2V (\u03c0)\u22121 \u2264 2d+ 1 , where the last inequality follows from the fact that \u03c0 is a 2-approximation of the G-optimal design. Hence, in total, by picking \u03c4 = \u03b5, we get that sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8\u22c6\u27e9| \u2264 16d\u03b5 . Thus, for any \u03b5 > 0, the total number of pulls of each arm is \u2126 ( d4 log(d/\u03b4) log2 log(d) log log log(d)/(\u03c12\u03b52) ) , to get sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8\u22c6\u27e9| \u2264 \u03b5 . G COMPUTATIONAL PERFORMANCE OF ALGORITHM 4: In this appendix, we discuss the barriers towards computational efficiency regarding Algorithm 4. The reasons why Algorithm 4 is computationally inefficient are the following: (a) we have to compute the arm in the set of active arms that has maximum correlation with the guess \u03b8\u0302i, (b) we have to eliminate arms based on this value and (c) we have to run at each batch the Frank-Wolfe algorithm (or some other optimization method needed for Lemma 5) in order to obtain an approximate G-optimal design. As a minimal assumption in what follows, we focus on the case where the action set A is convex and we have access to a separation oracle for it. Note that executing both (a) and (b) naively requires time exponential in d. However, on the one side, arm elimination (issue (b)) reduces to finding the intersection of the current active set with a halfspace H whose normal vector is \u03b8 and the threshold is, roughly speaking, the maximum correlation. This maximum correlation can also be computed efficiently. Finding an arm with (almost) maximum correlation relates to the problem of finding a point that maximizes a linear objective under the constraint that the point lies in the intersection of the active arm set with some linear constraints. Thus, we can use the ellipsoid algorithm to implement this step. The above discussion deals with issues (a) and (b) and, essentially, states that even with infinitely many actions, one could implement these steps efficiently. We now focus on issue (c). The FrankWolfe method first requires a proper initialization. As mentioned in Lattimore & Szepesva\u0301ri (2020), if the starting point is chosen to be the uniform distribution over A\u2032, then the number of iterations before getting a 2-approximate optimal design is roughly O\u0303(d). The issue is that since A\u2032 is exponential in d, it is not clear how to work with such an initialization efficiently. Notably there is a different initialization (Fedorov, 2013; Lattimore et al., 2020) with support O(d) for which the method runs in O(d log log(d)) rounds (see Note 3 at Section 21.2 of Lattimore & Szepesva\u0301ri (2020) and Lattimore et al. (2020)). There are two issues: first, one requires an oracle to provide this good initialization. Second, each iteration of the Frank-Wolfe method (with current design guess \u03c0) requires computing a point in the current active set with maximum V (\u03c0)\u22121-norm. As noted in Todd (2016), a good initialization for finding a G-optimal design, i.e., a minimum volume enclosing ellipsoid (MVEE) should be sufficiently sparse (compared to the number of active arms) and assign positive mass to arms that correspond to extreme points, i.e., points that are close to the border of MVEE. The work of Kumar & Yildirim (2005) provides an initial core set that depends only on d but not on the number of points. The algorithm works as follows: it runs for d iterations and, in each round, it adds 2 arms into the core set. Initially, we set the core set C0 = \u2205 and let \u03a8 = {0}. In each iteration i \u2208 [d], the algorithm draws a random direction vi in the orthogonal complement of \u03a8 (this step is reproducible thanks to the shared randomness) and computes the vectors in the active arms\u2019 set with the maximum and the minimum correlation with vi, say a+i , a \u2212 i . It then extends C0 \u2190 C0 \u222a {a+i , a \u2212 i } and sets \u03a8 \u2190 span(\u03a8, {a + i \u2212 a \u2212 i }). Hence, the runtime of this algorithm corresponds to the runtime of the tasks maxa\u2208A\u2032\u27e8a, vi\u27e9 and mina\u2208A\u2032\u27e8a, vi\u27e9. One can efficiently approximate these values using the ellipsoid algorithm and hence efficiently initialize the Frank-Wolfe algorithm as in Todd (2016) (e.g., set the weights uniformly 1/(2d)). Our second challenge deals with finding a point in the active arm set with maximum V (\u03c0)\u22121norm for some current guess \u03c0. Even if the current active set is a polytope, finding an exact norm maximizer is NP-hard (Freund & Orlin, 1985; Mangasarian & Shiau, 1986)2. Hence, one should focus on efficient approximation algorithms. We note that even a poly(d)-approximate maximizer is sufficient to get O\u0303(poly(d) \u221a T ) regret. Such an algorithm for polytopes, which gets an 1/d2approximation, is provided in Ye (1992); Vavasis (1993). As a general note, if we assume that we have access to an oracle O that computes a 2-approximate G-optimal design in time TO, then our Algorithm 4 runs in time polynomial in TO. 2In fact, even finding a constant factor approximation, for some appropriate constant, is NP-hard (Bellare & Rogaway, 1993).", "agent_review": [{"role": "user", "content": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Replicable Bandits\n\n1 INTRODUCTION: In order for scientific findings to be valid and reliable, the experimental process must be repeatable, and must provide coherent results and conclusions across these repetitions. In fact, lack of reproducibility has been a major issue in many scientific areas; a 2016 survey that appeared in Nature (Baker, 2016a) revealed that more than 70% of researchers failed in their attempt to reproduce another researcher\u2019s experiments. What is even more concerning is that over 50% of them failed to reproduce their own findings. Similar concerns have been raised by the machine learning community, e.g., the ICLR 2019 Reproducibility Challenge (Pineau et al., 2019) and NeurIPS 2019 Reproducibility Program (Pineau et al., 2021), due to the to the exponential increase in the number of publications and the reliability of the findings. The aforementioned empirical evidence has recently led to theoretical studies and rigorous definitions of reproducibility. In particular, the works of Impagliazzo et al. (2022) and Ahn et al. (2022) considered reproducibility as an algorithmic property through the lens of (offline) learning and convex optimization, respectively. In a similar vein, in the current work, we introduce the notion of reproducibility in the context of interactive learning and decision making. In particular, we study reproducible policy design for the fundamental setting of stochastic bandits. A multi-armed bandit (MAB) is a one-player game that is played over T rounds where there is a set of different arms/actions A of size |A| = K (in the more general case of linear bandits, we can consider even an infinite number of arms). In each round t = 1, 2, . . . , T , the player pulls an arm at \u2208 A and receives a corresponding reward rt. In the stochastic setting, the rewards of each arm are sampled in each round independently, from some fixed but unknown, distribution supported on [0, 1]. Crucially, each arm has a potentially different reward distribution, but the distribution of each arm is fixed over time. A bandit algorithm A at every round t takes as input the sequence of arm-reward pairs that it has seen so far, i.e., (a1, r1), . . . , (at\u22121, rt\u22121), then uses (potentially) some internal randomness \u03be to pull an arm at \u2208 A and, finally, observes the associated reward rt \u223c Dat . We propose the following natural notion of a reproducible bandit algorithm, which is inspired by the definition of Impagliazzo et al. (2022). Intuitively, a bandit algorithm is reproducible if two distinct executions of the algorithm, with internal randomness fixed between both runs, but with independent reward realizations, give the exact same sequence of played arms, with high probability. More formally, we have the following definition. Definition 1 (Reproducible Bandit Algorithm). Let \u03c1 \u2208 [0, 1]. We call a bandit algorithm A \u03c1reproducible in the stochastic setting if for any distribution Daj over [0, 1] of the rewards of the j-th arm aj \u2208 A, and for any two executions of A, where the internal randomness \u03be is shared across the executions, it holds that Pr \u03be,r(1),r(2) [( a (1) 1 , . . . , a (1) T ) = ( a (2) 1 , . . . , a (2) T )] \u2265 1\u2212 \u03c1 . Here, a(i)t = A(a (i) 1 , r (i) 1 , ..., a (i) t\u22121, r (i) t\u22121; \u03be) is the t-th action taken by the algorithm A in execution i \u2208 {1, 2}. The reason why we allow for some fixed internal randomness is that the algorithm designer has control over it, e.g., they can use the same seed for their (pseudo)random generator between two executions. Clearly, naively designing a reproducible bandit algorithm is not quite challenging. For instance, an algorithm that always pulls the same arm or an algorithm that plays the arms in a particular random sequence determined by the shared random seed \u03be are both reproducible. The caveat is that the performance of these algorithms in terms of expected regret will be quite poor. In this work, we aim to design bandit algorithms which are reproducible and enjoy small expected regret. In the stochastic setting, the (expected) regret after T rounds is defined as E[RT ] = T max a\u2208A \u00b5a \u2212E [ T\u2211 t=1 \u00b5at ] , where \u00b5a = Er\u223cDa [r] is the mean reward for arm a \u2208 A. In a similar manner, we can define the regret in the more general setting of linear bandits (see, Section 5) Hence, the overarching question in this work is the following: Is it possible to design reproducible bandit algorithms with small expected regret?: At a first glance, one might think that this is not possible, since it looks like reproducibility contradicts the exploratory behavior that a bandit algorithm should possess. However, our main results answer this question in the affirmative and can be summarized in Table 1. 1.1 RELATED WORK: Reproducibility In this work, we introduce the notion of reproducibility in the context of interactive learning and, in particular, in the fundamental setting of stochastic bandits. Close to our work, the notion of a reproducible algorithm in the context of learning was proposed by Impagliazzo et al. (2022), where it is shown how any statistical query algorithm can be made reproducible with a moderate increase in its sample complexity. Using this result, they provide reproducible algorithms for finding approximate heavy-hitters, medians, and the learning of half-spaces. Reproducibility has been also considered in the context of optimization by Ahn et al. (2022). We mention that in Ahn et al. (2022) the notion of a reproducible algorithm is different from our work and that of Impagliazzo et al. (2022), in the sense that the outputs of two different executions of the algorithm do not need to be exactly the same. From a more application-oriented perspective, Shamir & Lin (2022) study irreproducibility in recommendation systems and propose the use of smooth activations (instead of ReLUs) to improve recommendation reproducibility. In general, the reproducibility crisis is reported in various scientific disciplines (Ioannidis, 2005; McNutt, 2014; Baker, 2016b; Goodman et al., 2016; Lucic et al., 2018; Henderson et al., 2018). For more details we refer to the report of the NeurIPS 2019 Reproducibility Program (Pineau et al., 2021) and the ICLR 2019 Reproducibility Challenge (Pineau et al., 2019). Bandit Algorithms Stochastic multi-armed bandits for the general setting without structure have been studied extensively (Slivkins, 2019; Lattimore & Szepesva\u0301ri, 2020; Bubeck et al., 2012b; Auer et al., 2002; Cesa-Bianchi & Fischer, 1998; Kaufmann et al., 2012a; Audibert et al., 2010; Agrawal & Goyal, 2012; Kaufmann et al., 2012b). In this setting, the optimum regret achievable is O ( log(T ) \u2211 i:\u2206i>0 \u2206\u22121 ) ; this is achieved, e.g., by the upper confidence bound (UCB) algorithm of Auer et al. (2002). The setting of d-dimensional linear stochastic bandits is also well-explored (Dani et al., 2008; Abbasi-Yadkori et al., 2011) under the well-specified linear reward model, achieving (near) optimal problem-independent regret of O(d \u221a T log(T )) (Lattimore & Szepesva\u0301ri, 2020). Note that the best-known lower bound is \u2126(d \u221a T ) (Dani et al., 2008) and that the number of arms can, in principle, be unbounded. For a finite number of arms K, the best known upper bound is O(d \u221a T log(K)) (Bubeck et al., 2012a). Our work focuses on the design of reproducible bandit algorithms and we hence consider only stochastic environments. In general, there is also extensive work in adversarial bandits and we refer the interested reader to Lattimore & Szepesva\u0301ri (2020). Batched Bandits While sequential bandit problems have been studied for almost a century, there is much interest in the batched setting too. In many settings, like medical trials, one has to take a lot of actions in parallel and observe their rewards later. The works of Auer & Ortner (2010) and CesaBianchi et al. (2013) provided sequential bandit algorithms which can easily work in the batched setting. The works of Gao et al. (2019) and Esfandiari et al. (2021) are focusing exclusively on the batched setting. Our work on reproducible bandits builds upon some of the techniques from these two lines of work. 2 STOCHASTIC BANDITS AND REPRODUCIBILITY: In this section, we first highlight the main challenges in order to guarantee reproducibility and then discuss how the results of Impagliazzo et al. (2022) can be applied in our setting. 2.1 WARM-UP I: NAIVE REPRODUCIBILITY AND CHALLENGES: Let us consider the stochastic two-arm setting (K = 2) and a bandit algorithm A with two independent executions, A1 and A2. The algorithm Ai plays the sequence 1, 2, 1, 2, . . . until some, potentially random, round Ti \u2208 N after which one of the two arms is eliminated and, from that point, the algorithm picks the winning arm ji \u2208 {1, 2}. The algorithm A is \u03c1-reproducible if and only if T1 = T2 and j1 = j2 with probability 1\u2212 \u03c1. Assume that |\u00b51 \u2212 \u00b52| = \u2206 where \u00b5i is the mean of the distribution of the i-th arm. If we assume that \u2206 is known, then we can run the algorithm for T1 = T2 = C\u22062 log(1/\u03c1) for some universal constant C > 0 and obtain that, with probability 1 \u2212 \u03c1, it will hold that \u00b5\u0302(j)1 \u2248 \u00b51 and \u00b5\u0302 (j) 2 \u2248 \u00b52 for j \u2208 {1, 2}, where \u00b5\u0302(j)i is the estimation of arm\u2019s i mean during execution j. Hence, knowing \u2206 implies that the stopping criterion of the algorithm A is deterministic and that, with high probability, the winning arm will be detected at time T1 = T2. This will make the algorithm \u03c1-reproducible. Observe that when K = 2, the only obstacle to reproducibility is that the algorithm should decide at the same time to select the winning arm and the selection must be the same in the two execution threads. In the presence of multiple arms, there exists the additional constraint that the above conditions must be satisfied during, potentially, multiple arm eliminations. Hence, the two questions arising from the above discussion are (i) how to modify the above approach when \u2206 is unknown and (ii) how to deal with K > 2 arms. A potential solution to the second question (on handling K > 2 arms) is the Execute-Then-Commit (ETC) strategy. Consider the stochastic K-arm bandit setting. For any \u03c1 \u2208 (0, 1), the ETC algorithm with known \u2206 = mini \u2206i and horizon T that uses m = 4\u22062 log(1/\u03c1) deterministic exploration phases before commitment is \u03c1-reproducible. The intuition is exactly the same as in the K = 2 case. The caveats of this approach are that it assumes that \u2206 is known and that the obtained regret is quite unsatisfying. In particular, it achieves regret bounded by m \u2211 i\u2208[K] \u2206i + \u03c1 \u00b7 (T \u2212mK) \u2211 i\u2208[k] \u2206i. Next, we discuss how to improve the regret bound without knowing the gaps \u2206i. Before designing new algorithms, we will inspect the guarantees that can be obtained by combining ideas from previous results in the bandits literature and the recent work in reproducible learning of Impagliazzo et al. (2022). 2.2 WARM-UP II: BANDIT ALGORITHMS AND REPRODUCIBLE MEAN ESTIMATION: First, we remark that we work in the stochastic setting and the distributions of the rewards of the two arms are subgaussian. Thus, the problem of estimating their mean is an instance of a statistical query for which we can use the algorithm of Impagliazzo et al. (2022) to get a reproducible mean estimator for the distributions of the rewards of the arms. Proposition 2 (Reproducible Mean Estimation (Impagliazzo et al., 2022)). Let \u03c4, \u03b4, \u03c1 \u2208 [0, 1]. There exists a \u03c1-reproducible algorithm ReprMeanEstimation that draws \u2126 ( log(1/\u03b4) \u03c42(\u03c1\u2212\u03b4)2 ) samples from a distribution with mean \u00b5 and computes an estimate \u00b5\u0302 that satisfies |\u00b5\u0302 \u2212 \u00b5| \u2264 \u03c4 with probability at least 1\u2212 \u03b4. Notice that we are working in the regime where \u03b4 \u226a \u03c1, so the sample complexity is \u2126 ( log(1/\u03b4) \u03c42\u03c12 ) . The straightforward approach is to try to use an optimal multi-armed algorithm for the stochastic setting, such as UCB or arm-elimination (Even-Dar et al., 2006), combined with the reproducible mean estimator. However, it is not hard to see that this approach does not give meaningful results: if we want to achieve reproducibility \u03c1 we need to call the reproducible mean estimator routine with parameter \u03c1/(KT ), due to the union bound that we need to take. This means that we need to pull every arm at least K2T 2 times, so the regret guarantee becomes vacuous. This gives us the first key insight to tackle the problem: we need to reduce the number of calls to the mean estimator. Hence, we will draw inspiration from the line of work in stochastic batched bandits (Gao et al., 2019; Esfandiari et al., 2021) to derive reproducible bandit algorithms. 3 REPRODUCIBLE MEAN ESTIMATION FOR BATCHED BANDITS: As a first step, we would like to show how one could combine the existing reproducible algorithms of Impagliazzo et al. (2022) with the batched bandits approach of Esfandiari et al. (2021) to get some preliminary non-trivial results. We build an algorithm for the K-arm setting, where the gaps \u2206j are unknown to the learner. Let \u03b4 be the confidence parameter of the arm elimination algorithm and \u03c1 be the reproducibility guarantee we want to achieve. Our approach is the following: let us, deterministically, split the time interval into sub-intervals of increasing length. We treat each subinterval as a batch of samples where we pull each active arm the same number of times and use the reproducible mean estimation algorithm to, empirically, compute the true mean. At the end of each batch, we decide to eliminate some arm j using the standard UCB estimate. Crucially, if we condition on the event that all the calls to the reproducible mean estimator return the same number, then the algorithm we propose is reproducible. Theorem 3. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm (presented in Algorithm 1) for the stochastic bandit problem with K arms and gaps (\u2206j)j\u2208[K] whose expected regret is E[RT ] \u2264 C \u00b7 K2 log2(T ) \u03c12 \u2211 j:\u2206j>0 ( \u2206j + log(KT log(T )) \u2206j ) , where C > 0 is an absolute numerical constant, and its running time is polynomial in K,T and 1/\u03c1. Algorithm 1 Mean-Estimation Based Reproducible Algorithm for Stochastic MAB (Theorem 3) 1: Input: time horizon T, number of arms K, reproducibility \u03c1 2: Initialization: B \u2190 log(T ), q \u2190 T 1/B , c0 \u2190 0, A \u2190 [K], r \u2190 T , \u00b5\u0302a \u2190 0,\u2200a \u2208 A 3: for i = 1 to B \u2212 1 do 4: if \u230aqi\u230b \u00b7 |A| > r then 5: break 6: ci = ci\u22121 + \u230aqi\u230b 7: Pull every arm a \u2208 A for \u230aqi\u230b times 8: for a \u2208 A do 9: \u00b5\u0302a \u2190 ReprMeanEstimation(\u03b4 = 1/(2KTB), \u03c4 = 1, \u221a log(2KTB)/ci, \u03c1 \u2032 = \u03c1/(KB)) \u25b7 Proposition 2 10: r \u2190 r \u2212 |A| \u00b7 \u230aqi\u230b 11: for a \u2208 A do 12: if \u00b5\u0302a < maxa\u2208A \u00b5\u0302a \u2212 2\u03c4 then 13: Remove a from A 14: In the last batch play argmaxa\u2208A \u00b5\u0302a The above result, whose proof can be found in Appendix A, states that, by combining the tools from Impagliazzo et al. (2022) and Esfandiari et al. (2021), we can design a reproducible bandit algorithm with (instance-dependent) expected regret O(K2 log3(T )/\u03c12). Notice that the regret guarantee has an extra K2 log2(T )/\u03c12 factor compared to its non-reproducible counterpart in Esfandiari et al. (2021) (Theorem 5.1). This is because, due to a union bound over the rounds and the arms, we need to call the reproducible mean estimator with parameter \u03c1/(K log(T )). In the next section, we show how to get rid of the log2(T ) by designing a new algorithm. 4 IMPROVED ALGORITHMS FOR REPRODUCIBLE STOCHASTIC BANDITS: While the previous result provides a non-trivial regret bound, it is not optimal with respect to the time horizon T . In this section, we show to improve it by designing a new algorithm, presented in Algorithm 2, which satisfies the guarantees of Theorem 4 and, essentially, decreases the dependence on the time horizon T from log3(T ) to log(T ). Our main result for reproducible stochastic multiarmed bandits with K arms follows. Theorem 4. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm (presented in Algorithm 2) for the stochastic bandit problem with K arms and gaps (\u2206j)j\u2208[K] whose expected regret is E[RT ] \u2264 C \u00b7 K2 \u03c12 \u2211 j:\u2206j>0 ( \u2206j + log(KT log(T )) \u2206j ) , where C > 0 is an absolute numerical constant, and its running time is polynomial in K,T and 1/\u03c1. Note that, compared to the non-reproducible setting, we incur an extra factor of K2/\u03c12 in the regret. The proof can be found in Appendix B. Let us now describe how Algorithm 2 works. We decompose the time horizon into B = log(T ) batches. Without the reproducibility constraint, one could draw qi samples in batch i from each arm and estimate the mean reward. With the reproducibility constraint, we have to boost this: in each batch i, we pull each active arm O(\u03b2qi) times, for some q to be determined, where \u03b2 = O(K2/\u03c12) is the reproducibility blow-up. Using these samples, we compute the empirical mean \u00b5\u0302(i)\u03b1 for any active arm \u03b1. Note that U\u0303i in Algorithm 2 corresponds to the size of the actual confidence interval of the estimation and Ui corresponds to the confidence interval of an algorithm that does not use the \u03b2-blow-up in the number of samples. The novelty of our approach comes from the choice of the interval around the mean of the maximum arm: we pick a threshold uniformly at random from an interval of size Ui/2 around the maximum mean. Then, the algorithm checks whether \u00b5\u0302(i)a + U\u0303i < max \u00b5\u0302 (i) a\u2032 \u2212 U i, where max runs over the active arms a\u2032 in batch i, and eliminates arms accordingly. To prove the result we show that there are three regions that some arm j can be in relative to the confidence interval of the best arm in batch i (cf. Appendix B). If Algorithm 2 Reproducible Algorithm for Stochastic Multi-Armed Bandits (Theorem 4) 1: Input: time horizon T, number of arms K, reproducibility \u03c1 2: Initialization: B \u2190 log(T ), q \u2190 T 1/B , c0 \u2190 0, A0 \u2190 [K], r \u2190 T , \u00b5\u0302a \u2190 0,\u2200a \u2208 A0 3: \u03b2 \u2190 \u230amax{K2/\u03c12, 2304}\u230b 4: for i = 1 to B \u2212 1 do 5: if \u03b2\u230aqi\u230b \u00b7 |Ai| > r then 6: break 7: Ai \u2190 Ai\u22121 8: for a \u2208 Ai do 9: Pull arm a for \u03b2\u230aqi\u230b times 10: Compute the empirical mean \u00b5\u0302(i)\u03b1 11: ci \u2190 ci\u22121 + \u230aqi\u230b 12: c\u0303i \u2190 \u03b2ci 13: U\u0303i \u2190 \u221a 2 ln(2KTB)/c\u0303i 14: Ui \u2190 \u221a 2 ln(2KTB)/ci 15: U i \u2190 Uni[Ui/2, Ui] 16: r \u2190 r \u2212 \u03b2 \u00b7 |Ai| \u00b7 \u230aqi\u230b 17: for a \u2208 Ai do 18: if \u00b5\u0302(i)a + U\u0303i < maxa\u2208Ai \u00b5\u0302 (i) a \u2212 U i then 19: Remove a from Ai 20: In the last batch play argmaxa\u2208AB\u22121 \u00b5\u0302 (B\u22121) a it lies in two of these regions, then the decision of whether to keep it or discard it is the same in both executions of the algorithm. However, if it is in the third region, the decision could be different between parallel executions, and since it relies on some external and unknown randomness, it is not clear how to reason about it. To overcome this issue, we use the random threshold to argue about the probability that the decision between two executions differs. The crucial observation that allows us to get rid of the extra log2(T ) factor is that there are correlations between consecutive batches: we prove that if some arm j lies in this \u201cbad\u201d region in some batch i, then it will be outside this region after a constant number of batches. 5 REPRODUCIBLE STOCHASTIC LINEAR BANDITS: We now investigate reproducibility in the more general setting of stochastic linear bandits. In this setting, each arm is a vector a \u2208 Rd belonging to some action set A \u2286 Rd, and there is a parameter \u03b8\u22c6 \u2208 Rd unknown to the player. In round t, the player chooses some action at \u2208 A and receives a reward rt = \u27e8\u03b8\u22c6, at\u27e9 + \u03b7t, where \u03b7t is a zero-mean 1-subgaussian random variable independent of any other source of randomness. This means that E[\u03b7t] = 0 and satisfies E[exp(\u03bb\u03b7t)] \u2264 exp(\u03bb2/2) for any \u03bb \u2208 R. For normalization purposes, it is standard to assume that \u2225\u03b8\u22c6\u22252 \u2264 1 and supa\u2208A \u2225a\u22252 \u2264 1. In the linear setting, the expected regret after T pulls a1, . . . , aT can be written as E[RT ] = T sup a\u2208A \u27e8\u03b8\u22c6, a\u27e9 \u2212E [ T\u2211 t=1 \u27e8\u03b8\u22c6, at\u27e9 ] . In Section 5.1 we provide results for the finite action space case, i.e., when |A| = K. Next, in Section 5.2, we study reproducible linear bandit algorithms when dealing with infinite action spaces. In the following, we work in the regime where T \u226b d. We underline that our approach leverages connections of stochastic linear bandits with G-optimal experiment design, core sets constructions, and least-squares estimators. Roughly speaking, the goal of G-optimal design is to find a (small) subset of arms A\u2032, which is called the core set, and define a distribution \u03c0 over them with the following property: for any \u03b5 > 0, \u03b4 > 0 pulling only these arms for an appropriate number of times and computing the least-squares estimate \u03b8\u0302 guarantees that supa\u2208A\u27e8a, \u03b8\u2217\u2212 \u03b8\u0302\u27e9 \u2264 \u03b5, with probability 1\u2212\u03b4. For an extensive discussion, we refer to Chapters 21 and 22 of Lattimore & Szepesva\u0301ri (2020). 5.1 FINITE ACTION SET: We first introduce a lemma that allows us to reduce the size of the action set that our algorithm has to search over. Lemma 5 (See Chapters 21 and 22 in Lattimore & Szepesva\u0301ri (2020)). For any finite action set A that spans Rd and any \u03b4, \u03b5 > 0, there exists an algorithm that, in time polynomial in d, computes a multi-set of \u0398(d log(1/\u03b4)/\u03b52+d log log d) actions (possibly with repetitions) such that (i) they span Rd and (ii) if we perform these actions in a batched stochastic d-dimensional linear bandits setting with true parameter \u03b8\u22c6 \u2208 Rd and let \u03b8\u0302 be the least-squares estimate for \u03b8\u22c6, then, for any a \u2208 A, with probability at least 1\u2212 \u03b4, we have \u2223\u2223\u2223\u2329a, \u03b8\u22c6 \u2212 \u03b8\u0302\u232a\u2223\u2223\u2223 \u2264 \u03b5. Essentially, the multi-set in Lemma 5 is obtained using an approximate G-optimal design algorithm. Thus, it is crucial to check whether this can be done in a reproducible manner. Recall that the above set of distinct actions is called the core set and is the solution of an (approximate) G-optimal design problem. To be more specific, consider a distribution \u03c0 : A \u2192 [0, 1] and define V (\u03c0) =\u2211 a\u2208A \u03c0(a)aa \u22a4 \u2208 Rd\u00d7d and g(\u03c0) = supa\u2208A \u2225a\u22252V (\u03c0)\u22121 . The distribution \u03c0 is called a design and the goal of G-optimal design is to find a design that minimizes g. Since the number of actions is finite, this problem reduces to an optimization problem which can be solved efficiently using standard optimization methods (e.g., the Frank-Wolfe method). Since the initialization is the same, the algorithm that finds the optimal (or an approximately optimal) design is reproducible under the assumption that the gradients and the projections do not have numerical errors. This perspective is orthogonal to the work of Ahn et al. (2022), that defines reproducibility from a different viewpoint. Algorithm 3 Reproducible Algorithm for Stochastic Linear Bandits (Theorem 6) 1: Input: number of arms K, time horizon T, reproducibility \u03c1 2: Initialization: B \u2190 log(T ), q \u2190 (T/c)1/B , A \u2190 [K], r \u2190 T 3: \u03b2 \u2190 \u230amax{K2/\u03c12, 2304}\u230b 4: for i = 1 to B \u2212 1 do 5: \u03b5\u0303i = \u221a d log(KT 2)/(\u03b2qi) 6: \u03b5i = \u221a d log(KT 2)/qi 7: ni = 10d log(KT 2)/\u03b52i 8: a1, . . . , ani \u2190 multi-set given by Lemma 5 with parameters \u03b4 = 1/(KT 2) and \u03b5 = \u03b5\u0303i 9: if ni > r then 10: break 11: Pull every arm a1, . . . , ani and receive rewards r1, . . . , rni 12: Compute the LSE \u03b8\u0302i \u2190 (\u2211ni j=1 aja T j )\u22121 (\u2211ni j=1 ajrj ) 13: \u03b5i \u2190 Uni[\u03b5i/2, \u03b5i]:  14: r \u2190 r \u2212 ni: 15: for a \u2208 A do 16: if \u27e8a, \u03b8\u0302i\u27e9+ \u03b5\u0303i < maxa\u2208A\u27e8a, \u03b8\u0302i\u27e9 \u2212 \u03b5i then 17: Remove a from A 18: In the last batch play argmaxa\u2208A\u27e8a, \u03b8\u0302B\u22121\u27e9 In our batched bandit algorithm (Algorithm 3), the multi-set of arms a1, . . . , ani computed in each batch is obtained via a deterministic algorithm with runtime poly(K, d), where |A| = K. Hence, the multi-set will be the same in two different executions of the algorithm. On the other hand, the LSE will not be since it depends on the stochastic rewards. We apply the techniques that we developed in the reproducible stochastic MAB setting in order to design our algorithm. Our main result for reproducible d-dimensional stochastic linear bandits with K arms follows. For the proof, we refer to Appendix C. Theorem 6. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm for the stochastic d-dimensional linear bandit problem with K arms whose expected regret is E[RT ] \u2264 C \u00b7 K2 \u03c12 \u221a dT log(KT ) , where C > 0 is an absolute numerical constant, and its running time is polynomial in d,K, T and 1/\u03c1. Note that the best known non-reproducible algorithm achieves an upper bound of O\u0303( \u221a dT log(K)) and, hence, our algorithm incurs a reproducibility overhead of order K2/\u03c12. The intuition behind the proof is similar to the multi-armed bandit setting in Section 4. 5.2 INFINITE ACTION SET: Let us proceed to the setting where the action set A is unbounded. Unfortunately, even when d = 1, we cannot directly get an algorithm that has satisfactory regret guarantees by discretizing the space and using Algorithm 3. The approach of Esfandiari et al. (2021) is to discretize the action space and use an 1/T -net to cover it, i.e. a set A\u2032 \u2286 A such that for all a \u2208 A there exists some a\u2032 \u2208 A\u2032 with ||a \u2212 a\u2032||2 \u2264 1/T . It is known that there exists such a net of size at most (3T )d (Vershynin, 2018, Corollary 4.2.13). Then, they apply the algorithm for the finite arms setting, increasing their regret guarantee by a factor of \u221a d. However, our reproducible algorithm for this setting contains an additional factor of K2 in the regret bound. Thus, even when d = 1, our regret guarantee is greater than T, so the bound is vacuous. One way to fix this issue and get a sublinear regret guarantee is to use a smaller net. We use a 1/T 1/(4d+2)\u2212net that has size at most (3T ) d 4d+2 and this yields an expected regret of order O(T 4d+1/(4d+2) \u221a d log(T )/\u03c12). For further details, we refer to Appendix D. Even though the regret guarantee we managed to get using the smaller net of Appendix D is sublinear in T , it is not a satisfactory bound. The next step is to provide an algorithm for the infinite action setting using a reproducible LSE subroutine combined with the batching approach of Esfandiari et al. (2021). We will make use of the next lemma. Lemma 7 (Section 21.2 Note 3 of Lattimore & Szepesva\u0301ri (2020)). There exists a deterministic algorithm that, given an action space A \u2286 Rd, computes a 2-approximate G-optimal design \u03c0 with a core set of size O(d log log(d)). We additionally prove the next useful lemma, which, essentially, states that we can assume without loss of generality that every arm in the support of \u03c0 has mass at least \u2126(1/(d log(d))). We refer to Appendix F.1 for the proof. Lemma 8 (Effective Support). Let \u03c0 be the distribution that corresponds to the 2-approximate optimal G-design of Lemma 7 with input A. Assume that \u03c0(a) \u2264 c/(d log(d)), where c > 0 is some absolute numerical constant, for some arm a in the core set. Then, we can construct a distribution \u03c0\u0302 such that, for any arm a in the core set, \u03c0\u0302(a) \u2265 C/(d log(d)), where C > 0 is an absolute constant, so that it holds sup a\u2032\u2208A \u2225a\u2032\u22252V (\u03c0\u0302)\u22121 \u2264 4d . The upcoming lemma is a reproducible algorithm for the least-squares estimator and, essentially, builds upon Lemma 7 and Lemma 8. Its proof can be found at Appendix F.2. Lemma 9 (Reproducible LSE). Let \u03c1, \u03b5 \u2208 (0, 1] and 0 < \u03b4 \u2264 min{\u03c1, 1/d}1. Consider an environment of d-dimensional stochastic linear bandits with infinite action space A. Assume that \u03c0 is a 4-approximate optimal design with associated core set C as computed by Lemma 7 with input A. There exists a \u03c1-reproducible algorithm that pulls each arm a \u2208 C a total of \u2126 ( d4 log(d/\u03b4) log2 log(d) log log log(d) \u03b52\u03c12 ) times and outputs an estimate \u03b8SQ that satisfies supa\u2208A |\u27e8a, \u03b8SQ \u2212 \u03b8\u22c6\u27e9| \u2264 \u03b5 , with probability at least 1\u2212 \u03b4. The main result for the infinite actions\u2019 case, obtained by Algorithm 4, follows. Its proof can be found at Appendix E. 1We can handle the case of 0 < \u03b4 \u2264 d by paying an extra log d factor in the sample complexity. Algorithm 4 Reproducible LSE Algorithm for Stochastic Infinite Action Set (Theorem 10) 1: Input: time horizon T, action set A \u2286 Rd, reproducibility \u03c1 2: A\u2032 \u2190 1/T -net of A 3: Initialization: r \u2190 T,B \u2190 log(T ), q \u2190 (T/c)1/B 4: for i = 1 to B \u2212 1 do 5: qi denotes the number of pulls of all arms before the reproducibuility blow-up 6: \u03b5i = c \u00b7 d \u221a log(T )/qi 7: The blow-up is Mi = qi \u00b7 d3 log(d) log2 log(d) log log log(d) log2(T )/\u03c12 8: a1, . . . , a|Ci| \u2190 core set Ci of the design given by Lemma 7 with parameter A\u2032 9: if \u2308Mi\u2309 > r then 10: break 11: Pull every arm aj for Ni = \u2308Mi\u2309/|Ci| rounds and receive rewards r(j)1 , ..., r (j) Ni for j \u2208 [|Ci|] 12: Si = {(aj , r(j)t ) : t \u2208 [Ni], j \u2208 [|Ci|]} 13: \u03b8\u0302i \u2190 ReproducibleLSE(Si, \u03c1\u2032 = \u03c1/(dB), \u03b4 = 1/(2|A\u2032|T 2), \u03c4 = min{\u03b5i, 1}) 14: r \u2190 r \u2212 \u2308Mi\u2309 15: for a \u2208 A\u2032 do 16: if \u27e8a, \u03b8\u0302i\u27e9 < maxa\u2208A\u2032\u27e8a, \u03b8\u0302i\u27e9 \u2212 2\u03b5i then 17: Remove a from A\u2032 18: In the last batch play argmaxa\u2208A\u2032\u27e8a, \u03b8\u0302B\u22121\u27e9 19: 20: ReproducibleLSE(S, \u03c1, \u03b4, \u03c4) 21: for a \u2208 C do 22: v(a)\u2190 ReproducibleSQ(\u03d5 : x \u2208 R 7\u2192 x \u2208 R, S, \u03c1, \u03b4, \u03c4) \u25b7 Impagliazzo et al. (2022) 23: return ( \u2211 j\u2208|S| aja \u22a4 j ) \u22121 \u00b7 ( \u2211 a\u2208C a na v(a)) Theorem 10. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm (Algorithm 4) for the stochastic d-dimensional linear bandit problem with infinite action set whose expected regret is E[RT ] \u2264 C \u00b7 d4 log(d) log2 log(d) log log log(d) \u03c12 \u221a T log3/2(T ) , where C > 0 is an absolute numerical constant, and its running time is polynomial in T d and 1/\u03c1. Our algorithm for the infinite arm linear bandit case enjoys an expected regret of order O\u0303(poly(d) \u221a T ). We underline that the dependence of the regret on the time horizon is (almost) optimal, and we incur an extra d3 factor in the regret guarantee compared to the non-reproducible algorithm of Esfandiari et al. (2021). We now comment on the time complexity of our algorithm. Remark 11. The current implementation of our algorithm requires time exponential in d. However, for a general convex setA, given access to a separation oracle for it and an oracle that computes an (approximate) G-optimal design, we can execute it in polynomial time and with polynomially many calls to the oracle. Notably, when A is a polytope such oracles exist. We underline that computational complexity issues also arise in the traditional setting of linear bandits with an infinite number of arms and the computational overhead that the reproducibility requirement adds is minimal. For further details, we refer to Appendix G. 6 CONCLUSION AND FUTURE DIRECTIONS: In this paper, we have provided a formal notion of reproducibility for stochastic bandits and we have developed algorithms for the multi-armed bandit and the linear bandit settings that satisfy this notion and enjoy a small regret decay compared to their non-reproducible counterparts. An immediate future direction would be to find the optimal dependence on the number of arms K and the dimension d. Notice that the dependence on \u03c1 is optimal, and this follows from the lower bound in Impagliazzo et al. (2022). Our ideas can be applied to more complicated settings, like misspecified linear bandits (Ghosh et al., 2017), and give similar results. We hope and believe that our paper will inspire future works in reproducible interactive learning algorithms. A THE PROOF OF THEOREM 3: Theorem. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm (Algorithm 1) for the stochastic bandit problem with K arms and gaps (\u2206j)j\u2208[K] whose expected regret is E[RT ] \u2264 C \u00b7 K2 log2(T ) \u03c12 \u2211 j:\u2206j>0 ( \u2206j + log(2KT log(T )) \u2206j ) , where C > 0 is an absolute numerical constant, and its running time is polynomial in K,T and 1/\u03c1. Proof. First, we claim that the algorithm is \u03c1-reproducible: since the elimination decisions are taken in the same iterates and are based solely on the mean estimations, the reproducibility of the algorithm of Proposition 2 implies the reproducibility of the whole algorithm. In particular, Pr[(a1, ..., aT ) \u0338= (a\u20321, ..., a\u2032T )] = Pr[\u2203i \u2208 [B],\u2203j \u2208 [K] : \u00b5\u0302 (i) j was not reproducible] \u2264 \u03c1 . During each batch i, we draw for any active arm \u230aqi\u230b fresh samples for a total of ci samples and use the reproducible mean estimation algorithm to estimate its mean. For an active arm, at the end of some batch i \u2208 [B], we say that its estimation is \u201ccorrect\u201d if the estimation of its mean is within\u221a log(2KTB)/ci from the true mean. Using Proposition 2, the estimation of any active arm at the end of any batch (except possibly the last batch) is correct with probability at least 1\u2212 1/(2KTB) and so, by the union bound, the probability that the estimation is incorrect for some arm at the end of some batch is bounded by 1/T . We remark that when \u03b4 < \u03c1, the sample complexity of Proposition 2 reduces to O(log(1/\u03b4)/(\u03c42\u03c12)). Let E denote the event that our estimates are correct. The total expected regret can be bounded as E[RT ] \u2264 T \u00b7 1/T +E[RT |E ] . It suffices to bound the second term of the RHS and hence we can assume that each gap is correctly estimated within an additive factor of \u221a log(2KTB)/ci after batch i. First, due to the elimination condition, we get that the best arm is never eliminated. Next, we have that E[RT |E ] = \u2211 j:\u2206j>0 \u2206j E[Tj |E ] , where Tj is the total number of pulls of arm j. Fix a sub-optimal arm j and assume that i + 1 was the last batch it was active. Since this arm is not eliminated at the end of batch i, and the estimations are correct, we have that \u2206j \u2264 \u221a log(2KTB)/ci , and so ci \u2264 log(2KTB)/\u22062j . Hence, the number of pulls to get the desired bound due to Proposition 2 is (since we need to pull an arm ci/\u03c121 times in order to get an estimate at distance\u221a log(1/\u03b4)/c2i with probability 1\u2212 \u03b4 in a \u03c11-reproducible manner when \u03b4 < \u03c11) Tj \u2264 ci+1/\u03c121 = q/\u03c121(1 + ci) \u2264 q/\u03c121 \u00b7 (1 + log(2KTB)/\u22062j ) . This implies that the total regret is bounded by E[RT ] \u2264 1 + q/\u03c121 \u00b7 \u2211 j:\u2206j>0 ( \u2206j + log(2KTB) \u2206j ) . We finally set q = T 1/B and B = log(T ). Moreover, we have that \u03c11 = \u03c1/(KB). These yield E[RT ] \u2264 K2 log2(T ) \u03c12 \u2211 j:\u2206j>0 ( \u2206j + log(2KT log(T )) \u2206j ) . This completes the proof. B THE PROOF OF THEOREM 4: Theorem. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm (Algorithm 2) for the stochastic bandit problem with K arms and gaps (\u2206j)j\u2208[K] whose expected regret is E[RT ] \u2264 C \u00b7 K2 \u03c12 \u2211 j:\u2206j>0 (\u2206j + log(KT log(T ))/\u2206j) , for some absolute numerical constant C > 0, and its running time is polynomial in K,T and 1/\u03c1. For intuition, we begin with a non tight analysis which however provides the main ideas behind the actual proof. Non Tight Analysis Assume that the environment has K arms with unknown means \u00b5i and let T be the number of rounds. Consider B to the total number of batches and \u03b2 > 1. We set q = T 1/B . In each batch i \u2208 [B], we pull each arm \u03b2\u230a qi\u230b times. Hence, after the i-th batch, we will have drawn c\u0303i = \u2211 1\u2264j\u2264i \u03b2\u230aqj\u230b independent and identically distributed samples from each arm. Let us also set ci = \u2211 1\u2264j\u2264i\u230aqj\u230b. Let us fix i \u2208 [B]. Using Hoeffding\u2019s bound for subgaussian concentration, the length of the confidence bound for arm j \u2208 [K] that guarantees 1 \u2212 \u03b4 probability of success (in the sense that the empirical estimate \u00b5\u0302j will be close to the true \u00b5j) is equal to U\u0303i = \u221a 2 log(1/\u03b4)/c\u0303i , when the estimator uses c\u0303i samples. Also, let Ui = \u221a 2 log(1/\u03b4)/ci . Assume that the active arms at the batch iteration i lie in the set Ai. Consider the estimates {\u00b5\u0302(i)j }i\u2208[B],j\u2208Ai , where \u00b5\u0302 (i) j is the empirical mean of arm j using c\u0303i samples. We will eliminate an arm j at the end of the batch iteration i if \u00b5\u0302 (i) j + U\u0303i \u2264 max t\u2208Ai \u00b5\u0302 (i) t \u2212 U i , where U i \u223c Uni[Ui/2, Ui]. For the remaining of the proof, we condition on the event E that for every arm j \u2208 [K] and every batch i \u2208 [B] the true mean is within U\u0303i from the empirical one. We first argue about the reproducibility of our algorithm. Consider a fixed round i (end of i-th batch) and a fixed arm j. Let i\u22c6 be the optimal empirical arm after the i-th batch. Let \u00b5\u0302(i) \u2032 j , \u00b5\u0302 (i)\u2032 i\u22c6 the empirical estimates of arms j, i \u22c6 after the i-th batch, under some other execution of the algorithm. We condition on the event E \u2032 for the other execution as well. Notice that |\u00b5\u0302(i) \u2032 j \u2212 \u00b5\u0302 (i) j | \u2264 2U\u0303i, |\u00b5\u0302 (i)\u2032 i\u22c6 \u2212 \u00b5\u0302 (i) i\u22c6 | \u2264 2U\u0303i. Notice that, since the randomness of U i is shared, if \u00b5\u0302 (i) j + U\u0303i \u2265 \u00b5\u0302 (i) i\u22c6 \u2212 U i + 4U\u0303i, then the arm j will not be eliminated after the i-th batch in some other execution of the algorithm as well. Similarly, if \u00b5\u0302(i)j + U\u0303i < \u00b5\u0302 (i) i\u22c6 \u2212U i \u2212 4U\u0303i the the arm j will get eliminated after the i-th batch in some other execution of the algorithm as well. In particular, this means that if \u00b5\u0302(i)j \u2212 2U\u0303i > \u00b5\u0302 (i) i\u22c6 + U\u0303i \u2212 Ui/2 then the arm j will not get eliminated in some other execution of the algorithm and if \u00b5\u0302(i)j + 5U\u0303i < \u00b5\u0302 (i) i\u22c6 \u2212 Ui then the arm j will also get eliminated in some other execution of the algorithm with probability 1 under the event E \u2229 E \u2032. We call the above two cases good since they preserve reproducibility. Thus, it suffices to bound the probability that the decision about arm j will be different between the two executions when we are in neither of these cases. Then, the worst case bound due to the mass of the uniform probability measure is 16 \u221a 2 log(1/\u03b4)/c\u0303i\u221a 2 log(1/\u03b4)/ci . This implies that the probability mass of the bad event is at most 16 \u221a ci/c\u0303i = 16 \u221a 1/\u03b2. A union bound over all arms and batches yields that the probability that two distinct executions differ in at least one pull is Pr[(a1, . . . , aT ) \u0338= (a\u20321, . . . , a\u2032T )] \u2264 16KB \u221a 1/\u03b2 + 2\u03b4 , and since \u03b4 \u2264 \u03c1 it suffices to pick \u03b2 = 768K2B2/\u03c12. We now focus on the regret of our algorithm. Let us set \u03b4 = 1/(KTB). Fix a sub-optimal arm j and assume that batch i+ 1 was the last batch that is was active. We obtain that the total number of pulls of this arm is Tj \u2264 c\u0303i+1 \u2264 \u03b2q(1 + ci) \u2264 \u03b2q(1 + 8 log(1/\u03b4)/\u22062j ) From the reproducibility analysis, it suffices to take \u03b2 of order K2 log2(T )/\u03c12 and so E[RT ] \u2264 T \u00b71/T+E[RT |E ] = 1+ \u2211 j:\u2206j>0 \u2206j E[Tj |E ] \u2264 C \u00b7K2 log2(T ) \u03c12 \u2211 j:\u2206j>0 ( \u2206j + log(KT log(T )) \u2206j ) , for some absolute constant C > 0. Notice that the above analysis which uses a naive union bound does not yield the desired regret bound. We next provide a more tight analysis of the same algorithm that achieves the regret bound of Theorem 4. Improved Analysis (The Proof of Theorem 4) In the previous analysis, we used a union bound over all arms and all batches in order to control the probability of the bad event. However, we can obtain an improved regret bound as follows. Fix a sub-optimal arm i \u2208 [K] and let t be the first round that it appears in the bad event. We claim that after a constant number of rounds, this arm will be eliminated. This will shave the O(log2(T )) factor from the regret bound. Essentially, as indicated in the previous proof, the bad event corresponds to the case where the randomness of the cut-off threshold U can influence the decision of whether the algorithm eliminates an arm or not. The intuition is that during the rounds t and t+1, given that the two intervals intersected at round t, we know that the probability that they intersect again is quite small since the interval of the optimal mean is moving upwards, the interval of the sub-optimal mean is concentrating around the guess and the two estimations have been moved by at most a constant times the interval\u2019s length. Since the bad event occurs at round t, we know that \u00b5\u0302 (t) j \u2208 [ \u00b5\u0302 (t) t\u22c6 \u2212 Ut \u2212 5U\u0303t, \u00b5\u0302 (t) t\u22c6 \u2212 Ut/2 + 3U\u0303t ] . In the above \u00b5\u0302tt\u22c6 is the estimate of the optimal mean at round t whose index is denoted by t \u22c6. Now assume that the bad event for arm j also occurs at round t+ k. Then, we have that \u00b5\u0302 (t+k) j \u2208 [ \u00b5\u0302 (t+k) (t+k)\u22c6 \u2212 Ut+k \u2212 5U\u0303t+k, \u00b5\u0302 (t+k) (t+k)\u22c6 \u2212 Ut+k/2 + 3U\u0303t+k ] . First, notice that since the concentration inequality under event E holds for rounds t, t+ k we have that \u00b5\u0302(t+k)j \u2264 \u00b5\u0302 (t) j + U\u0303t + U\u0303t+k. Thus, combining it with the above inequalities gives us \u00b5\u0302 (t+k) (t+k)\u22c6 \u2212 Ut+k \u2212 5U\u0303t+k \u2264 \u00b5\u0302 (t+k) j \u2264 \u00b5\u0302 (t) j + U\u0303t + U\u0303t+k \u2264 \u00b5\u0302 (t) t\u22c6 \u2212 Ut/2 + 4U\u0303t + U\u0303t+k. We now compare \u00b5\u0302(t)t\u22c6 , \u00b5\u0302 (t+k) (t+k)\u22c6 . Let o denote the optimal arm. We have that \u00b5\u0302 (t+k) (t+k)\u22c6 \u2265 \u00b5\u0302 (t+k) o \u2265 \u00b5o \u2212 U\u0303t+k \u2265 \u00b5t\u22c6 \u2212 U\u0303t+k \u2265 \u00b5\u0302 (t) t\u22c6 \u2212 U\u0303t \u2212 U\u0303t+k. This gives us that \u00b5\u0302 (t) t\u22c6 \u2212 Ut+k \u2212 6U\u0303t+k \u2212 U\u0303t \u2264 \u00b5\u0302 (t+k) (t+k)\u22c6 \u2212 Ut+k \u2212 5U\u0303t+k. Thus, we have established that \u00b5\u0302 (t) t\u22c6 \u2212 Ut+k \u2212 6U\u0303t+k \u2212 U\u0303t \u2264 \u00b5\u0302 (t) t\u22c6 \u2212 Ut/2 + 4U\u0303t + U\u0303t+k =\u21d2 Ut+k \u2265 Ut/2\u2212 7U\u0303t+k \u2212 5U\u0303t \u2265 Ut/2\u2212 12U\u0303t. Since \u03b2 \u2265 2304, we get that 12U\u0303t \u2264 Ut/4. Thus, we get that Ut+k \u2265 Ut/4. Notice that Ut+k Ut = \u221a ct ct+k , thus it immediately follows that ct ct+k \u2265 1 16 =\u21d2 q t+1 \u2212 1 qt+k+1 \u2212 1 \u2265 1 16 =\u21d2 16 ( 1\u2212 1 qt+1 ) \u2265 qk \u2212 1 qt+1 =\u21d2 qk \u2264 16 + 1 qt+1 \u2264 17 =\u21d2 k log q \u2264 log 17 =\u21d2 k \u2264 5, when we pick B = log(T ) batches. Thus, for every arm the bad event can happen at most 6 times, by taking a union bound over the K arms we see that the probability that our algorithm is not reproducible is at most O(K \u221a 1/\u03b2), so picking \u03b2 = \u0398(K2/\u03c12) suffices to get the result. C THE PROOF OF THEOREM 6: Theorem. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm for the stochastic ddimensional linear bandit problem with K arms whose expected regret is E[RT ] \u2264 O ( K2 \u03c12 \u221a dT log(KT ) ) , and its running time is polynomial in d,K, T and 1/\u03c1. Proof. Let c, C be the constants hidden in Lemma 5, i.e., the size of the multi-set is in [cd log(1/\u03b4)/\u03b52, Cd log(1/\u03b4)/\u03b52]. We know that the size of each batch ni \u2208 [cqi, Cqi] (see Lemma 5), so by the end of the B \u2212 1 batch we will have less than nB pulls left. Hence, the number of batches is at most B. We first define the event E that the estimates of all arms after the end of each batch are accurate, i.e., for every active arm a at the beginning of the i-th batch, at the end of the batch we have that\u2223\u2223\u2223\u2329a, \u03b8\u0302i \u2212 \u03b8\u22c6\u232a\u2223\u2223\u2223 \u2264 \u03b5\u0303i. Since \u03b4 = 1/(KT 2) and there are at most T batches and K active arms in each batch, a simple union bound shows that E happens with probability at least 1 \u2212 1/T. We condition on the event E throughout the rest of the proof. We now argue about the regret bound of our algorithm. We first show that any optimal arm a\u2217 will not get eliminated. Indeed, consider any sub-optimal arm a \u2208 [K] and any batch i \u2208 [B]. Under the event E we have that \u27e8a, \u03b8\u0302i\u27e9 \u2212 \u27e8a\u2217, \u03b8\u0302i\u27e9 \u2264 (\u27e8a, \u03b8\u2217\u27e9+ \u03b5\u0303i)\u2212 (\u27e8a\u2217, \u03b8\u2217\u27e9 \u2212 \u03b5\u0303i) < 2\u03b5\u0303i < \u03b5i + \u03b5i. Next, we need to bound the number of times we pull some fixed suboptimal arm a \u2208 [K]. We let \u2206 = \u27e8a\u2217 \u2212 a, \u03b8\u2217\u27e9 denote the gap and we let i be the smallest integer such that \u03b5i < \u2206/4. We claim that this arm will get eliminated by the end of batch i. Indeed, \u27e8a\u2217, \u03b8\u0302i\u27e9 \u2212 \u27e8a, \u03b8\u0302i\u27e9 \u2265 (\u27e8a\u2217, \u03b8\u0302i\u27e9 \u2212 \u03b5\u0303i)\u2212 (\u27e8a, \u03b8\u0302i\u27e9+ \u03b5\u0303i) = \u2206\u2212 2\u03b5\u0303i > 4\u03b5i \u2212 2\u03b5\u0303i > \u03b5\u0303i + \u03b5i. This shows that during any batch i, all the active arms have gap at most 4\u03b5i\u22121. Thus, the regret of the algorithm conditioned on the event E is at most B\u2211 i=1 4ni\u03b5i\u22121 \u2264 4\u03b2C B\u2211 i=1 qi \u221a d log(KT 2)/qi\u22121 \u2264 6\u03b2Cq \u221a d log(KT ) B\u22121\u2211 i=0 qi/2 \u2264 O ( \u03b2qB/2+1 \u221a d log(KT ) ) = O ( K2 \u03c12 qB/2+1 \u221a d log(KT ) ) = O ( K2 \u03c12 q \u221a dT log(KT ) ) . Thus, the overall regret is bounded by \u03b4 \u00b7 T + (1 \u2212 \u03b4) \u00b7 O ( K2 \u03c12 q \u221a dT log(KT ) ) = O ( K2 \u03c12 q \u221a dT log(KT ) ) . We now argue about the reproducibility of our algorithm. The analysis follows in a similar fashion as in Theorem 4. Let \u03b8\u0302i, \u03b8\u0302\u2032i be the LSE after the i-th batch, under two different executions of the algorithm and assume that the set of active arms. We condition on the event E \u2032 for the other execution as well. Assume that the set of active arms is the same under both executions at the beginning of batch i. Notice that since the set that is guaranteed by Lemma 5 is computed by a deterministic algorithm, both executions will pull the same arms in batch i. Consider a suboptimal arm a and let ai\u2217 = argmaxa\u2208A\u27e8\u03b8\u0302i, a\u27e9, a\u2032i\u2217 = argmaxa\u2208A\u27e8\u03b8\u0302\u2032i, a\u27e9. Under the event E \u2229 E \u2032 we have that |\u27e8a, \u03b8\u0302i \u2212 \u03b8\u0302\u2032i\u27e9| \u2264 2\u03b5\u0303i, |\u27e8ai\u2217 , \u03b8\u0302i \u2212 \u03b8\u0302\u2032i\u27e9| \u2264 2\u03b5\u0303i, and |\u27e8a\u2032i\u2217 , \u03b8\u0302\u2032i\u27e9 \u2212 \u27e8ai\u2217 , \u03b8\u0302i\u27e9| \u2264 2\u03b5\u0303i. Notice that, since the randomness of \u03b5i is shared, if \u27e8a, \u03b8\u0302i\u27e9 + \u03b5\u0303i \u2265 \u27e8ai\u2217 , \u03b8\u0302i\u27e9 \u2212 \u03b5i + 4\u03b5\u0303i, then the arm a will not be eliminated after the i-th batch in some other execution of the algorithm as well. Similarly, if \u27e8a, \u03b8\u0302i\u27e9+ \u03b5\u0303i < \u27e8ai\u2217 , \u03b8\u0302i\u27e9\u2212 \u03b5i\u2212 4\u03b5\u0303i the the arm a will get eliminated after the i-th batch in some other execution of the algorithm as well. In particular, this means that if \u27e8a, \u03b8\u0302i\u27e9\u22122\u03b5\u0303i > \u27e8ai\u2217 , \u03b8\u0302i\u27e9+\u03b5\u0303i\u2212\u03b5i/2 then the arm a will not get eliminated in some other execution of the algorithm and if \u27e8a, \u03b8\u0302i\u27e9+5\u03b5\u0303i < \u27e8ai\u2217 , \u03b8\u0302i\u27e9 \u2212 \u03b5i then the arm j will also get eliminated in some other execution of the algorithm with probability 1 under the event E \u2229E \u2032. Thus, it suffices to bound the probability that the decision about arm j will be different between the two executions when we are in neither of these cases. Then, the worst case bound due to the mass of the uniform probability measure is 16 \u221a d log(1/\u03b4)/c\u0303i\u221a d log(1/\u03b4)/ci . This implies that the probability mass of the bad event is at most 16 \u221a ci/c\u0303i = 16 \u221a 1/\u03b2. A naive union bound would require us to pick \u03b2 = \u0398(K2 log2 T/\u03c12). We next show to avoid the log2 T factor. Fix a sub-optimal arm a \u2208 [K] and let t be the first round that it appears in the bad event. Since the bad event occurs at round t, we know that \u27e8a, \u03b8\u0302t\u27e9 \u2208 [ \u27e8at\u2217 , \u03b8\u0302t\u27e9 \u2212 \u03b5t \u2212 5\u03b5\u0303t, \u27e8at\u2217 , \u03b8\u0302t\u27e9 \u2212 \u03b5t/2 + 3\u03b5\u0303t ] . In the above, at\u2217 is the optimal arm at round t w.r.t. the LSE. Now assume that the bad event for arm a also occurs at round t+ k. Then, we have that \u27e8a, \u03b8\u0302t+k\u27e9 \u2208 [ \u27e8a(t+k)\u2217 , \u03b8\u0302t+k\u27e9 \u2212 \u03b5t+k \u2212 5\u03b5\u0303t+k, \u27e8a(t+k)\u2217 , \u03b8\u0302t+k\u27e9 \u2212 \u03b5t/2 + 3\u03b5\u0303t+k ] . First, notice that since the concentration inequality under event E holds for rounds t, t+ k we have that \u27e8a, \u03b8\u0302t+k\u27e9 \u2264 \u27e8a, \u03b8\u0302t\u27e9+ \u03b5\u0303t + \u03b5\u0303t+k. Thus, combining it with the above inequalities gives us \u27e8a(t+k)\u2217 , \u03b8\u0302t+k\u27e9\u2212 \u03b5t+k \u2212 5\u03b5\u0303t+k \u2264 \u27e8a, \u03b8\u0302t+k\u27e9 \u2264 \u27e8a, \u03b8\u0302t\u27e9+ \u03b5\u0303t + \u03b5\u0303t+k \u2264 \u27e8at\u2217 , \u03b8\u0302t\u27e9\u2212 \u03b5t/2+ 4\u03b5\u0303t + \u03b5\u0303t+k. We now compare \u27e8at\u2217 , \u03b8\u0302t\u27e9, \u27e8a(t+k)\u2217 , \u03b8\u0302t+k\u27e9. Let a\u2217 denote the optimal arm. We have that \u27e8a(t+k)\u2217 , \u03b8\u0302t+k\u27e9 \u2265 \u27e8a\u2217, \u03b8\u0302t+k\u27e9 \u2265 \u27e8a\u2217, \u03b8\u2217\u27e9 \u2212 \u03b5\u0303t+k \u2265 \u27e8at\u2217 , \u03b8\u2217\u27e9 \u2212 \u03b5\u0303t+k \u2265 \u27e8at\u2217 , \u03b8\u0302t\u27e9 \u2212 \u03b5\u0303t+k \u2212 \u03b5\u0303t. This gives us that \u27e8at\u2217 , \u03b8\u0302t\u27e9 \u2212 \u03b5t+k \u2212 6\u03b5\u0303t+k \u2212 \u03b5\u0303t \u2264 \u27e8a(t+k)\u2217 , \u03b8\u0302t+k\u27e9 \u2212 \u03b5t+k \u2212 5\u03b5\u0303t+k. Thus, we have established that \u27e8at\u2217 , \u03b8\u0302t\u27e9 \u2212 \u03b5t+k \u2212 6\u03b5\u0303t+k \u2212 \u03b5\u0303t \u2264 \u27e8at\u2217 , \u03b8\u0302t\u27e9 \u2212 \u03b5t/2 + 4\u03b5\u0303t + \u03b5\u0303t+k =\u21d2 \u03b5t+k \u2265 \u03b5t/2\u2212 7\u03b5\u0303t+k \u2212 5\u03b5\u0303t \u2265 \u03b5t/2\u2212 12\u03b5\u0303t. Since \u03b2 \u2265 2304, we get that 12\u03b5\u0303t \u2264 \u03b5t/4. Thus, we get that \u03b5t+k \u2265 \u03b5t/4. Notice that \u03b5t+k \u03b5t = \u221a qt qt+k , thus it immediately follows that qt qt+k \u2265 1 16 =\u21d2 qk \u2264 16 =\u21d2 k log q \u2264 log 16 =\u21d2 k \u2264 4, when we pick B = log(T ) batches. Thus, for every arm the bad event can happen at most 5 times, by taking a union bound over the K arms we see that the probability that our algorithm is not reproducible is at most O(K \u221a 1/\u03b2), so picking \u03b2 = \u0398(K2/\u03c12) suffices to get the result. D NAIVE APPLICATION OF ALGORITHM 3 WITH INFINITE ACTION SPACE: We use a 1/T 1/(4d+2)\u2212net that has size at most (3T ) d 4d+2 . Let A\u2032 be the new set of arms. We then run Algorithm 3 using A\u2032. This gives us the following result, that is proved right after. Corollary 12. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There is a \u03c1-reproducible algorithm for the stochastic ddimensional linear bandit problem with infinite arms whose expected regret is at most E[RT ] \u2264 C \u00b7 T 4d+1 4d+2 \u03c12 \u221a d log(T ) , where C > 0 is an absolute numerical constant. Proof. Since K \u2264 (3T ) d 4d+2 , we have that T sup a\u2208A\u2032 \u27e8a, \u03b8\u2217\u27e9 \u2212E [ T\u2211 i=1 \u27e8at, \u03b8\u2217\u27e9 ] \u2264 O ( (3T ) 2d 4d+2 \u03c12 \u221a dT log ( T (3T ) d 4d+2 )) = O ( T 4d+1 4d+2 \u03c12 \u221a d log(T ) ) Comparing to the best arm in A, we have that: T sup a\u2208A \u27e8a, \u03b8\u2217\u27e9 \u2212E [ T\u2211 i=1 \u27e8at, \u03b8\u2217\u27e9 ] = ( T sup a\u2208A \u27e8a, \u03b8\u2217\u27e9 \u2212 T sup a\u2208A\u2032 \u27e8a, \u03b8\u2217\u27e9 ) + ( T sup a\u2208A\u2032 \u27e8a, \u03b8\u2217\u27e9 \u2212E [ T\u2211 i=1 \u27e8at, \u03b8\u2217\u27e9 ]) Our choice of the 1/T 1/(4d+2)-net implies that for every a \u2208 A there exists some a\u2032 \u2208 A\u2032 such that ||a \u2212 a\u2032||2 \u2264 1/T 1/(4d+2). Thus, supa\u2208A\u27e8a, \u03b8\u2217\u27e9 \u2212 supa\u2032\u2208A\u2032\u27e8a\u2032, \u03b8\u2217\u27e9 \u2264 ||a \u2212 a\u2032||2||\u03b8\u2217||2 \u2264 1/T 1/(4d+2). Thus, the total regret is at most T \u00b7 1/T 1/(4d+2) +O ( T 4d+1 4d+2 \u03c12 \u221a d log(T ) ) = O ( T 4d+1 4d+2 \u03c12 \u221a d log(T ) ) . E THE PROOF OF THEOREM 10: Theorem. Let T \u2208 N, \u03c1 \u2208 (0, 1]. There exists a \u03c1-reproducible algorithm (Algorithm 4) for the stochastic d-dimensional linear bandit problem with infinite action set whose expected regret is E[RT ] \u2264 O ( d4 log(d) log2 log(d) log log log(d) \u03c12 \u221a T log3/2(T ) ) , and its running time is polynomial in T d and 1/\u03c1. Proof. First, the algorithm is \u03c1-reproducible since in each batch we use a reproducible LSE subroutine with parameter \u03c1\u2032 = \u03c1/B. This implies that Pr[(a1, ..., aT ) \u0338= (a\u20321, ..., a\u2032T )] = Pr[\u2203i \u2208 [B] : \u03b8\u0302i was not reproducible] \u2264 \u03c1 . Let us fix a batch iteration i \u2208 [B \u2212 1]. Set Ci be the core set computed by Lemma 7. The algorithm first pulls ni = Cd4 log(d/\u03b4) log2 log(d) log log log(d) \u03b52i\u03c1 \u20322 times each one of the arms of the i-th core set Ci, as indicated by Lemma 9 and computes the LSE \u03b8\u0302i in a reproducible way using the algorithm of Lemma 9. Let E be the event that over all batches the estimations are correct. We pick \u03b4 = 1/(2|A\u2032|T 2) so that this good event does hold with probability at least 1\u2212 1/T . Our goal is to control the expected regret which can be written as E[RT ] = T sup a\u2208A \u27e8a, \u03b8\u22c6\u27e9 \u2212E T\u2211 t=1 \u27e8at, \u03b8\u22c6\u27e9 . We have that T sup a\u2208A \u27e8a, \u03b8\u22c6\u27e9 \u2212 T sup a\u2032\u2208A\u2032 \u27e8a\u2032, \u03b8\u22c6\u27e9 \u2264 1 , since A\u2032 is a deterministic 1/T -net of A. Also, let us set the expected regret of the bounded action sub-problem as E[R\u2032T ] = T sup a\u2032\u2208A\u2032 \u27e8a\u2032, \u03b8\u22c6\u27e9 \u2212E T\u2211 t=1 \u27e8at, \u03b8\u22c6\u27e9 . We can now employ the analysis of the finite arm case. During batch i, any active arm has gap at most 4\u03b5i\u22121, so the instantaneous regret in any round is not more than 4\u03b5i\u22121. The expected regret conditional on the good event E is upper bounded by E[R\u2032T |E ] \u2264 B\u2211 i=1 4Mi\u03b5i\u22121 , where Mi is the total number of pulls in batch i (using the reproducibility blow-up) and \u03b5i\u22121 is the error one would achieve by drawing qi samples (ignoring the blow-up). Then, for some absolute constant C > 0, we have that E[R\u2032T |E ] \u2264 B\u2211 i=1 4 ( qi d3 log(d) log2 log(d) log log log(d) log2 T \u03c12 ) \u00b7 \u221a d2 log(T )/qi\u22121 , which yields that E[R\u2032T |E ] \u2264 C d4 log(d) log2 log(d) log log log(d) log(T ) \u221a log(T ) \u03c12 \u00b7 S , where we set S := B\u2211 i=1 qi q(i\u22121)/2 = q1/2 B\u2211 i=1 qi/2 = q(1+B)/2 . We pick B = log(T ) and get that, if q = T 1/B then S = \u0398( \u221a T ). We remark that this choice of q is valid since B\u2211 i=1 qi = qB+1 \u2212 q q \u2212 1 = \u0398(qB)\u2212 1 \u2265 T\u03c1 2 d3 log(d) log2 log(d) log log log(d) . Hence, we have that E[R\u2032T |E ] \u2264 O ( d4 log(d) log2 log(d) log log log(d) \u03c12 \u221a T log3/2(T ) ) . Note that when E does not hold, we can bound the expected regret by 1/T \u00b7 T = 1. This implies that the overall regret E[RT ] \u2264 2 + E[R\u2032T |E ] and so it satisfies the desired bound and the proof is complete. F DEFERRED LEMMATA: F.1 THE PROOF OF LEMMA 8 Proof. Consider the distribution \u03c0 that is a 2-approximation to the optimal G-design and has support |C| = O(d log log d). Let C\u2032 be the set of arms in the support such that \u03c0(a) \u2264 c/d log d. We consider \u03c0\u0303 = (1 \u2212 x)\u03c0 + xa, where a \u2208 C\u2032 and x will be specified later. Consider now the matrix V (\u03c0\u0303). Using the Sherman-Morrison formula, we have that V (\u03c0\u0303)\u22121 = 1 1\u2212 x V (\u03c0)\u22121 \u2212 xV (\u03c0) \u22121aa\u22a4V (\u03c0)\u22121 (1\u2212 x)2 ( 1 + 11\u2212x ||a|| 2 V (\u03c0)\u22121 ) = 1 1\u2212 x ( V (\u03c0)\u22121 \u2212 xV (\u03c0) \u22121aa\u22a4V (\u03c0)\u22121 1\u2212 x+ ||a||2V (\u03c0)\u22121 ) . Consider any arm a\u2032. Then, ||a\u2032||2V (\u03c0\u0303)\u22121 = 1 1\u2212 x ||a||2V (\u03c0)\u22121 \u2212 x 1\u2212 x \u00b7 (a \u22a4V (\u03c0)\u22121a\u2032)2 1\u2212 x+ ||a||2V (\u03c0)\u22121 \u2264 1 1\u2212 x ||a||2V (\u03c0)\u22121 . Note that we apply this transformation at most O(d log log d) times. Let \u03c0\u0302 be the distribution we end up with. We see that ||a\u2032||2V (\u03c0\u0302)\u22121 \u2264 ( 1 1\u2212 x )cd log log d ||a||2V (\u03c0)\u22121 \u2264 2 ( 1 1\u2212 x )cd log log d d. Notice that there is a constant c\u2032 such that when x = c\u2032/d log d we have that ( 1 1\u2212x )cd log log d \u2264 2. Moreover, notice that the mass of every arm is at least x(1 \u2212 x)|C| \u2265 x \u2212 |C|x2 = c\u2032/(d log(d)) \u2212 c\u2032\u2032d log log d/(d2 log2(d)) \u2265 c/(d log(d)), for some absolute numerical constant c > 0. This concludes the claim. F.2 THE PROOF OF LEMMA 9 Proof. The proof works when we can treat \u2126(\u2308d log(1/\u03b4)\u03c0(a)/\u03b52\u2309) as \u2126(d log(1/\u03b4)\u03c0(a)/\u03b52), i.e., as long as \u03c0(a) = \u2126(\u03b52/d log(1/\u03b4)). In the regime we are in, this point is handled thanks to Lemma 8. Combining the following proof with Lemma 8, we can obtain the desired result. We underline that we work in the fixed design setting: the arms ai are deterministically chosen independently of the rewards ri. Assume that the core set of Lemma 7 is the set C. Fix the multi-set S = {(ai, ri) : i \u2208 [M ]}, where each arm a lies in the core set and is pulled na = \u0398(d\u03c0(a) log(1/\u03b4)/\u03b5 2) times. Hence, we have that M = \u2211 a\u2208C na = \u0398 ( d log(1/\u03b4)/\u03b52) ) . Let also V = \u2211 i\u2208[M ] aia \u22a4 i . The least-squares estimator can be written as \u03b8 (\u03b5) LSE = V \u22121 \u2211 i\u2208[M ] airi = V \u22121 \u2211 a\u2208C a \u2211 i\u2208[na] ri(a) , where each a lies in the core set (deterministically) and ri(a) is the i-th reward generated independently by the linear regression process \u27e8\u03b8\u22c6, a\u27e9+\u03be, where \u03be is a fresh zero mean sub-gaussian random variable. Our goal is to reproducibly estimate the value \u2211 i\u2208[na] ri(a) for any a. This is sufficient since two independent executions of the algorithm share the set C and na for any a. Note that the above sum is a random variable. In the following, we condition on the high-probability event that the average reward of the arm a is \u03b5-close to the expected one, i.e., the value \u27e8\u03b8\u22c6, a\u27e9. This happens with probability at least 1 \u2212 \u03b4/(2|C|), given \u2126\u0303(d\u03c0(a) log(d/\u03b4)/\u03b52) samples from arm a \u2208 C. In order to guarantee reproducibility, we will apply a result from Impagliazzo et al. (2022). Since we will union bound over all arms in the core set and |C| = O(d log log(d)) (via Lemma 7), we will make use of a (\u03c1/|C|)-reproducible algorithm that gives an estimate v(a) \u2208 R such that |\u27e8\u03b8\u22c6, a\u27e9 \u2212 v(a)| \u2264 \u03c4 , with probability at least 1\u2212 \u03b4/(2|C|). For \u03b4 < \u03c1, the algorithm uses Sa = \u2126 ( d2 log(d/\u03b4) log2 log(d) log log log(d)/(\u03c12\u03c42) ) many samples from the linear regression with fixed arm a \u2208 C. Since we have conditioned on the randomness of ri(a) for any i, we get\u2223\u2223\u2223\u2223\u2223\u2223 1na \u2211 i\u2208[na] ri(a)\u2212 v(a) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b5+ \u03c4 , with probability at least 1 \u2212 \u03b4/|C|. Hence, by repeating this approach for all arms in the core set, we set \u03b8SQ = V \u22121 \u2211 a\u2208C a na v(a). Let us condition on the randomness of the estimate \u03b8 (\u03b5) LSE. We have that sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8\u22c6\u27e9| \u2264 sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8(\u03b5)LSE\u27e9|+ sup a\u2032\u2208A |\u27e8a\u2032, \u03b8(\u03b5)LSE \u2212 \u03b8 \u22c6\u27e9| . Note that the second term is \u03b5 with probability at least 1\u2212 \u03b4 via Lemma 5. Our next goal is to tune the accuracy \u03c4 \u2208 (0, 1) so that the first term yields another \u03b5 error. For the first term, we have that sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8(\u03b5)LSE\u27e9| \u2264 sup a\u2032\u2208A \u2223\u2223\u2223\u2223\u2223\u27e8a\u2032, V \u22121\u2211 a\u2208C a na (\u03b5+ \u03c4)\u27e9 \u2223\u2223\u2223\u2223\u2223 Note that V = Cd log(d/\u03b4)\u03b52 \u2211 a\u2208C \u03c0(a)aa \u22a4 and so V \u22121 = \u03b5 2 Cd log(d/\u03b4)V (\u03c0) \u22121, for some absolute constant C > 0. This implies that sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8(\u03b5)LSE\u27e9| \u2264 (\u03b5+ \u03c4) sup a\u2032\u2208A \u2223\u2223\u2223\u2223\u2223 \u2329 a\u2032, \u03b52 Cd log(d/\u03b4) V (\u03c0)\u22121 \u2211 a\u2208C Cd log(d/\u03b4)\u03c0(a) \u03b52 a \u232a\u2223\u2223\u2223\u2223\u2223 . Hence, we get that sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8(\u03b5)LSE\u27e9| \u2264 (\u03b5+ \u03c4) sup a\u2032\u2208A \u2223\u2223\u2223\u2223\u2223 \u2329 a\u2032, V (\u03c0)\u22121 \u2211 a\u2208C \u03c0(a)a \u232a\u2223\u2223\u2223\u2223\u2223 . Consider a fixed arm a\u2032 \u2208 A. Then,\u2223\u2223\u2223\u2223\u2223 \u2329 a\u2032, V (\u03c0)\u22121 \u2211 a\u2208C \u03c0(a)a \u232a\u2223\u2223\u2223\u2223\u2223 \u2264\u2211 a\u2208C \u03c0(a) \u2223\u2223\u27e8a\u2032, V (\u03c0)\u22121a\u27e9\u2223\u2223 \u2264 \u2211 a\u2208C \u03c0(a) ( 1 + \u2223\u2223\u27e8a\u2032, V (\u03c0)\u22121a\u27e9\u2223\u22232) = 1 + \u2211 a\u2208C \u03c0(a) \u2223\u2223\u27e8a\u2032, V (\u03c0)\u22121a\u27e9\u2223\u22232 = 1 + ||a\u2032||2V (\u03c0)\u22121 \u2264 2d+ 1 , where the last inequality follows from the fact that \u03c0 is a 2-approximation of the G-optimal design. Hence, in total, by picking \u03c4 = \u03b5, we get that sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8\u22c6\u27e9| \u2264 16d\u03b5 . Thus, for any \u03b5 > 0, the total number of pulls of each arm is \u2126 ( d4 log(d/\u03b4) log2 log(d) log log log(d)/(\u03c12\u03b52) ) , to get sup a\u2032\u2208A |\u27e8a\u2032, \u03b8SQ \u2212 \u03b8\u22c6\u27e9| \u2264 \u03b5 . G COMPUTATIONAL PERFORMANCE OF ALGORITHM 4: In this appendix, we discuss the barriers towards computational efficiency regarding Algorithm 4. The reasons why Algorithm 4 is computationally inefficient are the following: (a) we have to compute the arm in the set of active arms that has maximum correlation with the guess \u03b8\u0302i, (b) we have to eliminate arms based on this value and (c) we have to run at each batch the Frank-Wolfe algorithm (or some other optimization method needed for Lemma 5) in order to obtain an approximate G-optimal design. As a minimal assumption in what follows, we focus on the case where the action set A is convex and we have access to a separation oracle for it. Note that executing both (a) and (b) naively requires time exponential in d. However, on the one side, arm elimination (issue (b)) reduces to finding the intersection of the current active set with a halfspace H whose normal vector is \u03b8 and the threshold is, roughly speaking, the maximum correlation. This maximum correlation can also be computed efficiently. Finding an arm with (almost) maximum correlation relates to the problem of finding a point that maximizes a linear objective under the constraint that the point lies in the intersection of the active arm set with some linear constraints. Thus, we can use the ellipsoid algorithm to implement this step. The above discussion deals with issues (a) and (b) and, essentially, states that even with infinitely many actions, one could implement these steps efficiently. We now focus on issue (c). The FrankWolfe method first requires a proper initialization. As mentioned in Lattimore & Szepesva\u0301ri (2020), if the starting point is chosen to be the uniform distribution over A\u2032, then the number of iterations before getting a 2-approximate optimal design is roughly O\u0303(d). The issue is that since A\u2032 is exponential in d, it is not clear how to work with such an initialization efficiently. Notably there is a different initialization (Fedorov, 2013; Lattimore et al., 2020) with support O(d) for which the method runs in O(d log log(d)) rounds (see Note 3 at Section 21.2 of Lattimore & Szepesva\u0301ri (2020) and Lattimore et al. (2020)). There are two issues: first, one requires an oracle to provide this good initialization. Second, each iteration of the Frank-Wolfe method (with current design guess \u03c0) requires computing a point in the current active set with maximum V (\u03c0)\u22121-norm. As noted in Todd (2016), a good initialization for finding a G-optimal design, i.e., a minimum volume enclosing ellipsoid (MVEE) should be sufficiently sparse (compared to the number of active arms) and assign positive mass to arms that correspond to extreme points, i.e., points that are close to the border of MVEE. The work of Kumar & Yildirim (2005) provides an initial core set that depends only on d but not on the number of points. The algorithm works as follows: it runs for d iterations and, in each round, it adds 2 arms into the core set. Initially, we set the core set C0 = \u2205 and let \u03a8 = {0}. In each iteration i \u2208 [d], the algorithm draws a random direction vi in the orthogonal complement of \u03a8 (this step is reproducible thanks to the shared randomness) and computes the vectors in the active arms\u2019 set with the maximum and the minimum correlation with vi, say a+i , a \u2212 i . It then extends C0 \u2190 C0 \u222a {a+i , a \u2212 i } and sets \u03a8 \u2190 span(\u03a8, {a + i \u2212 a \u2212 i }). Hence, the runtime of this algorithm corresponds to the runtime of the tasks maxa\u2208A\u2032\u27e8a, vi\u27e9 and mina\u2208A\u2032\u27e8a, vi\u27e9. One can efficiently approximate these values using the ellipsoid algorithm and hence efficiently initialize the Frank-Wolfe algorithm as in Todd (2016) (e.g., set the weights uniformly 1/(2d)). Our second challenge deals with finding a point in the active arm set with maximum V (\u03c0)\u22121norm for some current guess \u03c0. Even if the current active set is a polytope, finding an exact norm maximizer is NP-hard (Freund & Orlin, 1985; Mangasarian & Shiau, 1986)2. Hence, one should focus on efficient approximation algorithms. We note that even a poly(d)-approximate maximizer is sufficient to get O\u0303(poly(d) \u221a T ) regret. Such an algorithm for polytopes, which gets an 1/d2approximation, is provided in Ye (1992); Vavasis (1993). As a general note, if we assume that we have access to an oracle O that computes a 2-approximate G-optimal design in time TO, then our Algorithm 4 runs in time polynomial in TO. 2In fact, even finding a constant factor approximation, for some appropriate constant, is NP-hard (Bellare & Rogaway, 1993)."}, {"role": "assistant", "content": "The agents have provided detailed reports. Now, let's analyze the reports and identify key points for discussion and synthesis.\n\n*   **BanditAlgoExpert:** The agent provided a comprehensive analysis of hypothetical bandit algorithms, highlighting the strengths and weaknesses of each algorithm. The agent correctly identified the optimal regret bounds for stochastic MAB and linear bandit settings. The agent also pointed out the importance of constant factors and the limitations of the algorithms.\n*   **ReproducibilityExpert:** The agent provided a detailed explanation of the formal definition of reproducibility and the assumptions required for reproducibility. The agent correctly pointed out that the paper builds upon the work of Impagliazzo et al. (2022) and that the paper's reproducibility guarantees apply to the simulated environment with controlled PRNGs, not necessarily to the behavior of the algorithm in a truly live, unpredictable environment.\n*   **OptimizationTheorist:** The agent provided a detailed analysis of the optimization aspects of the algorithms, the properties of the core sets used in the linear bandit setting, and the computational complexity of the proposed algorithms. The agent correctly pointed out the $O(d^3)$ bottleneck for matrix inversion and the limitations of core sets in high-dimensional settings.\n\nBased on the agent reports, here are some key points to consider:\n\n*   **Novelty:** The paper introduces a novel concept of reproducibility in the bandit setting. The combination of reproducibility with bandit algorithms is interesting and relevant.\n*   **Technical Correctness:** The regret analysis and reproducibility guarantees need to be carefully verified. The agents have raised some concerns about the assumptions made in the analysis and the limitations of the algorithms.\n*   **Significance:** The paper provides a trade-off between reproducibility and regret. The significance of the trade-off needs to be assessed. Is the trade-off practically relevant?\n*   **Computational Feasibility:** The computational complexity of the proposed algorithms needs to be carefully considered. Are the algorithms computationally feasible for practical applications, especially in high-dimensional settings?\n\nNow, let's synthesize the agent reports and provide a final recommendation.\n"}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "assistant", "content": "After careful consideration of the agent reports, I recommend a **Reject** decision for the following reasons:\n\n*   **Limited Practical Significance:** While the concept of reproducible bandits is novel and theoretically interesting, the practical significance of the proposed algorithms is limited. The K^2/\u03c1^2 overhead in the regret bound makes the algorithms less appealing for real-world applications.\n*   **Computational Complexity:** The computational complexity of the proposed algorithms, especially in the linear bandit setting with infinite action spaces, is a major concern. The exponential dependence on the dimension d makes the algorithms computationally infeasible for high-dimensional problems.\n*   **Reproducibility Scope:** The reproducibility guarantees apply to simulated environments with controlled PRNGs, not necessarily to the behavior of the algorithm in a truly live, unpredictable environment. This limits the practical impact of the reproducibility claims.\n\nWhile the paper makes a theoretical contribution by introducing the concept of reproducible bandits, the limitations in practical significance and computational feasibility outweigh the theoretical novelty.\n\n**FINAL DECISION: Reject**\n"}], "ground_truth": "Accept: poster", "response_history": "After careful consideration of the agent reports, I recommend a **Reject** decision for the following reasons:\n\n*   **Limited Practical Significance:** While the concept of reproducible bandits is novel and theoretically interesting, the practical significance of the proposed algorithms is limited. The K^2/\u03c1^2 overhead in the regret bound makes the algorithms less appealing for real-world applications.\n*   **Computational Complexity:** The computational complexity of the proposed algorithms, especially in the linear bandit setting with infinite action spaces, is a major concern. The exponential dependence on the dimension d makes the algorithms computationally infeasible for high-dimensional problems.\n*   **Reproducibility Scope:** The reproducibility guarantees apply to simulated environments with controlled PRNGs, not necessarily to the behavior of the algorithm in a truly live, unpredictable environment. This limits the practical impact of the reproducibility claims.\n\nWhile the paper makes a theoretical contribution by introducing the concept of reproducible bandits, the limitations in practical significance and computational feasibility outweigh the theoretical novelty.\n\n**FINAL DECISION: Reject**\n", "elapsed_time": 150.27007007598877, "reviewer_agents": {"BanditAlgoExpert": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in bandit algorithms and regret analysis.", "system_prompt": "You are an expert in bandit algorithms and regret analysis. You will carefully examine the proposed bandit algorithms, verify the correctness of the regret analysis, and compare the performance of the algorithms to existing bandit algorithms. Pay close attention to the assumptions made in the analysis and the limitations of the algorithms.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "ReproducibilityExpert": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in reproducibility and algorithm stability.", "system_prompt": "You are an expert in reproducibility and algorithm stability. You will focus on the formal definition of reproducibility, the reproducibility guarantees provided by the paper, and the connection to the work of Impagliazzo et al. (2022). Carefully examine the assumptions required for reproducibility and potential limitations.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "OptimizationTheorist": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in optimization theory and computational complexity.", "system_prompt": "You are an expert in optimization theory and computational complexity. You will focus on the optimization aspects of the algorithms, the properties of the core sets used in the linear bandit setting, and the computational complexity of the proposed algorithms. Pay close attention to the scalability of the algorithms to high-dimensional settings.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}}}
