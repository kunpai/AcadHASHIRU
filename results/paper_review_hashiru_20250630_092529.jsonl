{"paper_id": "Pgtn4l6eKjv", "prompt": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: DySR: Adaptive Super-Resolution via Algorithm and System Co-design\n\n1 INTRODUCTION: Deep super-resolution (SR) has been widely used in applications such as medical imaging Li et al. (2021), satellite imaging Shermeyer & Van Etten (2019), and image restoration Qiu et al. (2019). SR has attracted lots of attentions in recent years due to the surging demands in mobile services such as video conference, content sharing, and video streaming, where it helps provide high-resolution visual content from low-resolution data source Zhang et al. (2020); Li et al. (2020; 2021). SR models are resource demanding Li et al. (2021); Lu & Hu (2022) and need to meet Quality of Service (QoS) standards to provide good user experience in visual services. Examples of QoS including meeting a minimum framerate and avoiding interruptions so that users perceive smooth motions. This, however, is challenging for mobile devices where computing and memory resources are limited and the availability of which also depends on other running applications. To meet QoS for different mobile devices, existing works develop models for specific devices Liu et al. (2021b); Lee et al. (2019); Ayazoglu (2021) or use Neural Architecture Search (NAS) Chu et al. (2021); Guo et al. (2020); Huang et al. (2021) to generate multiple hardware-tailored models. However, none of these approaches considers the fluctuating resource environment of mobile devices and often leads to poor QoS. One potential way to achieve good QoS is to dynamically adapt the model based on available resources. The challenges are two folds. First, how to design an adaptive model. Second, how to enable model adaption in a live inference system. To enable adaptive model, we employ NAS to generate a set of models with different sizes so that the most profitable model is used under each resource availability situation to ensure a steady framerate while maximizing the model performance. Unfortunately, none of existing machine learning frameworks supports live model adaption. To enable model adaption in actual system, we explore two ideas. The first idea is to use an assemble method to keep all models loaded in the system at all times to avoid model switching overhead. However, such a method results in a significantly larger memory footprint, which is unsuitable for mobile devices. The second idea is to load a single model at a time, but the the model switching overhead is high as it interrupts the steaming for 1-3 seconds each time it switches models, leading to even worse QoS. To achieve low resource consumption while minimizing the model switching overhead, we propose DySR, an algorithm and system co-design approach for adaptive SR. To keep a small memory footprint and minimize adaption overhead, DySR employs an adaption-aware one-shot NAS approach, where a large meta-graph is trained in one-shot such that sub-graphs share kernel operation weights while exploring the best tradeoffs between performance and frames-per-second (FPS). During inference, the meta-graph is fully loaded in the memory and operations are dynamically adapted according to the real-time resource availability in an incremental manner, i.e., only affected operations are swapped or rerouted. This way, the amount of data moved during an adaption is minimized to significantly reduce the model adaption overhead and avoid any interruption. We evaluate DySR against baselines across a wide variety of hardware (from powerful GPUs to low-end mobile processors) using image and video SR datasets (e.g., Urban100 Huang et al. (2015) and Vimeo90k Xue et al. (2019)). Results show that our adaption-aware one-shot NAS approach can generate models close to the Pareto frontier of the performance vs. FPS tradeoffs and DySR can maintain a steady framerate throughput with low memory footprint (40% less compared to ensemble method). 2 RELATED WORKS: SR. Dong et al. (2014) is among the first works that employs deep learning models for superresolution. Since then deeper and more complex models such as Soh et al. (2019); Nazeri et al. (2019) were proposed for better performance. Generative Adversarial Networks (GANs) Creswell et al. (2018); Wang et al. (2019); Ahn et al. (2018); Wang et al. (2018a) and its variations Prajapati et al. (2021); Guo et al. (2020); Shahsavari et al. (2021) have been shown to be highly effective in tackling this task. Attention mechanisms were introduced to SR as well Zhao et al. (2020); Mei et al. (2021); Chen et al. (2021). Methods such as network pruning, knowledge distillation, and quantization have been applied to reduce computational overhead of existing SR deep learning models Jiang et al. (2021); Zhang et al. (2021b); Hong et al. (2022); Wang et al. (2021). However, all the above efforts focus on building a single model for each hardware and do not consider the dynamic resource environment in mobile devices, and thus fall short in meeting streaming QoS for mobile devices. NAS. Earlier neural architecture search methods rely on Reinforcement Learning (RL) Zoph & Le (2016) and evolutionary algorithms Lu et al. (2018); van Wyk & Bosman (2019) for architecture engineering. However, these methods are extremely resource demanding and often require thousands of GPU hours. Later works such as Wen et al. (2020); Jin et al. (2019) introduce performance prediction, shared weight training, and proxy training to speed up the architecture engineering process. DARTS Liu et al. (2018) and its followups Chen et al. (2019); Wu et al. (2019) adopt a differentiable architecture search paradigm. The once-for-all work Cai et al. (2019) proposes the idea of generating a single model for multiple hardware deployments though pruning and model swapping is needed for each deployment. One-shot NAS Bender et al. (2018) and its variations Huang & Chu (2021); Zhang et al. (2021a); Zhao et al. (2021) can generate models with few search iterations and have been explored for SR Chu et al. (2021); Guo et al. (2020); Liu et al. (2021a); Zhan et al. (2021) but existing works only focus on designing a single model and do not consider QoS for streaming on mobile devices. In Section 5, we compare our method with existing SR models. The results show that our model achieves Pareto optimal performance while meeting QoS. 3 MOTIVATION AND CHALLENGES: On mobile devices, SR tasks are often running along with other applications, e.g., users often watch streaming videos while browsing websites or doing online shopping; making video calls while playing games or taking notes. Therefore, the available computing and memory resources for SR is constantly changing on the already resource constraint mobile devices. We demonstrate the impact of changing available resources on framerate for static models running on real mobile devices. We test a set of state-of-the-art SR models on the mobile devices they were targeted for and manually limit the amount of resource available over time, i.e., by limiting GPU power and adding background loads to CPU. We use a workload trace to show how the utilization changes over time, see Figure 1. We observe that reducing the processing power available for the models over time results in significant frames-per-second (FPS) drop. These results demonstrate that state-of-the-art SR models do not perform as expected under practical circumstances, and to maintain QoS (i.e., minimum FPS), models need to be adapted according to the available resources. To fill this gap, however, is challenging. First, we need to design a set of models that can achieve Pareto-optimal performance under different framerate. However, no existing works can generate such a set of models. Second, we need to adapt models within milliseconds in the real-time system to meet QoS requirements. Unfortunately, no existing machine learning frameworks support real-time model adaption. One potential solution is to switch between a list of static models according to the available resources. We prototype this idea and perform experiments on different devices, see Fig. 2. Here, we unload and reload static models as soon as the utilizations change at the intervals 5, 10, and 15 seconds, see (a), (b), and (c) respectively. Across all devices, we observe an interruption of service during each model swapping period. As pointed out in Liao et al. (2019), such interruption is the time it takes to read the new model from storage and initialize all the parameters in memory. Preloading multiple static models as one is a potential solution to advoid such interruption. However, the assembled model has a significantly higher memory consumption, see Fig. 2d, which is not practical for resource constraint mobile devices. 4 ALGORITHM AND SYSTEM CO-DESIGN: DySR To address the aforementioned challenges, in this section, we adopt an algorithm and system codesign approach and propose DySR. DySR employs adaptive-aware one-shot NAS to create a set of models that achieve Pareto-optimal performance under different framerate with minimum adaption overhead during runtime. DySR also introduces an incremental adaption approach to further reduce the model adaption overhead to meet QoS. 4.1 ADAPTION-AWARE ONE-SHOT NEURAL ARCHITECTURE SEARCH: Neural Architecture Search (NAS) allows creating multiple models in an automated fashion, and thus we choose it for model generation. However, for SR, existing NAS methods only target for a finding a single model with the highest performance while ignoring the framerate and model adaption overhand. To provide QoS on mobile devices, we design an adaptive-aware one-shot NAS method for generating a set of models that achieves Pareto-optimal performance with framerate constraint and low model adaption overhead under different hardware devices and resource availability. Following Liu et al. (2018); Bender et al. (2018), we define the architecture search space as S, which is represented as a single Directed Acyclic Graph (DAG) containing all possible path and operation combinations, called the meta-graph. The main idea here is to train a meta-graph such that it contains sub-graph models with Pareto-optimal performance across a range of framerates. The sub-graphs are all part of the same meta-graph, so that during inference the full meta-graph is deployed and the sub-graphs can be adapted in real time to maintain QoS using sub-graph adaption policy. Adaption-aware Sub-graph Selection. The generation of adaption-aware sub-graphs requires two unique criteria to be fulfilled during the NAS search phase to be effective. First, in order to reduce resource consumption, we need to reduce total memory of the meta-graph to be significantly less than an assembled model. Second, we need to minimize the sub-graph adaption time to ensure uninterrupted service. In order to keep memory consumption at a minimum, we design our meta-graph space to share operations between sub-graphs wherever possible (e.g. two sub-graphs requiring a 3x3x64 convolutional operation at the same layer will use the same meta-graph operation, and thus have intersecting execution paths). To keep the switching time between models low, we need to reduce the number of execution path re-routings between sub-graphs to a minimum. Luckily, we find that both of these properties are related based on the observation that the number of total operations meta-graph and the number of path re-routes are inversely proportional to the number of shared operations. In other words, the more operations that are shared among sub-graphs, the less number of redundant operations there are and so meta-graph memory size is reduced. At the same time, more shared operations also means more common execution paths between sub-graphs and so less number of paths need to be changed while adapting. We use this observation to develop the custom search policy to generate our adaptive models. Thus the objective of our sub-graph selection policy can be formally defined as the constrained joint optimization problem - P \u2217 = argmax Eval(A(\u03b1,D)) + \u00b5(\u03b1,A\u2212 \u03b1) s.t. fps(\u03b1,D) \u2265 F \u03c1min (1) Here, P is the HR quality metric (i.e. PSNR or SSIM), Eval(A(\u03b1,D)) is the evaluation value of sub-graph \u03b1 of meta-graph A on dataset D, \u00b5 is the number of operations \u03b1 has shared with all other sub-graph A \u2212 \u03b1. fps is the frames-per-second and F \u03c1min is the minimum FPS allowed under available resource \u03c1. Note that \u03c1 can change over time and can be generalizable to any type of computational resource even though in our case we mainly demonstrate using the available processing power. The implementation details on how we solve this is described in detail below. Meta-Graph Design. The first step in designing a NAS framework is the Meta-graph. This needs to be diverse enough to generate a wide range of efficient candidate architectures with competitive performance with state-of-the-art. In our case, we focus on designing a meta-graph with a variety of inference FPS with a good set of PSNR values, rather than developing a single novel cell architecture that beats all state-of-the-art SR models. We design our meta-graph as a cell-based architecture (see Figure 3a). The input layer and the last output layer are kept constant, and the architecture between them consists of layers of cells followed by an upsampling layer. During the search phase, we sample paths within each searchable cell such that only one type of cell block is chosen at any one time. Existing works have already proposed a wide variety of cell blocks which were demonstrably efficient and also have many operations in common, and so we use them for our search space. Specifically, we use the cells from CARN Ahn et al. (2018), FALSR Chu et al. (2021), RCAB Zhang et al. (2018), AutoGAN Fu et al. (2020), WDSR Yu et al. (2018), ESRGAN Wang et al. (2018a) and MoreMNAS Chu et al. (2020) since they are well-known, efficient, self-contained blocks. Another large benefit is that all these cell blocks share many of the same operation types, making it possible for them to share weights. The most commonly shared operations between them are the convolutional and dense layers. As such, we set their parameter search spaces as: \u2022 Convolutional Filters - 1x1, 3x3, 5x5, \u2022 Convolutional Channels - 3, 16, 32, 64, \u2022 Dense - 64, 128, 256. Cell blocks for RCAB, AutoGAN, ESGRAN and FALSR also have attention and residual connections, which we can enable or disable as paths since we find that while their inclusion result in an increase in performance, it is not significant at the higher end models when considering the FPS increases. Layers. The number of layers is one of the most important factors when determining the overall FPS and is therefore a very important search parameter. Additionally, different layer types perform distinctly with different layer numbers Wang et al. (2018a); Ahn et al. (2018); Chu et al. (2021). Therefore, we set the range of our number of layers of cells from between 15 to 50 with intervals/groups of 5 for every single cell type. Lastly, for our upscaling layer, we keep the choices between four types \u2013 RCAN Zhang et al. (2018), Deconvolutional Dong et al. (2016), Depth2Space Bhardwaj et al. (2021) and Multi-branch bicubic Behjati et al. (2021). We have a total of number of possible cell types of 12 when including blocks with and without attention and residual networks and a total number of parameter combinations of 3 * 4 * 3 = 36, giving us a total number of 12 * 36 = 432 cell combinations. With possible layer combinations of 3 to 10, we have a permutation formula 432P10+ 432P9+ 432P8+ ...432P3 times 4 for the number of possible upscaling layers, which gives us a total of 5.8e1023 possible networks. Sampling Paths. Given such a large number of possible graphs, we need an efficient method for traversal. Fortunately, our hard constraint of Eq. 1 for FPS can play an important part in reducing the space here. We first start with setting the number of layers to the minimum possible (15 in our case) and start random uniform sampling models without replacement. We profile them on the desired hardware to ensure the FPS is under the Fmax. If not, it is permanently discarded. Once we have Nl models, we move to the next layer limit and iterate until we have reached the last layer limit I (50 in our current setting) which gives us a total of Nl \u2217 I models. Note that for our scenario, we need multiple models across a wide FPS spectrum such that for the same dataset so that we have choices on which models to deploy for different hardware and levels of resource availability. Therefore, before starting the training we first determine suitable models by binning them. We select how many bins of FPS we need as B with each bin having a window size of 200ms, and we bin our profiled models within each bin. We then train and evaluate each model within each bin for 5 epochs and evaluate them which gives us a sufficient estimate of final model performance. We now need to use an evaluation criteria to rank Nb models per bin. For this method, we set a desired PSNR threshold for bin b decided by the Pareto frontier. For example, from figure 8 we see that models on 1080Ti with 20 FPS needs to have at least 34 PSNR to be competitive, so we select that as the threshold. Then we take all models above that threshold and rank them based on the number of intersecting operations with others \u00b5 from Eq.1 We iterate this process until all bins B have Nb models. This way, we have a total of Nb \u2217 B models for exploration. The parameters Nl, Nb and B are very important for determining the search and model efficiency for our framework and their choice is discussed later in Section 5. We throw out all other models which are not within the bins and redeploy the meta-graph with only the selected model operations. This reduces the number of models in the search space drastically and reduces weight sharing which further alleviates the co-sharing issue. Training. After reducing the search space to Nb \u2217 B models, we then train them till convergence. As mentioned above, we perform GAN training. Our predictor models are the searched models and for our discriminator we use the pre-trained VGG-16 model. We use the Adam optimizer with loss function for the generator defined as - lgen = 1e 2 \u2217 |xhr \u2212G(xlr)|+ lvgg(G(xlr, xhr)) + (5e\u22123 \u2212mean(D(G(xlr)))) (2) where xhr and xlr are the HR and LR images respectively, G is the generator network, lvgg is the VGG loss Wang et al. (2018b) and D is the discriminator. This loss is discussed in detail in Wang et al. (2018b). 4.2 ADAPTIVE SUB-GRAPHS: Standard implementations employ static graphs with fixed inputs and outputs pre-set between operations and are not changed during forward passes. As a result, traditionally designed meta-graphs cannot change operation sequences in real time. Instead, they have to reroute the batches among different model inputs for inference. For our case, we have a single large model in the form of a meta-graph. It contains sub-graph architectures which share operations. In order to switch between sub-graphs, we can change the input-output paths for the operations. We take advantage of the non-static graph property to implement this. We first define the sub-graphs as a network with the nodes representing the operations and the input-output variables as the end-point for paths between them. We store each of the meta-graph operations as node objects with its input and output variable references within. During inference, we can change the input/output variables of one operation node to point to and from another node. This effectively decides the execution path of the input batch, thereby defining the sub-graph architecture itself. Since we have all the operations with their trained weights loaded in memory, we can call every available operation with our desired input values at any time. Thus, when we want to execute a specific sub-graph path, we start by first selecting the required operations from the list of all operation nodes. We then create the sub-graph architecture by routing the output variables (which are pointers to Tensors in memory) of one operation to the input of the next desired operation by changing the variable references. We can do this by applying the equal operation of the output variable of one operation node to the input variable of the next operation node, which essentially links them together into a direct execution path. Since we can set the output pointer variables to the input variables at any time between forward passes without disrupting the computation results, we can re-route paths at a per-batch granularity. Thus, by changing the pointer variables of inputoutputs, we can select sub-graphs architectures as per our requirements. This allows us to adapt between sub-graphs in real-time without degrading QoS. 4.3 MODEL ADAPTION POLICY: Algorithm 1 Model Selection Policy Inputs: Meta-graph after training A\u2217, all hardware resources \u03c1, maximum allowed FPS Fmax, empty profile table T , \u03b1sel is the list of sub-graphs that meet the selection criteria. for each \u03b1i in A\u2217 do for each \u03c1j do Ti,j = Profile(\u03b1i, \u03c1j) end for end for while True do \u03c1c = CurrentResources(), \u03b1sel = [] for each \u03b1i in A\u2217 do if Ti,c \u2264 Fmax then \u03b1sel += \u03b1i end if end for \u03b1sel = Sort(\u03b1sel) based on max PSNR Switch to model \u03b1sel[0] end while We design an adaption policy to decide the model adaption strategy in real time for a given resource availability. This is done in two steps. First, we generate a profiling table which contains the framerate of each model under different resource availability (e.g., utilization) for a device. We profile each model using 20 image frames and use the average latency to infer framerate. The profiling cost is no. of models \u2217 resource availability granularity. This step is a one-time offline effort and thus the profiling cost is not a major overhead. During the real-time SR task, we keep track of the resource availability of system via resource monitoring tools. Once we observe a change in resource availability, we adapt the model by using the search criteria \u03b1sel where fps(\u03b1sel) \u2264 Fmax and sort the candidates based on their performance. We use the adaptive sub-graph mechanism to select the top performing sub-graph in real time and use adapt to it to maintain QoS (see Alg. 1 for details). 5 EVALUATION:  5.1 TRAINING SETUP: We implement DySR using PyTorch and perform the search and training using 4 A100 GPUs, taking 21 GPU days to complete per run. For training the searched models till convergence, we use Div2K training dataset with 64x64 patch size, batch size of 16 and learning rate of 0.0043. For the search parameters, we use the values 15, 2 and 5 for Nl, Nb and B respectively. For the video superresolution dataset, we train with Vimeo90k Xue et al. (2019). The LR images and frames were generated by bicubic downsampling them by a factor of 2 and 4. To generate the Pareto Optimality curve, we use the the models as shown in Figure 6a). 5.2 BASELINES AND PARAMETERS We now describe our choice of baselines and search parameters. Since no other papers (to our knowledge) address this problem in the SR scope, we create our baseline model by combining all the models from Figure 6a (referred to as Assembled). It contains high-performing models over a wide FPS spectrum, which is what our DySR should ideally generate as well and so makes for a fair comparison. As mentioned above, the parameters Nb and B are very important in determining the quality of the models. Together they determine the total number of sub-graphs generated which in turn determines the amount of memory taken up by the meta-graph. It also indirectly determines how much weight is shared by the sub-graphs (more sub-graphs means more chances of operations being shared). In Figure 6b we show the memory consumed when executing DySR, Assembled and the single static CARN model after applying dynamic 8-bit quantization for all of them Pagliari et al. (2018). As we tune the Nb and B, we get 5, 10, 15 and 20 models (no. of models = Nb \u2217B). We see that even with 15 models, DySR has almost equal consumption to Assembled due to weight sharing among sub-graphs. With 5 models, DySR only consumes around twice as much memory compared to the single model, thus illustrating the memory efficiency of our weight-shared NAS system. Since Assembled contains 10 models, we select 10 models for our DySR as well for fair comparison for the rest of the experiments. Table 4 shows that with equal number of models, our framework has less number of parameters and size on disk. 5.3 PARETO OPTIMALITY: We now test our searched and fully trained sub-graphs against state-of-the-art models. Figure 7 shows the PSNR vs. FPS performance of each of the 10 sub-networks in DySR for different datasets. The sub-graphs are evaluated by passing the input through the execution path of the specific subgraphs and their corresponding inference time measured. We observe here that across both im- age super-resolution and video super-resolution datasets for 2X and 4X upscaling, our sub-graphs achieve Pareto Optimality for the PSNR vs. FPS tradeoff created by the state-of-the-art models across the full FPS spectrum, demonstrating the efficiency of our search space. We next evaluate our generated models against a diverse set of hardware, and the results are shown in Figure 8. We use a mobile CPU Snapdragon 855, a laptop CPU Intel i5-560M, a desktop grade GPU 1080Ti and a server-grade GPU A100 (see Table 5). This figure shows the results for 2X scaling for the Div2k dataset (other datasets show similar results, so we eliminate them for lack of space). We search our models on the 1080Ti to be above 15 FPS. Therefore, the model\u2019s FPS shift for different hardware but still have models above 15 FPS (as was our search criteria). We see that across all these hardware, the Pareto Optimality of our generated sub-graphs are maintained, demonstrating that even with one search run we can generate efficient models for inference for such a wide variety of models. This property allows for model adaptivity since this allows the selection of models with any desired FPS for any hardware, and ensures close to the optimal HR output quality. Along with the ability to change models on-the-fly, DySR allows for optimal PSNR vs. FPS tradeoff for resource adaptivity as well, as will be demonstrated next. 5.4 DYNAMIC RESOURCE ADAPTIVITY: To demonstrate the resource adaptivity of our adaptive sub-graphs and model selection policy, we use three traces to generate real world resource fluctuation scenarios. We use a bursty trace to simulate extreme changes in resource availability, stable to illustrate a relatively small variation in resource availability and random. Since resource adaptivity has rarely been discussed in the SR context, we create two possible baselines. The static baseline contains a single model deployed at any one time from the list of state-of-the-art models used above for generating the Pareto Optimality curve. Assembled baseline contains all the models from the state-of-the-art list deployed at the same time to represent the scenario of keeping models preloaded. Based on available resources, we use our real-time swapping policy to choose the best model for both our DySR as well as the two baselines. Figure 9 shows the results of our experiment using the Div2k dataset on 1080Ti for 2x scaling. The top row shows the trace used, the middle row shows the CDF of the resulting FPS fluctuation over time, and the bottom row shows the CDF of the PSNR values. The CDFs represent the probability of the FPS or PSNR being less than a certain value, therefore the higher y-axis values at lower p values mean the better they perform. Here we observe that across all traces, DySR achieves overall higher values of PSNR compared to the baselines. The FPS for Assembled and DySR are somewhat similar due to both having little to no reloading overhead. However, Assembled performs worse for PSNR due to having less granularity of models available across the full FPS spectrum. For example, between 20 to 25 FPS in Figure 7 for Div2K with 2X upscaling, we see that our framework generates four models but the Assembled only has two. Therefore, Assembled has less options and tends to choose models with lower PSNR in order to be able to maintain its FPS. The Static models perform the worst due to having many interruption of services as expected. For the PSNR values, Static has a significant portion of its CDF distribution at 0 and therefore performs much worse than the others, especially at the lower p values. Our framework outperforms all baselines by having the highest PSNR probabilities. Another observation is that the largest improvement in PSNR and FPS gained from DySR is for the bursty workload. This is mainly due to there being more model switches required here compared to the other two workloads. This amplifies the benefits derived from the model selection policy and granularity and thus a greater difference is observed. As the workloads grow more stable, the numbers vary less, illustrating that the more unstable a system the more it should benefit from our solutions. Based on all these observations, we conclude that DySR outperforms both baselines and can address the resource adaptivity issue. 6 CONCLUSION: In this paper, we focus on providing QoS for super resolution used in steaming services of mobile devices. We propose an algorithm and system co-design approach DySR, which employs an adaptive-aware one-shot NAS to generate models with Pareto-optimal performance, low switching overhead, and framerate constraint that can be adapted in real-time via our low cost incremental adaption method. DySR outperforms existing state-of-the-art models as well as the simple assemble method we explored by achieving the best performance while providing a steady framerate throughput with a memory footprint that is 40% less. A APPENDIX: You may include other additional sections here.", "agent_review": [{"role": "user", "content": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: DySR: Adaptive Super-Resolution via Algorithm and System Co-design\n\n1 INTRODUCTION: Deep super-resolution (SR) has been widely used in applications such as medical imaging Li et al. (2021), satellite imaging Shermeyer & Van Etten (2019), and image restoration Qiu et al. (2019). SR has attracted lots of attentions in recent years due to the surging demands in mobile services such as video conference, content sharing, and video streaming, where it helps provide high-resolution visual content from low-resolution data source Zhang et al. (2020); Li et al. (2020; 2021). SR models are resource demanding Li et al. (2021); Lu & Hu (2022) and need to meet Quality of Service (QoS) standards to provide good user experience in visual services. Examples of QoS including meeting a minimum framerate and avoiding interruptions so that users perceive smooth motions. This, however, is challenging for mobile devices where computing and memory resources are limited and the availability of which also depends on other running applications. To meet QoS for different mobile devices, existing works develop models for specific devices Liu et al. (2021b); Lee et al. (2019); Ayazoglu (2021) or use Neural Architecture Search (NAS) Chu et al. (2021); Guo et al. (2020); Huang et al. (2021) to generate multiple hardware-tailored models. However, none of these approaches considers the fluctuating resource environment of mobile devices and often leads to poor QoS. One potential way to achieve good QoS is to dynamically adapt the model based on available resources. The challenges are two folds. First, how to design an adaptive model. Second, how to enable model adaption in a live inference system. To enable adaptive model, we employ NAS to generate a set of models with different sizes so that the most profitable model is used under each resource availability situation to ensure a steady framerate while maximizing the model performance. Unfortunately, none of existing machine learning frameworks supports live model adaption. To enable model adaption in actual system, we explore two ideas. The first idea is to use an assemble method to keep all models loaded in the system at all times to avoid model switching overhead. However, such a method results in a significantly larger memory footprint, which is unsuitable for mobile devices. The second idea is to load a single model at a time, but the the model switching overhead is high as it interrupts the steaming for 1-3 seconds each time it switches models, leading to even worse QoS. To achieve low resource consumption while minimizing the model switching overhead, we propose DySR, an algorithm and system co-design approach for adaptive SR. To keep a small memory footprint and minimize adaption overhead, DySR employs an adaption-aware one-shot NAS approach, where a large meta-graph is trained in one-shot such that sub-graphs share kernel operation weights while exploring the best tradeoffs between performance and frames-per-second (FPS). During inference, the meta-graph is fully loaded in the memory and operations are dynamically adapted according to the real-time resource availability in an incremental manner, i.e., only affected operations are swapped or rerouted. This way, the amount of data moved during an adaption is minimized to significantly reduce the model adaption overhead and avoid any interruption. We evaluate DySR against baselines across a wide variety of hardware (from powerful GPUs to low-end mobile processors) using image and video SR datasets (e.g., Urban100 Huang et al. (2015) and Vimeo90k Xue et al. (2019)). Results show that our adaption-aware one-shot NAS approach can generate models close to the Pareto frontier of the performance vs. FPS tradeoffs and DySR can maintain a steady framerate throughput with low memory footprint (40% less compared to ensemble method). 2 RELATED WORKS: SR. Dong et al. (2014) is among the first works that employs deep learning models for superresolution. Since then deeper and more complex models such as Soh et al. (2019); Nazeri et al. (2019) were proposed for better performance. Generative Adversarial Networks (GANs) Creswell et al. (2018); Wang et al. (2019); Ahn et al. (2018); Wang et al. (2018a) and its variations Prajapati et al. (2021); Guo et al. (2020); Shahsavari et al. (2021) have been shown to be highly effective in tackling this task. Attention mechanisms were introduced to SR as well Zhao et al. (2020); Mei et al. (2021); Chen et al. (2021). Methods such as network pruning, knowledge distillation, and quantization have been applied to reduce computational overhead of existing SR deep learning models Jiang et al. (2021); Zhang et al. (2021b); Hong et al. (2022); Wang et al. (2021). However, all the above efforts focus on building a single model for each hardware and do not consider the dynamic resource environment in mobile devices, and thus fall short in meeting streaming QoS for mobile devices. NAS. Earlier neural architecture search methods rely on Reinforcement Learning (RL) Zoph & Le (2016) and evolutionary algorithms Lu et al. (2018); van Wyk & Bosman (2019) for architecture engineering. However, these methods are extremely resource demanding and often require thousands of GPU hours. Later works such as Wen et al. (2020); Jin et al. (2019) introduce performance prediction, shared weight training, and proxy training to speed up the architecture engineering process. DARTS Liu et al. (2018) and its followups Chen et al. (2019); Wu et al. (2019) adopt a differentiable architecture search paradigm. The once-for-all work Cai et al. (2019) proposes the idea of generating a single model for multiple hardware deployments though pruning and model swapping is needed for each deployment. One-shot NAS Bender et al. (2018) and its variations Huang & Chu (2021); Zhang et al. (2021a); Zhao et al. (2021) can generate models with few search iterations and have been explored for SR Chu et al. (2021); Guo et al. (2020); Liu et al. (2021a); Zhan et al. (2021) but existing works only focus on designing a single model and do not consider QoS for streaming on mobile devices. In Section 5, we compare our method with existing SR models. The results show that our model achieves Pareto optimal performance while meeting QoS. 3 MOTIVATION AND CHALLENGES: On mobile devices, SR tasks are often running along with other applications, e.g., users often watch streaming videos while browsing websites or doing online shopping; making video calls while playing games or taking notes. Therefore, the available computing and memory resources for SR is constantly changing on the already resource constraint mobile devices. We demonstrate the impact of changing available resources on framerate for static models running on real mobile devices. We test a set of state-of-the-art SR models on the mobile devices they were targeted for and manually limit the amount of resource available over time, i.e., by limiting GPU power and adding background loads to CPU. We use a workload trace to show how the utilization changes over time, see Figure 1. We observe that reducing the processing power available for the models over time results in significant frames-per-second (FPS) drop. These results demonstrate that state-of-the-art SR models do not perform as expected under practical circumstances, and to maintain QoS (i.e., minimum FPS), models need to be adapted according to the available resources. To fill this gap, however, is challenging. First, we need to design a set of models that can achieve Pareto-optimal performance under different framerate. However, no existing works can generate such a set of models. Second, we need to adapt models within milliseconds in the real-time system to meet QoS requirements. Unfortunately, no existing machine learning frameworks support real-time model adaption. One potential solution is to switch between a list of static models according to the available resources. We prototype this idea and perform experiments on different devices, see Fig. 2. Here, we unload and reload static models as soon as the utilizations change at the intervals 5, 10, and 15 seconds, see (a), (b), and (c) respectively. Across all devices, we observe an interruption of service during each model swapping period. As pointed out in Liao et al. (2019), such interruption is the time it takes to read the new model from storage and initialize all the parameters in memory. Preloading multiple static models as one is a potential solution to advoid such interruption. However, the assembled model has a significantly higher memory consumption, see Fig. 2d, which is not practical for resource constraint mobile devices. 4 ALGORITHM AND SYSTEM CO-DESIGN: DySR To address the aforementioned challenges, in this section, we adopt an algorithm and system codesign approach and propose DySR. DySR employs adaptive-aware one-shot NAS to create a set of models that achieve Pareto-optimal performance under different framerate with minimum adaption overhead during runtime. DySR also introduces an incremental adaption approach to further reduce the model adaption overhead to meet QoS. 4.1 ADAPTION-AWARE ONE-SHOT NEURAL ARCHITECTURE SEARCH: Neural Architecture Search (NAS) allows creating multiple models in an automated fashion, and thus we choose it for model generation. However, for SR, existing NAS methods only target for a finding a single model with the highest performance while ignoring the framerate and model adaption overhand. To provide QoS on mobile devices, we design an adaptive-aware one-shot NAS method for generating a set of models that achieves Pareto-optimal performance with framerate constraint and low model adaption overhead under different hardware devices and resource availability. Following Liu et al. (2018); Bender et al. (2018), we define the architecture search space as S, which is represented as a single Directed Acyclic Graph (DAG) containing all possible path and operation combinations, called the meta-graph. The main idea here is to train a meta-graph such that it contains sub-graph models with Pareto-optimal performance across a range of framerates. The sub-graphs are all part of the same meta-graph, so that during inference the full meta-graph is deployed and the sub-graphs can be adapted in real time to maintain QoS using sub-graph adaption policy. Adaption-aware Sub-graph Selection. The generation of adaption-aware sub-graphs requires two unique criteria to be fulfilled during the NAS search phase to be effective. First, in order to reduce resource consumption, we need to reduce total memory of the meta-graph to be significantly less than an assembled model. Second, we need to minimize the sub-graph adaption time to ensure uninterrupted service. In order to keep memory consumption at a minimum, we design our meta-graph space to share operations between sub-graphs wherever possible (e.g. two sub-graphs requiring a 3x3x64 convolutional operation at the same layer will use the same meta-graph operation, and thus have intersecting execution paths). To keep the switching time between models low, we need to reduce the number of execution path re-routings between sub-graphs to a minimum. Luckily, we find that both of these properties are related based on the observation that the number of total operations meta-graph and the number of path re-routes are inversely proportional to the number of shared operations. In other words, the more operations that are shared among sub-graphs, the less number of redundant operations there are and so meta-graph memory size is reduced. At the same time, more shared operations also means more common execution paths between sub-graphs and so less number of paths need to be changed while adapting. We use this observation to develop the custom search policy to generate our adaptive models. Thus the objective of our sub-graph selection policy can be formally defined as the constrained joint optimization problem - P \u2217 = argmax Eval(A(\u03b1,D)) + \u00b5(\u03b1,A\u2212 \u03b1) s.t. fps(\u03b1,D) \u2265 F \u03c1min (1) Here, P is the HR quality metric (i.e. PSNR or SSIM), Eval(A(\u03b1,D)) is the evaluation value of sub-graph \u03b1 of meta-graph A on dataset D, \u00b5 is the number of operations \u03b1 has shared with all other sub-graph A \u2212 \u03b1. fps is the frames-per-second and F \u03c1min is the minimum FPS allowed under available resource \u03c1. Note that \u03c1 can change over time and can be generalizable to any type of computational resource even though in our case we mainly demonstrate using the available processing power. The implementation details on how we solve this is described in detail below. Meta-Graph Design. The first step in designing a NAS framework is the Meta-graph. This needs to be diverse enough to generate a wide range of efficient candidate architectures with competitive performance with state-of-the-art. In our case, we focus on designing a meta-graph with a variety of inference FPS with a good set of PSNR values, rather than developing a single novel cell architecture that beats all state-of-the-art SR models. We design our meta-graph as a cell-based architecture (see Figure 3a). The input layer and the last output layer are kept constant, and the architecture between them consists of layers of cells followed by an upsampling layer. During the search phase, we sample paths within each searchable cell such that only one type of cell block is chosen at any one time. Existing works have already proposed a wide variety of cell blocks which were demonstrably efficient and also have many operations in common, and so we use them for our search space. Specifically, we use the cells from CARN Ahn et al. (2018), FALSR Chu et al. (2021), RCAB Zhang et al. (2018), AutoGAN Fu et al. (2020), WDSR Yu et al. (2018), ESRGAN Wang et al. (2018a) and MoreMNAS Chu et al. (2020) since they are well-known, efficient, self-contained blocks. Another large benefit is that all these cell blocks share many of the same operation types, making it possible for them to share weights. The most commonly shared operations between them are the convolutional and dense layers. As such, we set their parameter search spaces as: \u2022 Convolutional Filters - 1x1, 3x3, 5x5, \u2022 Convolutional Channels - 3, 16, 32, 64, \u2022 Dense - 64, 128, 256. Cell blocks for RCAB, AutoGAN, ESGRAN and FALSR also have attention and residual connections, which we can enable or disable as paths since we find that while their inclusion result in an increase in performance, it is not significant at the higher end models when considering the FPS increases. Layers. The number of layers is one of the most important factors when determining the overall FPS and is therefore a very important search parameter. Additionally, different layer types perform distinctly with different layer numbers Wang et al. (2018a); Ahn et al. (2018); Chu et al. (2021). Therefore, we set the range of our number of layers of cells from between 15 to 50 with intervals/groups of 5 for every single cell type. Lastly, for our upscaling layer, we keep the choices between four types \u2013 RCAN Zhang et al. (2018), Deconvolutional Dong et al. (2016), Depth2Space Bhardwaj et al. (2021) and Multi-branch bicubic Behjati et al. (2021). We have a total of number of possible cell types of 12 when including blocks with and without attention and residual networks and a total number of parameter combinations of 3 * 4 * 3 = 36, giving us a total number of 12 * 36 = 432 cell combinations. With possible layer combinations of 3 to 10, we have a permutation formula 432P10+ 432P9+ 432P8+ ...432P3 times 4 for the number of possible upscaling layers, which gives us a total of 5.8e1023 possible networks. Sampling Paths. Given such a large number of possible graphs, we need an efficient method for traversal. Fortunately, our hard constraint of Eq. 1 for FPS can play an important part in reducing the space here. We first start with setting the number of layers to the minimum possible (15 in our case) and start random uniform sampling models without replacement. We profile them on the desired hardware to ensure the FPS is under the Fmax. If not, it is permanently discarded. Once we have Nl models, we move to the next layer limit and iterate until we have reached the last layer limit I (50 in our current setting) which gives us a total of Nl \u2217 I models. Note that for our scenario, we need multiple models across a wide FPS spectrum such that for the same dataset so that we have choices on which models to deploy for different hardware and levels of resource availability. Therefore, before starting the training we first determine suitable models by binning them. We select how many bins of FPS we need as B with each bin having a window size of 200ms, and we bin our profiled models within each bin. We then train and evaluate each model within each bin for 5 epochs and evaluate them which gives us a sufficient estimate of final model performance. We now need to use an evaluation criteria to rank Nb models per bin. For this method, we set a desired PSNR threshold for bin b decided by the Pareto frontier. For example, from figure 8 we see that models on 1080Ti with 20 FPS needs to have at least 34 PSNR to be competitive, so we select that as the threshold. Then we take all models above that threshold and rank them based on the number of intersecting operations with others \u00b5 from Eq.1 We iterate this process until all bins B have Nb models. This way, we have a total of Nb \u2217 B models for exploration. The parameters Nl, Nb and B are very important for determining the search and model efficiency for our framework and their choice is discussed later in Section 5. We throw out all other models which are not within the bins and redeploy the meta-graph with only the selected model operations. This reduces the number of models in the search space drastically and reduces weight sharing which further alleviates the co-sharing issue. Training. After reducing the search space to Nb \u2217 B models, we then train them till convergence. As mentioned above, we perform GAN training. Our predictor models are the searched models and for our discriminator we use the pre-trained VGG-16 model. We use the Adam optimizer with loss function for the generator defined as - lgen = 1e 2 \u2217 |xhr \u2212G(xlr)|+ lvgg(G(xlr, xhr)) + (5e\u22123 \u2212mean(D(G(xlr)))) (2) where xhr and xlr are the HR and LR images respectively, G is the generator network, lvgg is the VGG loss Wang et al. (2018b) and D is the discriminator. This loss is discussed in detail in Wang et al. (2018b). 4.2 ADAPTIVE SUB-GRAPHS: Standard implementations employ static graphs with fixed inputs and outputs pre-set between operations and are not changed during forward passes. As a result, traditionally designed meta-graphs cannot change operation sequences in real time. Instead, they have to reroute the batches among different model inputs for inference. For our case, we have a single large model in the form of a meta-graph. It contains sub-graph architectures which share operations. In order to switch between sub-graphs, we can change the input-output paths for the operations. We take advantage of the non-static graph property to implement this. We first define the sub-graphs as a network with the nodes representing the operations and the input-output variables as the end-point for paths between them. We store each of the meta-graph operations as node objects with its input and output variable references within. During inference, we can change the input/output variables of one operation node to point to and from another node. This effectively decides the execution path of the input batch, thereby defining the sub-graph architecture itself. Since we have all the operations with their trained weights loaded in memory, we can call every available operation with our desired input values at any time. Thus, when we want to execute a specific sub-graph path, we start by first selecting the required operations from the list of all operation nodes. We then create the sub-graph architecture by routing the output variables (which are pointers to Tensors in memory) of one operation to the input of the next desired operation by changing the variable references. We can do this by applying the equal operation of the output variable of one operation node to the input variable of the next operation node, which essentially links them together into a direct execution path. Since we can set the output pointer variables to the input variables at any time between forward passes without disrupting the computation results, we can re-route paths at a per-batch granularity. Thus, by changing the pointer variables of inputoutputs, we can select sub-graphs architectures as per our requirements. This allows us to adapt between sub-graphs in real-time without degrading QoS. 4.3 MODEL ADAPTION POLICY: Algorithm 1 Model Selection Policy Inputs: Meta-graph after training A\u2217, all hardware resources \u03c1, maximum allowed FPS Fmax, empty profile table T , \u03b1sel is the list of sub-graphs that meet the selection criteria. for each \u03b1i in A\u2217 do for each \u03c1j do Ti,j = Profile(\u03b1i, \u03c1j) end for end for while True do \u03c1c = CurrentResources(), \u03b1sel = [] for each \u03b1i in A\u2217 do if Ti,c \u2264 Fmax then \u03b1sel += \u03b1i end if end for \u03b1sel = Sort(\u03b1sel) based on max PSNR Switch to model \u03b1sel[0] end while We design an adaption policy to decide the model adaption strategy in real time for a given resource availability. This is done in two steps. First, we generate a profiling table which contains the framerate of each model under different resource availability (e.g., utilization) for a device. We profile each model using 20 image frames and use the average latency to infer framerate. The profiling cost is no. of models \u2217 resource availability granularity. This step is a one-time offline effort and thus the profiling cost is not a major overhead. During the real-time SR task, we keep track of the resource availability of system via resource monitoring tools. Once we observe a change in resource availability, we adapt the model by using the search criteria \u03b1sel where fps(\u03b1sel) \u2264 Fmax and sort the candidates based on their performance. We use the adaptive sub-graph mechanism to select the top performing sub-graph in real time and use adapt to it to maintain QoS (see Alg. 1 for details). 5 EVALUATION:  5.1 TRAINING SETUP: We implement DySR using PyTorch and perform the search and training using 4 A100 GPUs, taking 21 GPU days to complete per run. For training the searched models till convergence, we use Div2K training dataset with 64x64 patch size, batch size of 16 and learning rate of 0.0043. For the search parameters, we use the values 15, 2 and 5 for Nl, Nb and B respectively. For the video superresolution dataset, we train with Vimeo90k Xue et al. (2019). The LR images and frames were generated by bicubic downsampling them by a factor of 2 and 4. To generate the Pareto Optimality curve, we use the the models as shown in Figure 6a). 5.2 BASELINES AND PARAMETERS We now describe our choice of baselines and search parameters. Since no other papers (to our knowledge) address this problem in the SR scope, we create our baseline model by combining all the models from Figure 6a (referred to as Assembled). It contains high-performing models over a wide FPS spectrum, which is what our DySR should ideally generate as well and so makes for a fair comparison. As mentioned above, the parameters Nb and B are very important in determining the quality of the models. Together they determine the total number of sub-graphs generated which in turn determines the amount of memory taken up by the meta-graph. It also indirectly determines how much weight is shared by the sub-graphs (more sub-graphs means more chances of operations being shared). In Figure 6b we show the memory consumed when executing DySR, Assembled and the single static CARN model after applying dynamic 8-bit quantization for all of them Pagliari et al. (2018). As we tune the Nb and B, we get 5, 10, 15 and 20 models (no. of models = Nb \u2217B). We see that even with 15 models, DySR has almost equal consumption to Assembled due to weight sharing among sub-graphs. With 5 models, DySR only consumes around twice as much memory compared to the single model, thus illustrating the memory efficiency of our weight-shared NAS system. Since Assembled contains 10 models, we select 10 models for our DySR as well for fair comparison for the rest of the experiments. Table 4 shows that with equal number of models, our framework has less number of parameters and size on disk. 5.3 PARETO OPTIMALITY: We now test our searched and fully trained sub-graphs against state-of-the-art models. Figure 7 shows the PSNR vs. FPS performance of each of the 10 sub-networks in DySR for different datasets. The sub-graphs are evaluated by passing the input through the execution path of the specific subgraphs and their corresponding inference time measured. We observe here that across both im- age super-resolution and video super-resolution datasets for 2X and 4X upscaling, our sub-graphs achieve Pareto Optimality for the PSNR vs. FPS tradeoff created by the state-of-the-art models across the full FPS spectrum, demonstrating the efficiency of our search space. We next evaluate our generated models against a diverse set of hardware, and the results are shown in Figure 8. We use a mobile CPU Snapdragon 855, a laptop CPU Intel i5-560M, a desktop grade GPU 1080Ti and a server-grade GPU A100 (see Table 5). This figure shows the results for 2X scaling for the Div2k dataset (other datasets show similar results, so we eliminate them for lack of space). We search our models on the 1080Ti to be above 15 FPS. Therefore, the model\u2019s FPS shift for different hardware but still have models above 15 FPS (as was our search criteria). We see that across all these hardware, the Pareto Optimality of our generated sub-graphs are maintained, demonstrating that even with one search run we can generate efficient models for inference for such a wide variety of models. This property allows for model adaptivity since this allows the selection of models with any desired FPS for any hardware, and ensures close to the optimal HR output quality. Along with the ability to change models on-the-fly, DySR allows for optimal PSNR vs. FPS tradeoff for resource adaptivity as well, as will be demonstrated next. 5.4 DYNAMIC RESOURCE ADAPTIVITY: To demonstrate the resource adaptivity of our adaptive sub-graphs and model selection policy, we use three traces to generate real world resource fluctuation scenarios. We use a bursty trace to simulate extreme changes in resource availability, stable to illustrate a relatively small variation in resource availability and random. Since resource adaptivity has rarely been discussed in the SR context, we create two possible baselines. The static baseline contains a single model deployed at any one time from the list of state-of-the-art models used above for generating the Pareto Optimality curve. Assembled baseline contains all the models from the state-of-the-art list deployed at the same time to represent the scenario of keeping models preloaded. Based on available resources, we use our real-time swapping policy to choose the best model for both our DySR as well as the two baselines. Figure 9 shows the results of our experiment using the Div2k dataset on 1080Ti for 2x scaling. The top row shows the trace used, the middle row shows the CDF of the resulting FPS fluctuation over time, and the bottom row shows the CDF of the PSNR values. The CDFs represent the probability of the FPS or PSNR being less than a certain value, therefore the higher y-axis values at lower p values mean the better they perform. Here we observe that across all traces, DySR achieves overall higher values of PSNR compared to the baselines. The FPS for Assembled and DySR are somewhat similar due to both having little to no reloading overhead. However, Assembled performs worse for PSNR due to having less granularity of models available across the full FPS spectrum. For example, between 20 to 25 FPS in Figure 7 for Div2K with 2X upscaling, we see that our framework generates four models but the Assembled only has two. Therefore, Assembled has less options and tends to choose models with lower PSNR in order to be able to maintain its FPS. The Static models perform the worst due to having many interruption of services as expected. For the PSNR values, Static has a significant portion of its CDF distribution at 0 and therefore performs much worse than the others, especially at the lower p values. Our framework outperforms all baselines by having the highest PSNR probabilities. Another observation is that the largest improvement in PSNR and FPS gained from DySR is for the bursty workload. This is mainly due to there being more model switches required here compared to the other two workloads. This amplifies the benefits derived from the model selection policy and granularity and thus a greater difference is observed. As the workloads grow more stable, the numbers vary less, illustrating that the more unstable a system the more it should benefit from our solutions. Based on all these observations, we conclude that DySR outperforms both baselines and can address the resource adaptivity issue. 6 CONCLUSION: In this paper, we focus on providing QoS for super resolution used in steaming services of mobile devices. We propose an algorithm and system co-design approach DySR, which employs an adaptive-aware one-shot NAS to generate models with Pareto-optimal performance, low switching overhead, and framerate constraint that can be adapted in real-time via our low cost incremental adaption method. DySR outperforms existing state-of-the-art models as well as the simple assemble method we explored by achieving the best performance while providing a steady framerate throughput with a memory footprint that is 40% less. A APPENDIX: You may include other additional sections here."}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetBudget'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'directory': '.'}, name='ListFiles'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Budget retrieved successfully\",\n  \"output\": {\n    \"total_resource_budget\": 100,\n    \"current_resource_usage\": 0,\n    \"current_remaining_resource_budget\": 100,\n    \"total_expense_budget\": 10000,\n    \"current_expense\": 0.022884150000000002,\n    \"current_remaining_expense_budget\": 9999.97711585\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetBudget` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Files listed successfully\",\n  \"error\": null,\n  \"output\": [\n    \"favicon.ico\",\n    \"requirements.txt\",\n    \"HASHIRU_LOGO.png\",\n    \"HASHIRU_ARCH.png\",\n    \"deleteAgents.py\",\n    \"CITATION.cff\",\n    \"tests\",\n    \"__pycache__\",\n    \"start.py\",\n    \"README.md\",\n    \"results\",\n    \"HASHIRU.png\",\n    \".gitignore\",\n    \".env\",\n    \"static\",\n    \".gitattributes\",\n    \"app.py\",\n    \"HASHIRU_2.png\",\n    \".git\",\n    \"paper_review_hashiru_20250627_201357.jsonl\",\n    \"src\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ListFiles` with \n```json\n{\n  \"directory\": \".\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetBudget', response={'result': {'status': 'success', 'message': 'Budget retrieved successfully', 'output': {'total_resource_budget': 100, 'current_resource_usage': 0, 'current_remaining_resource_budget': 100, 'total_expense_budget': 10000, 'current_expense': 0.022884150000000002, 'current_remaining_expense_budget': 9999.97711585}}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ListFiles', response={'result': {'status': 'success', 'message': 'Files listed successfully', 'error': None, 'output': ['favicon.ico', 'requirements.txt', 'HASHIRU_LOGO.png', 'HASHIRU_ARCH.png', 'deleteAgents.py', 'CITATION.cff', 'tests', '__pycache__', 'start.py', 'README.md', 'results', 'HASHIRU.png', '.gitignore', '.env', 'static', '.gitattributes', 'app.py', 'HASHIRU_2.png', '.git', 'paper_review_hashiru_20250627_201357.jsonl', 'src']}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'website': 'iclr.cc', 'query': 'ICLR 2023 review criteria'}, name='GoogleSearchTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": [\n    \"https://iclr.cc/Conferences/2023/ReviewerGuide\",\n    \"https://blog.iclr.cc/2023/04/12/ethics-review-process-for-iclr-2023/\",\n    \"https://blog.iclr.cc/category/iclr-2023/\",\n    \"https://blog.iclr.cc/tag/reviewing/\",\n    \"https://blog.iclr.cc/tag/awards/\",\n    \"https://blog.iclr.cc/2023/04/05/announcing-notable-reviewers-and-area-chairs-at-iclr-2023/\",\n    \"https://iclr.cc/Conferences/2025/FAQ\",\n    \"https://iclr.cc/Conferences/2019/Reviewer_Guidelines\",\n    \"https://iclr.cc/Conferences/2023/CallForTinyPapers\",\n    \"https://iclr.cc/Conferences/2022/ReviewerGuide\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `GoogleSearchTool` with \n```json\n{\n  \"website\": \"iclr.cc\",\n  \"query\": \"ICLR 2023 review criteria\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GoogleSearchTool', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': ['https://iclr.cc/Conferences/2023/ReviewerGuide', 'https://blog.iclr.cc/2023/04/12/ethics-review-process-for-iclr-2023/', 'https://blog.iclr.cc/category/iclr-2023/', 'https://blog.iclr.cc/tag/reviewing/', 'https://blog.iclr.cc/tag/awards/', 'https://blog.iclr.cc/2023/04/05/announcing-notable-reviewers-and-area-chairs-at-iclr-2023/', 'https://iclr.cc/Conferences/2025/FAQ', 'https://iclr.cc/Conferences/2019/Reviewer_Guidelines', 'https://iclr.cc/Conferences/2023/CallForTinyPapers', 'https://iclr.cc/Conferences/2022/ReviewerGuide']}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'output_type': 'full_text', 'url': 'https://iclr.cc/Conferences/2023/ReviewerGuide'}, name='GetWebsite'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR 2022 Reviewer Instructions\\n\\n\\n\\n\\n\\nSkip to yearly menu bar\\nSkip to main content\\n\\nMain Navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            ICLR\\n        \\n\\n\\n\\n\\n                        Help/FAQ\\n                    \\n\\n\\n\\n\\n\\n\\n                        Contact ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n                        Downloads\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Blog\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n                        Privacy Policy\\n                    \\n\\n\\n\\n\\n\\n\\n                        Create Profile\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reset Password\\n                    \\n\\n\\n\\n\\n\\n\\n                        Journal To Conference Track\\n                    \\n\\n\\n\\n\\n\\n\\n                        Diversity & Inclusion\\n                    \\n\\n\\n\\n\\n\\n\\n                        Proceedings at OpenReview\\n                    \\n\\n\\n\\n\\n\\n\\n                        Future Meetings\\n                    \\n\\n\\n\\n\\n\\n\\n                        Press\\n                    \\n\\n\\n\\n\\n\\n\\n                        Exhibitor Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Twitter\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        My Stuff\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Login\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Select Year: (2023)\\n        \\n\\n\\n2026\\n                \\n\\n\\n\\n2025\\n                \\n\\n\\n\\n2024\\n                \\n\\n\\n\\n2023\\n                \\n\\n\\n\\n2022\\n                \\n\\n\\n\\n2021\\n                \\n\\n\\n\\n2020\\n                \\n\\n\\n\\n2019\\n                \\n\\n\\n\\n2018\\n                \\n\\n\\n\\n2017\\n                \\n\\n\\n\\n2016\\n                \\n\\n\\n\\n2015\\n                \\n\\n\\n\\n2014\\n                \\n\\n\\n\\n2013\\n                \\n\\n\\n\\n\\n\\n\\n                        Dates\\n                    \\n\\n\\n\\n\\n            Calls\\n        \\n\\n\\n\\n\\n                        Call for Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Tiny Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Workshops\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Blog Posts\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Socials\\n                    \\n\\n\\n\\n\\n\\n\\n            Guides\\n        \\n\\n\\n\\n\\n                        Author Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewer Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Area Chair Guide\\n                    \\n\\n\\n\\n\\n\\n\\n            Attend\\n        \\n\\n\\n\\n\\n                        Register\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visa and Travel  Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        Hotels\\n                    \\n\\n\\n\\n\\n\\n\\n                        Child Care\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visiting Kigali\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n            Organization\\n        \\n\\n\\n\\n\\n                        ICLR Board\\n                    \\n\\n\\n\\n\\n\\n\\n                        Organizing Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Program Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Ethics Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        Conference Site\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nICLR 2023 Reviewer Guide\\nThank you for agreeing to serve as an ICLR 2023 reviewer. Your contribution as a reviewer is paramount to creating an exciting and high-quality program. We ask that:\\n\\nYour reviews are timely and substantive.\\nYou follow the reviewing guidelines below.\u00a0\\nYou adhere to our Code of Ethics in your role as a reviewer. You must also adhere to our Code of Conduct.\\n\\nThis guide is intended to help you understand the ICLR 2023 decision process and your role within it. It contains:\\n\\nAn outline of the main reviewer tasks\\nStep-by-step reviewing instructions (especially relevant for reviewers that are new to ICLR)\\nReview examples\\nAn FAQ.\\n\\n\u00a0\\n\\n\\nWe\u2019re counting on you\\nAs a reviewer you are central to the program creation process for ICLR 2023. Your Area Chairs (ACs), Senior Area Chairs (SACs) and the Program Chairs (PCs) will rely greatly on your expertise and your diligent and thorough reviews to make decisions on each paper. Therefore, your role as a reviewer is critical to ensuring a strong program for ICLR 2023.\\nHigh-quality reviews are also very valuable for helping authors improve their work, whether it is eventually accepted by ICLR 2023, or not. Therefore it is important to treat each valid ICLR 2023 submission with equal care.\\nAs a token of our appreciation for your essential work, top reviewers will be acknowledged permanently on the ICLR 2023 website. Furthermore, top and high quality reviewers will receive special acknowledgement during the opening ceremony and free registration to ICLR 2023.\\n\u00a0\\n\\nMain reviewer tasks\\nThe main reviewer tasks and dates are as follows:\\n\\nCreate or update your OpenReview profile (September 30 2022)\\nBid on papers (September 30 2022 - October 06 2022)\\nWrite a constructive, thorough and timely review (October 12 2022 - October 24 2022)\\nInitial paper reviews released (November 4 2022)\\nDiscuss with authors and other reviewers to clarify and improve the paper (November 4 2022 - November 18 2022)\\nFlag any potential CoE violations and/or concerns (by November 18 2022)\\nProvide a recommendation to the area chair assigned to the paper (by November 18 2022)\\nVirtual Meeting with AC if paper you reviewed falls into borderline papers (November 18 2022 - December 12 2022)\\nProvide a final recommendation to the area chair assigned to the paper (after Virtual Meeting)\\n\\n\u00a0\\n\\n\\n\u00a0\\nCode of Ethics\\nAll ICLR participants, including reviewers, are required to adhere to the ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics). All reviewers are required to read the Code of Ethics and adhere to it. The Code of Ethics applies to all conference participation, including paper submission, reviewing, and paper discussion.\u00a0\\nAs part of the review process, reviewers are asked to raise potential violations of the ICLR Code of Ethics. Note that authors are encouraged to discuss questions and potential issues regarding the Code of Ethics as part of their submission. This discussion is not counted against the maximum page limit of the paper and should be included as a separate section.\\n\u00a0\\n\\n\u00a0\\nReviewing a submission: step-by-step\\nSummarized in one sentence, a review aims to determine whether a submission will bring sufficient value to the community and contribute new knowledge. The process can be broken down into the following main reviewer tasks:\\n\u00a0\\n\\n\\nRead the paper: It\u2019s important to carefully read through the entire paper, and to look up any related work and citations that will help you comprehensively evaluate it. Be sure to give yourself sufficient time for this step.\\nWhile reading, consider the following:\\n\\nObjective of the work: What is the goal of the paper? Is it to better address a known application or problem, draw attention to a new application or problem, or to introduce and/or explain a new theoretical finding? A combination of these? Different objectives will require different considerations as to potential value and impact.\\nStrong points: is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g. theoretically, algorithmically, etc.)?\\nWeak points: is it weak in any of the aspects listed in b.?\\nBe mindful of potential biases and try to be open-minded about the value and interest a paper can hold for the entire ICLR community, even if it may not be very interesting for you.\\n\\n\\nAnswer four key questions for yourself, to make a recommendation to Accept or Reject:\\n\\nWhat is the specific question and/or problem tackled by the paper?\\nIs the approach well motivated, including being well-placed in the literature?\\nDoes the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\\nWhat is the significance of the work? Does it contribute new knowledge and sufficient value to the community? Note, this does not necessarily require state-of-the-art results. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge (incl., empirical, theoretical, for practitioners, etc).\\n\\n\\nWrite and submit your initial review, organizing it as follows:\u00a0\\n\\nSummarize what the paper claims to contribute. Be positive and constructive.\\nList strong and weak points of the paper. Be as comprehensive as possible.\\nClearly state your initial recommendation (accept or reject) with one or two key reasons for this choice.\\nProvide supporting arguments for your recommendation.\\nAsk questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.\u00a0\\nProvide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\\n\\n\\nComplete the CoE report: ICLR has adopted the following Code of Ethics (CoE). When submitting your review, you\u2019ll be asked to complete a CoE report for the paper. The report is a simple form with two questions. The first asks whether there is a potential violation of the CoE. The second is relevant only if there is a potential violation and asks the reviewer to explain why there may be a potential violation. In order to answer these questions, it is therefore important that you read the CoE before starting your reviews. \\n\\t\u00a0\\nEngage in discussion: The discussion phase at ICLR is different from most conferences in the AI/ML community. During this phase, reviewers, authors and area chairs engage in asynchronous discussion and authors are allowed to revise their submissions to address concerns that arise. It is crucial that you are actively engaged during this phase. Maintain a spirit of openness to changing your initial recommendation (either to a more positive or more negative) rating.\\nBorderline paper meeting: This year, we are introducing a new program for ACs to (virtually) meet and discuss with reviewers only for borderline cases. ACs will reach out to schedule this meeting. This is to ensure active discussions among reviewers, and well-thought-out decisions. ACs will schedule the meeting and facilitate the discussion. For a productive discussion, it is important to familiarize yourself with other reviewers' feedback prior to the meeting. Please note that we will be leveraging information for reviewers who failed to attend this meeting (excluding emergencies).\u00a0\\nProvide final recommendation: Update your review, taking into account the new information collected during the discussion phase, and any revisions to the submission. State your reasoning and what did/didn\u2019t change your recommendation throughout the discussion phase.\\n\\n\u00a0\\n\\n\u00a0\\nFor great in-depth resources on reviewing, see these resources:\\n\\nDaniel Dennet, Criticising with Kindness.\\nComprehensive advice: Mistakes Reviewers Make\\nViews from multiple reviewers: Last minute reviewing advice\\nPerspective from instructions to Area Chairs: Dear ACs.\\n\\n\u00a0\\n\\n\u00a0\\nReview Examples\\nHere are two sample reviews from previous conferences that give an example of what we consider a good review for the case of leaning-to-accept and leaning-to-reject.\\nReview for a Paper where Leaning-to-Accept\\nThis paper proposes a method, Dual-AC, for optimizing the actor (policy) and critic (value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor. In order to achieve that, they take the linear programming approach of solving the Bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems. The discussion on the deficiencies of the naive LP approach is mostly well done. Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without losing convexity by applying a regularization. They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements. Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms.\u00a0\\nOverall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments. Given these clarifications in an author response, I would be willing to increase the score.\u00a0\\nFor the theory, there are a few steps that need clarification and further clarification on novelty. For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results. It looks like Theorem 2 has already been shown in \\\"Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time\u201d. There is a statement that \u201cChen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting. However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization\u201d. Is your Theorem 2 somehow an extension? Is Theorem 3 completely new?\\nThis is particularly called into question due to the lack of assumptions about the function class for value functions. It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function). This assumption seems to be used right at the bottom of Page 17, where U^{pi*} = V^*. Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small. More about conditions on eta_v would be illuminating.\u00a0\\nThere is also one step in the theorem that I cannot verify. On Page 18, how is the squared removed for difference between U and Upi? The transition from the second line of the proof to the third line is not clear. It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2.\u00a0\\n\u00a0\\n\\nFor the experiments, the following should be addressed.\\n1. It would have been better to also show the performance graphs with and without the improvements for multiple domains.\\n2. The central contribution is extending the single step LP to a multi-step formulation. It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains.\\n3. Increasing k also comes at a computational cost. I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO).\\n4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization. It was also mentioned that increasing the regularization parameter size increases the convergence rate. Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization? In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned. Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity? A bit more discussion on these choices would be helpful.\u00a0\\n\u00a0\\n\\nMinor comments:\\n1. Page 2: In equation 5, there should not be a 'ds' in the dual variable constraint\\n\u00a0\\n\\n\u00a0\\nReview for a Paper where Leaning-to-Reject\\nThis paper introduces a variation on temporal difference learning for the function approximation case that attempts to resolve the issue of over-generalization across temporally-successive states. The new approach is applied to both linear and non-linear function approximation, and for prediction and control problems. The algorithmic contribution is demonstrated with a suite of experiments in classic benchmark control domains (Mountain Car and Acrobot), and in Pong.\\nThis paper should be rejected because (1) the algorithm is not well justified either by theory or practice, (2) the paper never clearly demonstrates the existence of problem they are trying to solve (nor differentiates it from the usual problem of generalizing well), (3) the experiments are difficult to understand, missing many details, and generally do not support a significant contribution, and (4) the paper is imprecise and unpolished.\\nMain argument\\nThe paper does not do a great job of demonstrating that the problem it is trying to solve is a real thing. There is no experiment in this paper that clearly shows how this temporal generalization problem is different from the need to generalize well with function approximation. The paper points to references to establish the existence of the problem, but for example the Durugkar and Stone paper is a workshop paper and the conference version of that paper was rejected from ICLR 2018 and the reviewers highlighted serious issues with the paper\u2014that is not work to build upon. Further the paper under review here claims this problem is most pressing in the non-linear case, but the analysis in section 4.1 is for the linear case.\u00a0\\nThe resultant algorithm does not seem well justified, and has a different fixed point than TD, but there is no discussion of this other than section 4.4, which does not make clear statements about the correctness of the algorithm or what it converges to. Can you provide a proof or any kind of evidence that the proposed approach is sound, or how it\u2019s fixed point relates to TD?\\nThe experiments do not provide convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches. There are so many missing details it is difficult to draw many conclusions:\\n\\nWhat was the policy used in exp1 for policy evaluation in MC?\\nWhy Fourier basis features?\\nIn MC with DQN how did you adjust the parameters and architecture for the MC task?\\nWas the reward in MC and Acrobot -1 per step or something else\\nHow did you tune the parameters in the MC and Acrobot experiments?\\nWhy so few runs in MC, none of the results presented are significant?\\nWhy is the performance so bad in MC?\\nDid you evaluate online learning or do tests with the greedy policy?\\nHow did you initialize the value functions and weights?\\nWhy did you use experience replay for the linear experiments?\\nIN MC and Acrobot why only a one layer MLP?\\n\\nIgnoring all that, the results are not convincing. Most of the results in the paper are not statistically significant. The policy evaluation results in MC show little difference to regular TD. The Pong results show DQN is actually better. This makes the reader wonder if the result with DQN on MC and Acrobot are only worse because you did not properly tune DQN for those domains, whereas the default DQN architecture is well tuned for Atari and that is why you method is competitive in the smaller domains.\u00a0\\nThe differences in the \u201caverage change in value plots\u201d are very small if the rewards are -1 per step. Can you provide some context to understand the significance of this difference? In the last experiment linear FA and MC, the step-size is set equal for all methods\u2014this is not a valid comparison. Your method may just work better with alpha = 0.1.\u00a0\\n\u00a0\\n\\nThe paper has many imprecise parts, here are a few:\\n\\nThe definition of the value function would be approximate not equals unless you specify some properties of the function approximation architecture. Same for the Bellman equation\\nequation 1 of section 2.1 is neither an algorithm or a loss function\\nTD does not minimize the squared TD. Saying that is the objective function of TD learning in not true\\nend of section 2.1 says \u201cIt is computed as\u201d but the following equation just gives a form for the partial derivative\\nequation 2, x is not bounded\u00a0\\nYou state TC-loss has an unclear solution property, I don\u2019t know what that means and I don\u2019t think your approach is well justified either\\nSection 4.1 assumes linear FA, but its implied up until paragraph 2 that it has not assumed linear\\ntreatment of n_t in alg differs from appendix (t is no time episode number)\\nYour method has a n_t parameter that is adapted according to a schedule seemingly giving it an unfair advantage over DQN.\\nOver-claim not supported by the results: \u201cwe see that HR-TD is able to find a representation that is better at keeping the target value separate than TC is \u201c. The results do not show this.\\nSection 4.4 does not seem to go anywhere or produce and tangible conclusions\\n\\n\u00a0\\n\\nThings to improve the paper that did not impact the score:\\n\\nIt\u2019s hard to follow how the prox operator is used in the development of the alg, this could use some higher level explanation\\nIntro p2 is about bootstrapping, use that term and remove the equations\\nIt\u2019s not clear why you are talking about stochastic vs deterministic in P3\\nPerhaps you should compare against a MC method in the experiments to demonstrate the problem with TD methods and generalization\\nSection 2: \u201ccan often be a regularization term\u201d >> can or must be?\\nupdate law is an odd term\\n\u00a0tends to alleviate\u201d >> odd phrase\\nsection 4 should come before section 3\\nAlg 1 in not helpful because it just references an equation\\nsection 4.4 is very confusing, I cannot follow the logic of the statements\u00a0\\nQ learning >> Q-learning\\nNot sure what you mean with the last sentence of p2 section 5\\nwhere are the results for Acrobot linear function approximation\\nappendix Q-learning with linear FA is not DQN (table 2)\\n\\n\u00a0\\n\\nFAQ for Reviewers\\nQ. The review I just submitted is not appearing! Did it go thru?\\nA. There seems to be some delay in the system for you to see your reviews. Just give a bit of time and refresh again.\\n\u00a0\\nQ. What is the borderline paper meeting between AC and reviewers about?\\nA. This year we require ACs to meet with reviewers for all borderline papers (defined and announced by PCs once all reviews are received). There will be one meeting per borderline paper. Once we receive all reviews, we will let ACs know which papers fall into this bucket. ACs will then schedule and run the meetings with the reviewers of the papers in question.\\n\u00a0\\n\\nQ: What if I can not attend the meeting?\\nA: Please work with your AC to find a meeting schedule that works for everyone. Note that the discussion phase is roughly 1 month long, and overlaps with the NeurIPS conference, in case in-person meeting is preferred. All reviewers agreed to participate in review meetings when they accepted the reviewer invitations. An AC has right to\u00a0 downweight the review of reviewers who refuse to attend the meeting.\\n\u00a0\\n\\nQ: How should I use supplementary material?\\nA: It is not necessary to read supplementary material but such material can often answer questions that arise while reading the main paper, so consider looking there before asking authors.\\n\u00a0\\n\\nQ: How should I handle a policy violation?\\nA: To flag a CoE violation related to a submission, please indicate it when submitting the CoE report for that paper. The AC will work with the PC and the ethics board to resolve the case. To discuss other violations (e.g. plagiarism, double submission, paper length, formatting, etc.), please contact either the AC/SAC or the PC as appropriate. You can do this by sending a confidential comment with the appropriate readership restrictions.\\n\u00a0\\n\\nQ: Am I allowed to ask for additional experiments?\\nA: You can ask for additional experiments. New experiments should not significantly change the content of the submission. Rather, they should be limited in scope and serve to more thoroughly validate existing results from the submission.\\n\u00a0\\n\\nQ: If a submission does not achieve state-of-the-art results, is that grounds for rejection?\\nA: No, a lack of state-of-the-art results does not by itself constitute grounds for rejection. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge. Submissions can achieve this without achieving state-of-the-art results.\\nQ: Are authors expected to cite and compare with very recent work? What about non peer-reviewed (e.g., ArXiv) papers? (updated on 7 November 2022)\\nA: We consider papers contemporaneous if they are published (available in online proceedings) within the last four months. That means, since our full paper deadline is September 28, if a paper was published (i.e., at a peer-reviewed venue) on or after May 28, 2022, authors are not required to compare their own work to that paper. Authors are encouraged to cite and discuss all relevant papers, but they may be excused for not knowing about papers not published in peer-reviewed conference proceedings or journals, which includes papers exclusively available on arXiv. Reviewers are encouraged to use their own good judgement and, if in doubt, discuss with their area chair.\u00a0\\n\u00a0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuccessful Page Load\\n\\n\\n\\n\\n\\n\\n\\n                        ICLR uses cookies for essential functions only. We do not sell your personal\\n                        information.\\n                        Our Privacy Policy \u00bb\u00a0\\n\\n\\nAccept\\n                            Cookies\\n                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            The ICLR Logo above may be used on presentations. Right-click and choose\\n                            download. It is a vector graphic and may be used at any scale.\\n                        \\n\\n\\n\\n\\n\\n                            Useful links\\n                        \\n\\n\\nSponsor / Exhibitor Information\\nPress\\n\\n\\n\\n\\n\\n\\n\\nContact\\n\\n 2710 E Corridor Dr, Appleton WI 54913\\n                            \\n\\n Email\\n\\n Phone: +1-920-268-4789\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR Proceedings at OpenReview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"\n}\n```\n", "metadata": {"title": "Invoking `GetWebsite` with \n```json\n{\n  \"output_type\": \"full_text\",\n  \"url\": \"https://iclr.cc/Conferences/2023/ReviewerGuide\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetWebsite', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR 2022 Reviewer Instructions\\n\\n\\n\\n\\n\\nSkip to yearly menu bar\\nSkip to main content\\n\\nMain Navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            ICLR\\n        \\n\\n\\n\\n\\n                        Help/FAQ\\n                    \\n\\n\\n\\n\\n\\n\\n                        Contact ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n                        Downloads\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Blog\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n                        Privacy Policy\\n                    \\n\\n\\n\\n\\n\\n\\n                        Create Profile\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reset Password\\n                    \\n\\n\\n\\n\\n\\n\\n                        Journal To Conference Track\\n                    \\n\\n\\n\\n\\n\\n\\n                        Diversity & Inclusion\\n                    \\n\\n\\n\\n\\n\\n\\n                        Proceedings at OpenReview\\n                    \\n\\n\\n\\n\\n\\n\\n                        Future Meetings\\n                    \\n\\n\\n\\n\\n\\n\\n                        Press\\n                    \\n\\n\\n\\n\\n\\n\\n                        Exhibitor Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Twitter\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        My Stuff\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Login\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Select Year: (2023)\\n        \\n\\n\\n2026\\n                \\n\\n\\n\\n2025\\n                \\n\\n\\n\\n2024\\n                \\n\\n\\n\\n2023\\n                \\n\\n\\n\\n2022\\n                \\n\\n\\n\\n2021\\n                \\n\\n\\n\\n2020\\n                \\n\\n\\n\\n2019\\n                \\n\\n\\n\\n2018\\n                \\n\\n\\n\\n2017\\n                \\n\\n\\n\\n2016\\n                \\n\\n\\n\\n2015\\n                \\n\\n\\n\\n2014\\n                \\n\\n\\n\\n2013\\n                \\n\\n\\n\\n\\n\\n\\n                        Dates\\n                    \\n\\n\\n\\n\\n            Calls\\n        \\n\\n\\n\\n\\n                        Call for Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Tiny Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Workshops\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Blog Posts\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Socials\\n                    \\n\\n\\n\\n\\n\\n\\n            Guides\\n        \\n\\n\\n\\n\\n                        Author Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewer Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Area Chair Guide\\n                    \\n\\n\\n\\n\\n\\n\\n            Attend\\n        \\n\\n\\n\\n\\n                        Register\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visa and Travel  Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        Hotels\\n                    \\n\\n\\n\\n\\n\\n\\n                        Child Care\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visiting Kigali\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n            Organization\\n        \\n\\n\\n\\n\\n                        ICLR Board\\n                    \\n\\n\\n\\n\\n\\n\\n                        Organizing Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Program Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Ethics Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        Conference Site\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nICLR 2023 Reviewer Guide\\nThank you for agreeing to serve as an ICLR 2023 reviewer. Your contribution as a reviewer is paramount to creating an exciting and high-quality program. We ask that:\\n\\nYour reviews are timely and substantive.\\nYou follow the reviewing guidelines below.\\xa0\\nYou adhere to our Code of Ethics in your role as a reviewer. You must also adhere to our Code of Conduct.\\n\\nThis guide is intended to help you understand the ICLR 2023 decision process and your role within it. It contains:\\n\\nAn outline of the main reviewer tasks\\nStep-by-step reviewing instructions (especially relevant for reviewers that are new to ICLR)\\nReview examples\\nAn FAQ.\\n\\n\\xa0\\n\\n\\nWe\u2019re counting on you\\nAs a reviewer you are central to the program creation process for ICLR 2023. Your Area Chairs (ACs), Senior Area Chairs (SACs) and the Program Chairs (PCs) will rely greatly on your expertise and your diligent and thorough reviews to make decisions on each paper. Therefore, your role as a reviewer is critical to ensuring a strong program for ICLR 2023.\\nHigh-quality reviews are also very valuable for helping authors improve their work, whether it is eventually accepted by ICLR 2023, or not. Therefore it is important to treat each valid ICLR 2023 submission with equal care.\\nAs a token of our appreciation for your essential work, top reviewers will be acknowledged permanently on the ICLR 2023 website. Furthermore, top and high quality reviewers will receive special acknowledgement during the opening ceremony and free registration to ICLR 2023.\\n\\xa0\\n\\nMain reviewer tasks\\nThe main reviewer tasks and dates are as follows:\\n\\nCreate or update your OpenReview profile (September 30 2022)\\nBid on papers (September 30 2022 - October 06 2022)\\nWrite a constructive, thorough and timely review (October 12 2022 - October 24 2022)\\nInitial paper reviews released (November 4 2022)\\nDiscuss with authors and other reviewers to clarify and improve the paper (November 4 2022 - November 18 2022)\\nFlag any potential CoE violations and/or concerns (by November 18 2022)\\nProvide a recommendation to the area chair assigned to the paper (by November 18 2022)\\nVirtual Meeting with AC if paper you reviewed falls into borderline papers (November 18 2022 - December 12 2022)\\nProvide a final recommendation to the area chair assigned to the paper (after Virtual Meeting)\\n\\n\\xa0\\n\\n\\n\\xa0\\nCode of Ethics\\nAll ICLR participants, including reviewers, are required to adhere to the ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics). All reviewers are required to read the Code of Ethics and adhere to it. The Code of Ethics applies to all conference participation, including paper submission, reviewing, and paper discussion.\\xa0\\nAs part of the review process, reviewers are asked to raise potential violations of the ICLR Code of Ethics. Note that authors are encouraged to discuss questions and potential issues regarding the Code of Ethics as part of their submission. This discussion is not counted against the maximum page limit of the paper and should be included as a separate section.\\n\\xa0\\n\\n\\xa0\\nReviewing a submission: step-by-step\\nSummarized in one sentence, a review aims to determine whether a submission will bring sufficient value to the community and contribute new knowledge. The process can be broken down into the following main reviewer tasks:\\n\\xa0\\n\\n\\nRead the paper: It\u2019s important to carefully read through the entire paper, and to look up any related work and citations that will help you comprehensively evaluate it. Be sure to give yourself sufficient time for this step.\\nWhile reading, consider the following:\\n\\nObjective of the work: What is the goal of the paper? Is it to better address a known application or problem, draw attention to a new application or problem, or to introduce and/or explain a new theoretical finding? A combination of these? Different objectives will require different considerations as to potential value and impact.\\nStrong points: is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g. theoretically, algorithmically, etc.)?\\nWeak points: is it weak in any of the aspects listed in b.?\\nBe mindful of potential biases and try to be open-minded about the value and interest a paper can hold for the entire ICLR community, even if it may not be very interesting for you.\\n\\n\\nAnswer four key questions for yourself, to make a recommendation to Accept or Reject:\\n\\nWhat is the specific question and/or problem tackled by the paper?\\nIs the approach well motivated, including being well-placed in the literature?\\nDoes the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\\nWhat is the significance of the work? Does it contribute new knowledge and sufficient value to the community? Note, this does not necessarily require state-of-the-art results. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge (incl., empirical, theoretical, for practitioners, etc).\\n\\n\\nWrite and submit your initial review, organizing it as follows:\\xa0\\n\\nSummarize what the paper claims to contribute. Be positive and constructive.\\nList strong and weak points of the paper. Be as comprehensive as possible.\\nClearly state your initial recommendation (accept or reject) with one or two key reasons for this choice.\\nProvide supporting arguments for your recommendation.\\nAsk questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.\\xa0\\nProvide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\\n\\n\\nComplete the CoE report: ICLR has adopted the following Code of Ethics (CoE). When submitting your review, you\u2019ll be asked to complete a CoE report for the paper. The report is a simple form with two questions. The first asks whether there is a potential violation of the CoE. The second is relevant only if there is a potential violation and asks the reviewer to explain why there may be a potential violation. In order to answer these questions, it is therefore important that you read the CoE before starting your reviews. \\n\\t\\xa0\\nEngage in discussion: The discussion phase at ICLR is different from most conferences in the AI/ML community. During this phase, reviewers, authors and area chairs engage in asynchronous discussion and authors are allowed to revise their submissions to address concerns that arise. It is crucial that you are actively engaged during this phase. Maintain a spirit of openness to changing your initial recommendation (either to a more positive or more negative) rating.\\nBorderline paper meeting: This year, we are introducing a new program for ACs to (virtually) meet and discuss with reviewers only for borderline cases. ACs will reach out to schedule this meeting. This is to ensure active discussions among reviewers, and well-thought-out decisions. ACs will schedule the meeting and facilitate the discussion. For a productive discussion, it is important to familiarize yourself with other reviewers\\' feedback prior to the meeting. Please note that we will be leveraging information for reviewers who failed to attend this meeting (excluding emergencies).\\xa0\\nProvide final recommendation: Update your review, taking into account the new information collected during the discussion phase, and any revisions to the submission. State your reasoning and what did/didn\u2019t change your recommendation throughout the discussion phase.\\n\\n\\xa0\\n\\n\\xa0\\nFor great in-depth resources on reviewing, see these resources:\\n\\nDaniel Dennet, Criticising with Kindness.\\nComprehensive advice: Mistakes Reviewers Make\\nViews from multiple reviewers: Last minute reviewing advice\\nPerspective from instructions to Area Chairs: Dear ACs.\\n\\n\\xa0\\n\\n\\xa0\\nReview Examples\\nHere are two sample reviews from previous conferences that give an example of what we consider a good review for the case of leaning-to-accept and leaning-to-reject.\\nReview for a Paper where Leaning-to-Accept\\nThis paper proposes a method, Dual-AC, for optimizing the actor (policy) and critic (value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor. In order to achieve that, they take the linear programming approach of solving the Bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems. The discussion on the deficiencies of the naive LP approach is mostly well done. Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without losing convexity by applying a regularization. They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements. Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms.\\xa0\\nOverall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments. Given these clarifications in an author response, I would be willing to increase the score.\\xa0\\nFor the theory, there are a few steps that need clarification and further clarification on novelty. For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results. It looks like Theorem 2 has already been shown in \"Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time\u201d. There is a statement that \u201cChen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting. However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization\u201d. Is your Theorem 2 somehow an extension? Is Theorem 3 completely new?\\nThis is particularly called into question due to the lack of assumptions about the function class for value functions. It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function). This assumption seems to be used right at the bottom of Page 17, where U^{pi*} = V^*. Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small. More about conditions on eta_v would be illuminating.\\xa0\\nThere is also one step in the theorem that I cannot verify. On Page 18, how is the squared removed for difference between U and Upi? The transition from the second line of the proof to the third line is not clear. It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2.\\xa0\\n\\xa0\\n\\nFor the experiments, the following should be addressed.\\n1. It would have been better to also show the performance graphs with and without the improvements for multiple domains.\\n2. The central contribution is extending the single step LP to a multi-step formulation. It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains.\\n3. Increasing k also comes at a computational cost. I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO).\\n4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization. It was also mentioned that increasing the regularization parameter size increases the convergence rate. Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization? In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned. Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity? A bit more discussion on these choices would be helpful.\\xa0\\n\\xa0\\n\\nMinor comments:\\n1. Page 2: In equation 5, there should not be a \\'ds\\' in the dual variable constraint\\n\\xa0\\n\\n\\xa0\\nReview for a Paper where Leaning-to-Reject\\nThis paper introduces a variation on temporal difference learning for the function approximation case that attempts to resolve the issue of over-generalization across temporally-successive states. The new approach is applied to both linear and non-linear function approximation, and for prediction and control problems. The algorithmic contribution is demonstrated with a suite of experiments in classic benchmark control domains (Mountain Car and Acrobot), and in Pong.\\nThis paper should be rejected because (1) the algorithm is not well justified either by theory or practice, (2) the paper never clearly demonstrates the existence of problem they are trying to solve (nor differentiates it from the usual problem of generalizing well), (3) the experiments are difficult to understand, missing many details, and generally do not support a significant contribution, and (4) the paper is imprecise and unpolished.\\nMain argument\\nThe paper does not do a great job of demonstrating that the problem it is trying to solve is a real thing. There is no experiment in this paper that clearly shows how this temporal generalization problem is different from the need to generalize well with function approximation. The paper points to references to establish the existence of the problem, but for example the Durugkar and Stone paper is a workshop paper and the conference version of that paper was rejected from ICLR 2018 and the reviewers highlighted serious issues with the paper\u2014that is not work to build upon. Further the paper under review here claims this problem is most pressing in the non-linear case, but the analysis in section 4.1 is for the linear case.\\xa0\\nThe resultant algorithm does not seem well justified, and has a different fixed point than TD, but there is no discussion of this other than section 4.4, which does not make clear statements about the correctness of the algorithm or what it converges to. Can you provide a proof or any kind of evidence that the proposed approach is sound, or how it\u2019s fixed point relates to TD?\\nThe experiments do not provide convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches. There are so many missing details it is difficult to draw many conclusions:\\n\\nWhat was the policy used in exp1 for policy evaluation in MC?\\nWhy Fourier basis features?\\nIn MC with DQN how did you adjust the parameters and architecture for the MC task?\\nWas the reward in MC and Acrobot -1 per step or something else\\nHow did you tune the parameters in the MC and Acrobot experiments?\\nWhy so few runs in MC, none of the results presented are significant?\\nWhy is the performance so bad in MC?\\nDid you evaluate online learning or do tests with the greedy policy?\\nHow did you initialize the value functions and weights?\\nWhy did you use experience replay for the linear experiments?\\nIN MC and Acrobot why only a one layer MLP?\\n\\nIgnoring all that, the results are not convincing. Most of the results in the paper are not statistically significant. The policy evaluation results in MC show little difference to regular TD. The Pong results show DQN is actually better. This makes the reader wonder if the result with DQN on MC and Acrobot are only worse because you did not properly tune DQN for those domains, whereas the default DQN architecture is well tuned for Atari and that is why you method is competitive in the smaller domains.\\xa0\\nThe differences in the \u201caverage change in value plots\u201d are very small if the rewards are -1 per step. Can you provide some context to understand the significance of this difference? In the last experiment linear FA and MC, the step-size is set equal for all methods\u2014this is not a valid comparison. Your method may just work better with alpha = 0.1.\\xa0\\n\\xa0\\n\\nThe paper has many imprecise parts, here are a few:\\n\\nThe definition of the value function would be approximate not equals unless you specify some properties of the function approximation architecture. Same for the Bellman equation\\nequation 1 of section 2.1 is neither an algorithm or a loss function\\nTD does not minimize the squared TD. Saying that is the objective function of TD learning in not true\\nend of section 2.1 says \u201cIt is computed as\u201d but the following equation just gives a form for the partial derivative\\nequation 2, x is not bounded\\xa0\\nYou state TC-loss has an unclear solution property, I don\u2019t know what that means and I don\u2019t think your approach is well justified either\\nSection 4.1 assumes linear FA, but its implied up until paragraph 2 that it has not assumed linear\\ntreatment of n_t in alg differs from appendix (t is no time episode number)\\nYour method has a n_t parameter that is adapted according to a schedule seemingly giving it an unfair advantage over DQN.\\nOver-claim not supported by the results: \u201cwe see that HR-TD is able to find a representation that is better at keeping the target value separate than TC is \u201c. The results do not show this.\\nSection 4.4 does not seem to go anywhere or produce and tangible conclusions\\n\\n\\xa0\\n\\nThings to improve the paper that did not impact the score:\\n\\nIt\u2019s hard to follow how the prox operator is used in the development of the alg, this could use some higher level explanation\\nIntro p2 is about bootstrapping, use that term and remove the equations\\nIt\u2019s not clear why you are talking about stochastic vs deterministic in P3\\nPerhaps you should compare against a MC method in the experiments to demonstrate the problem with TD methods and generalization\\nSection 2: \u201ccan often be a regularization term\u201d >> can or must be?\\nupdate law is an odd term\\n\\xa0tends to alleviate\u201d >> odd phrase\\nsection 4 should come before section 3\\nAlg 1 in not helpful because it just references an equation\\nsection 4.4 is very confusing, I cannot follow the logic of the statements\\xa0\\nQ learning >> Q-learning\\nNot sure what you mean with the last sentence of p2 section 5\\nwhere are the results for Acrobot linear function approximation\\nappendix Q-learning with linear FA is not DQN (table 2)\\n\\n\\xa0\\n\\nFAQ for Reviewers\\nQ. The review I just submitted is not appearing! Did it go thru?\\nA. There seems to be some delay in the system for you to see your reviews. Just give a bit of time and refresh again.\\n\\xa0\\nQ. What is the borderline paper meeting between AC and reviewers about?\\nA. This year we require ACs to meet with reviewers for all borderline papers (defined and announced by PCs once all reviews are received). There will be one meeting per borderline paper. Once we receive all reviews, we will let ACs know which papers fall into this bucket. ACs will then schedule and run the meetings with the reviewers of the papers in question.\\n\\xa0\\n\\nQ: What if I can not attend the meeting?\\nA: Please work with your AC to find a meeting schedule that works for everyone. Note that the discussion phase is roughly 1 month long, and overlaps with the NeurIPS conference, in case in-person meeting is preferred. All reviewers agreed to participate in review meetings when they accepted the reviewer invitations. An AC has right to\\xa0 downweight the review of reviewers who refuse to attend the meeting.\\n\\xa0\\n\\nQ: How should I use supplementary material?\\nA: It is not necessary to read supplementary material but such material can often answer questions that arise while reading the main paper, so consider looking there before asking authors.\\n\\xa0\\n\\nQ: How should I handle a policy violation?\\nA: To flag a CoE violation related to a submission, please indicate it when submitting the CoE report for that paper. The AC will work with the PC and the ethics board to resolve the case. To discuss other violations (e.g. plagiarism, double submission, paper length, formatting, etc.), please contact either the AC/SAC or the PC as appropriate. You can do this by sending a confidential comment with the appropriate readership restrictions.\\n\\xa0\\n\\nQ: Am I allowed to ask for additional experiments?\\nA: You can ask for additional experiments. New experiments should not significantly change the content of the submission. Rather, they should be limited in scope and serve to more thoroughly validate existing results from the submission.\\n\\xa0\\n\\nQ: If a submission does not achieve state-of-the-art results, is that grounds for rejection?\\nA: No, a lack of state-of-the-art results does not by itself constitute grounds for rejection. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge. Submissions can achieve this without achieving state-of-the-art results.\\nQ: Are authors expected to cite and compare with very recent work? What about non peer-reviewed (e.g., ArXiv) papers? (updated on 7 November 2022)\\nA: We consider papers contemporaneous if they are published (available in online proceedings) within the last four months. That means, since our full paper deadline is September 28, if a paper was published (i.e., at a peer-reviewed venue) on or after May 28, 2022, authors are not required to compare their own work to that paper. Authors are encouraged to cite and discuss all relevant papers, but they may be excused for not knowing about papers not published in peer-reviewed conference proceedings or journals, which includes papers exclusively available on arXiv. Reviewers are encouraged to use their own good judgement and, if in doubt, discuss with their area chair.\\xa0\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuccessful Page Load\\n\\n\\n\\n\\n\\n\\n\\n                        ICLR uses cookies for essential functions only. We do not sell your personal\\n                        information.\\n                        Our Privacy Policy \u00bb\\xa0\\n\\n\\nAccept\\n                            Cookies\\n                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            The ICLR Logo above may be used on presentations. Right-click and choose\\n                            download. It is a vector graphic and may be used at any scale.\\n                        \\n\\n\\n\\n\\n\\n                            Useful links\\n                        \\n\\n\\nSponsor / Exhibitor Information\\nPress\\n\\n\\n\\n\\n\\n\\n\\nContact\\n\\n 2710 E Corridor Dr, Appleton WI 54913\\n                            \\n\\n Email\\n\\n Phone: +1-920-268-4789\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR Proceedings at OpenReview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'max_results': 5, 'query': 'adaptive super-resolution neural architecture search'}, name='ArxivTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Found 5 paper(s) on arXiv\",\n  \"output\": [\n    {\n      \"title\": \"QuantNAS for super resolution: searching for efficient quantization-friendly architectures against quantization noise\",\n      \"authors\": [\n        \"Egor Shvetsov\",\n        \"Dmitry Osin\",\n        \"Alexey Zaytsev\",\n        \"Ivan Koryakovskiy\",\n        \"Valentin Buchnev\",\n        \"Ilya Trofimov\",\n        \"Evgeny Burnaev\"\n      ],\n      \"published\": \"2022-08-31T13:12:16+00:00\",\n      \"summary\": \"There is a constant need for high-performing and computationally efficient\\nneural network models for image super-resolution: computationally efficient\\nmodels can be used via low-capacity devices and reduce carbon footprints. One\\nway to obtain such models is to compress models, e.g. quantization. Another way\\nis a neural architecture search that automatically discovers new, more\\nefficient solutions. We propose a novel quantization-aware procedure, the\\nQuantNAS that combines pros of these two approaches. To make QuantNAS work, the\\nprocedure looks for quantization-friendly super-resolution models. The approach\\nutilizes entropy regularization, quantization noise, and Adaptive Deviation for\\nQuantization (ADQ) module to enhance the search procedure. The entropy\\nregularization technique prioritizes a single operation within each block of\\nthe search space. Adding quantization noise to parameters and activations\\napproximates model degradation after quantization, resulting in a more\\nquantization-friendly architectures. ADQ helps to alleviate problems caused by\\nBatch Norm blocks in super-resolution models. Our experimental results show\\nthat the proposed approximations are better for search procedure than direct\\nmodel quantization. QuantNAS discovers architectures with better PSNR/BitOps\\ntrade-off than uniform or mixed precision quantization of fixed architectures.\\nWe showcase the effectiveness of our method through its application to two\\nsearch spaces inspired by the state-of-the-art SR models and RFDN. Thus, anyone\\ncan design a proper search space based on an existing architecture and apply\\nour method to obtain better quality and efficiency.\\n  The proposed procedure is 30\\\\% faster than direct weight quantization and is\\nmore stable.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2208.14839v4\"\n    },\n    {\n      \"title\": \"Network Architecture Search for Domain Adaptation\",\n      \"authors\": [\n        \"Yichen Li\",\n        \"Xingchao Peng\"\n      ],\n      \"published\": \"2020-08-13T06:15:57+00:00\",\n      \"summary\": \"Deep networks have been used to learn transferable representations for domain\\nadaptation. Existing deep domain adaptation methods systematically employ\\npopular hand-crafted networks designed specifically for image-classification\\ntasks, leading to sub-optimal domain adaptation performance. In this paper, we\\npresent Neural Architecture Search for Domain Adaptation (NASDA), a principle\\nframework that leverages differentiable neural architecture search to derive\\nthe optimal network architecture for domain adaptation task. NASDA is designed\\nwith two novel training strategies: neural architecture search with\\nmulti-kernel Maximum Mean Discrepancy to derive the optimal architecture, and\\nadversarial training between a feature generator and a batch of classifiers to\\nconsolidate the feature generator. We demonstrate experimentally that NASDA\\nleads to state-of-the-art performance on several domain adaptation benchmarks.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2008.05706v1\"\n    },\n    {\n      \"title\": \"Hitchhiker's Guide to Super-Resolution: Introduction and Recent Advances\",\n      \"authors\": [\n        \"Brian Moser\",\n        \"Federico Raue\",\n        \"Stanislav Frolov\",\n        \"J\u00f6rn Hees\",\n        \"Sebastian Palacio\",\n        \"Andreas Dengel\"\n      ],\n      \"published\": \"2022-09-27T03:28:34+00:00\",\n      \"summary\": \"With the advent of Deep Learning (DL), Super-Resolution (SR) has also become\\na thriving research area. However, despite promising results, the field still\\nfaces challenges that require further research e.g., allowing flexible\\nupsampling, more effective loss functions, and better evaluation metrics. We\\nreview the domain of SR in light of recent advances, and examine\\nstate-of-the-art models such as diffusion (DDPM) and transformer-based SR\\nmodels. We present a critical discussion on contemporary strategies used in SR,\\nand identify promising yet unexplored research directions. We complement\\nprevious surveys by incorporating the latest developments in the field such as\\nuncertainty-driven losses, wavelet networks, neural architecture search, novel\\nnormalization methods, and the latests evaluation techniques. We also include\\nseveral visualizations for the models and methods throughout each chapter in\\norder to facilitate a global understanding of the trends in the field. This\\nreview is ultimately aimed at helping researchers to push the boundaries of DL\\napplied to SR.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2209.13131v2\"\n    },\n    {\n      \"title\": \"Revisiting L1 Loss in Super-Resolution: A Probabilistic View and Beyond\",\n      \"authors\": [\n        \"Xiangyu He\",\n        \"Jian Cheng\"\n      ],\n      \"published\": \"2022-01-25T04:04:44+00:00\",\n      \"summary\": \"Super-resolution as an ill-posed problem has many high-resolution candidates\\nfor a low-resolution input. However, the popular $\\\\ell_1$ loss used to best fit\\nthe given HR image fails to consider this fundamental property of\\nnon-uniqueness in image restoration. In this work, we fix the missing piece in\\n$\\\\ell_1$ loss by formulating super-resolution with neural networks as a\\nprobabilistic model. It shows that $\\\\ell_1$ loss is equivalent to a degraded\\nlikelihood function that removes the randomness from the learning process. By\\nintroducing a data-adaptive random variable, we present a new objective\\nfunction that aims at minimizing the expectation of the reconstruction error\\nover all plausible solutions. The experimental results show consistent\\nimprovements on mainstream architectures, with no extra parameter or computing\\ncost at inference time.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2201.10084v2\"\n    },\n    {\n      \"title\": \"Neural Architecture Search using Progressive Evolution\",\n      \"authors\": [\n        \"Nilotpal Sinha\",\n        \"Kuan-Wen Chen\"\n      ],\n      \"published\": \"2022-03-03T08:15:14+00:00\",\n      \"summary\": \"Vanilla neural architecture search using evolutionary algorithms (EA)\\ninvolves evaluating each architecture by training it from scratch, which is\\nextremely time-consuming. This can be reduced by using a supernet to estimate\\nthe fitness of every architecture in the search space due to its weight sharing\\nnature. However, the estimated fitness is very noisy due to the co-adaptation\\nof the operations in the supernet. In this work, we propose a method called\\npEvoNAS wherein the whole neural architecture search space is progressively\\nreduced to smaller search space regions with good architectures. This is\\nachieved by using a trained supernet for architecture evaluation during the\\narchitecture search using genetic algorithm to find search space regions with\\ngood architectures. Upon reaching the final reduced search space, the supernet\\nis then used to search for the best architecture in that search space using\\nevolution. The search is also enhanced by using weight inheritance wherein the\\nsupernet for the smaller search space inherits its weights from previous\\ntrained supernet for the bigger search space. Exerimentally, pEvoNAS gives\\nbetter results on CIFAR-10 and CIFAR-100 while using significantly less\\ncomputational resources as compared to previous EA-based methods. The code for\\nour paper can be found in https://github.com/nightstorm0909/pEvoNAS\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2203.01559v1\"\n    }\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ArxivTool` with \n```json\n{\n  \"max_results\": 5,\n  \"query\": \"adaptive super-resolution neural architecture search\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ArxivTool', response={'result': {'status': 'success', 'message': 'Found 5 paper(s) on arXiv', 'output': [{'title': 'QuantNAS for super resolution: searching for efficient quantization-friendly architectures against quantization noise', 'authors': ['Egor Shvetsov', 'Dmitry Osin', 'Alexey Zaytsev', 'Ivan Koryakovskiy', 'Valentin Buchnev', 'Ilya Trofimov', 'Evgeny Burnaev'], 'published': '2022-08-31T13:12:16+00:00', 'summary': 'There is a constant need for high-performing and computationally efficient\\nneural network models for image super-resolution: computationally efficient\\nmodels can be used via low-capacity devices and reduce carbon footprints. One\\nway to obtain such models is to compress models, e.g. quantization. Another way\\nis a neural architecture search that automatically discovers new, more\\nefficient solutions. We propose a novel quantization-aware procedure, the\\nQuantNAS that combines pros of these two approaches. To make QuantNAS work, the\\nprocedure looks for quantization-friendly super-resolution models. The approach\\nutilizes entropy regularization, quantization noise, and Adaptive Deviation for\\nQuantization (ADQ) module to enhance the search procedure. The entropy\\nregularization technique prioritizes a single operation within each block of\\nthe search space. Adding quantization noise to parameters and activations\\napproximates model degradation after quantization, resulting in a more\\nquantization-friendly architectures. ADQ helps to alleviate problems caused by\\nBatch Norm blocks in super-resolution models. Our experimental results show\\nthat the proposed approximations are better for search procedure than direct\\nmodel quantization. QuantNAS discovers architectures with better PSNR/BitOps\\ntrade-off than uniform or mixed precision quantization of fixed architectures.\\nWe showcase the effectiveness of our method through its application to two\\nsearch spaces inspired by the state-of-the-art SR models and RFDN. Thus, anyone\\ncan design a proper search space based on an existing architecture and apply\\nour method to obtain better quality and efficiency.\\n  The proposed procedure is 30\\\\% faster than direct weight quantization and is\\nmore stable.', 'pdf_url': 'http://arxiv.org/pdf/2208.14839v4'}, {'title': 'Network Architecture Search for Domain Adaptation', 'authors': ['Yichen Li', 'Xingchao Peng'], 'published': '2020-08-13T06:15:57+00:00', 'summary': 'Deep networks have been used to learn transferable representations for domain\\nadaptation. Existing deep domain adaptation methods systematically employ\\npopular hand-crafted networks designed specifically for image-classification\\ntasks, leading to sub-optimal domain adaptation performance. In this paper, we\\npresent Neural Architecture Search for Domain Adaptation (NASDA), a principle\\nframework that leverages differentiable neural architecture search to derive\\nthe optimal network architecture for domain adaptation task. NASDA is designed\\nwith two novel training strategies: neural architecture search with\\nmulti-kernel Maximum Mean Discrepancy to derive the optimal architecture, and\\nadversarial training between a feature generator and a batch of classifiers to\\nconsolidate the feature generator. We demonstrate experimentally that NASDA\\nleads to state-of-the-art performance on several domain adaptation benchmarks.', 'pdf_url': 'http://arxiv.org/pdf/2008.05706v1'}, {'title': \"Hitchhiker's Guide to Super-Resolution: Introduction and Recent Advances\", 'authors': ['Brian Moser', 'Federico Raue', 'Stanislav Frolov', 'J\u00f6rn Hees', 'Sebastian Palacio', 'Andreas Dengel'], 'published': '2022-09-27T03:28:34+00:00', 'summary': 'With the advent of Deep Learning (DL), Super-Resolution (SR) has also become\\na thriving research area. However, despite promising results, the field still\\nfaces challenges that require further research e.g., allowing flexible\\nupsampling, more effective loss functions, and better evaluation metrics. We\\nreview the domain of SR in light of recent advances, and examine\\nstate-of-the-art models such as diffusion (DDPM) and transformer-based SR\\nmodels. We present a critical discussion on contemporary strategies used in SR,\\nand identify promising yet unexplored research directions. We complement\\nprevious surveys by incorporating the latest developments in the field such as\\nuncertainty-driven losses, wavelet networks, neural architecture search, novel\\nnormalization methods, and the latests evaluation techniques. We also include\\nseveral visualizations for the models and methods throughout each chapter in\\norder to facilitate a global understanding of the trends in the field. This\\nreview is ultimately aimed at helping researchers to push the boundaries of DL\\napplied to SR.', 'pdf_url': 'http://arxiv.org/pdf/2209.13131v2'}, {'title': 'Revisiting L1 Loss in Super-Resolution: A Probabilistic View and Beyond', 'authors': ['Xiangyu He', 'Jian Cheng'], 'published': '2022-01-25T04:04:44+00:00', 'summary': 'Super-resolution as an ill-posed problem has many high-resolution candidates\\nfor a low-resolution input. However, the popular $\\\\ell_1$ loss used to best fit\\nthe given HR image fails to consider this fundamental property of\\nnon-uniqueness in image restoration. In this work, we fix the missing piece in\\n$\\\\ell_1$ loss by formulating super-resolution with neural networks as a\\nprobabilistic model. It shows that $\\\\ell_1$ loss is equivalent to a degraded\\nlikelihood function that removes the randomness from the learning process. By\\nintroducing a data-adaptive random variable, we present a new objective\\nfunction that aims at minimizing the expectation of the reconstruction error\\nover all plausible solutions. The experimental results show consistent\\nimprovements on mainstream architectures, with no extra parameter or computing\\ncost at inference time.', 'pdf_url': 'http://arxiv.org/pdf/2201.10084v2'}, {'title': 'Neural Architecture Search using Progressive Evolution', 'authors': ['Nilotpal Sinha', 'Kuan-Wen Chen'], 'published': '2022-03-03T08:15:14+00:00', 'summary': 'Vanilla neural architecture search using evolutionary algorithms (EA)\\ninvolves evaluating each architecture by training it from scratch, which is\\nextremely time-consuming. This can be reduced by using a supernet to estimate\\nthe fitness of every architecture in the search space due to its weight sharing\\nnature. However, the estimated fitness is very noisy due to the co-adaptation\\nof the operations in the supernet. In this work, we propose a method called\\npEvoNAS wherein the whole neural architecture search space is progressively\\nreduced to smaller search space regions with good architectures. This is\\nachieved by using a trained supernet for architecture evaluation during the\\narchitecture search using genetic algorithm to find search space regions with\\ngood architectures. Upon reaching the final reduced search space, the supernet\\nis then used to search for the best architecture in that search space using\\nevolution. The search is also enhanced by using weight inheritance wherein the\\nsupernet for the smaller search space inherits its weights from previous\\ntrained supernet for the bigger search space. Exerimentally, pEvoNAS gives\\nbetter results on CIFAR-10 and CIFAR-100 while using significantly less\\ncomputational resources as compared to previous EA-based methods. The code for\\nour paper can be found in https://github.com/nightstorm0909/pEvoNAS', 'pdf_url': 'http://arxiv.org/pdf/2203.01559v1'}]}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Okay, I have gathered the necessary information to proceed with the review.\n\n**Step 1: Review Preparation and Resource Analysis**\n\n*   Budget: Sufficient for creating agents and using tools.\n*   Tools: All necessary tools are available.\n*   ICLR 2023 Review Criteria: Focus on novelty, correctness, significance, and clarity. Constructive feedback is essential.\n*   Venue focus: ICLR emphasizes novel and impactful contributions to machine learning.\n\n**Step 2: Paper Context Research and Novelty Investigation**\n\n*   The paper addresses the problem of providing Quality of Service (QoS) for super-resolution (SR) on mobile devices with fluctuating resources.\n*   It proposes DySR, an algorithm and system co-design approach using adaptive-aware one-shot NAS.\n*   The approach aims to generate Pareto-optimal models with low switching overhead and framerate constraints.\n*   Novelty lies in the adaptive NAS and the system co-design for real-time model adaptation on mobile devices.\n*   The paper considers the dynamic resource environment of mobile devices, which is often overlooked in existing SR and NAS research.\n*   The paper is not found to be under review elsewhere.\n\n**Step 3: Paper-Adaptive Reviewer Agent Creation and Coordination**\n\nBased on the paper's content, I will create three reviewer agents with the following specializations:\n\n1.  **Mobile Systems and QoS Expert (AgentMobileQoS):** Focuses on the system aspects, real-time performance, and the validity of the QoS claims, including the experimental setup on mobile devices.\n2.  **Neural Architecture Search Specialist (AgentNAS):** Evaluates the novelty and effectiveness of the proposed adaptive-aware one-shot NAS approach, comparing it to existing NAS techniques.\n3.  **Super-Resolution Domain Expert (AgentSRExpert):** Assesses the SR model architecture, performance compared to other SR models, and the quality of the super-resolved images/videos.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"gemini-2.5-flash-preview-05-20\": {\n      \"description\": \"Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math\",\n      \"create_expense_cost\": 0,\n      \"invoke_expense_cost\": 0.15,\n      \"output_expense_cost\": 0.6\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'gemini-2.5-flash-preview-05-20': {'description': 'Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math', 'create_expense_cost': 0, 'invoke_expense_cost': 0.15, 'output_expense_cost': 0.6}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'system_prompt': \"You are an expert in mobile systems and Quality of Service (QoS). You will evaluate the paper's system design, real-time performance, and the validity of the QoS claims, including the experimental setup on mobile devices. Pay close attention to the resource adaptivity and model selection policy.\", 'base_model': 'gemini-2.5-flash-preview-05-20', 'description': 'Expert in mobile systems and QoS, focusing on real-time performance and system aspects.', 'agent_name': 'AgentMobileQoS'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gemini-2.5-flash-preview-05-20', 'system_prompt': 'You are a specialist in Neural Architecture Search (NAS). You will evaluate the novelty and effectiveness of the proposed adaptive-aware one-shot NAS approach, comparing it to existing NAS techniques. Focus on the search space design, sampling strategy, and training process.', 'agent_name': 'AgentNAS', 'description': 'Specialist in Neural Architecture Search, evaluating the novelty and effectiveness of the proposed adaptive-aware one-shot NAS approach.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'AgentSRExpert', 'description': 'Super-Resolution domain expert, assessing the SR model architecture, performance, and the quality of the super-resolved images/videos.', 'base_model': 'gemini-2.5-flash-preview-05-20', 'system_prompt': 'You are a Super-Resolution (SR) domain expert. You will assess the SR model architecture, its performance compared to other SR models, and the quality of the super-resolved images/videos. Pay attention to the PSNR/SSIM metrics and visual results.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.97094635\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"system_prompt\": \"You are an expert in mobile systems and Quality of Service (QoS). You will evaluate the paper's system design, real-time performance, and the validity of the QoS claims, including the experimental setup on mobile devices. Pay close attention to the resource adaptivity and model selection policy.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"description\": \"Expert in mobile systems and QoS, focusing on real-time performance and system aspects.\",\n  \"agent_name\": \"AgentMobileQoS\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.97094635\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"system_prompt\": \"You are a specialist in Neural Architecture Search (NAS). You will evaluate the novelty and effectiveness of the proposed adaptive-aware one-shot NAS approach, comparing it to existing NAS techniques. Focus on the search space design, sampling strategy, and training process.\",\n  \"agent_name\": \"AgentNAS\",\n  \"description\": \"Specialist in Neural Architecture Search, evaluating the novelty and effectiveness of the proposed adaptive-aware one-shot NAS approach.\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.97094635\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"agent_name\": \"AgentSRExpert\",\n  \"description\": \"Super-Resolution domain expert, assessing the SR model architecture, performance, and the quality of the super-resolved images/videos.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"system_prompt\": \"You are a Super-Resolution (SR) domain expert. You will assess the SR model architecture, its performance compared to other SR models, and the quality of the super-resolved images/videos. Pay attention to the PSNR/SSIM metrics and visual results.\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.97094635}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.97094635}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.97094635}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "**Step 4: Specialized Multi-Perspective Review Execution**\n\nNow, I will instruct each agent to review the paper based on their expertise.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'AgentMobileQoS', 'prompt': 'Evaluate the system design, real-time performance, and QoS claims of the DySR paper. Focus on the experimental setup on mobile devices and the resource adaptivity and model selection policy. Provide a detailed assessment of the strengths and weaknesses, and a recommendation (Accept/Reject) with justification.'}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'AgentNAS', 'prompt': 'Evaluate the novelty and effectiveness of the adaptive-aware one-shot NAS approach in the DySR paper. Compare it to existing NAS techniques, focusing on the search space design, sampling strategy, and training process. Provide a detailed assessment of the strengths and weaknesses, and a recommendation (Accept/Reject) with justification.'}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Evaluate the SR model architecture, performance, and the quality of the super-resolved images/videos in the DySR paper. Pay attention to the PSNR/SSIM metrics and visual results. Compare the performance with other SR models. Provide a detailed assessment of the strengths and weaknesses, and a recommendation (Accept/Reject) with justification.', 'agent_name': 'AgentSRExpert'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"## Evaluation of the DySR Paper: Dynamic Super-Resolution for Mobile Devices\\n\\nThis evaluation focuses on the system design, real-time performance, QoS claims, experimental setup on mobile devices, resource adaptivity, and model selection policy of the DySR paper.\\n\\n### 1. System Design\\n\\n**Strengths:**\\n*   **Clear Problem Statement:** The paper effectively identifies the challenge of deploying high-quality Super-Resolution (SR) models on resource-constrained mobile devices while maintaining real-time performance and quality of service (QoS).\\n*   **Modular Architecture:** The proposed DySR system typically features a modular design, separating the core SR inference from a dynamic decision engine, resource monitoring, and a repository of diverse SR models. This modularity allows for easier component updates and testing.\\n*   **Adaptive Nature:** The core idea of dynamically selecting an SR model based on real-time device resource availability is a sound and practical approach to address the variability in mobile performance.\\n*   **Feedback Loop:** A well-designed DySR system usually incorporates a feedback loop where resource utilization and output quality (e.g., frame rate) are monitored to inform subsequent model selection decisions.\\n\\n**Weaknesses:**\\n*   **Overhead of Adaptivity:** While dynamic adaptation is crucial, the paper needs to thoroughly analyze and quantify the overhead introduced by the decision engine itself, including:\\n    *   **Monitoring Overhead:** The continuous polling of CPU, GPU, memory, and potentially temperature sensors consumes resources. Is this overhead negligible compared to SR inference?\\n    *   **Switching Latency:** The time taken to unload one SR model and load/initialize another can introduce noticeable latency or frame drops. How is this managed or amortized? Is there a \\\"cold start\\\" penalty for models?\\n*   **Complexity of Model Management:** Managing multiple SR models (loading, unloading, memory footprint) can be complex, especially on devices with limited RAM. How are memory pressure and caching handled?\\n*   **Lack of Predictive Capability (Potential):** Most reactive systems struggle with sudden resource changes. Does the system anticipate future resource availability or is it purely reactive? A purely reactive system might lead to oscillations or late adaptations.\\n*   **User QoS Preference Integration:** While the system adapts to *device* resources, it's often unclear how user-defined QoS preferences (e.g., \\\"prioritize visual quality even if it means lower FPS,\\\" or \\\"prioritize battery life\\\") are integrated into the decision-making process.\\n\\n### 2. Real-time Performance\\n\\n**Strengths:**\\n*   **Focus on Latency/Throughput:** The primary goal of achieving real-time SR demands a strong focus on processing latency and frames per second (FPS). The system aims to maintain a target FPS, which is a critical real-time metric.\\n*   **Demonstrated Improvement over Static:** Typically, DySR would show that its dynamic approach outperforms a static, single-model approach, which would either struggle to maintain FPS under load or provide unnecessarily low quality when resources are abundant.\\n\\n**Weaknesses:**\\n*   **Jitter and Smoothness:** While average FPS might be good, sudden model switches can introduce significant frame time jitter, leading to a choppy user experience. Is \\\"smoothness\\\" (e.g., variance in frame time) adequately measured and optimized?\\n*   **Worst-case Latency:** In real-time systems, worst-case latency is often more critical than average latency. What guarantees can the system provide regarding maximum processing delay, especially during adaptation phases?\\n*   **Impact of Thermal Throttling:** Mobile devices are highly susceptible to thermal throttling, which can drastically reduce sustained performance. Does the paper sufficiently account for this in its real-time performance evaluation, or are benchmarks conducted for short bursts?\\n*   **Energy Efficiency as a Real-time Constraint:** While often tied to performance, energy consumption is a distinct real-time constraint on mobile. A system might be fast but drain the battery quickly. Is energy consumption considered an active QoS parameter in the real-time feedback loop?\\n\\n### 3. QoS Claims\\n\\n**Strengths:**\\n*   **Explicit QoS Target:** The paper clearly states its intent to maintain QoS, typically defined by a combination of visual quality (e.g., PSNR, SSIM, LPIPS) and real-time performance (e.g., FPS).\\n*   **Trade-off Exploration:** By dynamically switching models, the system implicitly explores the trade-off space between visual quality and computational cost.\\n\\n**Weaknesses:**\\n*   **Incomplete QoS Definition:** While PSNR/SSIM and FPS are crucial, \\\"QoS\\\" for mobile SR should ideally encompass:\\n    *   **Energy Consumption:** A critical mobile constraint. Is battery drain monitored and optimized?\\n    *   **Thermal Performance:** Avoiding excessive heat buildup to prevent throttling and ensure user comfort.\\n    *   **Smoothness/Jitter:** As mentioned above, a stable frame rate is as important as a high average.\\n    *   **Perceptual Quality:** PSNR/SSIM are objective but don't always align with human perception. Are subjective user studies or perceptual metrics (like LPIPS) included?\\n*   **Validation under Stress:** Do the QoS claims hold up under extreme conditions, such as high background load, low battery, or sustained usage leading to thermal throttling? Often, evaluations are done in pristine lab conditions.\\n*   **QoS Metric Prioritization:** If multiple QoS metrics are in conflict (e.g., highest quality vs. lowest energy), how does the system prioritize them? Is this configurable?\\n\\n### 4. Experimental Setup on Mobile Devices\\n\\n**Strengths:**\\n*   **Real Hardware Testing:** Conducting experiments on actual mobile devices (smartphones, tablets) is a significant strength, as it directly assesses the system's performance in a realistic environment, avoiding the pitfalls of simulation or desktop-only evaluation.\\n*   **Device Specifications:** Providing detailed specifications of the mobile devices used (CPU, GPU, RAM, specific SoC model) is crucial for reproducibility and understanding the context of the results.\\n\\n**Weaknesses:**\\n*   **Limited Device Diversity:** Often, papers test on only one or two flagship devices. To claim generalizability, testing on a wider range of devices (e.g., different manufacturers, price points, SoC architectures like Snapdragon, MediaTek, Exynos) is essential. Different mobile GPUs (Adreno, Mali) have distinct performance characteristics for AI workloads.\\n*   **Measurement Accuracy and Reliability:**\\n    *   **Energy Measurement:** Measuring actual battery drain accurately on mobile devices requires specialized hardware (e.g., power meters) or reliable software APIs. Relying solely on `top` or Android's battery stats might be insufficient or imprecise.\\n    *   **Resource Monitoring:** How are CPU/GPU utilization, memory usage, and temperature precisely captured without introducing significant measurement perturbation? Are system-level tools (e.g., `adb shell dumpsys cpuinfo`, GPU profiling tools) used appropriately?\\n    *   **Background Processes:** Mobile OSes run numerous background processes. Were experiments conducted in a controlled environment (e.g., \\\"airplane mode\\\" with minimal background apps) or a realistic one? If realistic, how was variability accounted for?\\n*   **Thermal Management:** How was thermal throttling observed and mitigated (or intentionally triggered for evaluation)? Without addressing this, sustained performance numbers can be misleading.\\n*   **Input Data Representativeness:** Are the video streams used for evaluation representative of real-world mobile use cases (e.g., varying scene complexity, motion, lighting conditions)?\\n\\n### 5. Resource Adaptivity\\n\\n**Strengths:**\\n*   **Dynamic Response to Resource Changes:** The core of DySR is its ability to react to fluctuating resource availability, which is inherent in mobile environments (e.g., foreground/background app switching, network activity, thermal effects).\\n*   **Granular Resource Monitoring:** The system monitors key resources like CPU, GPU, and memory, providing the necessary input for adaptive decisions.\\n\\n**Weaknesses:**\\n*   **Adaptation Granularity and Responsiveness:** How quickly does the system react to resource changes? Is the adaptation instantaneous, or is there a noticeable delay? What's the smallest time quantum for adaptation?\\n*   **Oscillation/Thrashing:** A common problem in adaptive systems is \\\"thrashing,\\\" where the system rapidly switches between models due to minor fluctuations around a threshold, leading to instability and increased overhead. Is there a hysteresis mechanism or smoothing applied to avoid this?\\n*   **Threshold Determination:** How are the thresholds for triggering model switches determined? Are they static, empirically derived, or dynamically learned? Static thresholds might not generalize well across devices or conditions.\\n*   **Lack of Proactive Adaptation:** As mentioned under System Design, purely reactive adaptation might be suboptimal. Can the system use historical data or simpler predictive models to anticipate resource constraints and adapt proactively?\\n\\n### 6. Model Selection Policy\\n\\n**Strengths:**\\n*   **Portfolio of Models:** Having a diverse set of pre-trained SR models with different quality-performance trade-offs is fundamental. This allows the system to operate across a wide range of resource conditions.\\n*   **Profiling-based Selection:** The policy likely relies on pre-profiling each SR model on the target hardware to understand its performance (latency, energy) and quality characteristics under various loads. This is a sound approach.\\n\\n**Weaknesses:**\\n*   **Cost Function Complexity:** Is the selection policy based on a simple heuristic (e.g., \\\"if FPS drops below X, switch to lower model\\\") or a more sophisticated cost function that balances multiple QoS metrics? A more advanced policy would consider battery, temperature, and visual quality concurrently.\\n*   **Model Transition Smoothness:** How are transitions between different SR models handled to ensure visual continuity? A sudden switch between models with different characteristics might be visually jarring. Is there a cross-fade or blend?\\n*   **Model Profile Accuracy:** Are the profiles of each SR model accurate across different operating conditions (e.g., cold start vs. warm, different thermal states)?\\n*   **Dynamic Reprofiling:** Mobile environments are dynamic. Do model profiles need to be updated online, or are offline profiles sufficient? What if a new background app is installed that significantly impacts baseline performance?\\n*   **Scalability of Model Set:** As the number of available SR models grows, the profiling and selection process can become more complex.\\n*   **Lack of Learning:** Is there any learning component (e.g., reinforcement learning) to refine the model selection policy over time based on observed performance and user feedback?\\n\\n---\\n\\n### Recommendation\\n\\n**Conditional Acceptance / Accept with Major Revisions**\\n\\n**Justification:**\\n\\nThe DySR paper tackles a highly relevant and challenging problem: enabling high-quality real-time Super-Resolution on diverse and resource-constrained mobile devices. The core idea of dynamic model selection based on resource availability is scientifically sound and has significant practical implications.\\n\\nHowever, the major concerns revolve around the **rigor of the experimental setup on mobile devices** and the **comprehensiveness of the QoS validation**. Mobile environments are notoriously complex and noisy. Without a robust and transparent methodology for measuring CPU/GPU utilization, power consumption, and thermal behavior, the real-time performance and QoS claims remain partially unsubstantiated.\\n\\n**Key areas requiring major revisions for acceptance include:**\\n\\n1.  **Enhanced Experimental Rigor:**\\n    *   **Detailed Power Analysis:** Provide precise battery consumption measurements using specialized tools or validated software APIs. Show energy-performance trade-offs.\\n    *   **Thermal Awareness:** Explicitly address thermal throttling. Show performance under sustained load, not just short bursts. Perhaps include temperature monitoring graphs.\\n    *   **Broader Device Testing:** Evaluate performance on a wider range of mobile SoC architectures and price points to demonstrate generalizability.\\n    *   **Background Process Control:** Clearly state the experimental conditions (e.g., minimal background apps vs. realistic scenarios with other apps running). Quantify the impact of background noise.\\n    *   **Jitter/Smoothness Metrics:** Beyond average FPS, provide data on frame time variability to ensure a smooth user experience.\\n\\n2.  **Comprehensive QoS Validation:**\\n    *   **Beyond PSNR/SSIM and FPS:** Incorporate energy efficiency, thermal stability, and potentially perceptual quality metrics (e.g., LPIPS, or ideally, user studies).\\n    *   **QoS under Stress:** Validate claims under various challenging scenarios (e.g., low battery, high ambient temperature, intense background activity).\\n\\n3.  **Detailed Analysis of Adaptivity Overheads:**\\n    *   **Quantify Monitoring & Switching Costs:** Provide precise measurements of the overhead introduced by resource monitoring and, critically, by the process of switching between SR models (loading/unloading, initialization time, memory implications).\\n    *   **Thrashing Mitigation:** Clearly explain and demonstrate how the system prevents or mitigates oscillation/thrashing between models.\\n\\n4.  **Refined Model Selection Policy:**\\n    *   **Clarity on Thresholds/Logic:** Detail how thresholds are determined and if they adapt. Explain the multi-objective optimization (if any) for selecting a model considering quality, performance, and energy.\\n    *   **Transition Management:** Discuss how visual smoothness is maintained during model transitions.\\n\\nIf these weaknesses are addressed with rigorous empirical evidence and detailed analysis, the paper has the potential to be a significant contribution to the field of mobile AI and real-time computer vision.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.96809835\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"AgentMobileQoS\",\n  \"prompt\": \"Evaluate the system design, real-time performance, and QoS claims of the DySR paper. Focus on the experimental setup on mobile devices and the resource adaptivity and model selection policy. Provide a detailed assessment of the strengths and weaknesses, and a recommendation (Accept/Reject) with justification.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The DySR paper proposes an \\\"adaptive-aware one-shot NAS approach\\\" for Dynamic Super-Resolution. This evaluation will assess its novelty and effectiveness by focusing on its search space design, sampling strategy, and training process, comparing it to existing NAS techniques.\\n\\n## Evaluation of DySR's Adaptive-Aware One-Shot NAS Approach\\n\\n### 1. Novelty\\n\\nThe primary novelty of DySR's approach lies in its **\\\"adaptive-aware\\\" capability within a one-shot NAS framework**, particularly tailored for the diverse and dynamic requirements of Super-Resolution (SR) tasks. While one-shot NAS is not new, the explicit integration of \\\"awareness\\\" for varying resource constraints (e.g., different upsampling factors, latency budgets, model sizes) *during the search and potentially training phases* is a significant step.\\n\\n*   **Existing One-Shot NAS:** Methods like SPOS (Single Path One-Shot) or ProxylessNAS focus on finding *one* optimal architecture (or a few) for a *single* set of constraints (e.g., best accuracy under a fixed latency).\\n*   **DySR's Novelty:** DySR aims to train a single supernet from which multiple highly efficient sub-networks can be derived, each optimally suited for *different, dynamically changing resource constraints*. This moves beyond finding a single Pareto-optimal point to enabling the *efficient derivation of a Pareto frontier* from a single supernet training run. This is crucial for real-world SR applications where requirements can change on the fly (e.g., high-quality offline vs. real-time mobile inference).\\n\\n### 2. Effectiveness\\n\\nThe effectiveness of DySR is demonstrated by its ability to:\\n1.  **Generate a spectrum of performant SR models:** It claims to find models that achieve competitive PSNR/SSIM while adhering to diverse latency and parameter budgets.\\n2.  **Reduce search costs:** Leveraging the one-shot paradigm significantly reduces the computational burden compared to traditional NAS methods like RL or evolutionary algorithms.\\n3.  **Adapt to dynamic requirements:** Its core promise is to provide on-demand sub-networks for varying upsampling scales and efficiency demands, which is highly effective for deployment scenarios.\\n\\n### 3. Detailed Analysis\\n\\n#### A. Search Space Design\\n\\n*   **Description:** The search space for DySR is typically composed of a directed acyclic graph (DAG) of potential operations and connections, forming a \\\"supernet.\\\" For SR, this would include common SR-specific operations like sub-pixel convolutions, various upsampling layers, residual blocks, attention modules, and different types of convolutions (e.g., depthwise separable, standard). The key aspect is that it must be rich enough to contain diverse high-performing architectures but constrained enough to be learnable.\\n*   **Comparison to Existing NAS:**\\n    *   **Strengths (relative to general NAS):** Likely incorporates operations and architectures proven effective for SR, making the search more focused and efficient. The design allows for flexible connectivity, enabling complex architectures.\\n    *   **Weaknesses:** The expressiveness of the supernet can be a double-edged sword. A very large supernet might lead to \\\"supernet collapse\\\" (where specific paths are not sufficiently trained) or \\\"unfair weight sharing\\\" (where some operations are more privileged than others due to sampling frequency). The paper needs to explicitly state how this search space is specifically designed to accommodate the \\\"adaptive\\\" aspect (e.g., including multiple upsampling pathways, different block complexities). If it's a generic SR search space, the \\\"adaptive-aware\\\" part primarily comes from sampling/training.\\n\\n#### B. Sampling Strategy\\n\\n*   **Description:** This is where the \\\"adaptive-aware\\\" mechanism is most prominent. During *supernet training*, the sampling strategy likely involves techniques to ensure the supernet weights are robust across a diverse set of subnetworks, potentially sampling paths under *varying resource constraints* or *different upsampling factors*. After the supernet is trained, the \\\"adaptive-aware\\\" part comes into full play: a search algorithm (e.g., evolutionary algorithm, greedy search, or a learned controller) navigates the *already trained* supernet to find specific sub-networks that precisely meet the dynamic resource requirements (e.g., find the best quality model that fits under 10ms latency for 4x upscaling).\\n*   **Comparison to Existing NAS:**\\n    *   **Strengths:**\\n        *   **Efficiency:** After supernet training, finding new sub-networks is extremely fast, often just a graph traversal and lookup, avoiding costly retraining or re-evaluations.\\n        *   **Multi-objective Optimization:** Enables the discovery of Pareto-optimal SR models for different latency/quality trade-offs from a *single* trained supernet. This is a significant advantage over methods that search for one optimal model at a time.\\n        *   **On-demand Adaptation:** Directly addresses the dynamic nature of SR deployment, where requirements can change.\\n    *   **Weaknesses:**\\n        *   **Supernet Robustness:** The challenge is to make the supernet weights generalizable enough so that *any* sampled path, even those rarely explored during training, performs well. If the sampling during training isn't sufficiently \\\"adaptive-aware\\\" (i.e., doesn't cover the full range of target constraints), the derived subnets might suffer.\\n        *   **Search Algorithm Post-training:** The quality of the final sub-networks heavily depends on the effectiveness of the *post-training search algorithm* (e.g., EA for latency-constrained search). While fast, this search still needs to be efficient and find good trade-offs.\\n\\n#### C. Training Process\\n\\n*   **Description:** The supernet is trained using a standard one-shot training methodology. This involves training the supernet weights such that any path sampled from it performs well. The \\\"adaptive-aware\\\" aspect here likely manifests in:\\n    1.  **Diverse Path Sampling:** During training, the supernet might not just sample random paths, but paths that represent various resource constraints or upsampling factors.\\n    2.  **Multi-objective Loss:** Potentially, the loss function could implicitly or explicitly encourage the supernet to learn weights robust to different operational points. For example, by regularizing the loss based on path complexity or having multiple heads for different upscaling factors.\\n    3.  **Progressive Training:** Often, one-shot methods employ progressive training (e.g., gradually increasing the search space complexity or path diversity) to stabilize supernet training.\\n*   **Comparison to Existing NAS:**\\n    *   **Strengths:**\\n        *   **Computational Efficiency:** Training one large supernet is vastly more efficient than training thousands of individual models (RL/EA NAS) or performing differentiable searches that can be unstable (DARTS).\\n        *   **Weight Sharing:** The core of one-shot NAS, enabling parameter efficiency during the search phase.\\n        *   **Tailored for SR:** The loss function would be optimized for SR metrics (e.g., L1/L2, perceptual loss, SSIM).\\n    *   **Weaknesses:**\\n        *   **Supernet Optimization Challenge:** Training a robust supernet is notoriously difficult. Issues like \\\"collapse\\\" (where the supernet performs well, but sampled subnets don't), \\\"fairness\\\" of weight sharing, and difficulty in generalizing to unseen paths are common.\\n        *   **Training Stability:** Large supernets can be unstable to train, requiring careful hyperparameter tuning and regularization.\\n        *   **\\\"Adaptive-Awareness\\\" Integration:** The critical aspect is *how* the \\\"adaptive-aware\\\" part is woven into the training. If it's just standard one-shot training and then adaptive *sampling*, the novelty is mostly in the sampling. If the *training itself* is made aware (e.g., by dynamically adjusting path probabilities based on target constraints), then it's a stronger contribution.\\n\\n### 4. Strengths & Weaknesses\\n\\n**Strengths:**\\n*   **Efficiency:** Leverages the one-shot paradigm for significant computational savings in the search process compared to RL/EA NAS.\\n*   **Adaptability for Dynamic Requirements:** Crucially enables the efficient discovery of multiple high-performing SR models tailored to varying upsampling factors, latency budgets, and model sizes from a single trained supernet. This is a highly practical capability for real-world deployment.\\n*   **Practicality for SR:** Addresses a key challenge in SR deployment where resource constraints are often fluid.\\n*   **Strong Performance:** Likely achieves competitive or superior performance to hand-crafted models and other NAS methods within its targeted efficiency envelopes.\\n\\n**Weaknesses:**\\n*   **Supernet Training Difficulty:** Like all one-shot NAS methods, training a robust and generalizable supernet can be challenging, prone to \\\"supernet collapse\\\" or sub-optimal performance for certain sampled paths.\\n*   **\\\"Fairness\\\" of Weight Sharing:** Ensuring that all operations and connections within the supernet are equally well-trained is difficult, potentially leading to bias towards frequently sampled paths.\\n*   **Search Space Limitation:** While tailored for SR, the chosen search space inherently limits the architectures that can be found. Novel, truly out-of-distribution architectures might be missed.\\n*   **Post-training Search Dependency:** While fast, the effectiveness of finding truly optimal adaptive models relies on an efficient and effective post-training search algorithm (e.g., evolutionary search).\\n*   **Generalizability Beyond SR:** The \\\"adaptive-aware\\\" mechanisms, while powerful for SR, might need significant modification for other computer vision tasks if their constraints are fundamentally different.\\n\\n### 5. Recommendation\\n\\n**Recommendation: Accept**\\n\\n**Justification:**\\n\\nThe DySR paper proposes a valuable and timely advancement in the field of Neural Architecture Search, particularly for applications like Super-Resolution where dynamic resource constraints are prevalent. Its \\\"adaptive-aware\\\" one-shot NAS approach addresses a critical gap: how to efficiently derive multiple optimal architectures from a single search, each tailored to different performance-resource trade-offs.\\n\\nThe novelty primarily lies in the *effective integration of \\\"awareness\\\" for varying deployment constraints into the one-shot paradigm*. While one-shot NAS and multi-objective optimization are not new individually, their coherent combination to serve *on-demand adaptive model generation* is a significant contribution. The efficiency gains from the one-shot training combined with the flexibility of adaptive sampling post-training make this approach highly practical and impactful.\\n\\nWhile it shares common weaknesses with other one-shot NAS methods (e.g., supernet training challenges), its specific focus on dynamically adapting to SR requirements and the potential to generate a Pareto front of models from a single training run make it a strong and innovative solution. The paper effectively tackles a real-world problem in deploying deep learning models in resource-constrained and dynamic environments.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.9671919\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"AgentNAS\",\n  \"prompt\": \"Evaluate the novelty and effectiveness of the adaptive-aware one-shot NAS approach in the DySR paper. Compare it to existing NAS techniques, focusing on the search space design, sampling strategy, and training process. Provide a detailed assessment of the strengths and weaknesses, and a recommendation (Accept/Reject) with justification.\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"As a Super-Resolution (SR) domain expert, I will evaluate the DySR paper based on its model architecture, performance metrics, visual quality, and comparative analysis.\\n\\n---\\n\\n### Evaluation of the DySR Model\\n\\n**Paper/Model Name:** DySR (Dynamic Super-Resolution) - *Assuming a hypothetical or a general \\\"DySR\\\" concept if a specific paper isn't linked, focusing on the common characteristics of dynamic SR models.* If you have a specific paper in mind, please provide the full title or link for a more precise review. For this assessment, I'll proceed based on the common approaches and goals of models named \\\"DySR\\\" or focusing on dynamic inference in SR.\\n\\n**Core Idea:** The primary objective of DySR models is to introduce *dynamic inference*, allowing a flexible trade-off between computational cost (FLOPs, parameters, latency) and reconstruction quality (PSNR/SSIM). This is crucial for real-world deployments where resource constraints vary, or the complexity of input images differs. Instead of a single fixed model, DySR aims to offer a spectrum of choices, ideally achieving competitive performance across this spectrum.\\n\\n#### 1. Model Architecture\\n\\n**Strengths:**\\n*   **Adaptive Computation:** The core strength lies in its ability to adapt computation based on input complexity or user-defined performance targets. This typically involves:\\n    *   **Multi-Branch/Multi-Exit Design:** Often, DySR models employ a backbone with multiple \\\"exit points\\\" or \\\"branches,\\\" each progressively refining the SR output. Simpler inputs can exit early with less computation, while complex ones proceed further.\\n    *   **Dynamic Decision Module:** A key component is a controller or decision network (e.g., a lightweight classifier, a reinforcement learning agent, or a gate mechanism) that decides when to terminate inference or which branch to activate for a given input. This module learns to correlate input features with the optimal computational path.\\n    *   **Feature Reusability:** A well-designed DySR reuses features from earlier layers for subsequent stages, minimizing redundant computations.\\n*   **Hierarchical Processing:** By having different levels of detail processing, DySR can inherently handle varying levels of detail in the input image more efficiently.\\n*   **Practical Applicability:** The architectural design is inherently practical, allowing deployment on devices ranging from mobile phones to high-end GPUs by simply adjusting the inference policy.\\n\\n**Weaknesses:**\\n*   **Training Complexity:** Training a dynamic model is typically more complex than training a static one. It often involves:\\n    *   **Multiple Loss Functions:** Optimizing for losses at multiple exit points simultaneously, often with weighted sums.\\n    *   **Policy Learning:** Training the dynamic decision module itself can be challenging, requiring careful design of reward functions (if RL is used) or effective regularization.\\n    *   **Balancing Act:** Ensuring that the early exit points are sufficiently capable while the later ones achieve SOTA quality requires careful architectural balancing.\\n*   **Overhead of Decision Module:** The dynamic decision module, however lightweight, introduces some computational and memory overhead. This overhead must be carefully minimized to ensure net gains in efficiency.\\n*   **Potential for Suboptimal Decisions:** The decision module might occasionally make suboptimal choices, leading to an early exit for a complex region or unnecessary computation for a simple one, impacting consistent quality.\\n\\n#### 2. Performance Evaluation (PSNR/SSIM and Computational Cost)\\n\\n**Strengths:**\\n*   **Excellent Pareto Front:** The most significant performance claim of DySR models is their ability to achieve a superior Pareto front (PSNR/SSIM vs. FLOPs/Params) compared to combining several static models. This means for a given computational budget, DySR offers higher quality, or for a given quality target, it requires less computation.\\n*   **Competitive at Extremes:**\\n    *   **High Efficiency:** At its lowest computational settings, DySR should be competitive with or even outperform dedicated lightweight SR models (e.g., FSRCNN, ESPCN) while maintaining better quality.\\n    *   **High Quality:** At its highest computational settings, it should achieve quality comparable to SOTA static models (e.g., RCAN, SwinIR, HAT), albeit possibly not surpassing them by a large margin as its core focus is flexibility.\\n*   **Demonstrated on Standard Benchmarks:** Performance is typically validated on widely accepted SR benchmarks (Set5, Set14, BSD100, Urban100, DIV2K validation), making comparisons robust.\\n\\n**Weaknesses:**\\n*   **Absolute SOTA Gap:** While competitive, it might not always achieve the absolute highest PSNR/SSIM scores when compared to the largest, most computationally expensive SOTA static models (which are designed purely for maximum quality without considering dynamic inference). This is an acceptable trade-off given its objective.\\n*   **Inconsistent Performance per Input:** While overall averages are good, there might be subtle inconsistencies in quality for *individual* images due to the dynamic decision-making process. For instance, one image might be deemed \\\"easy\\\" and processed lightly, leading to slightly lower quality than if it had been processed by the full model, even if visually it seemed complex.\\n\\n#### 3. Visual Quality Assessment\\n\\n**Strengths:**\\n*   **Sharpness and Detail Preservation:** For its target computational budget, DySR should produce sharp edges and preserve fine details, avoiding common artifacts like blurring, ringing, or aliasing.\\n*   **Texture Reconstruction:** It should adeptly reconstruct textures, preventing artificial patterns or overly smooth regions.\\n*   **Consistent Quality (Generally):** Across different exit points, the visual quality should degrade gracefully. At higher computational loads, it should be visually indistinguishable from high-quality static models.\\n*   **Adaptability to Content:** The dynamic nature *should* visually manifest as better resource allocation, meaning complex regions are processed more thoroughly than simpler, flat regions, leading to a more efficient and visually pleasing output overall.\\n\\n**Weaknesses:**\\n*   **Potential for Artifacts in Edge Cases:** If the dynamic decision module misjudges an image's complexity, it could lead to visible artifacts or sub-optimal reconstruction (e.g., slightly blurred textures or less sharp edges) in specific areas that should have received more computation.\\n*   **Difficulty in Direct Comparison:** Visually comparing dynamic models is challenging because their output quality is *conditional* on the chosen computation level. Side-by-side comparisons often require showing multiple outputs for the same input at different efficiency levels.\\n\\n#### 4. Comparison with Other SR Models\\n\\n**Strengths (against different categories):**\\n*   **Vs. Lightweight Static Models (e.g., FSRCNN, ESPCN, MobileSR):** DySR significantly outperforms these models in terms of quality for a comparable computational budget, or achieves similar quality with even less computation due to its adaptive nature.\\n*   **Vs. High-Quality Static Models (e.g., EDSR, RCAN, SwinIR, HAT):** While DySR might not always beat the absolute peak PSNR of these models when they run at full capacity, it offers a *flexibility* they lack. DySR can achieve very close to their performance at a fraction of their maximum computational cost, or match their performance while providing the option to scale down.\\n*   **Vs. Other Dynamic/Adaptive SR Models:** This is where DySR truly needs to differentiate itself. A strong DySR paper would demonstrate a better PSNR-FLOPs curve (i.e., higher PSNR for the same FLOPs, or lower FLOPs for the same PSNR) compared to existing adaptive SR methods (e.g., DSR, some adaptive FSRCNN variants, etc.). This superiority often comes from a more efficient backbone, a more robust dynamic decision mechanism, or a better training strategy.\\n\\n**Weaknesses:**\\n*   **Not a \\\"Panacea\\\":** DySR doesn't necessarily render all other SR models obsolete. For applications with *fixed* and *ample* computational resources where only the absolute best quality matters, a purpose-built, non-dynamic SOTA model might still slightly edge it out.\\n*   **Fair Comparison Challenges:** Presenting fair comparisons can be tricky. It's not just about one PSNR number, but a curve. Ensuring all models are run on the same hardware and with identical pre/post-processing is critical.\\n\\n---\\n\\n### Recommendation\\n\\n**Recommendation: Accept**\\n\\n**Justification:**\\n\\nThe concept behind DySR and its implementations represent a crucial advancement in the field of Super-Resolution, moving from purely quality-focused or purely efficiency-focused models to a more practical, adaptive paradigm.\\n\\n*   **Addresses a Critical Real-World Need:** The ability to dynamically adjust computational cost based on varying device capabilities, network bandwidth, or content complexity is paramount for widespread SR deployment in applications like real-time video streaming, mobile imaging, and resource-constrained embedded systems.\\n*   **Strong Pareto Optimality:** If the paper successfully demonstrates a superior PSNR-FLOPs Pareto front compared to existing methods, it signifies a significant contribution. This indicates that it's not just a \\\"jack of all trades\\\" but a highly optimized solution across a spectrum of performance points.\\n*   **Architectural Innovation:** The design of efficient multi-branch architectures and robust dynamic decision modules pushes the boundaries of SR model design beyond static, monolithic structures.\\n*   **Tangible Benefits:** The gains in efficiency (lower FLOPs, faster inference) for comparable quality, or higher quality for similar FLOPs, are tangible benefits that will resonate with practitioners and researchers alike.\\n\\nWhile there are inherent complexities in training and potential subtle inconsistencies, the overall value proposition of a dynamic SR model far outweighs these challenges. DySR represents a step forward in making SR models truly deployable and adaptable in diverse environments.\\n\\n**Caveats/Further Research Suggestions:**\\n*   **Robustness of Dynamic Policy:** Investigate the robustness of the dynamic decision module to out-of-distribution inputs.\\n*   **Real-time Latency Analysis:** Beyond FLOPs, a detailed analysis of actual inference latency on various hardware platforms would strengthen the efficiency claims.\\n*   **Extension to Video SR:** Applying dynamic inference to video SR, considering temporal consistency and adaptive frame processing, would be a valuable future direction.\\n*   **User Controllability:** Exploring ways for end-users or applications to easily specify their desired quality/speed trade-off in a fine-grained manner during deployment.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.9662841\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Evaluate the SR model architecture, performance, and the quality of the super-resolved images/videos in the DySR paper. Pay attention to the PSNR/SSIM metrics and visual results. Compare the performance with other SR models. Provide a detailed assessment of the strengths and weaknesses, and a recommendation (Accept/Reject) with justification.\",\n  \"agent_name\": \"AgentSRExpert\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': '## Evaluation of the DySR Paper: Dynamic Super-Resolution for Mobile Devices\\n\\nThis evaluation focuses on the system design, real-time performance, QoS claims, experimental setup on mobile devices, resource adaptivity, and model selection policy of the DySR paper.\\n\\n### 1. System Design\\n\\n**Strengths:**\\n*   **Clear Problem Statement:** The paper effectively identifies the challenge of deploying high-quality Super-Resolution (SR) models on resource-constrained mobile devices while maintaining real-time performance and quality of service (QoS).\\n*   **Modular Architecture:** The proposed DySR system typically features a modular design, separating the core SR inference from a dynamic decision engine, resource monitoring, and a repository of diverse SR models. This modularity allows for easier component updates and testing.\\n*   **Adaptive Nature:** The core idea of dynamically selecting an SR model based on real-time device resource availability is a sound and practical approach to address the variability in mobile performance.\\n*   **Feedback Loop:** A well-designed DySR system usually incorporates a feedback loop where resource utilization and output quality (e.g., frame rate) are monitored to inform subsequent model selection decisions.\\n\\n**Weaknesses:**\\n*   **Overhead of Adaptivity:** While dynamic adaptation is crucial, the paper needs to thoroughly analyze and quantify the overhead introduced by the decision engine itself, including:\\n    *   **Monitoring Overhead:** The continuous polling of CPU, GPU, memory, and potentially temperature sensors consumes resources. Is this overhead negligible compared to SR inference?\\n    *   **Switching Latency:** The time taken to unload one SR model and load/initialize another can introduce noticeable latency or frame drops. How is this managed or amortized? Is there a \"cold start\" penalty for models?\\n*   **Complexity of Model Management:** Managing multiple SR models (loading, unloading, memory footprint) can be complex, especially on devices with limited RAM. How are memory pressure and caching handled?\\n*   **Lack of Predictive Capability (Potential):** Most reactive systems struggle with sudden resource changes. Does the system anticipate future resource availability or is it purely reactive? A purely reactive system might lead to oscillations or late adaptations.\\n*   **User QoS Preference Integration:** While the system adapts to *device* resources, it\\'s often unclear how user-defined QoS preferences (e.g., \"prioritize visual quality even if it means lower FPS,\" or \"prioritize battery life\") are integrated into the decision-making process.\\n\\n### 2. Real-time Performance\\n\\n**Strengths:**\\n*   **Focus on Latency/Throughput:** The primary goal of achieving real-time SR demands a strong focus on processing latency and frames per second (FPS). The system aims to maintain a target FPS, which is a critical real-time metric.\\n*   **Demonstrated Improvement over Static:** Typically, DySR would show that its dynamic approach outperforms a static, single-model approach, which would either struggle to maintain FPS under load or provide unnecessarily low quality when resources are abundant.\\n\\n**Weaknesses:**\\n*   **Jitter and Smoothness:** While average FPS might be good, sudden model switches can introduce significant frame time jitter, leading to a choppy user experience. Is \"smoothness\" (e.g., variance in frame time) adequately measured and optimized?\\n*   **Worst-case Latency:** In real-time systems, worst-case latency is often more critical than average latency. What guarantees can the system provide regarding maximum processing delay, especially during adaptation phases?\\n*   **Impact of Thermal Throttling:** Mobile devices are highly susceptible to thermal throttling, which can drastically reduce sustained performance. Does the paper sufficiently account for this in its real-time performance evaluation, or are benchmarks conducted for short bursts?\\n*   **Energy Efficiency as a Real-time Constraint:** While often tied to performance, energy consumption is a distinct real-time constraint on mobile. A system might be fast but drain the battery quickly. Is energy consumption considered an active QoS parameter in the real-time feedback loop?\\n\\n### 3. QoS Claims\\n\\n**Strengths:**\\n*   **Explicit QoS Target:** The paper clearly states its intent to maintain QoS, typically defined by a combination of visual quality (e.g., PSNR, SSIM, LPIPS) and real-time performance (e.g., FPS).\\n*   **Trade-off Exploration:** By dynamically switching models, the system implicitly explores the trade-off space between visual quality and computational cost.\\n\\n**Weaknesses:**\\n*   **Incomplete QoS Definition:** While PSNR/SSIM and FPS are crucial, \"QoS\" for mobile SR should ideally encompass:\\n    *   **Energy Consumption:** A critical mobile constraint. Is battery drain monitored and optimized?\\n    *   **Thermal Performance:** Avoiding excessive heat buildup to prevent throttling and ensure user comfort.\\n    *   **Smoothness/Jitter:** As mentioned above, a stable frame rate is as important as a high average.\\n    *   **Perceptual Quality:** PSNR/SSIM are objective but don\\'t always align with human perception. Are subjective user studies or perceptual metrics (like LPIPS) included?\\n*   **Validation under Stress:** Do the QoS claims hold up under extreme conditions, such as high background load, low battery, or sustained usage leading to thermal throttling? Often, evaluations are done in pristine lab conditions.\\n*   **QoS Metric Prioritization:** If multiple QoS metrics are in conflict (e.g., highest quality vs. lowest energy), how does the system prioritize them? Is this configurable?\\n\\n### 4. Experimental Setup on Mobile Devices\\n\\n**Strengths:**\\n*   **Real Hardware Testing:** Conducting experiments on actual mobile devices (smartphones, tablets) is a significant strength, as it directly assesses the system\\'s performance in a realistic environment, avoiding the pitfalls of simulation or desktop-only evaluation.\\n*   **Device Specifications:** Providing detailed specifications of the mobile devices used (CPU, GPU, RAM, specific SoC model) is crucial for reproducibility and understanding the context of the results.\\n\\n**Weaknesses:**\\n*   **Limited Device Diversity:** Often, papers test on only one or two flagship devices. To claim generalizability, testing on a wider range of devices (e.g., different manufacturers, price points, SoC architectures like Snapdragon, MediaTek, Exynos) is essential. Different mobile GPUs (Adreno, Mali) have distinct performance characteristics for AI workloads.\\n*   **Measurement Accuracy and Reliability:**\\n    *   **Energy Measurement:** Measuring actual battery drain accurately on mobile devices requires specialized hardware (e.g., power meters) or reliable software APIs. Relying solely on `top` or Android\\'s battery stats might be insufficient or imprecise.\\n    *   **Resource Monitoring:** How are CPU/GPU utilization, memory usage, and temperature precisely captured without introducing significant measurement perturbation? Are system-level tools (e.g., `adb shell dumpsys cpuinfo`, GPU profiling tools) used appropriately?\\n    *   **Background Processes:** Mobile OSes run numerous background processes. Were experiments conducted in a controlled environment (e.g., \"airplane mode\" with minimal background apps) or a realistic one? If realistic, how was variability accounted for?\\n*   **Thermal Management:** How was thermal throttling observed and mitigated (or intentionally triggered for evaluation)? Without addressing this, sustained performance numbers can be misleading.\\n*   **Input Data Representativeness:** Are the video streams used for evaluation representative of real-world mobile use cases (e.g., varying scene complexity, motion, lighting conditions)?\\n\\n### 5. Resource Adaptivity\\n\\n**Strengths:**\\n*   **Dynamic Response to Resource Changes:** The core of DySR is its ability to react to fluctuating resource availability, which is inherent in mobile environments (e.g., foreground/background app switching, network activity, thermal effects).\\n*   **Granular Resource Monitoring:** The system monitors key resources like CPU, GPU, and memory, providing the necessary input for adaptive decisions.\\n\\n**Weaknesses:**\\n*   **Adaptation Granularity and Responsiveness:** How quickly does the system react to resource changes? Is the adaptation instantaneous, or is there a noticeable delay? What\\'s the smallest time quantum for adaptation?\\n*   **Oscillation/Thrashing:** A common problem in adaptive systems is \"thrashing,\" where the system rapidly switches between models due to minor fluctuations around a threshold, leading to instability and increased overhead. Is there a hysteresis mechanism or smoothing applied to avoid this?\\n*   **Threshold Determination:** How are the thresholds for triggering model switches determined? Are they static, empirically derived, or dynamically learned? Static thresholds might not generalize well across devices or conditions.\\n*   **Lack of Proactive Adaptation:** As mentioned under System Design, purely reactive adaptation might be suboptimal. Can the system use historical data or simpler predictive models to anticipate resource constraints and adapt proactively?\\n\\n### 6. Model Selection Policy\\n\\n**Strengths:**\\n*   **Portfolio of Models:** Having a diverse set of pre-trained SR models with different quality-performance trade-offs is fundamental. This allows the system to operate across a wide range of resource conditions.\\n*   **Profiling-based Selection:** The policy likely relies on pre-profiling each SR model on the target hardware to understand its performance (latency, energy) and quality characteristics under various loads. This is a sound approach.\\n\\n**Weaknesses:**\\n*   **Cost Function Complexity:** Is the selection policy based on a simple heuristic (e.g., \"if FPS drops below X, switch to lower model\") or a more sophisticated cost function that balances multiple QoS metrics? A more advanced policy would consider battery, temperature, and visual quality concurrently.\\n*   **Model Transition Smoothness:** How are transitions between different SR models handled to ensure visual continuity? A sudden switch between models with different characteristics might be visually jarring. Is there a cross-fade or blend?\\n*   **Model Profile Accuracy:** Are the profiles of each SR model accurate across different operating conditions (e.g., cold start vs. warm, different thermal states)?\\n*   **Dynamic Reprofiling:** Mobile environments are dynamic. Do model profiles need to be updated online, or are offline profiles sufficient? What if a new background app is installed that significantly impacts baseline performance?\\n*   **Scalability of Model Set:** As the number of available SR models grows, the profiling and selection process can become more complex.\\n*   **Lack of Learning:** Is there any learning component (e.g., reinforcement learning) to refine the model selection policy over time based on observed performance and user feedback?\\n\\n---\\n\\n### Recommendation\\n\\n**Conditional Acceptance / Accept with Major Revisions**\\n\\n**Justification:**\\n\\nThe DySR paper tackles a highly relevant and challenging problem: enabling high-quality real-time Super-Resolution on diverse and resource-constrained mobile devices. The core idea of dynamic model selection based on resource availability is scientifically sound and has significant practical implications.\\n\\nHowever, the major concerns revolve around the **rigor of the experimental setup on mobile devices** and the **comprehensiveness of the QoS validation**. Mobile environments are notoriously complex and noisy. Without a robust and transparent methodology for measuring CPU/GPU utilization, power consumption, and thermal behavior, the real-time performance and QoS claims remain partially unsubstantiated.\\n\\n**Key areas requiring major revisions for acceptance include:**\\n\\n1.  **Enhanced Experimental Rigor:**\\n    *   **Detailed Power Analysis:** Provide precise battery consumption measurements using specialized tools or validated software APIs. Show energy-performance trade-offs.\\n    *   **Thermal Awareness:** Explicitly address thermal throttling. Show performance under sustained load, not just short bursts. Perhaps include temperature monitoring graphs.\\n    *   **Broader Device Testing:** Evaluate performance on a wider range of mobile SoC architectures and price points to demonstrate generalizability.\\n    *   **Background Process Control:** Clearly state the experimental conditions (e.g., minimal background apps vs. realistic scenarios with other apps running). Quantify the impact of background noise.\\n    *   **Jitter/Smoothness Metrics:** Beyond average FPS, provide data on frame time variability to ensure a smooth user experience.\\n\\n2.  **Comprehensive QoS Validation:**\\n    *   **Beyond PSNR/SSIM and FPS:** Incorporate energy efficiency, thermal stability, and potentially perceptual quality metrics (e.g., LPIPS, or ideally, user studies).\\n    *   **QoS under Stress:** Validate claims under various challenging scenarios (e.g., low battery, high ambient temperature, intense background activity).\\n\\n3.  **Detailed Analysis of Adaptivity Overheads:**\\n    *   **Quantify Monitoring & Switching Costs:** Provide precise measurements of the overhead introduced by resource monitoring and, critically, by the process of switching between SR models (loading/unloading, initialization time, memory implications).\\n    *   **Thrashing Mitigation:** Clearly explain and demonstrate how the system prevents or mitigates oscillation/thrashing between models.\\n\\n4.  **Refined Model Selection Policy:**\\n    *   **Clarity on Thresholds/Logic:** Detail how thresholds are determined and if they adapt. Explain the multi-objective optimization (if any) for selecting a model considering quality, performance, and energy.\\n    *   **Transition Management:** Discuss how visual smoothness is maintained during model transitions.\\n\\nIf these weaknesses are addressed with rigorous empirical evidence and detailed analysis, the paper has the potential to be a significant contribution to the field of mobile AI and real-time computer vision.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.96809835}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The DySR paper proposes an \"adaptive-aware one-shot NAS approach\" for Dynamic Super-Resolution. This evaluation will assess its novelty and effectiveness by focusing on its search space design, sampling strategy, and training process, comparing it to existing NAS techniques.\\n\\n## Evaluation of DySR\\'s Adaptive-Aware One-Shot NAS Approach\\n\\n### 1. Novelty\\n\\nThe primary novelty of DySR\\'s approach lies in its **\"adaptive-aware\" capability within a one-shot NAS framework**, particularly tailored for the diverse and dynamic requirements of Super-Resolution (SR) tasks. While one-shot NAS is not new, the explicit integration of \"awareness\" for varying resource constraints (e.g., different upsampling factors, latency budgets, model sizes) *during the search and potentially training phases* is a significant step.\\n\\n*   **Existing One-Shot NAS:** Methods like SPOS (Single Path One-Shot) or ProxylessNAS focus on finding *one* optimal architecture (or a few) for a *single* set of constraints (e.g., best accuracy under a fixed latency).\\n*   **DySR\\'s Novelty:** DySR aims to train a single supernet from which multiple highly efficient sub-networks can be derived, each optimally suited for *different, dynamically changing resource constraints*. This moves beyond finding a single Pareto-optimal point to enabling the *efficient derivation of a Pareto frontier* from a single supernet training run. This is crucial for real-world SR applications where requirements can change on the fly (e.g., high-quality offline vs. real-time mobile inference).\\n\\n### 2. Effectiveness\\n\\nThe effectiveness of DySR is demonstrated by its ability to:\\n1.  **Generate a spectrum of performant SR models:** It claims to find models that achieve competitive PSNR/SSIM while adhering to diverse latency and parameter budgets.\\n2.  **Reduce search costs:** Leveraging the one-shot paradigm significantly reduces the computational burden compared to traditional NAS methods like RL or evolutionary algorithms.\\n3.  **Adapt to dynamic requirements:** Its core promise is to provide on-demand sub-networks for varying upsampling scales and efficiency demands, which is highly effective for deployment scenarios.\\n\\n### 3. Detailed Analysis\\n\\n#### A. Search Space Design\\n\\n*   **Description:** The search space for DySR is typically composed of a directed acyclic graph (DAG) of potential operations and connections, forming a \"supernet.\" For SR, this would include common SR-specific operations like sub-pixel convolutions, various upsampling layers, residual blocks, attention modules, and different types of convolutions (e.g., depthwise separable, standard). The key aspect is that it must be rich enough to contain diverse high-performing architectures but constrained enough to be learnable.\\n*   **Comparison to Existing NAS:**\\n    *   **Strengths (relative to general NAS):** Likely incorporates operations and architectures proven effective for SR, making the search more focused and efficient. The design allows for flexible connectivity, enabling complex architectures.\\n    *   **Weaknesses:** The expressiveness of the supernet can be a double-edged sword. A very large supernet might lead to \"supernet collapse\" (where specific paths are not sufficiently trained) or \"unfair weight sharing\" (where some operations are more privileged than others due to sampling frequency). The paper needs to explicitly state how this search space is specifically designed to accommodate the \"adaptive\" aspect (e.g., including multiple upsampling pathways, different block complexities). If it\\'s a generic SR search space, the \"adaptive-aware\" part primarily comes from sampling/training.\\n\\n#### B. Sampling Strategy\\n\\n*   **Description:** This is where the \"adaptive-aware\" mechanism is most prominent. During *supernet training*, the sampling strategy likely involves techniques to ensure the supernet weights are robust across a diverse set of subnetworks, potentially sampling paths under *varying resource constraints* or *different upsampling factors*. After the supernet is trained, the \"adaptive-aware\" part comes into full play: a search algorithm (e.g., evolutionary algorithm, greedy search, or a learned controller) navigates the *already trained* supernet to find specific sub-networks that precisely meet the dynamic resource requirements (e.g., find the best quality model that fits under 10ms latency for 4x upscaling).\\n*   **Comparison to Existing NAS:**\\n    *   **Strengths:**\\n        *   **Efficiency:** After supernet training, finding new sub-networks is extremely fast, often just a graph traversal and lookup, avoiding costly retraining or re-evaluations.\\n        *   **Multi-objective Optimization:** Enables the discovery of Pareto-optimal SR models for different latency/quality trade-offs from a *single* trained supernet. This is a significant advantage over methods that search for one optimal model at a time.\\n        *   **On-demand Adaptation:** Directly addresses the dynamic nature of SR deployment, where requirements can change.\\n    *   **Weaknesses:**\\n        *   **Supernet Robustness:** The challenge is to make the supernet weights generalizable enough so that *any* sampled path, even those rarely explored during training, performs well. If the sampling during training isn\\'t sufficiently \"adaptive-aware\" (i.e., doesn\\'t cover the full range of target constraints), the derived subnets might suffer.\\n        *   **Search Algorithm Post-training:** The quality of the final sub-networks heavily depends on the effectiveness of the *post-training search algorithm* (e.g., EA for latency-constrained search). While fast, this search still needs to be efficient and find good trade-offs.\\n\\n#### C. Training Process\\n\\n*   **Description:** The supernet is trained using a standard one-shot training methodology. This involves training the supernet weights such that any path sampled from it performs well. The \"adaptive-aware\" aspect here likely manifests in:\\n    1.  **Diverse Path Sampling:** During training, the supernet might not just sample random paths, but paths that represent various resource constraints or upsampling factors.\\n    2.  **Multi-objective Loss:** Potentially, the loss function could implicitly or explicitly encourage the supernet to learn weights robust to different operational points. For example, by regularizing the loss based on path complexity or having multiple heads for different upscaling factors.\\n    3.  **Progressive Training:** Often, one-shot methods employ progressive training (e.g., gradually increasing the search space complexity or path diversity) to stabilize supernet training.\\n*   **Comparison to Existing NAS:**\\n    *   **Strengths:**\\n        *   **Computational Efficiency:** Training one large supernet is vastly more efficient than training thousands of individual models (RL/EA NAS) or performing differentiable searches that can be unstable (DARTS).\\n        *   **Weight Sharing:** The core of one-shot NAS, enabling parameter efficiency during the search phase.\\n        *   **Tailored for SR:** The loss function would be optimized for SR metrics (e.g., L1/L2, perceptual loss, SSIM).\\n    *   **Weaknesses:**\\n        *   **Supernet Optimization Challenge:** Training a robust supernet is notoriously difficult. Issues like \"collapse\" (where the supernet performs well, but sampled subnets don\\'t), \"fairness\" of weight sharing, and difficulty in generalizing to unseen paths are common.\\n        *   **Training Stability:** Large supernets can be unstable to train, requiring careful hyperparameter tuning and regularization.\\n        *   **\"Adaptive-Awareness\" Integration:** The critical aspect is *how* the \"adaptive-aware\" part is woven into the training. If it\\'s just standard one-shot training and then adaptive *sampling*, the novelty is mostly in the sampling. If the *training itself* is made aware (e.g., by dynamically adjusting path probabilities based on target constraints), then it\\'s a stronger contribution.\\n\\n### 4. Strengths & Weaknesses\\n\\n**Strengths:**\\n*   **Efficiency:** Leverages the one-shot paradigm for significant computational savings in the search process compared to RL/EA NAS.\\n*   **Adaptability for Dynamic Requirements:** Crucially enables the efficient discovery of multiple high-performing SR models tailored to varying upsampling factors, latency budgets, and model sizes from a single trained supernet. This is a highly practical capability for real-world deployment.\\n*   **Practicality for SR:** Addresses a key challenge in SR deployment where resource constraints are often fluid.\\n*   **Strong Performance:** Likely achieves competitive or superior performance to hand-crafted models and other NAS methods within its targeted efficiency envelopes.\\n\\n**Weaknesses:**\\n*   **Supernet Training Difficulty:** Like all one-shot NAS methods, training a robust and generalizable supernet can be challenging, prone to \"supernet collapse\" or sub-optimal performance for certain sampled paths.\\n*   **\"Fairness\" of Weight Sharing:** Ensuring that all operations and connections within the supernet are equally well-trained is difficult, potentially leading to bias towards frequently sampled paths.\\n*   **Search Space Limitation:** While tailored for SR, the chosen search space inherently limits the architectures that can be found. Novel, truly out-of-distribution architectures might be missed.\\n*   **Post-training Search Dependency:** While fast, the effectiveness of finding truly optimal adaptive models relies on an efficient and effective post-training search algorithm (e.g., evolutionary search).\\n*   **Generalizability Beyond SR:** The \"adaptive-aware\" mechanisms, while powerful for SR, might need significant modification for other computer vision tasks if their constraints are fundamentally different.\\n\\n### 5. Recommendation\\n\\n**Recommendation: Accept**\\n\\n**Justification:**\\n\\nThe DySR paper proposes a valuable and timely advancement in the field of Neural Architecture Search, particularly for applications like Super-Resolution where dynamic resource constraints are prevalent. Its \"adaptive-aware\" one-shot NAS approach addresses a critical gap: how to efficiently derive multiple optimal architectures from a single search, each tailored to different performance-resource trade-offs.\\n\\nThe novelty primarily lies in the *effective integration of \"awareness\" for varying deployment constraints into the one-shot paradigm*. While one-shot NAS and multi-objective optimization are not new individually, their coherent combination to serve *on-demand adaptive model generation* is a significant contribution. The efficiency gains from the one-shot training combined with the flexibility of adaptive sampling post-training make this approach highly practical and impactful.\\n\\nWhile it shares common weaknesses with other one-shot NAS methods (e.g., supernet training challenges), its specific focus on dynamically adapting to SR requirements and the potential to generate a Pareto front of models from a single training run make it a strong and innovative solution. The paper effectively tackles a real-world problem in deploying deep learning models in resource-constrained and dynamic environments.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.9671919}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'As a Super-Resolution (SR) domain expert, I will evaluate the DySR paper based on its model architecture, performance metrics, visual quality, and comparative analysis.\\n\\n---\\n\\n### Evaluation of the DySR Model\\n\\n**Paper/Model Name:** DySR (Dynamic Super-Resolution) - *Assuming a hypothetical or a general \"DySR\" concept if a specific paper isn\\'t linked, focusing on the common characteristics of dynamic SR models.* If you have a specific paper in mind, please provide the full title or link for a more precise review. For this assessment, I\\'ll proceed based on the common approaches and goals of models named \"DySR\" or focusing on dynamic inference in SR.\\n\\n**Core Idea:** The primary objective of DySR models is to introduce *dynamic inference*, allowing a flexible trade-off between computational cost (FLOPs, parameters, latency) and reconstruction quality (PSNR/SSIM). This is crucial for real-world deployments where resource constraints vary, or the complexity of input images differs. Instead of a single fixed model, DySR aims to offer a spectrum of choices, ideally achieving competitive performance across this spectrum.\\n\\n#### 1. Model Architecture\\n\\n**Strengths:**\\n*   **Adaptive Computation:** The core strength lies in its ability to adapt computation based on input complexity or user-defined performance targets. This typically involves:\\n    *   **Multi-Branch/Multi-Exit Design:** Often, DySR models employ a backbone with multiple \"exit points\" or \"branches,\" each progressively refining the SR output. Simpler inputs can exit early with less computation, while complex ones proceed further.\\n    *   **Dynamic Decision Module:** A key component is a controller or decision network (e.g., a lightweight classifier, a reinforcement learning agent, or a gate mechanism) that decides when to terminate inference or which branch to activate for a given input. This module learns to correlate input features with the optimal computational path.\\n    *   **Feature Reusability:** A well-designed DySR reuses features from earlier layers for subsequent stages, minimizing redundant computations.\\n*   **Hierarchical Processing:** By having different levels of detail processing, DySR can inherently handle varying levels of detail in the input image more efficiently.\\n*   **Practical Applicability:** The architectural design is inherently practical, allowing deployment on devices ranging from mobile phones to high-end GPUs by simply adjusting the inference policy.\\n\\n**Weaknesses:**\\n*   **Training Complexity:** Training a dynamic model is typically more complex than training a static one. It often involves:\\n    *   **Multiple Loss Functions:** Optimizing for losses at multiple exit points simultaneously, often with weighted sums.\\n    *   **Policy Learning:** Training the dynamic decision module itself can be challenging, requiring careful design of reward functions (if RL is used) or effective regularization.\\n    *   **Balancing Act:** Ensuring that the early exit points are sufficiently capable while the later ones achieve SOTA quality requires careful architectural balancing.\\n*   **Overhead of Decision Module:** The dynamic decision module, however lightweight, introduces some computational and memory overhead. This overhead must be carefully minimized to ensure net gains in efficiency.\\n*   **Potential for Suboptimal Decisions:** The decision module might occasionally make suboptimal choices, leading to an early exit for a complex region or unnecessary computation for a simple one, impacting consistent quality.\\n\\n#### 2. Performance Evaluation (PSNR/SSIM and Computational Cost)\\n\\n**Strengths:**\\n*   **Excellent Pareto Front:** The most significant performance claim of DySR models is their ability to achieve a superior Pareto front (PSNR/SSIM vs. FLOPs/Params) compared to combining several static models. This means for a given computational budget, DySR offers higher quality, or for a given quality target, it requires less computation.\\n*   **Competitive at Extremes:**\\n    *   **High Efficiency:** At its lowest computational settings, DySR should be competitive with or even outperform dedicated lightweight SR models (e.g., FSRCNN, ESPCN) while maintaining better quality.\\n    *   **High Quality:** At its highest computational settings, it should achieve quality comparable to SOTA static models (e.g., RCAN, SwinIR, HAT), albeit possibly not surpassing them by a large margin as its core focus is flexibility.\\n*   **Demonstrated on Standard Benchmarks:** Performance is typically validated on widely accepted SR benchmarks (Set5, Set14, BSD100, Urban100, DIV2K validation), making comparisons robust.\\n\\n**Weaknesses:**\\n*   **Absolute SOTA Gap:** While competitive, it might not always achieve the absolute highest PSNR/SSIM scores when compared to the largest, most computationally expensive SOTA static models (which are designed purely for maximum quality without considering dynamic inference). This is an acceptable trade-off given its objective.\\n*   **Inconsistent Performance per Input:** While overall averages are good, there might be subtle inconsistencies in quality for *individual* images due to the dynamic decision-making process. For instance, one image might be deemed \"easy\" and processed lightly, leading to slightly lower quality than if it had been processed by the full model, even if visually it seemed complex.\\n\\n#### 3. Visual Quality Assessment\\n\\n**Strengths:**\\n*   **Sharpness and Detail Preservation:** For its target computational budget, DySR should produce sharp edges and preserve fine details, avoiding common artifacts like blurring, ringing, or aliasing.\\n*   **Texture Reconstruction:** It should adeptly reconstruct textures, preventing artificial patterns or overly smooth regions.\\n*   **Consistent Quality (Generally):** Across different exit points, the visual quality should degrade gracefully. At higher computational loads, it should be visually indistinguishable from high-quality static models.\\n*   **Adaptability to Content:** The dynamic nature *should* visually manifest as better resource allocation, meaning complex regions are processed more thoroughly than simpler, flat regions, leading to a more efficient and visually pleasing output overall.\\n\\n**Weaknesses:**\\n*   **Potential for Artifacts in Edge Cases:** If the dynamic decision module misjudges an image\\'s complexity, it could lead to visible artifacts or sub-optimal reconstruction (e.g., slightly blurred textures or less sharp edges) in specific areas that should have received more computation.\\n*   **Difficulty in Direct Comparison:** Visually comparing dynamic models is challenging because their output quality is *conditional* on the chosen computation level. Side-by-side comparisons often require showing multiple outputs for the same input at different efficiency levels.\\n\\n#### 4. Comparison with Other SR Models\\n\\n**Strengths (against different categories):**\\n*   **Vs. Lightweight Static Models (e.g., FSRCNN, ESPCN, MobileSR):** DySR significantly outperforms these models in terms of quality for a comparable computational budget, or achieves similar quality with even less computation due to its adaptive nature.\\n*   **Vs. High-Quality Static Models (e.g., EDSR, RCAN, SwinIR, HAT):** While DySR might not always beat the absolute peak PSNR of these models when they run at full capacity, it offers a *flexibility* they lack. DySR can achieve very close to their performance at a fraction of their maximum computational cost, or match their performance while providing the option to scale down.\\n*   **Vs. Other Dynamic/Adaptive SR Models:** This is where DySR truly needs to differentiate itself. A strong DySR paper would demonstrate a better PSNR-FLOPs curve (i.e., higher PSNR for the same FLOPs, or lower FLOPs for the same PSNR) compared to existing adaptive SR methods (e.g., DSR, some adaptive FSRCNN variants, etc.). This superiority often comes from a more efficient backbone, a more robust dynamic decision mechanism, or a better training strategy.\\n\\n**Weaknesses:**\\n*   **Not a \"Panacea\":** DySR doesn\\'t necessarily render all other SR models obsolete. For applications with *fixed* and *ample* computational resources where only the absolute best quality matters, a purpose-built, non-dynamic SOTA model might still slightly edge it out.\\n*   **Fair Comparison Challenges:** Presenting fair comparisons can be tricky. It\\'s not just about one PSNR number, but a curve. Ensuring all models are run on the same hardware and with identical pre/post-processing is critical.\\n\\n---\\n\\n### Recommendation\\n\\n**Recommendation: Accept**\\n\\n**Justification:**\\n\\nThe concept behind DySR and its implementations represent a crucial advancement in the field of Super-Resolution, moving from purely quality-focused or purely efficiency-focused models to a more practical, adaptive paradigm.\\n\\n*   **Addresses a Critical Real-World Need:** The ability to dynamically adjust computational cost based on varying device capabilities, network bandwidth, or content complexity is paramount for widespread SR deployment in applications like real-time video streaming, mobile imaging, and resource-constrained embedded systems.\\n*   **Strong Pareto Optimality:** If the paper successfully demonstrates a superior PSNR-FLOPs Pareto front compared to existing methods, it signifies a significant contribution. This indicates that it\\'s not just a \"jack of all trades\" but a highly optimized solution across a spectrum of performance points.\\n*   **Architectural Innovation:** The design of efficient multi-branch architectures and robust dynamic decision modules pushes the boundaries of SR model design beyond static, monolithic structures.\\n*   **Tangible Benefits:** The gains in efficiency (lower FLOPs, faster inference) for comparable quality, or higher quality for similar FLOPs, are tangible benefits that will resonate with practitioners and researchers alike.\\n\\nWhile there are inherent complexities in training and potential subtle inconsistencies, the overall value proposition of a dynamic SR model far outweighs these challenges. DySR represents a step forward in making SR models truly deployable and adaptable in diverse environments.\\n\\n**Caveats/Further Research Suggestions:**\\n*   **Robustness of Dynamic Policy:** Investigate the robustness of the dynamic decision module to out-of-distribution inputs.\\n*   **Real-time Latency Analysis:** Beyond FLOPs, a detailed analysis of actual inference latency on various hardware platforms would strengthen the efficiency claims.\\n*   **Extension to Video SR:** Applying dynamic inference to video SR, considering temporal consistency and adaptive frame processing, would be a valuable future direction.\\n*   **User Controllability:** Exploring ways for end-users or applications to easily specify their desired quality/speed trade-off in a fine-grained manner during deployment.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.9662841}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "**Step 5: Collaborative Review Discussion and Consensus Building**\n\nThe agents have provided their individual assessments. Now, I will analyze their feedback and identify areas of agreement and disagreement.\n\n*   **AgentMobileQoS:** Recommends \"Accept with Major Revisions,\" highlighting concerns about the rigor of the experimental setup on mobile devices and the comprehensiveness of the QoS validation.\n*   **AgentNAS:** Recommends \"Accept,\" emphasizing the novelty and effectiveness of the adaptive-aware one-shot NAS approach.\n*   **AgentSRExpert:** Recommends \"Accept,\" highlighting the practical applicability and the potential for excellent Pareto optimality.\n\nThe agents generally agree that the paper presents a valuable contribution. However, AgentMobileQoS raises significant concerns about the experimental validation, particularly on mobile devices. This concern needs to be addressed.\n\nI will synthesize the reviews, giving more weight to the concerns raised by AgentMobileQoS due to the importance of real-world validation in this domain.\n\n**Step 6: Methodology Memory Management**\n\n*   Store the successful agent specialization pattern for this paper (Mobile Systems Expert, NAS Specialist, SR Expert).\n*   Store the importance of rigorous experimental validation on real devices for system-focused papers.\n*   Store the need to address QoS comprehensively, including energy consumption and thermal performance.\n\n**Step 7: Comprehensive Review Synthesis and Balanced Recommendation**\n\nThe DySR paper presents a novel and promising approach for adaptive super-resolution on mobile devices. The use of an adaptive-aware one-shot NAS to generate Pareto-optimal models is a significant contribution. The paper addresses a critical need for QoS in mobile streaming services.\n\nHowever, the experimental validation, particularly on mobile devices, needs to be significantly strengthened. The paper should provide more detailed information on power consumption, thermal management, and performance under realistic workloads. The QoS metrics should be more comprehensive, including energy efficiency and thermal stability.\n\n**FINAL DECISION: Reject**\n\n**Justification:**\n\nWhile the proposed approach is novel and promising, the lack of rigorous experimental validation on mobile devices and the incomplete QoS evaluation are significant weaknesses. The paper has the potential to be a valuable contribution, but it requires major revisions to address these concerns. The current version does not provide sufficient evidence to support its claims.\n"}], "ground_truth": "Accept: poster", "response_history": "**Step 5: Collaborative Review Discussion and Consensus Building**\n\nThe agents have provided their individual assessments. Now, I will analyze their feedback and identify areas of agreement and disagreement.\n\n*   **AgentMobileQoS:** Recommends \"Accept with Major Revisions,\" highlighting concerns about the rigor of the experimental setup on mobile devices and the comprehensiveness of the QoS validation.\n*   **AgentNAS:** Recommends \"Accept,\" emphasizing the novelty and effectiveness of the adaptive-aware one-shot NAS approach.\n*   **AgentSRExpert:** Recommends \"Accept,\" highlighting the practical applicability and the potential for excellent Pareto optimality.\n\nThe agents generally agree that the paper presents a valuable contribution. However, AgentMobileQoS raises significant concerns about the experimental validation, particularly on mobile devices. This concern needs to be addressed.\n\nI will synthesize the reviews, giving more weight to the concerns raised by AgentMobileQoS due to the importance of real-world validation in this domain.\n\n**Step 6: Methodology Memory Management**\n\n*   Store the successful agent specialization pattern for this paper (Mobile Systems Expert, NAS Specialist, SR Expert).\n*   Store the importance of rigorous experimental validation on real devices for system-focused papers.\n*   Store the need to address QoS comprehensively, including energy consumption and thermal performance.\n\n**Step 7: Comprehensive Review Synthesis and Balanced Recommendation**\n\nThe DySR paper presents a novel and promising approach for adaptive super-resolution on mobile devices. The use of an adaptive-aware one-shot NAS to generate Pareto-optimal models is a significant contribution. The paper addresses a critical need for QoS in mobile streaming services.\n\nHowever, the experimental validation, particularly on mobile devices, needs to be significantly strengthened. The paper should provide more detailed information on power consumption, thermal management, and performance under realistic workloads. The QoS metrics should be more comprehensive, including energy efficiency and thermal stability.\n\n**FINAL DECISION: Reject**\n\n**Justification:**\n\nWhile the proposed approach is novel and promising, the lack of rigorous experimental validation on mobile devices and the incomplete QoS evaluation are significant weaknesses. The paper has the potential to be a valuable contribution, but it requires major revisions to address these concerns. The current version does not provide sufficient evidence to support its claims.\n", "elapsed_time": 117.12195611000061, "reviewer_agents": {"AgentMobileQoS": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in mobile systems and QoS, focusing on real-time performance and system aspects.", "system_prompt": "You are an expert in mobile systems and Quality of Service (QoS). You will evaluate the paper's system design, real-time performance, and the validity of the QoS claims, including the experimental setup on mobile devices. Pay close attention to the resource adaptivity and model selection policy.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "AgentNAS": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Specialist in Neural Architecture Search, evaluating the novelty and effectiveness of the proposed adaptive-aware one-shot NAS approach.", "system_prompt": "You are a specialist in Neural Architecture Search (NAS). You will evaluate the novelty and effectiveness of the proposed adaptive-aware one-shot NAS approach, comparing it to existing NAS techniques. Focus on the search space design, sampling strategy, and training process.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "AgentSRExpert": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Super-Resolution domain expert, assessing the SR model architecture, performance, and the quality of the super-resolved images/videos.", "system_prompt": "You are a Super-Resolution (SR) domain expert. You will assess the SR model architecture, its performance compared to other SR models, and the quality of the super-resolved images/videos. Pay attention to the PSNR/SSIM metrics and visual results.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}}}
