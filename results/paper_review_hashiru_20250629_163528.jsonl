{"paper_id": "xQXHkKI_7TG", "prompt": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Local Coefficient Optimization in Federated Learning\n\n1 INTRODUCTION: Federated learning emerges as a promising approach to build a large-scale cooperative learning system among multiple clients without sharing their raw data. However, due to non-i.i.d data distributed on different local clients, directly defining the global objective as the average of the local loss will not give the proper objective function for the facing task. Mohri et al. (2019) propose the Agnostic Federated Learning where the proposed algorithm guarantee the worst performance for each local task. However, in a different situation, we may face different tasks. It is not enough for us to solve each local task well, but to solve any possible tasks. However, it is impossible to give an objective function that works well for all different potential tasks without any prior knowledge. Thus, by collecting a small validation set as the knowledge of the facing task, we want to solve the following problem: min x f0(w \u2217(x)) s.t. w\u2217(x) \u2208 argmin w N\u2211 i=1 x(i)fi(w) x \u2208 X = {x|x \u2265 0, \u2225x\u22251 = 1}, (1) where f0 corresponds to the validation error which can be sensed in the global server, and fi is the local objective function corresponding to the ith local client. Once problem 1 is solved in real-world applications, the optimal x\u2217 can be used to determine the contribution degree of each local worker. This can help fairly distribute the benefits of federated learning to each local worker. To solve the bi-level optimization problem (1), under some mild conditions, Kolstad & Lasdon (1990) propose and algorithm that calculate gradient of x directly, where the gradient calculate is based on implicit function theorem and the gradient will involved the inverse of Hessian matrix of lower optimization objective function. But, due to the large parameter dimension of w, it is impossible to take the inverse of the Hessian or solve the linear system related to the Hessian. Meanwhile, due to a large amount of data in the local device, it is hard to directly estimate the gradient or the Hessian of the local function fi. Only stochastic gradient and stochastic hessian can be accessed. Thus, Ghadimi & Wang (2018) propose BSA algorithm where the inverse of Hessian is approximated by a series of the power of Hessian. Khanduri et al. (2021) propose SUSTAIN algorithm for solving stochastic bi-level optimization problems with smaller sample complexity. Similar to Ghadimi & Wang (2018), they need an extra loop of the Hessian-vector product to approximate the product of the Hessian inverse with some vector. However, in the federated learning setting, each local device may not be online during the optimization process, making it difficult to estimate the Hessian inverse. Fortunately, with the assumption that the lower-level functions have some smoothness property (e.g., L2 Lipschitz Hessian, which has been assumed in many bi-level analyses (e.g. Ghadimi & Wang (2018))), we can estimate the inverse of Hessian based on the previous Hessian estimation. Thus, we can get a more accurate estimation than those estimated from the scratch. Specifically, Ghadimi & Wang (2018) use series of matrix (using \u2211K k=0(I \u2212 \u03b7H)k to approximate 1 \u03b7H \u22121 with certain \u03b7). For different w, to approximate H\u22121 with series, we need to go through the entire series from the beginning, which is hard to utilize the Lipschitz smooth of H with respect to w. Thus, we introduce a new variable \u03bb to keep track of the information of the inversion of Hessian matrix . To connect Hessian of lower-level optimization problems and the new variable \u03bb, under strongly convex assumption for each fi, we rewrite the constraints to a non-linear equations constraints (\u2207 \u2211N i=1 x (i)fi(w) = 0), and then introduce the Lagrange function with new constraints. From the KKT conditions, it holds that the optimal solution of \u03bb is enough for us to compute the gradient of x instead of estimate the inverse of Hessian matrix. To solve a constrained optimization with non-linear equality constraints, adding the norm square of the equality constraint as an augmented term may not give the convexity to the augmented Lagrange function, which may not help the min-max optimization algorithm find the stationary point. Instead, as the function fi\u2019s are assumed to be strongly convex, adding function fi\u2019s as the augmented term can help introduce convexity and it will not change the stationary point of the min-max problem. Based on this new augmented Lagrange function, we prove that with stochastic gradient descent and ascent, w and \u03bb can converge to the KKT point. Meanwhile, by the implicit function theorem, when w and \u03bb are closed to the stationary point of min-max, the bias of estimating the gradient of x can be reduced to 0. Thus, with the primal-dual algorithm on w and \u03bb and stochastic projected gradient descent on x, we show the convergence of our algorithm. Finally, we compare our algorithm with other algorithms on a toy example and real datasets (MNIST and F-MNIST with Network LeNet-5). The experimental results show that the proposed algorithm can perform well in strongly convex cases and even in some non-convex cases (Neural Networks). We summarize our contributions as follows: \u2022 In Federated Learning, we formulate the local coefficient learning problem as a bi-level optimization problem, which gives a way to identify the dataset quality in each local client for some specific task (where a small validation set is given). \u2022 In bi-level optimization, we introduce a primal-dual framework and show the convergence of the whole algorithm in the constrained and stochastic setting. \u2022 For some specific optimization problems with non-linear constraints, we give a new augmented term. With the new augmented term, the primal variable and dual variable can converge to the KKT point of the original problems. 2 RELATED WORK:  2.1 PERSONALIZED FEDERATED LEARNING: The most related work in federated learning tasks will be personalized federated learning. A welltrained local personalized personalized model is needed for each local device in personalized federated learning. Jiang et al. (2019); Deng et al. (2020) propose a method that they train a global model and then fine-tune the trained global model to get the local model. T Dinh et al. (2020); Fallah et al. (2020) change the local objective function to make each local has the ability to be different and handle individual local tasks. Li et al. (2021) introduces a two-level optimization problem for seeking the best local model from great global models. All of these works do not involve a validation set as a reference, but they use a few gradient steps or simple modifications and hope the local model can both fit the local training data and use information from the global model (other local devices). Different from these works, we explicitly formulate a bi-level optimization problem. By adding a validation set, it can be more clearly identified the correlation of information from the other devices and from its own. 2.2 STOCHASTIC BI-LEVEL OPTIMIZATION: Bi-level optimization problem has been studied for a long time. One of the simplest cases in bi-level optimization is the singleton case, where the lower-level optimization has a unique global optimal point. Without calculating the inversion of the Hessian matrix of the lower level optimization problem, there are two major algorithms. One Franceschi et al. (2017) is to approximate \u2202w \u2217(x) \u2202x by \u2202wT \u2202x where wT is the iterate after T steps gradient descent for the lower optimization problem. Using this method, in each iteration, we need to communicate N (number of local devices) vectors among the server and local devices which is not communication efficient. The other method Ghadimi & Wang (2018) is to approximate (\u22072wg(w))\u22121 by \u2211K i=0(I \u2212 \u03b7\u22072g(w))i, where g(w) is the objective function of lower-level optimization problem. Although Khanduri et al. (2021) point out that to approximate gradient for upper optimization function, we can get rid of taking the optimal point for lower optimization in each upper-level update optimization, which seems to get rid of double-loop approximation, it still needs a loop for approximating Hessian inverse with series. Guo & Yang (2021) uses SVRG to reduce the noise level of estimating stochastic gradient and Hessian to get better performance. Besides, all of the above works assume smoothness of the local Hessian, but none of them will apply the property directly into the algorithm. Different from the above work, we introduce a primal-dual framework into bi-level optimization, where the dual variable can record the information of Hessian. Also, Shi et al. (2005); Hansen et al. (1992) introduce the primal-dual framework, but they stay in quadratic regime or mix integer programming, which is non-trivial to extend the results to federated learning settings. 3 ALGORITHM DESIGN: Assume that each function of fi is a strongly convex function. Then, the optimal solution to the lower optimization problem becomes only a single point. Thus, with the implicit function theorem, we can calculate the gradient of f0(w\u2217(x)) with respect to x as follows. Proposition 1. Suppose fi\u2019s are strongly convex functions. Then for each x \u2208 X , it holds that \u2202f0(w \u2217(x)) \u2202x(i) = \u2212\u2207wf0(w\u2217(x))\u22a4 (\u2211N j=1 x (j)\u22072wfj(w\u2217(x)) )\u22121 \u2207wfi(w\u2217(x)). With the proposition 1, one can calculate the gradient of x, when w\u2217(x) and the inverse of Hessian are given. However, for large scale problems, none of these can be easily obtained. Fortunately, by noticing the convexity of each function fi, we can replace the first constraint w\u2217(x) \u2208 argminw \u2211N i=1 x (i)fi(w) with \u2207 \u2211N i=1 x (i)fi(w) = 0. For given x, we can formulate the following constrained optimization problem: min w f0(w) s.t. N\u2211 i=1 x(i)\u2207wfi(w) = 0, (2) By introducing the dual variable \u03bb, we can easily get the Lagrange function. To solve the Lagrange function efficiently, we propose the following augmented Lagrange function. Lx(w, \u03bb) = f0(w) + \u03bb \u22a4 N\u2211 i=1 x(i)\u2207wfi(w) + \u0393 N\u2211 i=1 x(i)fi(w). (3) With the definition, we can obtain the following two propositions directly. Proposition 2. Suppose fi\u2019s are strongly convex functions for i = 1, 2, \u00b7 \u00b7 \u00b7 , N , x(i) \u2265 0 for all i and \u2225x\u22251 = 1. Then, Problem (2) satisfies LICQ and its KKT conditions can be written as follows: \u2207wf0(w) + N\u2211 i=1 x(i)\u22072fi(w)\u03bb = 0 N\u2211 i=1 x(i)\u2207fi(w) = 0. Proposition 3. Suppose fi\u2019s are strongly convex functions for i = 1, 2, \u00b7 \u00b7 \u00b7 , N , x(i) \u2265 0 for all i and \u2225x\u22251 = 1. Then, the stationary point of minw max\u03bb Lx(w, \u03bb) is unique and satisfies the KKT conditions of problem (2). Let (w\u0302\u2217(x), \u03bb\u2217(x)) be the stationary point of minw max\u03bb Lx(w, \u03bb). From proposition 2, it holds that w\u0302\u2217(x) = w\u2217(x) and \u2202f0(w \u2217(x)) \u2202x(i) = \u03bb\u2217(x) \u22a4\u2207wfi(w\u2217(x)). (4) Thus, with the KKT poiont w\u2217(x) and \u03bb\u2217(x), we can estimate the gradient of x without estimating the inverse of Hessian. However, as \u03bb\u22a4 \u2211N i=1 x (i)\u2207wfi(w) can be highly non-convex function, which can be harmful to the optimization process. We add an additional constraint on the norm of \u03bb and define the constraint set ad \u039b. Thus, the problem (2) becomes min w max \u03bb\u2208\u039b Lx(w, \u03bb) = f0(w) + \u03bb \u22a4 N\u2211 i=1 x(i)\u2207wfi(w) + \u0393 N\u2211 i=1 x(i)fi(w). (5) We propose a double loop algorithm for solving problem (1). We show the algorithm in the Algorithm 1 and 2. In inner loop, we solve the augmented Lagrange for K steps. In each step, local client will receive the iterates wt,k and \u03bbt,k. After that, each local client will calculate \u2207\u0303fi(wt,k) and \u2207\u0302fi(wt,k) based on the back propagation through two independent batches. The term \u2207\u03032fi(wt,k)\u03bbt,k is calculated with auto-differentiable framework (i.e. Pytorch, TensorFlow) or with the closed-form multiplication. Then the local device sends gradient estimation \u2207\u0303wfi(wt,k) and the estimated product of Hessian and \u03bb (\u2207\u03032fi(wt,k)\u03bbt,k) to the server. For the server, in each step, the server will first send the primal variable (wt,k) and dual variable (\u03bbt, k) to all local clients. Then, the server will receive the estimated gradients and estimated product from some local clients. Because not all devices will stay online in each step, we define a set Activet,k which records the clients that participant the optimization in (t, k) step. With the vectors collected from local clients, the server will calculate the gradient estimator of wt,k and \u03bbt,k with respect to function Lxt(wt,k, \u03bbt,k). And then, wt,k will be updated by a gradient descent step and \u03bbt,k will be updated by a gradient ascent step. Different from local devices, after K inner loop update steps, based on the \u03bbt,K and gradient estimated in each local client, the server will calculate the gradient of x based on equation 4 and perform a projected gradient descent step on x. In addition, if the ith agent is not in Activet,K , we set the gradient of x(i) to be zero. Algorithm 1 The bi-level primal dual algorithm on local device i 1: for t = 1, 2, \u00b7 \u00b7 \u00b7 , T do 2: for k = 1, 2, \u00b7 \u00b7 \u00b7 ,K do 3: Receive wt,k, \u03bbt,k from the server; 4: Sample a mini-batch and calculate \u2207\u0303fi(wt,k); 5: Sample a mini-batch and calculate \u2207\u0302fi(wt,k); 6: Calculate \u2207\u03032wfi(wt,k)\u03bbt,k with back propagation on scalar \u2207\u0302wf(wt,k)\u03bbt,k; 7: Send \u2207\u03032fi(wt,k)\u03bbt and \u2207\u0303fi(wt,k) to the server; 8: end for 9: end for 4 THEORETICAL ANALYSIS: In this section, we analyze the convergence property of the proposed algorithm. First, we state some assumptions used in the analysis. (A1) f0, f1, \u00b7 \u00b7 \u00b7 , fN are lower bounded by f with L1 Lipschitz gradient . (A2) f1, \u00b7 \u00b7 \u00b7 , fN are \u00b5-strongly convex functions. (A3) f1, \u00b7 \u00b7 \u00b7 , fN has L2 Lipschitz Hessian. Algorithm 2 The Bi-level primal dual algorithm on the Server 1: Input: Initial x1, w1,1, \u03bb1,1, total iterations: K, T and step size: \u03b7w, \u03b7\u03bb, \u03b7x. 2: for t = 1, 2, \u00b7 \u00b7 \u00b7 , T do 3: for k = 1, 2, \u00b7 \u00b7 \u00b7 ,K do 4: Send wt,k, \u03bbt,k to each local device; 5: Receive \u2207\u0303fi(wt,k) and \u2207\u03032wfi(wt,k)\u03bbt from Activet,k; 6: gw = \u2207\u0303f0(wt,k) + N|Activet,k| \u2211 i\u2208Activet,k x (i) t \u2207\u03032wfi(wt,k)\u03bbt,k + \u0393\u2207\u0303fi(wt,k); 7: wt,k+1 = wt,k \u2212 \u03b7wgw; 8: g\u03bb = N |Activet,k| (\u2211 i\u2208Activet,k x (i) t \u2207\u0303wfi(wt,k) ) ; 9: \u03bbt,k+1 = \u03a0\u039b (\u03bbt,k + \u03b7\u03bbg\u03bb); 10: end for 11: gx(i) = N |Activet,K |\u03bb \u22a4 t,K\u2207\u0303wfi(wt,K) for i \u2208 Activet,K ; 12: gx(i) = 0 for i /\u2208 Activet,K ; 13: xt+1 = PX (xt \u2212 \u03b7xgx); 14: \u03bbt+1,1 = \u03bbt,K+1; 15: wt+1,1 = wt,K+1 16: end for 17: Output: xT , WT,K+1. (A4) Assume maxi\u2208{0,1,\u00b7\u00b7\u00b7 ,N} maxx\u2208X \u2225\u2207fi(w\u2217(x))\u2225 \u2264 Dw. Then, D\u03bb = maxx\u2208X \u2225\u03bb\u2217(x)\u2225 is upper bounded by Dw/\u00b5. (A5) Each local estimation is unbiased with bounded variance \u03c32. (A6) Activet,k is independent and sampled from the set of nonempty subset of {1, 2, \u00b7 \u00b7 \u00b7 , N}, where P (i \u2208 Activet,k) = p for all i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , N}. Remark 1. (A1),(A2),(A3) are commonly used in the convergence analysis for bi-level optimization problems (Ji et al., 2021; Chen et al., 2021; Khanduri et al., 2021). Unlike Ji et al. (2021); Chen et al. (2021), where they need to assume f0, f1, \u00b7 \u00b7 \u00b7 , fN to be L0 Lipschitz, we assume the gradient norm are bounded at optimal solution. Because for machine learning models, regularization will be add into objective function, makes the norm of the optimal solution not be large. When w\u2217(x) can be bounded by some constant. (A4) is reasonable in practice. Moreover, the Lipschitz assumption on function can directly infer (A4) with Dw = L0. (A5) is a common assumption used for stochastic gradient methods Ghadimi et al. (2016) and (A6) extend the assumption in Karimireddy et al. (2020) by giving the probability that a local devices will be chosen instead of uniformly sampling. Proposition 4. When \u039b = {\u03bb | \u2225\u03bb\u2225 \u2264 D\u03bb}, the stationary point of problem (5) is the KKT point of problem 2. With proposition 3 and 4, the stationary point of problem (5) is unique and we denote the stationary point as (w\u2217(x), \u03bb\u2217(x)). To give the convergence of the whole algorithm, firstly, we give the convergence guarantee for the inner loop. Theorem 1. For given x \u2208 X , when (A1) to (A6) holds, \u0393 > D\u03bbL2+L1\u00b5 and \u03b7w, \u03b7\u03bb = \u0398(1/ \u221a K), when randomly choose k\u0302 \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K} with equal probability it holds that E [\u2225\u2225\u2225\u2225\u03bb\u22a4k\u0302 \u2207wfi(wk\u0302)\u2212 \u2202f0(w\u2217(x))\u2202x(i) \u2225\u2225\u2225\u22252 ] = O(1/ \u221a K). Thus, with theorem1, the gradient of x can be \u201cwell\u201d estimated through the inner gradient descent ascent method when the number of inner loop steps is large enough. Then, We can obtain the following convergence result of the outer loop. Theorem 2. Suppose (A1) to (A6) holds, \u0393 > D\u03bbL2+L1\u00b5 , \u03b7w, \u03b7\u03bb = \u0398(1/ \u221a K), \u03b7x = \u0398(1/ \u221a T ) and randomly choosing k\u0302 \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K} with equal probability to approximate gradient of x. Define x\u0302 = argminy\u2208X (f0(w \u2217(y)) + \u03c12\u2225y \u2212 x\u2225 2) and \u2207\u0304\u03c1f0(w\u2217(x)) = \u03c1(x\u2212 x\u0302) for large \u03c1,it holds that 1 T T\u2211 t=1 E\u2225\u2207\u0304\u03c1f0(w\u2217(xt))\u22252 = O(1/ \u221a T + 1/ \u221a K). Remark 2. To achieve \u03f5-stationary point (E\u2225\u2207\u0304\u03c1f0(w\u2217(xt))\u22252 \u2264 \u03f5), O(1/\u03f54) samples are needed in each local client and in the server. Different from the previous bilevel works (e.g. Ghadimi & Wang (2018), Khanduri et al. (2021) and Franceschi et al. (2017)), we prove the convergence when optimization variable x has a convex constraint. 4.1 PROOF SKETCH OF THEOREM 1: To show the convergence of inner loop, we first construct a potential function for inner loop objective. Define \u03a6x(w, \u03bb) = Lx(w, \u03bb)\u2212 2d(\u03bb), where d(\u03bb) = minwLx(w, \u03bb) for given x. The intuition of defining this potential function is that Lx(w, \u03bb) is not necessarily decreasing in each iteration, as \u03bb is performing a gradient ascent step. Meanwhile, gradient \u03bb taken is an approximation of gradient of d(\u03bb). Thus, by subtracting d(\u03bb), we can obtain that \u03a6 will decrease during iterations. Therefore, the first thing is to show the lower bound of function \u03a6. Lemma 1 (Lower bound of \u03a6). Suppose (A1)-(A4) hold. It holds that \u03a6x(w, \u03bb) is bounded below by f . The proof of this lemma is basically due to the definition of \u03a6x(w, \u03bb) and d(\u03bb). Then, similar to the proof of gradient descent, we give a lemma that shows the descent of potential function under certain choices of hyperparameters. Lemma 2 (Potential function descent, proof can be found in Lemma 11 in Appendix). Suppose (A1)-(A6) hold. In addition, we assume \u0393 > D\u03bbL2+L1\u00b5 , it holds that E[\u03a6x(wt,k, \u03bbt,k)\u2212\u03a6x(wt,k+1, \u03bbt,k+1)] \u2264 \u2212C1E\u2225\u2207wLx(wt,k, \u03bbt,k)\u22252\u2212C2E[\u2225\u03bbt\u2212\u03bb\u2217t \u22252]+C3\u03c32, where \u03bb+t = \u03a0\u039b(\u03bbt+\u03b7\u03bb\u2207d(\u03bbt)), C1 = \u0398(\u03b7w\u2212\u03b72w\u2212\u03b72\u03bb\u2212\u03b7\u03bb), C2 = \u0398(\u03b7\u03bb) and C3 = O(\u03b72w+\u03b72\u03bb) Thus, when choosing sufficient small \u03b7w and \u03b7\u03bb, we can achieve positive C1 and C2. Together with the lower bound of the function \u03a6, the convergence of the inner algorithm can be shown. Because of the uniqueness of the KKT point, by choosing \u03b7w and \u03b7\u03bb in order of 1/ \u221a K, it can be shown that 1 K K\u2211 k=1 E\u2225wt,k \u2212 w\u2217(xt)\u22252 = O(1/ \u221a K), 1 K K\u2211 k=1 E\u2225\u03bbt,k \u2212 w\u2217(xt)\u22252 = O(1/ \u221a K). Therefore, with the convergence rate of wt,k and \u03bbt,k and equation 4, we can easily prove theorem 1. 4.2 PROOF SKETCH OF THEOREM 2: To apply stochastic gradient descent analysis on x, although we have smoothness for function f0, f1, \u00b7 \u00b7 \u00b7 , fN on w, we need to verify the smoothness of f0(w\u2217(x)) with respect to x. Lemma 3 (Convergence of stochastic gradient descent with biased gradient estimation, proof can be found in Lemma 14 in Appendix). Suppose function f(x) is lower bounded by f with L-Lipshitz gradient. g(x) is an unbiased gradient estimator of \u2207f(x) satisfying that expected norm of g(x) are bounded by G in domain X for function f . Then with update rule xt+1 = \u03a0X (xt \u2212 \u03b7x(g(xt) + \u03bet)), where \u03b7x = \u0398(1/ \u221a T ), X is a convex set and E\u2225\u03bet\u22252 \u2264 \u03f52. By defining x\u0302 = argminy\u2208X (f(y) + \u03c1 2\u2225y \u2212 x\u2225 2) and \u2207\u0304\u03c1f(x) = \u03c1(x\u2212 x\u0302), where \u03c1 = 2L, then it holds that 1 T E T\u2211 t=1 \u2225\u2207\u0304\u03c1f(xt)\u22252 = O(1/ \u221a T + \u03f52). As Lemma 3 suggests, when f0(w\u2217(x)) satisfying L-Lipschitz gradient, bounded estimation error and bounded gradient norm, the convergence rate can achieve O(1/ \u221a T ) with a error term related to estimation error. Theorem 1 shows the estimation error can be bounded by O(1/ \u221a K). Combining this two results we can prove Theorem 2. 5 EXPERIMENTAL RESULTS: In this section, we compare our algorithm with other bi-level optimization algorithms (BSA (Ghadimi & Wang, 2018), SUSTAIN (Khanduri et al., 2021) and RFHO (Franceschi et al., 2017)) in two cases: the toy example and two vision tasks. 5.1 TOY EXAMPLE: In this section, we apply algorithms to solve problem (1) with fi as follows: fi(w) = 1 2 \u2225Aiw \u2212Bi\u22252 + cos(a\u22a4i w \u2212 bi), where Ai \u2208 R30\u00d720, Bi \u2208 R30, ai \u2208 R20 and bi \u2208 R are all generated from Gaussian distribution. The variance of each component in Ai and ai is 1/ \u221a 20 and the variance of each component in Bi is 1/ \u221a 30 and variance of bi is 1. When generated function fi is not 0.1-strongly convex, we randomly generate a new one until we get strongly convex fi whose modular is not less than 0.1. Three local steps (K=1,5,10) are tested. Here, the local steps are used for w update for algorithm BSA, RFHO, and our algorithm, and the local steps are used for Hessian estimation for algorithm BSA and SUSTAIN. Because for this toy example, we can easily compute the Hessian matrix and its inverse, we test the algorithm using the inverse of estimated Hessian to compute the gradient of x named GD. We test two settings of the toy example. One is the deterministic setting, where no estimation noise or client disconnection will occur. In the other setting, we add white Gaussian noise with a noise level of 0.5 in each estimation (including gradient estimation and Hessian estimation). Also, each client has a 0.5 probability of connecting with the server. To evaluate the performance of different algorithms, we calculate the function value of f0(w\u2217(x)) and the stationary of x, i.e. x\u2212\u03a0X(x\u2212 0.001\u2207xf0(w\u2217(x))), where w\u2217(x) is approximate by 200 gradient steps. We take N = 15 and run 20 times and get the results of different algorithms. The results of deterministic setting are shown in Figure 1, and results of noise setting are shown in Figure 2. As it is shown in Figure 1, with local steps getting larger and larger, the performance of BSA, RFHO, and SUSTAIN is getting close to GD, while the performance of the primal-dual method is similar to GD whatever local step used in the algorithm even with only one single step. When noise is added in the Hessian, directly inverse may cause the biased estimation. Thus, the performance of GD gets much worse than it in the deterministic setting shown in Figure 2. Also, in Figure 2, our algorithm can perform better than other algorithms when the local step is small. When local steps increase to 10, BSA and our algorithm have competitive results. 5.2 VISION TASKS: In this section, we apply algorithms to train LeNet5(LeCun et al., 1998) on dataset MNIST(LeCun et al., 1998) and Fashion-MNIST(Xiao et al., 2017). To construct non-iid datasets on different local clients and the global server\u2019s validation set, we randomly pick 20 samples per label out of the whole training dataset and form the validation set. Then, the rest of the training data are divided into 3 sets, and each set will be assigned to a local client. The first client contains samples labeled as 0,1,2,3,4, the second client contains samples labeled as 5,6,7, and the third client contains samples labeled as 8,9 for all two datasets. To test the algorithm\u2019s ability to choose the proper coefficient of local clients, we add 7 noise nodes containing 5000 samples with random labels. We set the learning rate of w to be a constant learning rate without any decay selected from {0.1, 0.01, 0.001} for all training methods, and the learning rate of x is selected from {0.1, 0.01, 0.001, 0.0001}. The batch size for all three training cases is set to 64. \u0393 used in the proposed algorithm is set to be 1. For simplicity, we set the local step as 1. We run 2000 iterations for MNIST and 6000 iterations for Fashion-MNIST. Active probability is set in {0.5, 0.9, 1}. We compare the test accuracy among different methods. As a baseline, we report the test accuracy for training with the validation set only named val, training with the average loss of each client named avg, and training with x = (0.5, 0.3, 0.2, 0, \u00b7 \u00b7 \u00b7 , 0) named opt. All experiments run on V100 with Pytorch (Paszke et al., 2019). Results are shown in Figure 3, Figure 4 and Table 1. Figure 3 shows the test accuracy of the MNIST dataset with different active probabilities. Although SUSTAIN works better than the primal-dual algorithm when all local devices participant in the optimization process, when clients\u2019 participant rate decreases to 0.5, SUSTAIN works worse than ours method. Primal-dual become slower than SUSTAIN may be because of the initialization of the dual variable. When the dual variable is far from its real value it needs more time to get a good enough point. Other than SUSTAIN, our algorithm can converge faster and more stable to a high accuracy point. Further, we list the output of x and standard error of test accuracy for 5 different experiments for different algorithms in Table 1. According to Table 1, our algorithm can achieve a more stable output with respect to x, and the output x is more close to 0.5, 0.3, 0.2, which is related to the number of labels the first three clients holds. Figure 4 gives the test accuracy of training LeNet 5 on the Fashion-MNIST Dataset. Similar to the results of the MNIST dataset, when the clients\u2019 participant is high (0.9,1), SUSTAIN works slightly better than the primal-dual algorithm. But when more local devices disconnect to the server, the performance of SUSTAIN drops, while the primal dual algorithm remains fast convergence speed and high test accuracy. 6 CONCLUSION: In this paper, we proposed a primal-dual-based method for solving a bi-level optimization problem based on a federated learning tasking (local coefficient learning). We give a theoretical analysis that shows the convergence of the proposed algorithm. Though the analysis shows it needs more iterations for the algorithm to converge to an \u03f5-stationary point, it works well with a pretty small number of local steps in both toy case and neural network training. Other than that convergence rate can be improved (perhaps it should be in the order of O(1/ \u221a T ) instead of O(1/ \u221a T + 1/ \u221a K)), the initialization of dual variable affects the speed for convergence, which we leave as the future work. A CONVERGENCE OF INNER LOOP: For simplicity, in this section we simplified the inner loop problem as the following: min w g(w) s.t.\u2207h(w) = 0. Besides, the algorithm for solving the inner loop problem can be simplified as wt+1 = wt \u2212 \u03b7w ( \u2207\u0303g(wt) + \u2207\u03032h(wt)\u03bbt + \u0393\u2207\u0303h(wt) ) = wt \u2212 \u03b7w\u2207\u0303L(wt, \u03bbt) \u03bbt+1 = \u03a0\u039b ( \u03bbt + \u03b7\u03bb\u2207\u0303h(wt) ) Furthermore, the assumptions are as the following: (A1) g and h are differentiable strongly convex function with modular \u00b5, L1-Lipschitz gradient and lower bounded by f . (A2) h has L2-Lipschitz Hessian. (A3) \u039b = {\u03bb|\u2225\u03bb\u2225 \u2264 D\u03bb}, where D \u2265 \u2225\u22072h(w\u2217)\u22121\u2207g(w\u2217)\u2225 and w\u2217 = argminh(w). (A4) Given wt, \u2207\u0303g(wt), \u2207\u03032h(wt), \u2207\u0303h(wt) and \u2207\u0302h(wt) are independent to each other. Besides, all of them are unbiased estimators with bounded variance with respect to mean value and a bounded constant \u03c3, i.e. E [ \u2207\u0303g(wt) | wt ] = \u2207g(wt), E [\u2225\u2225\u2225\u2207\u0303g(wt)\u2212\u2207g(wt)\u2225\u2225\u22252 | wt] \u2264 p\u2225\u2207g(wt)\u22252 + \u03c32; E [ \u2207\u03032h(wt) | wt ] = \u22072h(wt), E [\u2225\u2225\u2225\u2207\u03032h(wt)\u2212\u22072h(wt)\u2225\u2225\u22252 F | wt ] \u2264 p\u2225\u22072h(wt)\u22252 + \u03c32; E [ \u2207\u0303h(wt) | wt ] = \u2207h(wt), E [\u2225\u2225\u2225\u2207\u0303h(wt)\u2212\u2207h(wt)\u2225\u2225\u22252 | wt] \u2264 p\u2225\u2207h(wt)\u22252 + \u03c32. Thus, it is easy to show E [ \u2207\u0303wL(wt, \u03bbt) | wt, \u03bbt ] = \u2207wL(wt, \u03bbt), and E [\u2225\u2225\u2225\u2207\u0303wL(wt, \u03bbt)\u2212\u2207wL(wt, \u03bbt)\u2225\u2225\u22252 | wt, \u03bbt] \u2264 (1+\u03932+D2\u03bb)(p\u2225\u2207wL(wt, \u03bbt)\u22252+\u03c32). First, we give some notations that will be used in this section. Definition 1. Let L(w, \u03bb) = g(w) + \u03bb\u22a4\u2207h(w) + \u0393h(w), d(\u03bb) = minwL(w, \u03bb), w\u2217(\u03bb) = argminw L(w, \u03bb), \u03bb+t = \u03a0\u039b(\u03bbt + \u03b7\u03bb\u2207h(wt))) and LL = (1 + \u0393)L1 +D\u03bbL2. Define potential function \u03a6(wt, \u03bbt) = L(wt, \u03bbt)\u2212 2d(\u03bbt). Lemma 4 (Descent of Lagrange function). For the function L, it holds that E [L(wt+1, \u03bbt+1)\u2212 L(wt, \u03bbt)] \u2264 E [ C0(\u03b7w)\u2225\u2207wL(wt, \u03bbt)\u22252 + LL\u03b7 2 w(1 + \u0393 2 +D2\u03bb)\u03c3 2 2 + (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1)) ] , where C0(\u03b7w) = LL\u03b7 2 w 2 + LL\u03b7 2 wp(1+\u0393 2+D2\u03bb) 2 \u2212 \u03b7w. Proof. Because L(w, \u03bb) has LL Lipschitz gradient, it holds that L(wt+1, \u03bbt) \u2264 L(wt, \u03bbt) + \u27e8\u2207wL(wt, \u03bbt), wt+1 \u2212 wt\u27e9+ LL 2 \u2225wt+1 \u2212 wt\u22252. By taking expectations with respect to wt and \u03bbt on both side of the above inequality, we obtain that E [L(wt+1, \u03bbt)\u2212 L(wt, \u03bbt) | wt, \u03bbt] \u2264 E [ \u27e8\u2207wL(wt, \u03bbt), wt+1 \u2212 wt\u27e9+ LL 2 \u2225wt+1 \u2212 wt\u22252 | wt, \u03bbt ] = E [\u2329 \u2207wL(wt, \u03bbt),\u2212\u03b7w\u2207\u0303wL(wt, \u03bbt) \u232a + LL\u03b7 2 w 2 \u2225\u2207wL(wt, \u03bbt)\u22252 + LL\u03b7 2 w 2 \u2225\u2207\u0303wL(wt, \u03bbt)\u2212\u2207wL(wt, \u03bbt)\u22252 | wt, \u03bbt ] = E [( LL\u03b7 2 w 2 + LL\u03b7 2 wp(1 + \u0393 2 +D2\u03bb) 2 \u2212 \u03b7w ) \u2225\u2207wL(wt, \u03bbt)\u22252 + LL\u03b7 2 w(1 + \u0393 2 +D2\u03bb)\u03c3 2 2 | wt, \u03bbt ] . (6) Meanwhile, it holds that E [L(wt+1, \u03bbt+1)\u2212 L(wt+1, \u03bbt) | wt, \u03bbt] = E [ (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1)|wt, \u03bbt ] (7) Combining (6), (7) and taking expectation on the conditional expectation, we can obtain desired result. Lemma 5. Suppose \u0393\u00b5 > D\u03bbL2 + L1, it holds that \u2225w\u2217(\u03bb1)\u2212 w\u2217(\u03bb2)\u2225 \u2264 \u03b21\u2225\u03bb1 \u2212 \u03bb2\u2225 for all \u03bb1, \u03bb2 \u2208 \u039b, where \u03b21 = L1\u0393\u00b5\u2212D\u03bbL2\u2212L1 . Proof. Note that L(w, \u03bb) is a strongly convex function with respect to the w in the domain \u03bb \u2208 \u039b with the modular (1 + \u0393)\u00b5\u2212D\u03bbL2. Thus, it holds that L(w\u2217(\u03bb1), \u03bb2)\u2212 L(w\u2217(\u03bb2), \u03bb2) \u2265 \u0393\u00b5\u2212D\u03bbL2 \u2212 L1 2 \u2225w\u2217(\u03bb1)\u2212 w\u2217(\u03bb2)\u22252. On the other hand, we have L(w\u2217(\u03bb1), \u03bb2)\u2212 L(w\u2217(\u03bb2).\u03bb2) = L(w\u2217(\u03bb1), \u03bb2)\u2212 L(w\u2217(\u03bb1), \u03bb1) + L(w\u2217(\u03bb1), \u03bb1)\u2212 L(w\u2217(\u03bb2), \u03bb1) + L(w\u2217(\u03bb2), \u03bb1)\u2212 L(w\u2217(\u03bb2), \u03bb2) \u2264 (\u03bb1 \u2212 \u03bb2)\u22a4(\u2207h(w\u2217(\u03bb2))\u2212\u2207h(w\u2217(\u03bb1)))\u2212 \u0393\u00b5\u2212D\u03bbL2 \u2212 L1 2 \u2225w\u2217(\u03bb1)\u2212 w\u2217(\u03bb2)\u22252. Thus, by combining the above two inequalities, with Cauchy-Schwarz inequality, we can obtain (\u0393\u00b5\u2212D\u03bbL2 \u2212 L1)\u2225w\u2217(\u03bb1)\u2212 w\u2217(\u03bb2)\u22252 \u2264 \u2225\u03bb1 \u2212 \u03bb2\u2225\u2225\u2207h(w\u2217(\u03bb1))\u2212\u2207h(w\u2217(\u03bb2))\u2225 \u2264 L1\u2225\u03bb1 \u2212 \u03bb2\u2225\u2225w\u2217(\u03bb1)\u2212 w\u2217(\u03bb2)\u2225. Hence, we get the desired result. Lemma 6 (Ascent of dual function). It holds that E [d(\u03bbt+1)\u2212 d(\u03bbt)] \u2265 E [ \u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(w\u2217(\u03bbt))\u27e9 \u2212 Ld 2 \u2225\u03bbt+1 \u2212 \u03bbt\u22252 ] . Proof. It can be calculated by the implicit function theorem that \u2207\u03bbd(\u03bb) = \u2207h(w\u2217(\u03bb)). Thus, we can obtain that for all \u03bb1, \u03bb2 \u2208 \u039b \u2225\u2207d(\u03bb1)\u2212\u2207d(\u03bb2)\u2225 = \u2225\u2207h(w\u2217(\u03bb1))\u2212\u2207h(w\u2217(\u03bb2))\u2225 \u2264 L1\u03b21\u2225\u03bb1 \u2212 \u03bb2\u2225. Therefore, d(\u03bb) is a differentiable function with Ld = L1\u03b21-Lipschitz gradient. With the definition of d(\u03bb), we have E [d(\u03bbt+1)\u2212 d(\u03bbt)] \u2265 E [ \u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(w\u2217(\u03bbt))\u27e9 \u2212 Ld 2 \u2225\u03bbt+1 \u2212 \u03bbt\u22252 ] Lemma 7. With the strongly convexity of L and h, it holds that \u2225w\u2217(\u03bbt)\u2212 w\u2217\u2225 \u2264 1 \u00b5 \u2225\u2207h(w\u2217(\u03bb))\u2225, and \u2225wt \u2212 w\u2217(\u03bbt)\u2225 \u2264 1 \u0393\u00b5\u2212D\u03bbL2 \u2212 L1 \u2225\u2207wL(wt, \u03bbt)\u2225 Proof. Because of the \u00b5-strongly convexity of h the inequality \u00b5\u2225w1\u2212w2\u2225 \u2264 \u2225\u2207h(w1)\u2212\u2207h(w2)\u2225 holds for all w1, w2. With the \u2207h(w\u2217) = 0, we get the result. Similar to h, because of \u0393\u00b5 \u2212 D\u03bbL2 \u2212 L1-strongly convexity of L, we can prove the second inequality. Lemma 8 (Descent of one step local SGD). It holds that E\u2225wt \u2212 wt+1\u22252 \u2264 E [ \u03b72w(1 + p)\u2225\u2207wL(wt, \u03bbt)\u22252 + \u03b72w\u03c32 ] Proof. E\u2225wt \u2212 wt+1\u22252 = E [ \u03b72w\u2225\u2207\u0303wL(wt, \u03bbt)\u22252 ] \u2264 E [ \u03b72w(1 + p)\u2225\u2207wL(wt, \u03bbt)\u22252 + \u03b72w\u03c32 ] Lemma 9. d(\u03bb) is a \u00b5 2 LL -strongly concave funtion. We define \u00b5d = \u00b5 2 LL . Proof. Let \u03b30 be the largest eigenvalue of \u22072d(\u03bb), \u03b31 be the largest eigenvalue of \u2202w \u2217(\u03bb) \u2202\u03bb . Then, it holds that \u03b3 \u2264 \u03b31\u00b5. Meanwhile, \u2202w \u2217(\u03bb) \u2202\u03bb = \u2212\u2207 2 wL(w \u2217(\u03bb), \u03bb)\u22121\u22072wh(w\u2217(\u03bb)). Thus, \u03b31 \u2264 \u2212 \u00b5 LL . Therefore, d(\u03bb) is a \u00b5 2 LL -strongly concave funtion. Lemma 10. It holds that E\u2225\u03bbt+1\u2212\u03bbt\u22252 \u2265 \u00b52d\u03b7 2 \u03bb 16 E\u2225\u03bbt\u2212\u03bb\u2217\u22252\u2212 \u00b5d\u03b7\u03bb 4 ( \u03b72\u03bb\u03c3 2 + ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) E\u2225wt \u2212 w\u2217(\u03bbt)\u22252 ) Proof. With the update rule of \u03bbt+1, it holds that E\u2225\u03bbt+1 \u2212 \u03bb\u2217\u22252 = E\u2225\u03a0\u039b(\u03bbt + \u03b7\u03bb\u2207h(wt))\u2212 \u03bb\u2217\u22252 \u2264 E\u2225\u03bbt + \u03b7\u03bb \u02dc\u2207h(wt)\u2212 \u03bb\u2217\u22252 = E [ \u2225\u03bbt \u2212 \u03bb\u2217\u22252 + 2\u03b7\u03bb\u27e8\u03bbt \u2212 \u03bb\u2217, \u2207\u0303h(wt)\u27e9+ \u03b72\u03bb\u2225\u2207\u0303h(wt)\u22252 ] = E [ \u2225\u03bbt \u2212 \u03bb\u2217\u22252 + 2\u03b7\u03bb\u27e8\u03bbt \u2212 \u03bb\u2217,\u2207h(w\u2217(\u03bbt))\u27e9 +2\u03b7\u03bb\u27e8\u03bbt \u2212 \u03bb\u2217,\u2207h(wt)\u2212\u2207h(w\u2217(\u03bbt))\u27e9+ \u03b72\u03bb\u2225\u2207\u0303h(wt)\u22252 ] , (8) where the last equality is because \u2207\u0303h(wt) is an unbiased estimator. Meanwhile, because d(\u03bb) is a strongly concave function, it holds that \u03b7\u03bb\u27e8\u03bbt \u2212 \u03bb\u2217,\u2207h(w\u2217(\u03bbt))\u27e9 \u2264 \u2212\u03b7\u03bb\u00b5d\u2225\u03bbt \u2212 \u03bb\u2217\u22252 (9) Further, it holds that E|\u2207\u0303h(wt)\u22252 = E\u2225\u2207\u0303h(wt) +\u2207h(wt)\u2212\u2207h(wt)\u22252 \u2264 \u03c32 + E(1 + p)\u2225\u2207h(wt)\u22252 \u2264 \u03c32 + E2(1 + p)\u2225\u2207h(w\u2217(\u03bbt)))\u22252 + 2(1 + p)\u2225\u2207h(w\u2217(\u03bbt)\u2212\u2207h(wt)\u22252 \u2264 \u03c32 + E2(1 + p)\u2225\u2207h(w\u2217(\u03bbt)))\u22252 + 2(1 + p)L21\u2225w\u2217(\u03bbt \u2212 wt\u22252 \u2264 \u03c32 + E2(1 + p)L2d\u2225\u03bbt \u2212 \u03bb\u2217\u22252 + 2(1 + p)L21\u2225w\u2217(\u03bbt)\u2212 wt\u22252 (10) For \u27e8\u03bbt \u2212 \u03bb\u2217,\u2207h(wt)\u2212\u2207h(w\u2217(\u03bbt))\u27e9, it holds that \u27e8\u03bbt \u2212 \u03bb\u2217,\u2207h(wt)\u2212\u2207h(w\u2217(\u03bbt))\u27e9 \u2264 \u00b5d 2 \u2225\u03bbt \u2212 \u03bb\u2217\u22252 + 1 2\u00b5d \u2225\u2207h(wt)\u2212\u2207h(w\u2217(\u03bbt)\u22252 \u2264 \u00b5d 2 \u2225\u03bbt \u2212 \u03bb\u2217\u22252 + L21 2\u00b5d \u2225wt \u2212 w\u2217(\u03bbt)\u22252 (11) Combining (8), (9), (10) and (11), it holds that E\u2225\u03bbt+1 \u2212 \u03bb\u2217\u22252 \u2264 (1\u2212 \u00b5d\u03b7\u03bb + 2(1 + p)L2d\u03b72\u03bb)E\u2225\u03bbt \u2212 \u03bb\u2217\u22252 + \u03b72\u03c32 + ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) E\u2225wt \u2212 w\u2217(\u03bbt)\u22252 When \u03b7\u03bb \u2264 \u00b5d4(1+p)L2d , it holds that E\u2225\u03bbt+1\u2212\u03bb\u2217\u22252 \u2264 (1\u2212 \u00b5d\u03b7\u03bb 2 )E\u2225\u03bb\u2217\u2212\u03bbt\u22252+\u03b72\u03bb\u03c32+ ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) E\u2225wt\u2212w\u2217(\u03bbt)\u22252. (12) It holds that E\u2225\u03bbt+1 \u2212 \u03bbt\u22252 = E\u2225\u03bbt+1 \u2212 \u03bb\u2217 + \u03bb\u2217 \u2212 \u03bbt\u22252 = E [ \u2225\u03bbt+1 \u2212 \u03bb\u2217\u22252 + \u2225\u03bb\u2217 \u2212 \u03bbt\u22252 + 2\u27e8\u03bbt+1 \u2212 \u03bb\u2217, \u03bb\u2217 \u2212 \u03bbt\u27e9 ] \u2265 E [ (1\u2212 \u03be)\u2225\u03bbt+1 \u2212 \u03bb\u2217\u22252 + ( 1\u2212 1 \u03be ) \u2225\u03bb\u2217 \u2212 \u03bbt\u22252 ] ,\u2200\u03be > 1 (13) Let \u03be = 1+ \u00b5d\u03b7\u03bb/4, (1\u2212 \u03be) ( 1\u2212 \u00b5d\u03b7\u03bb2 ) + ( 1\u2212 1\u03be ) \u2265 \u00b52d\u03b72\u03bb/16. combining (12) and (13), it holds that E\u2225\u03bbt+1 \u2212 \u03bbt\u22252 \u2265 ( (1\u2212 \u03be) ( 1\u2212 \u00b5d\u03b7\u03bb 2 ) + ( 1\u2212 1 \u03be )) \u2225\u03bb\u2217 \u2212 \u03bbt\u22252 + (1\u2212 \u03be) ( \u03b72\u03bb\u03c3 2 + ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) E\u2225wt \u2212 w\u2217(\u03bbt)\u22252 ) \u2265 \u00b5 2 d\u03b7 2 \u03bb 16 E\u2225\u03bbt \u2212 \u03bb\u2217\u22252 \u2212 \u00b5d\u03b7\u03bb 4 ( \u03b72\u03bb\u03c3 2 + ( 2\u03b72\u03bb(1 + p)L 2 1 \u0393\u00b5\u2212D\u03bbL2 \u2212 L1 + L21\u03b7\u03bb \u00b5d\u0393\u00b5\u2212D\u03bbL2 \u2212 L1 ) E\u2225wt \u2212 w\u2217(\u03bbt)\u22252 ) Thus, we get the desired result. Define a potential function \u03a6(wt, \u03bbt) = L(wt, \u03bbt) \u2212 2d(\u03bbt). According to the definition we have L(wt, \u03bbt) > d(\u03bbt). Then, \u03a6(wt, \u03bbt) \u2265 \u2212d(\u03bbt) \u2265 \u2212minw g(w) + \u0393h(w) \u2265 \u2212(1 + \u0393)f for all wt and \u03bbt \u2208 \u039b. Lemma 11 (Descent of potential function). It holds that E [\u03a6(wt+1, \u03bbt+1)\u2212 \u03a6(wt, \u03bbt)] \u2264 E [ C1\u2225\u2207wL(wt, \u03bbt)\u22252 + C2\u2225\u03bbt \u2212 \u03bb\u2217t \u22252 ] + C3 where C1 = \u2212\u03b7w + LL\u03b7 2 w 2 + LL\u03b7 2 wp(1 + \u0393 2 +D2\u03bb) 2 + 4L21\u03b7\u03bb\u03b7 2 w(1 + p) + 4L21\u03b7\u03bb (\u0393\u00b5\u2212D\u03bbL2 \u2212 L1)2 + \u00b5d 16 ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) C2 = \u2212 \u03b7\u03bb\u00b5 2 d 64 C3 = LL\u03b7 2 w(1 + \u0393 2 +D2\u03bb)\u03c3 2 2 + 2L21\u03b7\u03bb\u03b7 2 w\u03c3 2 + 2\u03b72\u03bb\u03c3 2 + \u00b5d\u03b7 2 \u03bb\u03c3 2 16 . Proof. With Lemma 4 and Lemma 6, it holds that E [\u03a6(wt+1, \u03bbt+1)\u2212 \u03a6(wt, \u03bbt)] \u2264 E [ C0(\u03b7w)\u2225\u2207wL(wt, \u03bbt)\u22252 + (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1)\u2212 2\u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(w\u2217(\u03bbt))\u27e9+ Ld\u2225\u03bbt+1 \u2212 \u03bbt\u22252 ] + LL\u03b7 2 w(1 + \u0393 2 +D2\u03bb)\u03c3 2 2 . We deal with each term as follows. For the second term and the third term, it holds that (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1)\u2212 2\u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(w\u2217(\u03bbt))\u27e9 = 2\u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(wt+1)\u2212\u2207h(w\u2217(\u03bbt))\u27e9 \u2212 (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1) \u2264 2L1\u2225\u03bbt+1 \u2212 \u03bbt\u2225\u2225wt+1 \u2212 w\u2217(\u03bbt)\u2225 \u2212 1 \u03b7\u03bb \u2225\u03bbt+1 \u2212 \u03bbt\u22252 \u2264 2L1\u2225\u03bbt+1 \u2212 \u03bbt\u2225 (\u2225wt \u2212 w\u2217(\u03bbt))\u2225+ \u2225wt+1 \u2212 wt\u2225)\u2212 1 \u03b7\u03bb \u2225\u03bbt+1 \u2212 \u03bbt\u22252 \u2264 ( 1 2\u03b7\u03bb \u2212 1 \u03b7\u03bb ) \u2225\u03bbt+1 \u2212 \u03bbt\u22252 + 4L21\u03b7\u03bb\u2225wt \u2212 w\u2217(\u03bbt))\u22252 + 4L21\u03b7\u03bb\u2225wt+1 \u2212 wt\u22252 By taking the expectation on the both side of inequality, it holds that E [ (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1)\u2212 2\u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(w\u2217(\u03bbt))\u27e9 ] \u2264 E [ \u2212 1 2\u03b7\u03bb \u2225\u03bbt+1 \u2212 \u03bbt\u22252 + 4L21\u03b7\u03bb\u2225wt \u2212 w\u2217(\u03bbt))\u22252 + 4L21\u03b7\u03bb\u2225wt+1 \u2212 wt\u22252 ] \u2264 E [ \u2212 1 2\u03b7\u03bb \u2225\u03bbt+1 \u2212 \u03bbt\u22252 + 4L21\u03b7\u03bb\u03b72w(1 + p)\u2225\u2207wL(wt, \u03bbt)\u22252 + 4L21\u03b7\u03bb (\u0393\u00b5\u2212D\u03bbL2 \u2212 L1)2 \u2225\u2207wL(wt, \u03bbt)2\u22252 ] + 2L21\u03b7\u03bb\u03b7 2 w\u03c3 2. Meanwhile, with Lemma 10 and Lemma 7 it holds that E\u2225\u03bbt+1 \u2212 \u03bbt\u22252 \u2265 \u00b5 2 d\u03b7 2 \u03bb 16 E\u2225\u03bbt \u2212 \u03bb\u2217\u22252 \u2212 \u00b5d\u03b7\u03bb 4 ( \u03b72\u03bb\u03c3 2 + ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) E\u2225wt \u2212 w\u2217(\u03bbt)\u22252 ) \u2265 \u00b5 2 d\u03b7 2 \u03bb 16 E\u2225\u03bbt \u2212 \u03bb\u2217\u22252 \u2212 \u00b5d\u03b7\u03bb 4 ( \u03b72\u03bb\u03c3 2 + ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) E\u2225\u2207wL(wt, \u03bbt)\u22252 ) Further, when \u03b7\u03bb \u2264 1/(4Ld), it holds that \u2212 12\u03b7\u03bb \u2225\u03bbt+1 \u2212 \u03bbt\u2225 2 + Ld\u2225\u03bbt+1 \u2212 \u03bbt\u22252 \u2264 14\u03b7\u03bb . Thus, it holds that E [\u03a6(wt+1, \u03bbt+1)\u2212 \u03a6(wt, \u03bbt)] \u2264 E [ C0(\u03b7w)\u2225\u2207wL(wt, \u03bbt)\u22252 + (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1)\u2212 2\u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(w\u2217(\u03bbt))\u27e9+ Ld\u2225\u03bbt+1 \u2212 \u03bbt\u22252 ] + LL\u03b7 2 w(1 + \u0393 2 +D2\u03bb)\u03c3 2 2 \u2264 E [ C1\u2225\u2207wL(wt, \u03bbt)\u22252 + C2\u2225\u03bbt \u2212 \u03bb\u2217t \u22252 + C3 ] , where C1 = \u2212\u03b7w + LL\u03b7 2 w 2 + LL\u03b7 2 wp(1 + \u0393 2 +D2\u03bb) 2 + 4L21\u03b7\u03bb\u03b7 2 w(1 + p) + 4L21\u03b7\u03bb (\u0393\u00b5\u2212D\u03bbL2 \u2212 L1)2 + \u00b5d 16 ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) C2 = \u2212 \u03b7\u03bb\u00b5 2 d 64 C3 = LL\u03b7 2 w(1 + \u0393 2 +D2\u03bb)\u03c3 2 2 + 2L21\u03b7\u03bb\u03b7 2 w\u03c3 2 + 2\u03b72\u03bb\u03c3 2 + \u00b5d\u03b7 2 \u03bb\u03c3 2 16 . Proof. Proof of the Inner Convergence By the definition of \u03a6, it holds that \u03a6(w, \u03bb) \u2265 \u2212d(\u03bb) \u2265 f . Thus, with Lemma 11, by summing up T terms, it holds that 1 K K\u2211 t=1 E \u03b7w 2 \u2225\u2207wL(wt, \u03bbt)\u22252 + \u03b7\u03bbL 2 L 8\u00b54 \u2225\u03bbt \u2212 \u03bb\u2217\u22252 \u2264 \u03a6(w1, \u03bb1)\u2212 f K + C3 Then, let \u03b7w = \u0398(1/ \u221a K) and \u03b7\u03bb = \u0398(1/ \u221a K), it holds that 1 K E K\u2211 t=1 \u2225\u03bbt \u2212 \u03bb\u2217\u22252 = O(1/ \u221a K) 1 K E K\u2211 t=1 \u2225wt \u2212 w\u2217\u22252 = O(1/ \u221a K) On the other hand, it holds that \u2225\u03bb\u22a4t \u2207wfi(wt)\u2212 \u03bb\u2217 \u22a4\u2207wfi(w\u2217)\u22252 \u2264 2\u2225\u2207wfi(w\u2217)\u22252\u2225\u03bbt \u2212 \u03bb\u2217\u22252 + 2\u2225\u03bbt\u22252\u2225\u2207fi(wt)\u2212\u2207fi(w\u2217)\u22252 \u2264 2D2w\u2225\u03bbt \u2212 \u03bb\u2217\u22252 + 2D2\u03bbL21\u2225wt \u2212 w\u2217\u22252 Therefore, it holds that 1 K E K\u2211 t=1 \u2225\u03bb\u22a4t \u2207wfi(wt)\u2212\u03bb\u2217 \u22a4\u2207wfi(w\u2217)\u22252 \u2264 2 K E K\u2211 t=1 D2w\u2225\u03bbt\u2212\u03bb\u2217\u22252+D2\u03bbL21\u2225wt\u2212w\u2217\u22252 = O(1/ \u221a K) B PROOF OF OUTER LOOP CONVERGENCE: In this section, we give the proof of the outer loop convergence. Lemma 12. Suppose (A1)-(A4) holds. Then, for all x1, x2 \u2208 X , it holds that \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225 \u2264 \u221a NDw\u00b5+ \u221a NDwL1 \u00b52 \u2225x1 \u2212 x2\u2225 Proof. Because \u2211N i=1 x (i) 2 fi(w) is \u00b5 is a strongly convex function, it holds that N\u2211 i=1 x (i) 2 (fi(w \u2217(x1))\u2212 fi(w\u2217(x2))) \u2265 \u00b5 2 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u22252. On the other hand, we have N\u2211 i=1 x (i) 2 (fi(w \u2217(x1))\u2212 fi(w\u2217(x2))) = N\u2211 i=1 x (i) 2 fi(w \u2217(x1))\u2212 N\u2211 i=1 x (i) 1 fi(w \u2217(x1)) + N\u2211 i=1 x (i) 1 fi(w \u2217(x1)) \u2212 N\u2211 i=1 x (i) 1 fi(w \u2217(x2)) + N\u2211 i=1 x (i) 1 fi(w \u2217(x2))\u2212 N\u2211 i=1 x (2) 2 fi(w \u2217(x2)) \u2264 N\u2211 i=1 (x (i) 1 \u2212 x (i) 2 )(fi(w \u2217(x2))\u2212 fi(w\u2217(x1)))\u2212 \u00b5 2 \u2225w\u2217(x2)\u2212 w\u2217(x1)\u22252. Meanwhile, we have |fi(w\u2217(x1))\u2212fi(w\u2217(x2))| \u2264 max(\u2225\u2207wf(w\u2217(x1))\u2225, \u2225\u2207wf(w\u2217(x2))\u2225)\u2225w\u2217(x1)\u2212w\u2217(x2)\u2225+ L1 2 \u2225w\u2217(x2)\u2212w\u2217(x1)\u22252 With strongly convexity, it holds that \u2225w\u2217(x2)\u2212 w\u2217(x1)\u2225 \u2264 1 \u00b5 \u2225\u2207f(w\u2217(x1))\u2212\u2207f(w\u2217(x2))\u2225 \u2264 2Dw \u00b5 . Thus, it holds that fi(w \u2217(x2))\u2212 fi(w\u2217(x1)) \u2264 Dw(1 + L1/\u00b5)\u2225w\u2217(x2)\u2212 w\u2217(x1)\u2225. Combining the above inequalities, it holds that \u00b5\u2225w\u2217(x1)\u2212w\u2217(x2)\u22252 \u2264 \u2225x1\u2212x2\u22251(max|fi(w\u2217(x1))\u2212fi(w\u2217(x2))|) \u2264 \u221a NDw(1+L1/\u00b5)\u2225w\u2217(x1)\u2212w\u2217(x2)\u2225 Lemma 13. Suppose (A1)-(A4) holds, then f0(w\u2217(x)) has Lipschitz gradient with Lipschitz constant( D2w \u00b52 + 2DwL1 \u00b5 ) \u221a NDw\u00b5+ \u221a NDwL1 \u00b52 . Proof. \u2202f0(w \u2217(x)) \u2202x(i) = \u2212\u2207wf0(w\u2217(x))\u22a4  N\u2211 j=1 x(j)\u22072w\u2207fj(w\u2217(x)) \u22121 \u2207wfi(w\u2217(x)) By the smoothness and strongly convexity it holds that\u2225\u2225\u2225\u2225\u2225\u2225\u2225  N\u2211 j=1 x (j) 1 \u22072w\u2207fj(w\u2217(x1)) \u22121 \u2212  N\u2211 j=1 x (j) 2 \u22072w\u2207fj(w\u2217(x2)) \u22121 \u2225\u2225\u2225\u2225\u2225\u2225\u2225 \u2264 1 \u00b52 \u2225\u2225\u2225\u2225\u2225\u2225 N\u2211 j=1 x (j) 1 \u22072w\u2207fj(w\u2217(x1))\u2212 N\u2211 j=1 x (j) 2 \u22072w\u2207fj(w\u2217(x2)) \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 1 \u00b52 max j \u2225\u22072w\u2207fj(w\u2217(x1))\u2212\u22072w\u2207fj(w\u2217(x2))\u2225 \u2264 L2 \u00b52 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225, where the first inequality is due to \u2225A(A\u22121 \u2212B\u22121)B\u2225 = \u2225A\u2212B\u2225 for all invertible matrices A,B. Then, it holds that\u2225\u2225\u2225\u2225\u2225\u2225\u2225  N\u2211 j=1 x (j) 1 \u22072w\u2207fj(w\u2217(x1)) \u22121 \u2207wfi(w\u2217(x1))\u2212  N\u2211 j=1 x (j) 2 \u22072w\u2207fj(w\u2217(x2)) \u22121 \u2207wfi(w\u2217(x2)) \u2225\u2225\u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225\u2225\u2225\u2225   N\u2211 j=1 x (j) 1 \u22072w\u2207fj(w\u2217(x1)) \u22121 \u2212  N\u2211 j=1 x (j) 2 \u22072w\u2207fj(w\u2217(x2)) \u22121 \u2207wfi(w\u2217(x1)) \u2225\u2225\u2225\u2225\u2225\u2225\u2225 + \u2225\u2225\u2225\u2225\u2225\u2225\u2225  N\u2211 j=1 x (j) 2 \u22072w\u2207fj(w\u2217(x2)) \u22121 (\u2207fi(w\u2217(x1))\u2212\u2207fi(w\u2217(x2))) \u2225\u2225\u2225\u2225\u2225\u2225\u2225 \u2264 Dw \u00b52 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225+ L1 \u00b5 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225. Thus, combining with the definition of gradient, it holds that\u2223\u2223\u2223\u2223\u2202f0(w\u2217(x1))\u2202x(i) \u2212 \u2202f0(w\u2217(x2))\u2202x(i) \u2223\u2223\u2223\u2223 \u2264 \u2225\u2207wf0(w\u2217(x1))\u2225 \u2225\u2225\u2225\u2225\u2225\u2225 ( N\u2211 j=1 x (j) 1 \u2207 2 w\u2207fj(w\u2217(x1)) )\u22121 \u2207wfi(w\u2217(x1))\u2212 ( N\u2211 j=1 x (j) 2 \u2207 2 w\u2207fj(w\u2217(x2)) )\u22121 \u2207wfi(w\u2217(x2)) \u2225\u2225\u2225\u2225\u2225\u2225 + \u2225\u2207wf0(w\u2217(x1))\u2212\u2207wf0(w\u2217(x2))\u2225 \u2225\u2225\u2225\u2225\u2225\u2225 ( N\u2211 j=1 x (j) 2 \u2207 2 w\u2207fj(w\u2217(x2)) )\u22121 \u2207wfi(w\u2217(x2)) \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 Dw( Dw \u00b52 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225+ L1 \u00b5 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225) + DwL1 \u00b5 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225 Therefore, combine with Lemma 12, we can obtain the result. Lemma 14. Suppose function f(x) is lower bounded by f with L-Lipshitz gradient. g(x) is an unbiased gradient estimator of \u2207f(x) satisfying that expected norm of g(x) are bounded by G in domain X for function f . Then with update rule xt+1 = \u03a0X (xt \u2212 \u03b7x(g(xt) + \u03bet)), where \u03b7x = \u0398(1/ \u221a T ), X is a convex set and E\u2225\u03bet\u22252 \u2264 \u03f52. By defining x\u0302 = argminy\u2208X (f(y)+ \u03c12\u2225y\u2212x\u2225 2) and \u2207\u0304\u03c1f(x) = \u03c1(x\u2212 x\u0302), where \u03c1 = 2L, then it holds that 1 T E T\u2211 t=1 \u2225\u2207\u0304\u03c1f(xt)\u22252 = O(1/ \u221a T + \u03f52). Proof. It holds that E ( f(x\u0302t+1 + \u03c1 2 \u2225xt+1 \u2212 x\u0302t+1\u22252 ) \u2264 E ( f(x\u0302t) + \u03c1 2 \u2225xt+1 \u2212 x\u0302t\u22252 ) = E ( f(x\u0302t) + \u03c1 2 \u2225\u03a0X (xt \u2212 \u03b7x(g(xt) + \u03bet))\u2212 x\u0302t\u22252 ) \u2264 E ( f(x\u0302t) + \u03c1 2 \u2225xt \u2212 \u03b7x(g(xt) + \u03bet)\u2212 x\u0302t\u22252 ) = E ( f(x\u0302t) + \u03c1 2 \u2225xt \u2212 x\u0302t\u22252 + \u03c1\u03b72x\u2225g(xt) + \u03bet\u22252 \u2212 \u03b7x\u03c1\u27e8xt \u2212 x\u0302t, g(xt)\u27e9 \u2212 \u03b7x\u03c1\u27e8xt \u2212 x\u0302t, \u03bet\u27e9 ) \u2264 E ( f(x\u0302t) + \u03c1 2 \u2225xt \u2212 x\u0302t\u22252 ) + \u03c1\u03b72x(G 2 + \u03f52)\u2212 \u03c1\u03b7xE ( f(xt)\u2212 f(x\u0302t)\u2212 L 2 \u2225xt \u2212 x\u0302t\u22252 ) + \u03c1\u03b7xE ( L 2 \u2225xt \u2212 x\u0302t\u22252 + 1 2L \u03f52 ) Then summing up the above inequality, it holds that \u03c1\u03b7x T\u2211 t=1 E ( f(xt)\u2212 f(x\u0302t)\u2212 L\u2225xt \u2212 x\u0302t\u22252 ) \u2264 E [ f(x\u03021) + \u03c1 2 \u2225x1 \u2212 x\u03021\u22252 \u2212 (f(x\u0302T+1 + \u03c1 2 \u2225xT+1 \u2212 x\u0302T+1\u22252) ] + T\u03c1\u03b72x(G 2 + \u03f52) + T\u03c1\u03b7x 2 \u03f52 \u2264 f(x\u03021) + \u03c1 2 \u2225x1 \u2212 x\u03021\u22252 \u2212 f) + T\u03c1\u03b72x(G2 + \u03f52) + T\u03c1\u03b7x 2L \u03f52 On the other hand, because x\u0302t = argminy\u2208X (f(y) + \u03c12\u2225xt \u2212 y\u2225 2) and function f(y) + \u03c12\u2225xt \u2212 y\u2225 2 is strongly convex with modular \u03c1\u2212 L, it holds that f(xt)\u2212 f(x\u0302t)\u2212 L\u2225xt \u2212 x\u0302t\u22252 = f(xt) + \u03c1 2 \u2225xt \u2212 xt\u22252 \u2212 (f(x\u0302t) + \u03c1 2 \u2225xt \u2212 x\u0302t\u22252) + \u03c1\u2212 L\u2212 1 2 \u2225xt \u2212 x\u0302t\u22252 \u2265 2\u03c1\u2212 3L 2 \u2225xt \u2212 x\u0302t\u22252 = L 2 \u2225xt \u2212 x\u0302t\u22252 Thus, combining the above inequalities, it holds that \u03c12 T T\u2211 t=1 E\u2225xt \u2212 x\u0302t\u22252 \u2264 2\u03c1 2 TL T\u2211 t=1 Ef(xt)\u2212 f(x\u0302t)\u2212 L\u2225xt \u2212 x\u0302t\u22252 \u2264 \u03c1f(x\u03021) + \u03c12 2 \u2225x1 \u2212 x\u03021\u2225 2 \u2212 \u03c1f) T\u03b7x + \u03c12\u03b7x(G 2 + \u03f52) + \u03c12 2L \u03f52. Therefore, when \u03b7x = O(1/ \u221a T ), 1T E \u2211T t=1 \u2225\u2207\u0304\u03c1f(xt)\u22252 = \u03c12 T \u2211T t=1 E\u2225xt \u2212 x\u0302t\u22252 = O(1/ \u221a T + \u03f52). Proof of Theorem 2. With Lemma 13, we can obtain f0(w\u2217(x)) has Lipschitz gradient on domain X . Define \u03be(i)t = \u03bb T t \u2207wfi(wt,k)\u2212 \u03bb\u2217(x)T\u2207wfi(w\u2217(x)) When we use \u03bbt,k and wt,k with random k, as it suggests in Theorem 1, the expected norm of \u03be ( t i) can be bounded by O(1/ \u221a K). Let x(i)t+1/2 = x (i) t \u2212 \u03b7x(g(xt)(i) + \u03bbTt \u2207wfi(wt,k)\u2212 \u03bb\u2217(xt)T\u2207wfi(w\u2217(xt))), Then, it holds that Eg(xt)(i) = E [ 1 \u03b7x (xt+1/2 \u2212 xt)\u2212 \u03bbTt \u2207wfi(wt,k) + \u03bb\u2217(xt)T\u2207wfi(w\u2217(xt))) ] = \u03bb\u2217(xt) T\u2207wfi(w\u2217(xt))) and E ( g(xt) (i))2 ) \u2264 (1 + p)\u2225\u03bb\u2217(xt)T\u2207wfi(w\u2217(xt)))\u22252 + \u03f52 + \u03c32 \u2264 (1 + p)D2\u03bbD2w + \u03f52 + \u03c32 Thus, together with Lemma 14, we can directly get the result. C ADDITIONAL EXPERIMENTAL RESULTS ON MNIST AND FASHION MNIST WITH LENET5: We use 10 clients in this experiment. The first 5 clients contains i.i.d. 9000 samples, the last 5 clients contains 9000 samples with random label. The rest setting is the same as it in the main text. The results are shown in Figure 5, and Figure 6.", "agent_review": [{"role": "user", "content": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Local Coefficient Optimization in Federated Learning\n\n1 INTRODUCTION: Federated learning emerges as a promising approach to build a large-scale cooperative learning system among multiple clients without sharing their raw data. However, due to non-i.i.d data distributed on different local clients, directly defining the global objective as the average of the local loss will not give the proper objective function for the facing task. Mohri et al. (2019) propose the Agnostic Federated Learning where the proposed algorithm guarantee the worst performance for each local task. However, in a different situation, we may face different tasks. It is not enough for us to solve each local task well, but to solve any possible tasks. However, it is impossible to give an objective function that works well for all different potential tasks without any prior knowledge. Thus, by collecting a small validation set as the knowledge of the facing task, we want to solve the following problem: min x f0(w \u2217(x)) s.t. w\u2217(x) \u2208 argmin w N\u2211 i=1 x(i)fi(w) x \u2208 X = {x|x \u2265 0, \u2225x\u22251 = 1}, (1) where f0 corresponds to the validation error which can be sensed in the global server, and fi is the local objective function corresponding to the ith local client. Once problem 1 is solved in real-world applications, the optimal x\u2217 can be used to determine the contribution degree of each local worker. This can help fairly distribute the benefits of federated learning to each local worker. To solve the bi-level optimization problem (1), under some mild conditions, Kolstad & Lasdon (1990) propose and algorithm that calculate gradient of x directly, where the gradient calculate is based on implicit function theorem and the gradient will involved the inverse of Hessian matrix of lower optimization objective function. But, due to the large parameter dimension of w, it is impossible to take the inverse of the Hessian or solve the linear system related to the Hessian. Meanwhile, due to a large amount of data in the local device, it is hard to directly estimate the gradient or the Hessian of the local function fi. Only stochastic gradient and stochastic hessian can be accessed. Thus, Ghadimi & Wang (2018) propose BSA algorithm where the inverse of Hessian is approximated by a series of the power of Hessian. Khanduri et al. (2021) propose SUSTAIN algorithm for solving stochastic bi-level optimization problems with smaller sample complexity. Similar to Ghadimi & Wang (2018), they need an extra loop of the Hessian-vector product to approximate the product of the Hessian inverse with some vector. However, in the federated learning setting, each local device may not be online during the optimization process, making it difficult to estimate the Hessian inverse. Fortunately, with the assumption that the lower-level functions have some smoothness property (e.g., L2 Lipschitz Hessian, which has been assumed in many bi-level analyses (e.g. Ghadimi & Wang (2018))), we can estimate the inverse of Hessian based on the previous Hessian estimation. Thus, we can get a more accurate estimation than those estimated from the scratch. Specifically, Ghadimi & Wang (2018) use series of matrix (using \u2211K k=0(I \u2212 \u03b7H)k to approximate 1 \u03b7H \u22121 with certain \u03b7). For different w, to approximate H\u22121 with series, we need to go through the entire series from the beginning, which is hard to utilize the Lipschitz smooth of H with respect to w. Thus, we introduce a new variable \u03bb to keep track of the information of the inversion of Hessian matrix . To connect Hessian of lower-level optimization problems and the new variable \u03bb, under strongly convex assumption for each fi, we rewrite the constraints to a non-linear equations constraints (\u2207 \u2211N i=1 x (i)fi(w) = 0), and then introduce the Lagrange function with new constraints. From the KKT conditions, it holds that the optimal solution of \u03bb is enough for us to compute the gradient of x instead of estimate the inverse of Hessian matrix. To solve a constrained optimization with non-linear equality constraints, adding the norm square of the equality constraint as an augmented term may not give the convexity to the augmented Lagrange function, which may not help the min-max optimization algorithm find the stationary point. Instead, as the function fi\u2019s are assumed to be strongly convex, adding function fi\u2019s as the augmented term can help introduce convexity and it will not change the stationary point of the min-max problem. Based on this new augmented Lagrange function, we prove that with stochastic gradient descent and ascent, w and \u03bb can converge to the KKT point. Meanwhile, by the implicit function theorem, when w and \u03bb are closed to the stationary point of min-max, the bias of estimating the gradient of x can be reduced to 0. Thus, with the primal-dual algorithm on w and \u03bb and stochastic projected gradient descent on x, we show the convergence of our algorithm. Finally, we compare our algorithm with other algorithms on a toy example and real datasets (MNIST and F-MNIST with Network LeNet-5). The experimental results show that the proposed algorithm can perform well in strongly convex cases and even in some non-convex cases (Neural Networks). We summarize our contributions as follows: \u2022 In Federated Learning, we formulate the local coefficient learning problem as a bi-level optimization problem, which gives a way to identify the dataset quality in each local client for some specific task (where a small validation set is given). \u2022 In bi-level optimization, we introduce a primal-dual framework and show the convergence of the whole algorithm in the constrained and stochastic setting. \u2022 For some specific optimization problems with non-linear constraints, we give a new augmented term. With the new augmented term, the primal variable and dual variable can converge to the KKT point of the original problems. 2 RELATED WORK:  2.1 PERSONALIZED FEDERATED LEARNING: The most related work in federated learning tasks will be personalized federated learning. A welltrained local personalized personalized model is needed for each local device in personalized federated learning. Jiang et al. (2019); Deng et al. (2020) propose a method that they train a global model and then fine-tune the trained global model to get the local model. T Dinh et al. (2020); Fallah et al. (2020) change the local objective function to make each local has the ability to be different and handle individual local tasks. Li et al. (2021) introduces a two-level optimization problem for seeking the best local model from great global models. All of these works do not involve a validation set as a reference, but they use a few gradient steps or simple modifications and hope the local model can both fit the local training data and use information from the global model (other local devices). Different from these works, we explicitly formulate a bi-level optimization problem. By adding a validation set, it can be more clearly identified the correlation of information from the other devices and from its own. 2.2 STOCHASTIC BI-LEVEL OPTIMIZATION: Bi-level optimization problem has been studied for a long time. One of the simplest cases in bi-level optimization is the singleton case, where the lower-level optimization has a unique global optimal point. Without calculating the inversion of the Hessian matrix of the lower level optimization problem, there are two major algorithms. One Franceschi et al. (2017) is to approximate \u2202w \u2217(x) \u2202x by \u2202wT \u2202x where wT is the iterate after T steps gradient descent for the lower optimization problem. Using this method, in each iteration, we need to communicate N (number of local devices) vectors among the server and local devices which is not communication efficient. The other method Ghadimi & Wang (2018) is to approximate (\u22072wg(w))\u22121 by \u2211K i=0(I \u2212 \u03b7\u22072g(w))i, where g(w) is the objective function of lower-level optimization problem. Although Khanduri et al. (2021) point out that to approximate gradient for upper optimization function, we can get rid of taking the optimal point for lower optimization in each upper-level update optimization, which seems to get rid of double-loop approximation, it still needs a loop for approximating Hessian inverse with series. Guo & Yang (2021) uses SVRG to reduce the noise level of estimating stochastic gradient and Hessian to get better performance. Besides, all of the above works assume smoothness of the local Hessian, but none of them will apply the property directly into the algorithm. Different from the above work, we introduce a primal-dual framework into bi-level optimization, where the dual variable can record the information of Hessian. Also, Shi et al. (2005); Hansen et al. (1992) introduce the primal-dual framework, but they stay in quadratic regime or mix integer programming, which is non-trivial to extend the results to federated learning settings. 3 ALGORITHM DESIGN: Assume that each function of fi is a strongly convex function. Then, the optimal solution to the lower optimization problem becomes only a single point. Thus, with the implicit function theorem, we can calculate the gradient of f0(w\u2217(x)) with respect to x as follows. Proposition 1. Suppose fi\u2019s are strongly convex functions. Then for each x \u2208 X , it holds that \u2202f0(w \u2217(x)) \u2202x(i) = \u2212\u2207wf0(w\u2217(x))\u22a4 (\u2211N j=1 x (j)\u22072wfj(w\u2217(x)) )\u22121 \u2207wfi(w\u2217(x)). With the proposition 1, one can calculate the gradient of x, when w\u2217(x) and the inverse of Hessian are given. However, for large scale problems, none of these can be easily obtained. Fortunately, by noticing the convexity of each function fi, we can replace the first constraint w\u2217(x) \u2208 argminw \u2211N i=1 x (i)fi(w) with \u2207 \u2211N i=1 x (i)fi(w) = 0. For given x, we can formulate the following constrained optimization problem: min w f0(w) s.t. N\u2211 i=1 x(i)\u2207wfi(w) = 0, (2) By introducing the dual variable \u03bb, we can easily get the Lagrange function. To solve the Lagrange function efficiently, we propose the following augmented Lagrange function. Lx(w, \u03bb) = f0(w) + \u03bb \u22a4 N\u2211 i=1 x(i)\u2207wfi(w) + \u0393 N\u2211 i=1 x(i)fi(w). (3) With the definition, we can obtain the following two propositions directly. Proposition 2. Suppose fi\u2019s are strongly convex functions for i = 1, 2, \u00b7 \u00b7 \u00b7 , N , x(i) \u2265 0 for all i and \u2225x\u22251 = 1. Then, Problem (2) satisfies LICQ and its KKT conditions can be written as follows: \u2207wf0(w) + N\u2211 i=1 x(i)\u22072fi(w)\u03bb = 0 N\u2211 i=1 x(i)\u2207fi(w) = 0. Proposition 3. Suppose fi\u2019s are strongly convex functions for i = 1, 2, \u00b7 \u00b7 \u00b7 , N , x(i) \u2265 0 for all i and \u2225x\u22251 = 1. Then, the stationary point of minw max\u03bb Lx(w, \u03bb) is unique and satisfies the KKT conditions of problem (2). Let (w\u0302\u2217(x), \u03bb\u2217(x)) be the stationary point of minw max\u03bb Lx(w, \u03bb). From proposition 2, it holds that w\u0302\u2217(x) = w\u2217(x) and \u2202f0(w \u2217(x)) \u2202x(i) = \u03bb\u2217(x) \u22a4\u2207wfi(w\u2217(x)). (4) Thus, with the KKT poiont w\u2217(x) and \u03bb\u2217(x), we can estimate the gradient of x without estimating the inverse of Hessian. However, as \u03bb\u22a4 \u2211N i=1 x (i)\u2207wfi(w) can be highly non-convex function, which can be harmful to the optimization process. We add an additional constraint on the norm of \u03bb and define the constraint set ad \u039b. Thus, the problem (2) becomes min w max \u03bb\u2208\u039b Lx(w, \u03bb) = f0(w) + \u03bb \u22a4 N\u2211 i=1 x(i)\u2207wfi(w) + \u0393 N\u2211 i=1 x(i)fi(w). (5) We propose a double loop algorithm for solving problem (1). We show the algorithm in the Algorithm 1 and 2. In inner loop, we solve the augmented Lagrange for K steps. In each step, local client will receive the iterates wt,k and \u03bbt,k. After that, each local client will calculate \u2207\u0303fi(wt,k) and \u2207\u0302fi(wt,k) based on the back propagation through two independent batches. The term \u2207\u03032fi(wt,k)\u03bbt,k is calculated with auto-differentiable framework (i.e. Pytorch, TensorFlow) or with the closed-form multiplication. Then the local device sends gradient estimation \u2207\u0303wfi(wt,k) and the estimated product of Hessian and \u03bb (\u2207\u03032fi(wt,k)\u03bbt,k) to the server. For the server, in each step, the server will first send the primal variable (wt,k) and dual variable (\u03bbt, k) to all local clients. Then, the server will receive the estimated gradients and estimated product from some local clients. Because not all devices will stay online in each step, we define a set Activet,k which records the clients that participant the optimization in (t, k) step. With the vectors collected from local clients, the server will calculate the gradient estimator of wt,k and \u03bbt,k with respect to function Lxt(wt,k, \u03bbt,k). And then, wt,k will be updated by a gradient descent step and \u03bbt,k will be updated by a gradient ascent step. Different from local devices, after K inner loop update steps, based on the \u03bbt,K and gradient estimated in each local client, the server will calculate the gradient of x based on equation 4 and perform a projected gradient descent step on x. In addition, if the ith agent is not in Activet,K , we set the gradient of x(i) to be zero. Algorithm 1 The bi-level primal dual algorithm on local device i 1: for t = 1, 2, \u00b7 \u00b7 \u00b7 , T do 2: for k = 1, 2, \u00b7 \u00b7 \u00b7 ,K do 3: Receive wt,k, \u03bbt,k from the server; 4: Sample a mini-batch and calculate \u2207\u0303fi(wt,k); 5: Sample a mini-batch and calculate \u2207\u0302fi(wt,k); 6: Calculate \u2207\u03032wfi(wt,k)\u03bbt,k with back propagation on scalar \u2207\u0302wf(wt,k)\u03bbt,k; 7: Send \u2207\u03032fi(wt,k)\u03bbt and \u2207\u0303fi(wt,k) to the server; 8: end for 9: end for 4 THEORETICAL ANALYSIS: In this section, we analyze the convergence property of the proposed algorithm. First, we state some assumptions used in the analysis. (A1) f0, f1, \u00b7 \u00b7 \u00b7 , fN are lower bounded by f with L1 Lipschitz gradient . (A2) f1, \u00b7 \u00b7 \u00b7 , fN are \u00b5-strongly convex functions. (A3) f1, \u00b7 \u00b7 \u00b7 , fN has L2 Lipschitz Hessian. Algorithm 2 The Bi-level primal dual algorithm on the Server 1: Input: Initial x1, w1,1, \u03bb1,1, total iterations: K, T and step size: \u03b7w, \u03b7\u03bb, \u03b7x. 2: for t = 1, 2, \u00b7 \u00b7 \u00b7 , T do 3: for k = 1, 2, \u00b7 \u00b7 \u00b7 ,K do 4: Send wt,k, \u03bbt,k to each local device; 5: Receive \u2207\u0303fi(wt,k) and \u2207\u03032wfi(wt,k)\u03bbt from Activet,k; 6: gw = \u2207\u0303f0(wt,k) + N|Activet,k| \u2211 i\u2208Activet,k x (i) t \u2207\u03032wfi(wt,k)\u03bbt,k + \u0393\u2207\u0303fi(wt,k); 7: wt,k+1 = wt,k \u2212 \u03b7wgw; 8: g\u03bb = N |Activet,k| (\u2211 i\u2208Activet,k x (i) t \u2207\u0303wfi(wt,k) ) ; 9: \u03bbt,k+1 = \u03a0\u039b (\u03bbt,k + \u03b7\u03bbg\u03bb); 10: end for 11: gx(i) = N |Activet,K |\u03bb \u22a4 t,K\u2207\u0303wfi(wt,K) for i \u2208 Activet,K ; 12: gx(i) = 0 for i /\u2208 Activet,K ; 13: xt+1 = PX (xt \u2212 \u03b7xgx); 14: \u03bbt+1,1 = \u03bbt,K+1; 15: wt+1,1 = wt,K+1 16: end for 17: Output: xT , WT,K+1. (A4) Assume maxi\u2208{0,1,\u00b7\u00b7\u00b7 ,N} maxx\u2208X \u2225\u2207fi(w\u2217(x))\u2225 \u2264 Dw. Then, D\u03bb = maxx\u2208X \u2225\u03bb\u2217(x)\u2225 is upper bounded by Dw/\u00b5. (A5) Each local estimation is unbiased with bounded variance \u03c32. (A6) Activet,k is independent and sampled from the set of nonempty subset of {1, 2, \u00b7 \u00b7 \u00b7 , N}, where P (i \u2208 Activet,k) = p for all i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , N}. Remark 1. (A1),(A2),(A3) are commonly used in the convergence analysis for bi-level optimization problems (Ji et al., 2021; Chen et al., 2021; Khanduri et al., 2021). Unlike Ji et al. (2021); Chen et al. (2021), where they need to assume f0, f1, \u00b7 \u00b7 \u00b7 , fN to be L0 Lipschitz, we assume the gradient norm are bounded at optimal solution. Because for machine learning models, regularization will be add into objective function, makes the norm of the optimal solution not be large. When w\u2217(x) can be bounded by some constant. (A4) is reasonable in practice. Moreover, the Lipschitz assumption on function can directly infer (A4) with Dw = L0. (A5) is a common assumption used for stochastic gradient methods Ghadimi et al. (2016) and (A6) extend the assumption in Karimireddy et al. (2020) by giving the probability that a local devices will be chosen instead of uniformly sampling. Proposition 4. When \u039b = {\u03bb | \u2225\u03bb\u2225 \u2264 D\u03bb}, the stationary point of problem (5) is the KKT point of problem 2. With proposition 3 and 4, the stationary point of problem (5) is unique and we denote the stationary point as (w\u2217(x), \u03bb\u2217(x)). To give the convergence of the whole algorithm, firstly, we give the convergence guarantee for the inner loop. Theorem 1. For given x \u2208 X , when (A1) to (A6) holds, \u0393 > D\u03bbL2+L1\u00b5 and \u03b7w, \u03b7\u03bb = \u0398(1/ \u221a K), when randomly choose k\u0302 \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K} with equal probability it holds that E [\u2225\u2225\u2225\u2225\u03bb\u22a4k\u0302 \u2207wfi(wk\u0302)\u2212 \u2202f0(w\u2217(x))\u2202x(i) \u2225\u2225\u2225\u22252 ] = O(1/ \u221a K). Thus, with theorem1, the gradient of x can be \u201cwell\u201d estimated through the inner gradient descent ascent method when the number of inner loop steps is large enough. Then, We can obtain the following convergence result of the outer loop. Theorem 2. Suppose (A1) to (A6) holds, \u0393 > D\u03bbL2+L1\u00b5 , \u03b7w, \u03b7\u03bb = \u0398(1/ \u221a K), \u03b7x = \u0398(1/ \u221a T ) and randomly choosing k\u0302 \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K} with equal probability to approximate gradient of x. Define x\u0302 = argminy\u2208X (f0(w \u2217(y)) + \u03c12\u2225y \u2212 x\u2225 2) and \u2207\u0304\u03c1f0(w\u2217(x)) = \u03c1(x\u2212 x\u0302) for large \u03c1,it holds that 1 T T\u2211 t=1 E\u2225\u2207\u0304\u03c1f0(w\u2217(xt))\u22252 = O(1/ \u221a T + 1/ \u221a K). Remark 2. To achieve \u03f5-stationary point (E\u2225\u2207\u0304\u03c1f0(w\u2217(xt))\u22252 \u2264 \u03f5), O(1/\u03f54) samples are needed in each local client and in the server. Different from the previous bilevel works (e.g. Ghadimi & Wang (2018), Khanduri et al. (2021) and Franceschi et al. (2017)), we prove the convergence when optimization variable x has a convex constraint. 4.1 PROOF SKETCH OF THEOREM 1: To show the convergence of inner loop, we first construct a potential function for inner loop objective. Define \u03a6x(w, \u03bb) = Lx(w, \u03bb)\u2212 2d(\u03bb), where d(\u03bb) = minwLx(w, \u03bb) for given x. The intuition of defining this potential function is that Lx(w, \u03bb) is not necessarily decreasing in each iteration, as \u03bb is performing a gradient ascent step. Meanwhile, gradient \u03bb taken is an approximation of gradient of d(\u03bb). Thus, by subtracting d(\u03bb), we can obtain that \u03a6 will decrease during iterations. Therefore, the first thing is to show the lower bound of function \u03a6. Lemma 1 (Lower bound of \u03a6). Suppose (A1)-(A4) hold. It holds that \u03a6x(w, \u03bb) is bounded below by f . The proof of this lemma is basically due to the definition of \u03a6x(w, \u03bb) and d(\u03bb). Then, similar to the proof of gradient descent, we give a lemma that shows the descent of potential function under certain choices of hyperparameters. Lemma 2 (Potential function descent, proof can be found in Lemma 11 in Appendix). Suppose (A1)-(A6) hold. In addition, we assume \u0393 > D\u03bbL2+L1\u00b5 , it holds that E[\u03a6x(wt,k, \u03bbt,k)\u2212\u03a6x(wt,k+1, \u03bbt,k+1)] \u2264 \u2212C1E\u2225\u2207wLx(wt,k, \u03bbt,k)\u22252\u2212C2E[\u2225\u03bbt\u2212\u03bb\u2217t \u22252]+C3\u03c32, where \u03bb+t = \u03a0\u039b(\u03bbt+\u03b7\u03bb\u2207d(\u03bbt)), C1 = \u0398(\u03b7w\u2212\u03b72w\u2212\u03b72\u03bb\u2212\u03b7\u03bb), C2 = \u0398(\u03b7\u03bb) and C3 = O(\u03b72w+\u03b72\u03bb) Thus, when choosing sufficient small \u03b7w and \u03b7\u03bb, we can achieve positive C1 and C2. Together with the lower bound of the function \u03a6, the convergence of the inner algorithm can be shown. Because of the uniqueness of the KKT point, by choosing \u03b7w and \u03b7\u03bb in order of 1/ \u221a K, it can be shown that 1 K K\u2211 k=1 E\u2225wt,k \u2212 w\u2217(xt)\u22252 = O(1/ \u221a K), 1 K K\u2211 k=1 E\u2225\u03bbt,k \u2212 w\u2217(xt)\u22252 = O(1/ \u221a K). Therefore, with the convergence rate of wt,k and \u03bbt,k and equation 4, we can easily prove theorem 1. 4.2 PROOF SKETCH OF THEOREM 2: To apply stochastic gradient descent analysis on x, although we have smoothness for function f0, f1, \u00b7 \u00b7 \u00b7 , fN on w, we need to verify the smoothness of f0(w\u2217(x)) with respect to x. Lemma 3 (Convergence of stochastic gradient descent with biased gradient estimation, proof can be found in Lemma 14 in Appendix). Suppose function f(x) is lower bounded by f with L-Lipshitz gradient. g(x) is an unbiased gradient estimator of \u2207f(x) satisfying that expected norm of g(x) are bounded by G in domain X for function f . Then with update rule xt+1 = \u03a0X (xt \u2212 \u03b7x(g(xt) + \u03bet)), where \u03b7x = \u0398(1/ \u221a T ), X is a convex set and E\u2225\u03bet\u22252 \u2264 \u03f52. By defining x\u0302 = argminy\u2208X (f(y) + \u03c1 2\u2225y \u2212 x\u2225 2) and \u2207\u0304\u03c1f(x) = \u03c1(x\u2212 x\u0302), where \u03c1 = 2L, then it holds that 1 T E T\u2211 t=1 \u2225\u2207\u0304\u03c1f(xt)\u22252 = O(1/ \u221a T + \u03f52). As Lemma 3 suggests, when f0(w\u2217(x)) satisfying L-Lipschitz gradient, bounded estimation error and bounded gradient norm, the convergence rate can achieve O(1/ \u221a T ) with a error term related to estimation error. Theorem 1 shows the estimation error can be bounded by O(1/ \u221a K). Combining this two results we can prove Theorem 2. 5 EXPERIMENTAL RESULTS: In this section, we compare our algorithm with other bi-level optimization algorithms (BSA (Ghadimi & Wang, 2018), SUSTAIN (Khanduri et al., 2021) and RFHO (Franceschi et al., 2017)) in two cases: the toy example and two vision tasks. 5.1 TOY EXAMPLE: In this section, we apply algorithms to solve problem (1) with fi as follows: fi(w) = 1 2 \u2225Aiw \u2212Bi\u22252 + cos(a\u22a4i w \u2212 bi), where Ai \u2208 R30\u00d720, Bi \u2208 R30, ai \u2208 R20 and bi \u2208 R are all generated from Gaussian distribution. The variance of each component in Ai and ai is 1/ \u221a 20 and the variance of each component in Bi is 1/ \u221a 30 and variance of bi is 1. When generated function fi is not 0.1-strongly convex, we randomly generate a new one until we get strongly convex fi whose modular is not less than 0.1. Three local steps (K=1,5,10) are tested. Here, the local steps are used for w update for algorithm BSA, RFHO, and our algorithm, and the local steps are used for Hessian estimation for algorithm BSA and SUSTAIN. Because for this toy example, we can easily compute the Hessian matrix and its inverse, we test the algorithm using the inverse of estimated Hessian to compute the gradient of x named GD. We test two settings of the toy example. One is the deterministic setting, where no estimation noise or client disconnection will occur. In the other setting, we add white Gaussian noise with a noise level of 0.5 in each estimation (including gradient estimation and Hessian estimation). Also, each client has a 0.5 probability of connecting with the server. To evaluate the performance of different algorithms, we calculate the function value of f0(w\u2217(x)) and the stationary of x, i.e. x\u2212\u03a0X(x\u2212 0.001\u2207xf0(w\u2217(x))), where w\u2217(x) is approximate by 200 gradient steps. We take N = 15 and run 20 times and get the results of different algorithms. The results of deterministic setting are shown in Figure 1, and results of noise setting are shown in Figure 2. As it is shown in Figure 1, with local steps getting larger and larger, the performance of BSA, RFHO, and SUSTAIN is getting close to GD, while the performance of the primal-dual method is similar to GD whatever local step used in the algorithm even with only one single step. When noise is added in the Hessian, directly inverse may cause the biased estimation. Thus, the performance of GD gets much worse than it in the deterministic setting shown in Figure 2. Also, in Figure 2, our algorithm can perform better than other algorithms when the local step is small. When local steps increase to 10, BSA and our algorithm have competitive results. 5.2 VISION TASKS: In this section, we apply algorithms to train LeNet5(LeCun et al., 1998) on dataset MNIST(LeCun et al., 1998) and Fashion-MNIST(Xiao et al., 2017). To construct non-iid datasets on different local clients and the global server\u2019s validation set, we randomly pick 20 samples per label out of the whole training dataset and form the validation set. Then, the rest of the training data are divided into 3 sets, and each set will be assigned to a local client. The first client contains samples labeled as 0,1,2,3,4, the second client contains samples labeled as 5,6,7, and the third client contains samples labeled as 8,9 for all two datasets. To test the algorithm\u2019s ability to choose the proper coefficient of local clients, we add 7 noise nodes containing 5000 samples with random labels. We set the learning rate of w to be a constant learning rate without any decay selected from {0.1, 0.01, 0.001} for all training methods, and the learning rate of x is selected from {0.1, 0.01, 0.001, 0.0001}. The batch size for all three training cases is set to 64. \u0393 used in the proposed algorithm is set to be 1. For simplicity, we set the local step as 1. We run 2000 iterations for MNIST and 6000 iterations for Fashion-MNIST. Active probability is set in {0.5, 0.9, 1}. We compare the test accuracy among different methods. As a baseline, we report the test accuracy for training with the validation set only named val, training with the average loss of each client named avg, and training with x = (0.5, 0.3, 0.2, 0, \u00b7 \u00b7 \u00b7 , 0) named opt. All experiments run on V100 with Pytorch (Paszke et al., 2019). Results are shown in Figure 3, Figure 4 and Table 1. Figure 3 shows the test accuracy of the MNIST dataset with different active probabilities. Although SUSTAIN works better than the primal-dual algorithm when all local devices participant in the optimization process, when clients\u2019 participant rate decreases to 0.5, SUSTAIN works worse than ours method. Primal-dual become slower than SUSTAIN may be because of the initialization of the dual variable. When the dual variable is far from its real value it needs more time to get a good enough point. Other than SUSTAIN, our algorithm can converge faster and more stable to a high accuracy point. Further, we list the output of x and standard error of test accuracy for 5 different experiments for different algorithms in Table 1. According to Table 1, our algorithm can achieve a more stable output with respect to x, and the output x is more close to 0.5, 0.3, 0.2, which is related to the number of labels the first three clients holds. Figure 4 gives the test accuracy of training LeNet 5 on the Fashion-MNIST Dataset. Similar to the results of the MNIST dataset, when the clients\u2019 participant is high (0.9,1), SUSTAIN works slightly better than the primal-dual algorithm. But when more local devices disconnect to the server, the performance of SUSTAIN drops, while the primal dual algorithm remains fast convergence speed and high test accuracy. 6 CONCLUSION: In this paper, we proposed a primal-dual-based method for solving a bi-level optimization problem based on a federated learning tasking (local coefficient learning). We give a theoretical analysis that shows the convergence of the proposed algorithm. Though the analysis shows it needs more iterations for the algorithm to converge to an \u03f5-stationary point, it works well with a pretty small number of local steps in both toy case and neural network training. Other than that convergence rate can be improved (perhaps it should be in the order of O(1/ \u221a T ) instead of O(1/ \u221a T + 1/ \u221a K)), the initialization of dual variable affects the speed for convergence, which we leave as the future work. A CONVERGENCE OF INNER LOOP: For simplicity, in this section we simplified the inner loop problem as the following: min w g(w) s.t.\u2207h(w) = 0. Besides, the algorithm for solving the inner loop problem can be simplified as wt+1 = wt \u2212 \u03b7w ( \u2207\u0303g(wt) + \u2207\u03032h(wt)\u03bbt + \u0393\u2207\u0303h(wt) ) = wt \u2212 \u03b7w\u2207\u0303L(wt, \u03bbt) \u03bbt+1 = \u03a0\u039b ( \u03bbt + \u03b7\u03bb\u2207\u0303h(wt) ) Furthermore, the assumptions are as the following: (A1) g and h are differentiable strongly convex function with modular \u00b5, L1-Lipschitz gradient and lower bounded by f . (A2) h has L2-Lipschitz Hessian. (A3) \u039b = {\u03bb|\u2225\u03bb\u2225 \u2264 D\u03bb}, where D \u2265 \u2225\u22072h(w\u2217)\u22121\u2207g(w\u2217)\u2225 and w\u2217 = argminh(w). (A4) Given wt, \u2207\u0303g(wt), \u2207\u03032h(wt), \u2207\u0303h(wt) and \u2207\u0302h(wt) are independent to each other. Besides, all of them are unbiased estimators with bounded variance with respect to mean value and a bounded constant \u03c3, i.e. E [ \u2207\u0303g(wt) | wt ] = \u2207g(wt), E [\u2225\u2225\u2225\u2207\u0303g(wt)\u2212\u2207g(wt)\u2225\u2225\u22252 | wt] \u2264 p\u2225\u2207g(wt)\u22252 + \u03c32; E [ \u2207\u03032h(wt) | wt ] = \u22072h(wt), E [\u2225\u2225\u2225\u2207\u03032h(wt)\u2212\u22072h(wt)\u2225\u2225\u22252 F | wt ] \u2264 p\u2225\u22072h(wt)\u22252 + \u03c32; E [ \u2207\u0303h(wt) | wt ] = \u2207h(wt), E [\u2225\u2225\u2225\u2207\u0303h(wt)\u2212\u2207h(wt)\u2225\u2225\u22252 | wt] \u2264 p\u2225\u2207h(wt)\u22252 + \u03c32. Thus, it is easy to show E [ \u2207\u0303wL(wt, \u03bbt) | wt, \u03bbt ] = \u2207wL(wt, \u03bbt), and E [\u2225\u2225\u2225\u2207\u0303wL(wt, \u03bbt)\u2212\u2207wL(wt, \u03bbt)\u2225\u2225\u22252 | wt, \u03bbt] \u2264 (1+\u03932+D2\u03bb)(p\u2225\u2207wL(wt, \u03bbt)\u22252+\u03c32). First, we give some notations that will be used in this section. Definition 1. Let L(w, \u03bb) = g(w) + \u03bb\u22a4\u2207h(w) + \u0393h(w), d(\u03bb) = minwL(w, \u03bb), w\u2217(\u03bb) = argminw L(w, \u03bb), \u03bb+t = \u03a0\u039b(\u03bbt + \u03b7\u03bb\u2207h(wt))) and LL = (1 + \u0393)L1 +D\u03bbL2. Define potential function \u03a6(wt, \u03bbt) = L(wt, \u03bbt)\u2212 2d(\u03bbt). Lemma 4 (Descent of Lagrange function). For the function L, it holds that E [L(wt+1, \u03bbt+1)\u2212 L(wt, \u03bbt)] \u2264 E [ C0(\u03b7w)\u2225\u2207wL(wt, \u03bbt)\u22252 + LL\u03b7 2 w(1 + \u0393 2 +D2\u03bb)\u03c3 2 2 + (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1)) ] , where C0(\u03b7w) = LL\u03b7 2 w 2 + LL\u03b7 2 wp(1+\u0393 2+D2\u03bb) 2 \u2212 \u03b7w. Proof. Because L(w, \u03bb) has LL Lipschitz gradient, it holds that L(wt+1, \u03bbt) \u2264 L(wt, \u03bbt) + \u27e8\u2207wL(wt, \u03bbt), wt+1 \u2212 wt\u27e9+ LL 2 \u2225wt+1 \u2212 wt\u22252. By taking expectations with respect to wt and \u03bbt on both side of the above inequality, we obtain that E [L(wt+1, \u03bbt)\u2212 L(wt, \u03bbt) | wt, \u03bbt] \u2264 E [ \u27e8\u2207wL(wt, \u03bbt), wt+1 \u2212 wt\u27e9+ LL 2 \u2225wt+1 \u2212 wt\u22252 | wt, \u03bbt ] = E [\u2329 \u2207wL(wt, \u03bbt),\u2212\u03b7w\u2207\u0303wL(wt, \u03bbt) \u232a + LL\u03b7 2 w 2 \u2225\u2207wL(wt, \u03bbt)\u22252 + LL\u03b7 2 w 2 \u2225\u2207\u0303wL(wt, \u03bbt)\u2212\u2207wL(wt, \u03bbt)\u22252 | wt, \u03bbt ] = E [( LL\u03b7 2 w 2 + LL\u03b7 2 wp(1 + \u0393 2 +D2\u03bb) 2 \u2212 \u03b7w ) \u2225\u2207wL(wt, \u03bbt)\u22252 + LL\u03b7 2 w(1 + \u0393 2 +D2\u03bb)\u03c3 2 2 | wt, \u03bbt ] . (6) Meanwhile, it holds that E [L(wt+1, \u03bbt+1)\u2212 L(wt+1, \u03bbt) | wt, \u03bbt] = E [ (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1)|wt, \u03bbt ] (7) Combining (6), (7) and taking expectation on the conditional expectation, we can obtain desired result. Lemma 5. Suppose \u0393\u00b5 > D\u03bbL2 + L1, it holds that \u2225w\u2217(\u03bb1)\u2212 w\u2217(\u03bb2)\u2225 \u2264 \u03b21\u2225\u03bb1 \u2212 \u03bb2\u2225 for all \u03bb1, \u03bb2 \u2208 \u039b, where \u03b21 = L1\u0393\u00b5\u2212D\u03bbL2\u2212L1 . Proof. Note that L(w, \u03bb) is a strongly convex function with respect to the w in the domain \u03bb \u2208 \u039b with the modular (1 + \u0393)\u00b5\u2212D\u03bbL2. Thus, it holds that L(w\u2217(\u03bb1), \u03bb2)\u2212 L(w\u2217(\u03bb2), \u03bb2) \u2265 \u0393\u00b5\u2212D\u03bbL2 \u2212 L1 2 \u2225w\u2217(\u03bb1)\u2212 w\u2217(\u03bb2)\u22252. On the other hand, we have L(w\u2217(\u03bb1), \u03bb2)\u2212 L(w\u2217(\u03bb2).\u03bb2) = L(w\u2217(\u03bb1), \u03bb2)\u2212 L(w\u2217(\u03bb1), \u03bb1) + L(w\u2217(\u03bb1), \u03bb1)\u2212 L(w\u2217(\u03bb2), \u03bb1) + L(w\u2217(\u03bb2), \u03bb1)\u2212 L(w\u2217(\u03bb2), \u03bb2) \u2264 (\u03bb1 \u2212 \u03bb2)\u22a4(\u2207h(w\u2217(\u03bb2))\u2212\u2207h(w\u2217(\u03bb1)))\u2212 \u0393\u00b5\u2212D\u03bbL2 \u2212 L1 2 \u2225w\u2217(\u03bb1)\u2212 w\u2217(\u03bb2)\u22252. Thus, by combining the above two inequalities, with Cauchy-Schwarz inequality, we can obtain (\u0393\u00b5\u2212D\u03bbL2 \u2212 L1)\u2225w\u2217(\u03bb1)\u2212 w\u2217(\u03bb2)\u22252 \u2264 \u2225\u03bb1 \u2212 \u03bb2\u2225\u2225\u2207h(w\u2217(\u03bb1))\u2212\u2207h(w\u2217(\u03bb2))\u2225 \u2264 L1\u2225\u03bb1 \u2212 \u03bb2\u2225\u2225w\u2217(\u03bb1)\u2212 w\u2217(\u03bb2)\u2225. Hence, we get the desired result. Lemma 6 (Ascent of dual function). It holds that E [d(\u03bbt+1)\u2212 d(\u03bbt)] \u2265 E [ \u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(w\u2217(\u03bbt))\u27e9 \u2212 Ld 2 \u2225\u03bbt+1 \u2212 \u03bbt\u22252 ] . Proof. It can be calculated by the implicit function theorem that \u2207\u03bbd(\u03bb) = \u2207h(w\u2217(\u03bb)). Thus, we can obtain that for all \u03bb1, \u03bb2 \u2208 \u039b \u2225\u2207d(\u03bb1)\u2212\u2207d(\u03bb2)\u2225 = \u2225\u2207h(w\u2217(\u03bb1))\u2212\u2207h(w\u2217(\u03bb2))\u2225 \u2264 L1\u03b21\u2225\u03bb1 \u2212 \u03bb2\u2225. Therefore, d(\u03bb) is a differentiable function with Ld = L1\u03b21-Lipschitz gradient. With the definition of d(\u03bb), we have E [d(\u03bbt+1)\u2212 d(\u03bbt)] \u2265 E [ \u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(w\u2217(\u03bbt))\u27e9 \u2212 Ld 2 \u2225\u03bbt+1 \u2212 \u03bbt\u22252 ] Lemma 7. With the strongly convexity of L and h, it holds that \u2225w\u2217(\u03bbt)\u2212 w\u2217\u2225 \u2264 1 \u00b5 \u2225\u2207h(w\u2217(\u03bb))\u2225, and \u2225wt \u2212 w\u2217(\u03bbt)\u2225 \u2264 1 \u0393\u00b5\u2212D\u03bbL2 \u2212 L1 \u2225\u2207wL(wt, \u03bbt)\u2225 Proof. Because of the \u00b5-strongly convexity of h the inequality \u00b5\u2225w1\u2212w2\u2225 \u2264 \u2225\u2207h(w1)\u2212\u2207h(w2)\u2225 holds for all w1, w2. With the \u2207h(w\u2217) = 0, we get the result. Similar to h, because of \u0393\u00b5 \u2212 D\u03bbL2 \u2212 L1-strongly convexity of L, we can prove the second inequality. Lemma 8 (Descent of one step local SGD). It holds that E\u2225wt \u2212 wt+1\u22252 \u2264 E [ \u03b72w(1 + p)\u2225\u2207wL(wt, \u03bbt)\u22252 + \u03b72w\u03c32 ] Proof. E\u2225wt \u2212 wt+1\u22252 = E [ \u03b72w\u2225\u2207\u0303wL(wt, \u03bbt)\u22252 ] \u2264 E [ \u03b72w(1 + p)\u2225\u2207wL(wt, \u03bbt)\u22252 + \u03b72w\u03c32 ] Lemma 9. d(\u03bb) is a \u00b5 2 LL -strongly concave funtion. We define \u00b5d = \u00b5 2 LL . Proof. Let \u03b30 be the largest eigenvalue of \u22072d(\u03bb), \u03b31 be the largest eigenvalue of \u2202w \u2217(\u03bb) \u2202\u03bb . Then, it holds that \u03b3 \u2264 \u03b31\u00b5. Meanwhile, \u2202w \u2217(\u03bb) \u2202\u03bb = \u2212\u2207 2 wL(w \u2217(\u03bb), \u03bb)\u22121\u22072wh(w\u2217(\u03bb)). Thus, \u03b31 \u2264 \u2212 \u00b5 LL . Therefore, d(\u03bb) is a \u00b5 2 LL -strongly concave funtion. Lemma 10. It holds that E\u2225\u03bbt+1\u2212\u03bbt\u22252 \u2265 \u00b52d\u03b7 2 \u03bb 16 E\u2225\u03bbt\u2212\u03bb\u2217\u22252\u2212 \u00b5d\u03b7\u03bb 4 ( \u03b72\u03bb\u03c3 2 + ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) E\u2225wt \u2212 w\u2217(\u03bbt)\u22252 ) Proof. With the update rule of \u03bbt+1, it holds that E\u2225\u03bbt+1 \u2212 \u03bb\u2217\u22252 = E\u2225\u03a0\u039b(\u03bbt + \u03b7\u03bb\u2207h(wt))\u2212 \u03bb\u2217\u22252 \u2264 E\u2225\u03bbt + \u03b7\u03bb \u02dc\u2207h(wt)\u2212 \u03bb\u2217\u22252 = E [ \u2225\u03bbt \u2212 \u03bb\u2217\u22252 + 2\u03b7\u03bb\u27e8\u03bbt \u2212 \u03bb\u2217, \u2207\u0303h(wt)\u27e9+ \u03b72\u03bb\u2225\u2207\u0303h(wt)\u22252 ] = E [ \u2225\u03bbt \u2212 \u03bb\u2217\u22252 + 2\u03b7\u03bb\u27e8\u03bbt \u2212 \u03bb\u2217,\u2207h(w\u2217(\u03bbt))\u27e9 +2\u03b7\u03bb\u27e8\u03bbt \u2212 \u03bb\u2217,\u2207h(wt)\u2212\u2207h(w\u2217(\u03bbt))\u27e9+ \u03b72\u03bb\u2225\u2207\u0303h(wt)\u22252 ] , (8) where the last equality is because \u2207\u0303h(wt) is an unbiased estimator. Meanwhile, because d(\u03bb) is a strongly concave function, it holds that \u03b7\u03bb\u27e8\u03bbt \u2212 \u03bb\u2217,\u2207h(w\u2217(\u03bbt))\u27e9 \u2264 \u2212\u03b7\u03bb\u00b5d\u2225\u03bbt \u2212 \u03bb\u2217\u22252 (9) Further, it holds that E|\u2207\u0303h(wt)\u22252 = E\u2225\u2207\u0303h(wt) +\u2207h(wt)\u2212\u2207h(wt)\u22252 \u2264 \u03c32 + E(1 + p)\u2225\u2207h(wt)\u22252 \u2264 \u03c32 + E2(1 + p)\u2225\u2207h(w\u2217(\u03bbt)))\u22252 + 2(1 + p)\u2225\u2207h(w\u2217(\u03bbt)\u2212\u2207h(wt)\u22252 \u2264 \u03c32 + E2(1 + p)\u2225\u2207h(w\u2217(\u03bbt)))\u22252 + 2(1 + p)L21\u2225w\u2217(\u03bbt \u2212 wt\u22252 \u2264 \u03c32 + E2(1 + p)L2d\u2225\u03bbt \u2212 \u03bb\u2217\u22252 + 2(1 + p)L21\u2225w\u2217(\u03bbt)\u2212 wt\u22252 (10) For \u27e8\u03bbt \u2212 \u03bb\u2217,\u2207h(wt)\u2212\u2207h(w\u2217(\u03bbt))\u27e9, it holds that \u27e8\u03bbt \u2212 \u03bb\u2217,\u2207h(wt)\u2212\u2207h(w\u2217(\u03bbt))\u27e9 \u2264 \u00b5d 2 \u2225\u03bbt \u2212 \u03bb\u2217\u22252 + 1 2\u00b5d \u2225\u2207h(wt)\u2212\u2207h(w\u2217(\u03bbt)\u22252 \u2264 \u00b5d 2 \u2225\u03bbt \u2212 \u03bb\u2217\u22252 + L21 2\u00b5d \u2225wt \u2212 w\u2217(\u03bbt)\u22252 (11) Combining (8), (9), (10) and (11), it holds that E\u2225\u03bbt+1 \u2212 \u03bb\u2217\u22252 \u2264 (1\u2212 \u00b5d\u03b7\u03bb + 2(1 + p)L2d\u03b72\u03bb)E\u2225\u03bbt \u2212 \u03bb\u2217\u22252 + \u03b72\u03c32 + ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) E\u2225wt \u2212 w\u2217(\u03bbt)\u22252 When \u03b7\u03bb \u2264 \u00b5d4(1+p)L2d , it holds that E\u2225\u03bbt+1\u2212\u03bb\u2217\u22252 \u2264 (1\u2212 \u00b5d\u03b7\u03bb 2 )E\u2225\u03bb\u2217\u2212\u03bbt\u22252+\u03b72\u03bb\u03c32+ ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) E\u2225wt\u2212w\u2217(\u03bbt)\u22252. (12) It holds that E\u2225\u03bbt+1 \u2212 \u03bbt\u22252 = E\u2225\u03bbt+1 \u2212 \u03bb\u2217 + \u03bb\u2217 \u2212 \u03bbt\u22252 = E [ \u2225\u03bbt+1 \u2212 \u03bb\u2217\u22252 + \u2225\u03bb\u2217 \u2212 \u03bbt\u22252 + 2\u27e8\u03bbt+1 \u2212 \u03bb\u2217, \u03bb\u2217 \u2212 \u03bbt\u27e9 ] \u2265 E [ (1\u2212 \u03be)\u2225\u03bbt+1 \u2212 \u03bb\u2217\u22252 + ( 1\u2212 1 \u03be ) \u2225\u03bb\u2217 \u2212 \u03bbt\u22252 ] ,\u2200\u03be > 1 (13) Let \u03be = 1+ \u00b5d\u03b7\u03bb/4, (1\u2212 \u03be) ( 1\u2212 \u00b5d\u03b7\u03bb2 ) + ( 1\u2212 1\u03be ) \u2265 \u00b52d\u03b72\u03bb/16. combining (12) and (13), it holds that E\u2225\u03bbt+1 \u2212 \u03bbt\u22252 \u2265 ( (1\u2212 \u03be) ( 1\u2212 \u00b5d\u03b7\u03bb 2 ) + ( 1\u2212 1 \u03be )) \u2225\u03bb\u2217 \u2212 \u03bbt\u22252 + (1\u2212 \u03be) ( \u03b72\u03bb\u03c3 2 + ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) E\u2225wt \u2212 w\u2217(\u03bbt)\u22252 ) \u2265 \u00b5 2 d\u03b7 2 \u03bb 16 E\u2225\u03bbt \u2212 \u03bb\u2217\u22252 \u2212 \u00b5d\u03b7\u03bb 4 ( \u03b72\u03bb\u03c3 2 + ( 2\u03b72\u03bb(1 + p)L 2 1 \u0393\u00b5\u2212D\u03bbL2 \u2212 L1 + L21\u03b7\u03bb \u00b5d\u0393\u00b5\u2212D\u03bbL2 \u2212 L1 ) E\u2225wt \u2212 w\u2217(\u03bbt)\u22252 ) Thus, we get the desired result. Define a potential function \u03a6(wt, \u03bbt) = L(wt, \u03bbt) \u2212 2d(\u03bbt). According to the definition we have L(wt, \u03bbt) > d(\u03bbt). Then, \u03a6(wt, \u03bbt) \u2265 \u2212d(\u03bbt) \u2265 \u2212minw g(w) + \u0393h(w) \u2265 \u2212(1 + \u0393)f for all wt and \u03bbt \u2208 \u039b. Lemma 11 (Descent of potential function). It holds that E [\u03a6(wt+1, \u03bbt+1)\u2212 \u03a6(wt, \u03bbt)] \u2264 E [ C1\u2225\u2207wL(wt, \u03bbt)\u22252 + C2\u2225\u03bbt \u2212 \u03bb\u2217t \u22252 ] + C3 where C1 = \u2212\u03b7w + LL\u03b7 2 w 2 + LL\u03b7 2 wp(1 + \u0393 2 +D2\u03bb) 2 + 4L21\u03b7\u03bb\u03b7 2 w(1 + p) + 4L21\u03b7\u03bb (\u0393\u00b5\u2212D\u03bbL2 \u2212 L1)2 + \u00b5d 16 ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) C2 = \u2212 \u03b7\u03bb\u00b5 2 d 64 C3 = LL\u03b7 2 w(1 + \u0393 2 +D2\u03bb)\u03c3 2 2 + 2L21\u03b7\u03bb\u03b7 2 w\u03c3 2 + 2\u03b72\u03bb\u03c3 2 + \u00b5d\u03b7 2 \u03bb\u03c3 2 16 . Proof. With Lemma 4 and Lemma 6, it holds that E [\u03a6(wt+1, \u03bbt+1)\u2212 \u03a6(wt, \u03bbt)] \u2264 E [ C0(\u03b7w)\u2225\u2207wL(wt, \u03bbt)\u22252 + (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1)\u2212 2\u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(w\u2217(\u03bbt))\u27e9+ Ld\u2225\u03bbt+1 \u2212 \u03bbt\u22252 ] + LL\u03b7 2 w(1 + \u0393 2 +D2\u03bb)\u03c3 2 2 . We deal with each term as follows. For the second term and the third term, it holds that (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1)\u2212 2\u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(w\u2217(\u03bbt))\u27e9 = 2\u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(wt+1)\u2212\u2207h(w\u2217(\u03bbt))\u27e9 \u2212 (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1) \u2264 2L1\u2225\u03bbt+1 \u2212 \u03bbt\u2225\u2225wt+1 \u2212 w\u2217(\u03bbt)\u2225 \u2212 1 \u03b7\u03bb \u2225\u03bbt+1 \u2212 \u03bbt\u22252 \u2264 2L1\u2225\u03bbt+1 \u2212 \u03bbt\u2225 (\u2225wt \u2212 w\u2217(\u03bbt))\u2225+ \u2225wt+1 \u2212 wt\u2225)\u2212 1 \u03b7\u03bb \u2225\u03bbt+1 \u2212 \u03bbt\u22252 \u2264 ( 1 2\u03b7\u03bb \u2212 1 \u03b7\u03bb ) \u2225\u03bbt+1 \u2212 \u03bbt\u22252 + 4L21\u03b7\u03bb\u2225wt \u2212 w\u2217(\u03bbt))\u22252 + 4L21\u03b7\u03bb\u2225wt+1 \u2212 wt\u22252 By taking the expectation on the both side of inequality, it holds that E [ (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1)\u2212 2\u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(w\u2217(\u03bbt))\u27e9 ] \u2264 E [ \u2212 1 2\u03b7\u03bb \u2225\u03bbt+1 \u2212 \u03bbt\u22252 + 4L21\u03b7\u03bb\u2225wt \u2212 w\u2217(\u03bbt))\u22252 + 4L21\u03b7\u03bb\u2225wt+1 \u2212 wt\u22252 ] \u2264 E [ \u2212 1 2\u03b7\u03bb \u2225\u03bbt+1 \u2212 \u03bbt\u22252 + 4L21\u03b7\u03bb\u03b72w(1 + p)\u2225\u2207wL(wt, \u03bbt)\u22252 + 4L21\u03b7\u03bb (\u0393\u00b5\u2212D\u03bbL2 \u2212 L1)2 \u2225\u2207wL(wt, \u03bbt)2\u22252 ] + 2L21\u03b7\u03bb\u03b7 2 w\u03c3 2. Meanwhile, with Lemma 10 and Lemma 7 it holds that E\u2225\u03bbt+1 \u2212 \u03bbt\u22252 \u2265 \u00b5 2 d\u03b7 2 \u03bb 16 E\u2225\u03bbt \u2212 \u03bb\u2217\u22252 \u2212 \u00b5d\u03b7\u03bb 4 ( \u03b72\u03bb\u03c3 2 + ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) E\u2225wt \u2212 w\u2217(\u03bbt)\u22252 ) \u2265 \u00b5 2 d\u03b7 2 \u03bb 16 E\u2225\u03bbt \u2212 \u03bb\u2217\u22252 \u2212 \u00b5d\u03b7\u03bb 4 ( \u03b72\u03bb\u03c3 2 + ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) E\u2225\u2207wL(wt, \u03bbt)\u22252 ) Further, when \u03b7\u03bb \u2264 1/(4Ld), it holds that \u2212 12\u03b7\u03bb \u2225\u03bbt+1 \u2212 \u03bbt\u2225 2 + Ld\u2225\u03bbt+1 \u2212 \u03bbt\u22252 \u2264 14\u03b7\u03bb . Thus, it holds that E [\u03a6(wt+1, \u03bbt+1)\u2212 \u03a6(wt, \u03bbt)] \u2264 E [ C0(\u03b7w)\u2225\u2207wL(wt, \u03bbt)\u22252 + (\u03bbt+1 \u2212 \u03bbt)\u22a4\u2207h(wt+1)\u2212 2\u27e8\u03bbt+1 \u2212 \u03bbt,\u2207h(w\u2217(\u03bbt))\u27e9+ Ld\u2225\u03bbt+1 \u2212 \u03bbt\u22252 ] + LL\u03b7 2 w(1 + \u0393 2 +D2\u03bb)\u03c3 2 2 \u2264 E [ C1\u2225\u2207wL(wt, \u03bbt)\u22252 + C2\u2225\u03bbt \u2212 \u03bb\u2217t \u22252 + C3 ] , where C1 = \u2212\u03b7w + LL\u03b7 2 w 2 + LL\u03b7 2 wp(1 + \u0393 2 +D2\u03bb) 2 + 4L21\u03b7\u03bb\u03b7 2 w(1 + p) + 4L21\u03b7\u03bb (\u0393\u00b5\u2212D\u03bbL2 \u2212 L1)2 + \u00b5d 16 ( 2\u03b72\u03bb(1 + p)L 2 1 + L21\u03b7\u03bb \u00b5d ) C2 = \u2212 \u03b7\u03bb\u00b5 2 d 64 C3 = LL\u03b7 2 w(1 + \u0393 2 +D2\u03bb)\u03c3 2 2 + 2L21\u03b7\u03bb\u03b7 2 w\u03c3 2 + 2\u03b72\u03bb\u03c3 2 + \u00b5d\u03b7 2 \u03bb\u03c3 2 16 . Proof. Proof of the Inner Convergence By the definition of \u03a6, it holds that \u03a6(w, \u03bb) \u2265 \u2212d(\u03bb) \u2265 f . Thus, with Lemma 11, by summing up T terms, it holds that 1 K K\u2211 t=1 E \u03b7w 2 \u2225\u2207wL(wt, \u03bbt)\u22252 + \u03b7\u03bbL 2 L 8\u00b54 \u2225\u03bbt \u2212 \u03bb\u2217\u22252 \u2264 \u03a6(w1, \u03bb1)\u2212 f K + C3 Then, let \u03b7w = \u0398(1/ \u221a K) and \u03b7\u03bb = \u0398(1/ \u221a K), it holds that 1 K E K\u2211 t=1 \u2225\u03bbt \u2212 \u03bb\u2217\u22252 = O(1/ \u221a K) 1 K E K\u2211 t=1 \u2225wt \u2212 w\u2217\u22252 = O(1/ \u221a K) On the other hand, it holds that \u2225\u03bb\u22a4t \u2207wfi(wt)\u2212 \u03bb\u2217 \u22a4\u2207wfi(w\u2217)\u22252 \u2264 2\u2225\u2207wfi(w\u2217)\u22252\u2225\u03bbt \u2212 \u03bb\u2217\u22252 + 2\u2225\u03bbt\u22252\u2225\u2207fi(wt)\u2212\u2207fi(w\u2217)\u22252 \u2264 2D2w\u2225\u03bbt \u2212 \u03bb\u2217\u22252 + 2D2\u03bbL21\u2225wt \u2212 w\u2217\u22252 Therefore, it holds that 1 K E K\u2211 t=1 \u2225\u03bb\u22a4t \u2207wfi(wt)\u2212\u03bb\u2217 \u22a4\u2207wfi(w\u2217)\u22252 \u2264 2 K E K\u2211 t=1 D2w\u2225\u03bbt\u2212\u03bb\u2217\u22252+D2\u03bbL21\u2225wt\u2212w\u2217\u22252 = O(1/ \u221a K) B PROOF OF OUTER LOOP CONVERGENCE: In this section, we give the proof of the outer loop convergence. Lemma 12. Suppose (A1)-(A4) holds. Then, for all x1, x2 \u2208 X , it holds that \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225 \u2264 \u221a NDw\u00b5+ \u221a NDwL1 \u00b52 \u2225x1 \u2212 x2\u2225 Proof. Because \u2211N i=1 x (i) 2 fi(w) is \u00b5 is a strongly convex function, it holds that N\u2211 i=1 x (i) 2 (fi(w \u2217(x1))\u2212 fi(w\u2217(x2))) \u2265 \u00b5 2 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u22252. On the other hand, we have N\u2211 i=1 x (i) 2 (fi(w \u2217(x1))\u2212 fi(w\u2217(x2))) = N\u2211 i=1 x (i) 2 fi(w \u2217(x1))\u2212 N\u2211 i=1 x (i) 1 fi(w \u2217(x1)) + N\u2211 i=1 x (i) 1 fi(w \u2217(x1)) \u2212 N\u2211 i=1 x (i) 1 fi(w \u2217(x2)) + N\u2211 i=1 x (i) 1 fi(w \u2217(x2))\u2212 N\u2211 i=1 x (2) 2 fi(w \u2217(x2)) \u2264 N\u2211 i=1 (x (i) 1 \u2212 x (i) 2 )(fi(w \u2217(x2))\u2212 fi(w\u2217(x1)))\u2212 \u00b5 2 \u2225w\u2217(x2)\u2212 w\u2217(x1)\u22252. Meanwhile, we have |fi(w\u2217(x1))\u2212fi(w\u2217(x2))| \u2264 max(\u2225\u2207wf(w\u2217(x1))\u2225, \u2225\u2207wf(w\u2217(x2))\u2225)\u2225w\u2217(x1)\u2212w\u2217(x2)\u2225+ L1 2 \u2225w\u2217(x2)\u2212w\u2217(x1)\u22252 With strongly convexity, it holds that \u2225w\u2217(x2)\u2212 w\u2217(x1)\u2225 \u2264 1 \u00b5 \u2225\u2207f(w\u2217(x1))\u2212\u2207f(w\u2217(x2))\u2225 \u2264 2Dw \u00b5 . Thus, it holds that fi(w \u2217(x2))\u2212 fi(w\u2217(x1)) \u2264 Dw(1 + L1/\u00b5)\u2225w\u2217(x2)\u2212 w\u2217(x1)\u2225. Combining the above inequalities, it holds that \u00b5\u2225w\u2217(x1)\u2212w\u2217(x2)\u22252 \u2264 \u2225x1\u2212x2\u22251(max|fi(w\u2217(x1))\u2212fi(w\u2217(x2))|) \u2264 \u221a NDw(1+L1/\u00b5)\u2225w\u2217(x1)\u2212w\u2217(x2)\u2225 Lemma 13. Suppose (A1)-(A4) holds, then f0(w\u2217(x)) has Lipschitz gradient with Lipschitz constant( D2w \u00b52 + 2DwL1 \u00b5 ) \u221a NDw\u00b5+ \u221a NDwL1 \u00b52 . Proof. \u2202f0(w \u2217(x)) \u2202x(i) = \u2212\u2207wf0(w\u2217(x))\u22a4  N\u2211 j=1 x(j)\u22072w\u2207fj(w\u2217(x)) \u22121 \u2207wfi(w\u2217(x)) By the smoothness and strongly convexity it holds that\u2225\u2225\u2225\u2225\u2225\u2225\u2225  N\u2211 j=1 x (j) 1 \u22072w\u2207fj(w\u2217(x1)) \u22121 \u2212  N\u2211 j=1 x (j) 2 \u22072w\u2207fj(w\u2217(x2)) \u22121 \u2225\u2225\u2225\u2225\u2225\u2225\u2225 \u2264 1 \u00b52 \u2225\u2225\u2225\u2225\u2225\u2225 N\u2211 j=1 x (j) 1 \u22072w\u2207fj(w\u2217(x1))\u2212 N\u2211 j=1 x (j) 2 \u22072w\u2207fj(w\u2217(x2)) \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 1 \u00b52 max j \u2225\u22072w\u2207fj(w\u2217(x1))\u2212\u22072w\u2207fj(w\u2217(x2))\u2225 \u2264 L2 \u00b52 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225, where the first inequality is due to \u2225A(A\u22121 \u2212B\u22121)B\u2225 = \u2225A\u2212B\u2225 for all invertible matrices A,B. Then, it holds that\u2225\u2225\u2225\u2225\u2225\u2225\u2225  N\u2211 j=1 x (j) 1 \u22072w\u2207fj(w\u2217(x1)) \u22121 \u2207wfi(w\u2217(x1))\u2212  N\u2211 j=1 x (j) 2 \u22072w\u2207fj(w\u2217(x2)) \u22121 \u2207wfi(w\u2217(x2)) \u2225\u2225\u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225\u2225\u2225\u2225   N\u2211 j=1 x (j) 1 \u22072w\u2207fj(w\u2217(x1)) \u22121 \u2212  N\u2211 j=1 x (j) 2 \u22072w\u2207fj(w\u2217(x2)) \u22121 \u2207wfi(w\u2217(x1)) \u2225\u2225\u2225\u2225\u2225\u2225\u2225 + \u2225\u2225\u2225\u2225\u2225\u2225\u2225  N\u2211 j=1 x (j) 2 \u22072w\u2207fj(w\u2217(x2)) \u22121 (\u2207fi(w\u2217(x1))\u2212\u2207fi(w\u2217(x2))) \u2225\u2225\u2225\u2225\u2225\u2225\u2225 \u2264 Dw \u00b52 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225+ L1 \u00b5 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225. Thus, combining with the definition of gradient, it holds that\u2223\u2223\u2223\u2223\u2202f0(w\u2217(x1))\u2202x(i) \u2212 \u2202f0(w\u2217(x2))\u2202x(i) \u2223\u2223\u2223\u2223 \u2264 \u2225\u2207wf0(w\u2217(x1))\u2225 \u2225\u2225\u2225\u2225\u2225\u2225 ( N\u2211 j=1 x (j) 1 \u2207 2 w\u2207fj(w\u2217(x1)) )\u22121 \u2207wfi(w\u2217(x1))\u2212 ( N\u2211 j=1 x (j) 2 \u2207 2 w\u2207fj(w\u2217(x2)) )\u22121 \u2207wfi(w\u2217(x2)) \u2225\u2225\u2225\u2225\u2225\u2225 + \u2225\u2207wf0(w\u2217(x1))\u2212\u2207wf0(w\u2217(x2))\u2225 \u2225\u2225\u2225\u2225\u2225\u2225 ( N\u2211 j=1 x (j) 2 \u2207 2 w\u2207fj(w\u2217(x2)) )\u22121 \u2207wfi(w\u2217(x2)) \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 Dw( Dw \u00b52 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225+ L1 \u00b5 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225) + DwL1 \u00b5 \u2225w\u2217(x1)\u2212 w\u2217(x2)\u2225 Therefore, combine with Lemma 12, we can obtain the result. Lemma 14. Suppose function f(x) is lower bounded by f with L-Lipshitz gradient. g(x) is an unbiased gradient estimator of \u2207f(x) satisfying that expected norm of g(x) are bounded by G in domain X for function f . Then with update rule xt+1 = \u03a0X (xt \u2212 \u03b7x(g(xt) + \u03bet)), where \u03b7x = \u0398(1/ \u221a T ), X is a convex set and E\u2225\u03bet\u22252 \u2264 \u03f52. By defining x\u0302 = argminy\u2208X (f(y)+ \u03c12\u2225y\u2212x\u2225 2) and \u2207\u0304\u03c1f(x) = \u03c1(x\u2212 x\u0302), where \u03c1 = 2L, then it holds that 1 T E T\u2211 t=1 \u2225\u2207\u0304\u03c1f(xt)\u22252 = O(1/ \u221a T + \u03f52). Proof. It holds that E ( f(x\u0302t+1 + \u03c1 2 \u2225xt+1 \u2212 x\u0302t+1\u22252 ) \u2264 E ( f(x\u0302t) + \u03c1 2 \u2225xt+1 \u2212 x\u0302t\u22252 ) = E ( f(x\u0302t) + \u03c1 2 \u2225\u03a0X (xt \u2212 \u03b7x(g(xt) + \u03bet))\u2212 x\u0302t\u22252 ) \u2264 E ( f(x\u0302t) + \u03c1 2 \u2225xt \u2212 \u03b7x(g(xt) + \u03bet)\u2212 x\u0302t\u22252 ) = E ( f(x\u0302t) + \u03c1 2 \u2225xt \u2212 x\u0302t\u22252 + \u03c1\u03b72x\u2225g(xt) + \u03bet\u22252 \u2212 \u03b7x\u03c1\u27e8xt \u2212 x\u0302t, g(xt)\u27e9 \u2212 \u03b7x\u03c1\u27e8xt \u2212 x\u0302t, \u03bet\u27e9 ) \u2264 E ( f(x\u0302t) + \u03c1 2 \u2225xt \u2212 x\u0302t\u22252 ) + \u03c1\u03b72x(G 2 + \u03f52)\u2212 \u03c1\u03b7xE ( f(xt)\u2212 f(x\u0302t)\u2212 L 2 \u2225xt \u2212 x\u0302t\u22252 ) + \u03c1\u03b7xE ( L 2 \u2225xt \u2212 x\u0302t\u22252 + 1 2L \u03f52 ) Then summing up the above inequality, it holds that \u03c1\u03b7x T\u2211 t=1 E ( f(xt)\u2212 f(x\u0302t)\u2212 L\u2225xt \u2212 x\u0302t\u22252 ) \u2264 E [ f(x\u03021) + \u03c1 2 \u2225x1 \u2212 x\u03021\u22252 \u2212 (f(x\u0302T+1 + \u03c1 2 \u2225xT+1 \u2212 x\u0302T+1\u22252) ] + T\u03c1\u03b72x(G 2 + \u03f52) + T\u03c1\u03b7x 2 \u03f52 \u2264 f(x\u03021) + \u03c1 2 \u2225x1 \u2212 x\u03021\u22252 \u2212 f) + T\u03c1\u03b72x(G2 + \u03f52) + T\u03c1\u03b7x 2L \u03f52 On the other hand, because x\u0302t = argminy\u2208X (f(y) + \u03c12\u2225xt \u2212 y\u2225 2) and function f(y) + \u03c12\u2225xt \u2212 y\u2225 2 is strongly convex with modular \u03c1\u2212 L, it holds that f(xt)\u2212 f(x\u0302t)\u2212 L\u2225xt \u2212 x\u0302t\u22252 = f(xt) + \u03c1 2 \u2225xt \u2212 xt\u22252 \u2212 (f(x\u0302t) + \u03c1 2 \u2225xt \u2212 x\u0302t\u22252) + \u03c1\u2212 L\u2212 1 2 \u2225xt \u2212 x\u0302t\u22252 \u2265 2\u03c1\u2212 3L 2 \u2225xt \u2212 x\u0302t\u22252 = L 2 \u2225xt \u2212 x\u0302t\u22252 Thus, combining the above inequalities, it holds that \u03c12 T T\u2211 t=1 E\u2225xt \u2212 x\u0302t\u22252 \u2264 2\u03c1 2 TL T\u2211 t=1 Ef(xt)\u2212 f(x\u0302t)\u2212 L\u2225xt \u2212 x\u0302t\u22252 \u2264 \u03c1f(x\u03021) + \u03c12 2 \u2225x1 \u2212 x\u03021\u2225 2 \u2212 \u03c1f) T\u03b7x + \u03c12\u03b7x(G 2 + \u03f52) + \u03c12 2L \u03f52. Therefore, when \u03b7x = O(1/ \u221a T ), 1T E \u2211T t=1 \u2225\u2207\u0304\u03c1f(xt)\u22252 = \u03c12 T \u2211T t=1 E\u2225xt \u2212 x\u0302t\u22252 = O(1/ \u221a T + \u03f52). Proof of Theorem 2. With Lemma 13, we can obtain f0(w\u2217(x)) has Lipschitz gradient on domain X . Define \u03be(i)t = \u03bb T t \u2207wfi(wt,k)\u2212 \u03bb\u2217(x)T\u2207wfi(w\u2217(x)) When we use \u03bbt,k and wt,k with random k, as it suggests in Theorem 1, the expected norm of \u03be ( t i) can be bounded by O(1/ \u221a K). Let x(i)t+1/2 = x (i) t \u2212 \u03b7x(g(xt)(i) + \u03bbTt \u2207wfi(wt,k)\u2212 \u03bb\u2217(xt)T\u2207wfi(w\u2217(xt))), Then, it holds that Eg(xt)(i) = E [ 1 \u03b7x (xt+1/2 \u2212 xt)\u2212 \u03bbTt \u2207wfi(wt,k) + \u03bb\u2217(xt)T\u2207wfi(w\u2217(xt))) ] = \u03bb\u2217(xt) T\u2207wfi(w\u2217(xt))) and E ( g(xt) (i))2 ) \u2264 (1 + p)\u2225\u03bb\u2217(xt)T\u2207wfi(w\u2217(xt)))\u22252 + \u03f52 + \u03c32 \u2264 (1 + p)D2\u03bbD2w + \u03f52 + \u03c32 Thus, together with Lemma 14, we can directly get the result. C ADDITIONAL EXPERIMENTAL RESULTS ON MNIST AND FASHION MNIST WITH LENET5: We use 10 clients in this experiment. The first 5 clients contains i.i.d. 9000 samples, the last 5 clients contains 9000 samples with random label. The rest setting is the same as it in the main text. The results are shown in Figure 5, and Figure 6."}, {"role": "assistant", "content": "**Step 5: Collaborative Review Discussion and Consensus Building**\n\nI will now analyze the agents' responses and facilitate a discussion to identify key strengths, weaknesses, and potential areas for improvement. I will focus on the following aspects:\n\n*   **Novelty:** Is the proposed approach truly novel compared to existing federated learning and bi-level optimization techniques?\n*   **Theoretical Soundness:** Are the assumptions made in the convergence analysis valid in the context of federated learning with non-convex models?\n*   **Practical Applicability:** Does the proposed approach offer significant performance gains without introducing excessive communication or computational overhead?\n*   **Experimental Validation:** Are the experimental results clear, complete, and relevant to the claims made in the paper?\n\nBased on the agents' assessments, the paper seems to have potential but also faces several challenges. The novelty of the approach is not entirely clear, as similar ideas have been explored in meta-learning and hyperparameter optimization for federated learning. The theoretical soundness is questionable due to the strong convexity assumption, which is unlikely to hold for deep learning models. The practical applicability depends on the trade-off between performance gains and communication/computational overhead.\n\nI will now synthesize these perspectives into a comprehensive review.\n"}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'MethodologyExpert'}, name='FireAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'NoveltyAssessor'}, name='FireAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'PracticalityAnalyst'}, name='FireAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent MethodologyExpert does not exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `FireAgent` with \n```json\n{\n  \"agent_name\": \"MethodologyExpert\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully fired.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.34350475\n}\n```\n", "metadata": {"title": "Invoking `FireAgent` with \n```json\n{\n  \"agent_name\": \"NoveltyAssessor\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Error occurred: Agent PracticalityAnalyst does not exists\",\n  \"output\": null\n}\n```\n", "metadata": {"title": "Invoking `FireAgent` with \n```json\n{\n  \"agent_name\": \"PracticalityAnalyst\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='FireAgent', response={'result': {'status': 'error', 'message': 'Error occurred: Agent MethodologyExpert does not exists', 'output': None}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='FireAgent', response={'result': {'status': 'success', 'message': 'Agent successfully fired.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.34350475}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='FireAgent', response={'result': {'status': 'error', 'message': 'Error occurred: Agent PracticalityAnalyst does not exists', 'output': None}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "**Summary Review:**\n\nThis paper introduces a primal-dual method for local coefficient optimization in federated learning, framing the problem as a bi-level optimization. The approach aims to determine the contribution of each local worker by using a small validation set. The paper presents a theoretical analysis of the algorithm's convergence and compares it with other bi-level optimization algorithms on a toy example and image classification tasks.\n\n**Strengths:**\n\n*   The paper addresses an important problem in federated learning: fairly distributing benefits and identifying dataset quality in local clients.\n*   The formulation of local coefficient learning as a bi-level optimization problem is a novel approach in the federated learning context.\n*   The introduction of a primal-dual framework into bi-level optimization is a valuable contribution.\n*   The experimental results demonstrate the potential of the proposed algorithm in strongly convex cases and even in some non-convex cases.\n\n**Weaknesses:**\n\n*   The assumption of strong convexity for local functions is a significant limitation, as it is unlikely to hold for complex models like neural networks.\n*   The novelty of the approach is not clearly established, as similar ideas have been explored in meta-learning and hyperparameter optimization for federated learning.\n*   The theoretical convergence analysis requires a large number of iterations, which may limit the practical applicability of the algorithm.\n*   The experimental results are not comprehensive enough to demonstrate the superiority of the proposed algorithm over existing methods in various federated learning scenarios.\n\n**Recommendations:**\n\n*   Relax the strong convexity assumption or provide a more detailed justification for its validity in the context of federated learning.\n*   Clearly differentiate the proposed approach from existing meta-learning and hyperparameter optimization techniques for federated learning.\n*   Provide a more thorough experimental evaluation, including comparisons with state-of-the-art federated learning algorithms on more diverse datasets and model architectures.\n*   Address the limitations of the theoretical convergence analysis and explore techniques to improve the convergence rate.\n*   Discuss the communication and computational overhead of the proposed algorithm and compare it with other federated learning methods.\n\n**Overall Assessment:**\n\nThe paper presents an interesting approach to local coefficient optimization in federated learning. However, the strong convexity assumption, limited novelty, and incomplete experimental evaluation raise concerns about the practical applicability and significance of the proposed algorithm.\n\n**FINAL DECISION: Reject**\n"}], "ground_truth": "Reject", "response_history": "**Summary Review:**\n\nThis paper introduces a primal-dual method for local coefficient optimization in federated learning, framing the problem as a bi-level optimization. The approach aims to determine the contribution of each local worker by using a small validation set. The paper presents a theoretical analysis of the algorithm's convergence and compares it with other bi-level optimization algorithms on a toy example and image classification tasks.\n\n**Strengths:**\n\n*   The paper addresses an important problem in federated learning: fairly distributing benefits and identifying dataset quality in local clients.\n*   The formulation of local coefficient learning as a bi-level optimization problem is a novel approach in the federated learning context.\n*   The introduction of a primal-dual framework into bi-level optimization is a valuable contribution.\n*   The experimental results demonstrate the potential of the proposed algorithm in strongly convex cases and even in some non-convex cases.\n\n**Weaknesses:**\n\n*   The assumption of strong convexity for local functions is a significant limitation, as it is unlikely to hold for complex models like neural networks.\n*   The novelty of the approach is not clearly established, as similar ideas have been explored in meta-learning and hyperparameter optimization for federated learning.\n*   The theoretical convergence analysis requires a large number of iterations, which may limit the practical applicability of the algorithm.\n*   The experimental results are not comprehensive enough to demonstrate the superiority of the proposed algorithm over existing methods in various federated learning scenarios.\n\n**Recommendations:**\n\n*   Relax the strong convexity assumption or provide a more detailed justification for its validity in the context of federated learning.\n*   Clearly differentiate the proposed approach from existing meta-learning and hyperparameter optimization techniques for federated learning.\n*   Provide a more thorough experimental evaluation, including comparisons with state-of-the-art federated learning algorithms on more diverse datasets and model architectures.\n*   Address the limitations of the theoretical convergence analysis and explore techniques to improve the convergence rate.\n*   Discuss the communication and computational overhead of the proposed algorithm and compare it with other federated learning methods.\n\n**Overall Assessment:**\n\nThe paper presents an interesting approach to local coefficient optimization in federated learning. However, the strong convexity assumption, limited novelty, and incomplete experimental evaluation raise concerns about the practical applicability and significance of the proposed algorithm.\n\n**FINAL DECISION: Reject**\n", "elapsed_time": 118.28799700737, "reviewer_agents": {"FedLRExpert": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in federated learning, focusing on novelty and practical applicability.", "system_prompt": "You are an expert in federated learning. Evaluate the paper's contribution to the field, focusing on its novelty, practical applicability, and potential impact on real-world federated learning scenarios. Assess the clarity and completeness of the experimental results and their relevance to the claims made in the paper.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "BiLevelOptAnalyst": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in bi-level optimization, focusing on theoretical soundness and convergence analysis.", "system_prompt": "You are an expert in bi-level optimization. Evaluate the paper's theoretical soundness, focusing on the correctness of the convergence analysis and the validity of the assumptions made. Assess the novelty and rigor of the proposed algorithm compared to existing bi-level optimization techniques.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "PrimalDualReviewer": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in primal-dual methods, focusing on correctness and efficiency of the framework.", "system_prompt": "You are an expert in primal-dual optimization methods. Evaluate the paper's use of a primal-dual framework for solving the bi-level optimization problem, focusing on its correctness, efficiency, and potential advantages over alternative approaches. Assess the clarity and completeness of the algorithm design and its implementation details.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}}}
