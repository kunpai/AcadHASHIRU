{"paper_id": "SWPFPk9Tm81", "prompt": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction\n\n1 INTRODUCTION: Since the first record of human electroencephalogram (EEG) performed almost a century ago (in 1924), EEG has been one of the most widely used non-invasive neural modalities that monitors brain activity in high temporal resolution (Koike et al., 2013; Mehta & Parasuraman, 2013; Sejnowski et al., 2014). Among a variety of modalities, EEG has extensive use in the clinical assessment of neurological and psychiatric conditions, as well as in the research of neuroscience, cognitive science, psychology, and brain-computer interfacing (BCI). EEG signals measure subtle fluctuations of the electrical field driven by local neuroelectrophysiological activity of a population of neurons in the brain cortex (Cohen, 2017). While the electrodes are placed on the surface of the scalp, undesired artifacts may introduce interruption in the measurements and distort the signal of interest. Even in a well-controlled laboratory with a well-trained subject who can maximally keep the body still and relaxed, the EEG signals, unfortunately, could be contaminated by inevitable behavioral and physiological artifacts such as eye blinks, reflective muscle movements, ocular activity, cardiac activity, etc (Croft & Barry, 2000; Wallstrom et al., 2004; Romero et al., 2008). In practice, it is difficult to identify and track the sources of artifacts entirely due to their diversity and non-stationarity. Noise cancellation and artifact removal remain major issues in EEG signal processing and decoding. Currently, numerous methods have been proposed to alleviate the influence of artifacts in EEG signals. Traditional EEG denoising algorithms include filtering, regression, data separation or decomposition (Makeig et al., 1995; Islam et al., 2016; Kothe & Jung, 2016). According to previous meta-analyses on EEG artifact removal literature (Urigu\u0308en & Garcia-Zapirain, 2015; Jiang et al., 2019), independent component analysis (ICA) is especially popular. It is majorly used in 45% of EEG denoising literature. ICA-based artifact removal estimates the component activity by unmixing the EEG data in the channel domain. Through manual or automatic identification, one can exclude the artifact components and then reconstruct the EEG data through back projection based on non-artifact components (Jung et al., 2000a). The fast growth of deep learning methods has drawn state-of-the-art performances in a variety of machine learning problems (LeCun et al., 2015). Lately, deep-learning-based EEG reconstruction has drawn much attention (Leite et al., 2018; Sun et al., 2020; Lopes et al., 2021; Lee et al., 2020; Chuang et al., 2022). Although these methods can effectively remove artifacts from artificial synthetic signals, their performance in reconstructing real EEG data has not yet been validated in terms of decoding labeled EEG data. Meanwhile, the model design of existing deep-learning-based techniques for EEG reconstruction rarely takes the characteristics of EEG into account. In this work, we propose CLEEGN, a ConvoLutional neural network for EEG reconstructioN. CLEEGN is capable of subject-independent EEG construction without any training/calibration for a new subject. The contributions of this work are three-fold: \u2022 a light-weight autoencoder CNN, CLEEGN, with a subject-independent framework that facilitates plug-and-play EEG reconstruction. \u2022 CLEEGN outperforms leading online/offline methods in providing reconstructed EEG data with the best decoding performance for BCI datasets. \u2022 with a novel model design dedicated to EEG reconstruction, CLEEGN characterizes patterns of EEG interpretable and provides neuroscientific insights. 2 RELATED WORK: Current processing techniques for EEG artifact removal are highly varied based on the context where the algorithm may apply. Earlier attempts of EEG denoising assumed that the EEG signals and artifacts appear in different frequency ranges. Based on the assumption, some significant artifacts can be eliminated by the linear filtering method during the online stage (Seifzadeh et al., 2014). Despite the advantages of low computational time, linear filtering hardly removes artifacts that distribute in an overlapped frequency range of EEG signals. Another approach, adaptive filtering (Schlo\u0308gl et al., 2007), estimates artifact signals through additional EOG, EMG, ECG channels and removes these noisy signals from the recording signals by regression. Nevertheless, this approach requires additional auxiliary electrodes and raises the cost and inconvenience in practical applications. The blind source separation (BSS) method in EEG denoising was developed by assuming that the recording EEG signals are linear combinations of the signals from noise sources and the brain neurons. One of the most well-known BSS method is independent component analysis (ICA) (Jung et al., 2000a;b), which is able to separate EEG signals into independent components (ICs) (Makeig et al., 1995). Traditionally, the artifact components extracted by ICA are determined and removed through manual inspection. Recently developed ICLabel can label the ICs provenance into seven different categories: brain, eye, heart, muscle, line noise, channel noise, and other (Pion-Tonachini et al., 2019). Artifact subspace reconstruction (ASR) is another automatic approach, which is based on the principal component analysis (PCA) method (Kothe & Jung, 2016). The ASR method selects relatively noiseless periods from the multi-channel EEG data as reference based on the data distribution. After projecting all EEG data to the principal-component domain, high-variance components projected from the artifacts are detected by a cutoff parameter k. The noiseless signals are reconstructed by preserving the components without carrying artifacts and back-projected to the time domain. The ASR method has been shown capable of improving the quality of ICA decomposition (Chang et al., 2020). Recently, neural network-based methods have been proposed to remove artifacts for EEG data. A variety of network structures have been applied to the framework for removing EEG artifacts and reconstructing clean EEG. A deep convolutional autoencoder (Leite et al., 2018) can enhance the peak-signal-to-noise ratio compared to the linear filtering method via a common CNN autoencoder structure, which has been widely used on image denoising. Their work shows that it seems practicable to transform the EEG waveform through a CNN structure. Later on, a combined framework integrating Bayesian deep learning and ICA (Lee et al., 2020) used thresholding of the EEG data distribution to discard ICs classified as ocular artifacts. These methods leverage the flexibility of deep learning model design and achieve improvements in their assessments. Considering the non- stationary property in EEG data and the degradation phenomenon during training, 1D-ResCNN (Sun et al., 2020) was proposed, which adopted an Inception-Residual module in the network structure. This network is able to remove EOG, ECG, EMG on single-channel synthesis EEG data. Instead of using synthesis EEG data, IC-U-Net (Chuang et al., 2022) was created, which generated pairings of noisy and noiseless EEG data through ICA as training data. The proposed neural network is a one-dimensional adaptation of U-Net architecture trained with the ensemble of four loss functions to minimize the difference between amplitude, velocity, acceleration, and frequency components from the signals. This work showed that their reconstructed signal has higher SNR and can surely increase the number of brain components classified by ICLabel. 3 MATERIALS AND METHODS: CLEEGN Architecture and Model Training. CLEEGN is designed to map multi-channel noisy EEG into a latent space and reconstruct it into noiseless EEG signals. The architecture of CLEEGN is shown in Figure 1. The architecture of the encoder is inspired by an existing CNN model designed for EEG recognition (Wei et al., 2019) and incorporates convolutional blocks that capture spatiotemporal characteristics of EEG data. The first convolution block is used to extract spatial EEG features through a convolutional layer containing C spatial filters whose shape is (C, 1), where C is the number of electrodes of the EEG signals. The second convolution block is used to extract temporal EEG features through a convolutional layer containing NF temporal filters with shape (1, \u230afs \u00d7 0.1\u230b), where fs denotes the sample frequency of the EEG signals. As for the decoder, we design an approximately symmetric structure to the encoder with three convolutional blocks. The first convolutional block decodes the EEG feature with NF temporal kernels with shape is (1, \u230afs\u00d70.1\u230b). The second block decodes the EEG feature by a convolution layer with C spatial kernels of shape (C, 1). The last convolutional block is for projecting the feature domain back to the original time domain. To maintain the shape between the model inputs and outputs, every convolution block applies zeropadding except the first block in the encoder. The detail of the CLEEGN architecture is available in Table 1. As illustrated in Figure 1, the objective of the proposed method is to minimize the difference between noiseless signals, Y , and the model output, Y\u0302 . The recording EEG data, X , is the combination of brain signals, Y , and the signals from multiple noise sources, N . CLEEGN is a denoising autoencoder that is intended to perform artifact removal on EEG data by creating a mapping between multi-channel noisy EEG signals, X , and noiseless signals, Y . The training process of CLEEGN utilizes pairing noisy raw EEG data and noiseless reference EEG data so that the model can learn to transform noisy EEG data into reconstructed EEG data with maximal similarity to the reference data. To generate large-scale reference EEG data, we adopted automatic denoising methods, ICLabel and ASR, to remove artifacts and reconstruct clean waveforms offline. The pairing EEG recording with C channels of continuous noisy/noiseless data was further segmented into training sample pairs. The input size is (C, T ), where T is the number of time points based on the sampling rate of the EEG data in 4-second segments. The windows size and stride are set as T and 0.5T . In the context of plug-and-play EEG reconstruction, CLEEGN was trained as a subject-independent model. To perform subject-independent training and testing, we grouped subjects into k disjoint sets. During the training process, one of the sets was left out for testing. Subjects\u2019 EEG data in the left-alone set were not involved in both training and validation. A complete experiment on a single dataset would result in k different models. The artifact removal performance of a model was evaluated by using the left-alone set. The number of subjects in one set and the EEG duration available for each subject depend on the experiment setting and the dataset used. EEG Reconstruction and Evaluation. The reconstructed EEG data of CLEEGN is generated through an online reconstruction simulation. In the online stage, the system performs artifact removal on a new subject using a pre-trained CLEEGN model without any requirement of training/calibration. Figure 2 shows the online EEG reconstruction simulated in offline. The EEG data from a subject in the left-alone set would be fed into the model sequentially. The stride size of the sliding window is set to 0.5 seconds to minimize the delay in online reconstruction. The fitness of the model can be evaluated by observing the waveform visually or measuring the similarity using the mean square error (MSE) between reconstructed and noiseless EEG. Meanwhile, we propose to use the decoding performance of labeled EEG data as an objective measurement of the reconstructed EEG quality. We employed EEGNet, a compact CNN for end-to-end EEG decoding (Lawhern et al., 2018), as the classifier to decode the labeled EEG data in our study. 4 EXPERIMENTS: Data and Model Fitting. In this work, we applied two labeled EEG datasets, the BCI-ERN dataset 1 and the MAMEM-SSVEP-II dataset 2. We estimate the classification performance by calculating the score of Area Under the Curve (AUC) in ERN binary classification and Top-1 accuracy in the SSVEP task. Figure 3 is the schematic diagram of the process of decoding performance estimation using the reconstructed EEG data. The segmented EEG epochs are divided into three splits within each subject: training set, validation set, and test set. The ratio between classes remained the same in each set. We perform 20 repeated runs with shuffled data to estimate the average performance of each decoding performance evaluation. Detailed descriptions of the two datasets are available in the appendix at A.2. Figure 4 presents the loss curve and the reconstruction result at the end of three different training iterations in a sample run using the ERN dataset. Through this exploration, we can look into the course of model fitting. In the early stage of training, the model reduces the large MSE loss by smoothing the input signals. To further minimize the error between reconstruction and reference (offline denoised signal), the model compensates the intensity to better fit the reference signals. At the convergent stage of training, we can see the reconstructed waveform is very similar to the one in reference data. 1https://www.kaggle.com/c/inria-bci-challenge 2https://www.mamem.eu/results/datasets/ Types of Reference Data. As ICLabel, ASR, and the hybrid ASR-ICLabel are employed to automatically generate large-scale noiseless reference EEG data offline, it is of our interest to investigate what type of reference EEG serves as the best noiseless reference data for the CLEEGN model training. Figure 5 presents the EEG waveforms sampled from Subject 2 in the ERN dataset. The EEG time series from top to bottom represent the recording of Fp1, T7, Cz, T8, and O2 channels. The nine sub-figures visualize the comparison between the noisy raw data and the reference data reconstructed by the ICLabel, ASR with or without ICLabel when the cutoff parameter k was set to 4, 8, 16, and 32. We can see the waveform of ICLabel and the ASR-ICLabel hybrid method with 4 different cutoff parameters k are similar, and the noiseless waveform reconstructed by CLEEGN are highly correlated to the result from these 5 methods. High-amplitude ocular artifacts such as blinking and eye saccades are phenomenal in frontal channels (Fp1) and muscular artifacts featuring high-frequency (around 24-30 Hz in the \u03b2 band) are observed on the T7 channel. CLEEGN, ICA, and ASR-ICLabel hybrid methods can eliminate these kinds of artifacts. Compared to the ICLabel and ASR-ICLabel hybrid methods, ASR is unable to eliminate the high-frequency EMG artifact for this dataset. The tolerance of large amplitude EOG artifact increases with the employment of larger cutoff parameter k. Limited by the reference data generated by ASR, CLEEGN preserves the high-frequency artifact. As for the large amplitude artifacts that failed to be identified by ASR-32 and ASR-16, we can see that CLEEGN is able to mitigate those EOG artifacts. As shown in Figure 6(a), the reference data prepared by ICLabel offer the lowest MSE, i.e. the best fitness for CLEEGN training. We also compare the types of reference EEG data and their corresponding CLEEGN reconstruction results in the decoding performance of the ERN EEG dataset to assess their data quality. As illustrated in Figure 6(b), CLEEGN reconstruction draws better performances for all denoising methods. Although the performance decreases with a smaller cutoff parameter k, CLEEGN can outperform the reference data used for its training. This result suggests that our method not only removes the artifact but also preserves informative brain activity in the EEG under our cross-subject training scheme. As the ICLabel provides the reference data with the best decoding performance and fitness, we chose ICLabel as the source of reference data for further experiments in this study. Training Data Size. We explored the effect of data length per subject in the training set regarding the fitness and the decoding performance of CLEEGN. The training data were segmented from the first 1, 2, 4, 10, 20, and 30 minutes of data in each EEG recording to investigate the trade-off between training time and reconstruction performance. Figure 7(a) shows the fitness of CLEEGN to the reference data. The model trained using the first 10 minutes obtain the minimal value among all duration configurations. Though the difference in MSE between 10, 20, and 30 minutes is not noticeable, Figure 7(b) shows that using the 10-minute training data yields the best decoding performance among all settings. Interestingly, with only one-minute training data from each subject, CLEEGN can achieve comparable decoding performance to the reference data. This indicates that CLEEGN retains its performance even when each subject only contributes a short recording for training. In addition, we explore the effect of the number of subjects included in the training set on the fitness of CLEEGN model training and the decoding performance of the reconstructed EEG data. We randomly reduced the number of subjects for training from 12 to 6, 4, and 2. Since the subset of subjects may influence the performance, we tested multiple combinations and averaged the results. With the decrease in the number of subjects, the MSE value and the standard error (light span area) increase in Figure 8(a), which indicates that the generalization ability of the CLEEGN model reduces when fewer subjects are included for training. In Figure 8(b), we can observe a slight decrease in decoding performance. We consider the number of subjects as an essential factor in the performance of CLEEGN, yet it requires only a few subjects to achieve a satisfactory performance compared to the reference data. Performance Comparison. We compare the performance of CLEEGN against three baseline methods, ICLabel, 1D-ResCNN, and IC-U-Net. Except for the ICLabel that operates offline, CLEEGN and the other two neural network-based methods perform a simulated online reconstruction based on the same training process with the reference data generated by ICLabel. Results of the ERN EEG dataset are shown in Table 2. CLEEGN has the best fitness to the reference data with the minimum MSE, the highest AUC score in the decoding performance, and the least parameters. For the SSVEP EEG data, although not having the minimum MSE, CLEEGN outperforms other methods in the decoding accuracy and the model size. The evaluation across the two datasets suggests an overall superiority of our proposed CLEEGN model over other existing neural network-based methods in online reconstruction. CLEEGN even provides a better reconstruction than the offline ICLabel. These promising results indicate the usability of CLEEGN in online training/calibration-free EEG reconstruction that truly meets the need for real-world applications of EEG-based BCI. Visualization. On account of the interpretability of the CLEEGN model, we visualized its intermediate latent features by mapping onto a 2-dimensional domain based on the principal component analysis (PCA) of the noisy raw EEG data (Wold et al., 1987). Figure 9(a) shows the principal component space and the noisy EEG data channel projections. We observe that the scatters of noisy EEG data retain the spatial relationship of the actual EEG electrodes. The arrangement of frontal electrodes (prefix in Fp, P) to the posterior (prefix in P, O) are projected along the x-axis from right to left. The projection from top to bottom matches the left-side electrodes (suffixed in odd numbers), central electrode (suffixed in z), and right-side electrodes (suffixed in even numbers). The projection suggests that EEG data of adjacent channels tend to show similar waveforms. The blue dots in Figure 9(b), (c) are the projection of latent features resulting from the first and second convolutional layers, which make up the encoder design of CLEEGN. The projection of latent features from the next two convolutional layers is presented in Figure 9(d), (e). From Figure 9(b) to (e), we can see the distribution range of the latent features shrink in the encoder design and expand in the decoder design of CLEEGN. Figure 9(f) compares the distribution of noisy data and CLEEGN reconstructed data, showing that the CLEEGN reconstructed data is a more compact cluster than the noisy EEG in the PCA space. This implies that the process within CLEEGN includes a projection of the original EEG data in the upstream layer, complex temporal filtering and combination in the midstream layers, and a final projection that converts the noiseless latent features back to the channel domain. 5 CONCLUSION: In this work, we have proposed CLEEGN, a novel convolutional neural network for plug-and-play automatic EEG reconstruction. The training of CLEEGN leverages the conventional offline denoising methods, ASR and ICLabel, with automatic component classification to generate abundant noiseless EEG data. The performance of CLEEGN was objectively validated using multiple evaluations including waveform observation; reconstruction error assessment; and decoding accuracy on well-studied, labeled datasets. The experiment results suggest that, even without any calibration, CLEEGN can predominantly preserve inherent brain activity. According to the decoding performance of reconstructed EEG data, CLEEGN outperforms other neural network-based denoising methods on both ERN and SSVEP EEG decoding. From the visualization of the waveform, CLEEGN can remove artifacts from different sources and the waveform is highly correlated to the reference data. Through the visualization of model parameters and latent features, we exhibit model behavior and reveal explainable insights related to existing knowledge of neuroscience. We foresee pervasive applications of CLEEGN in prospective works of EEG decoding and analysis. A APPENDIX:  A.1 NEURAL NETWORK TRAINING CONFIGURATION:  A.1.1 CLEEGN: For training CLEEGN, we used the Adam optimizer with an initial learning rate of 1e-3 without weight decay. Besides, the exponential learning rate scheduler is applied with a gamma of 0.8. As for the loss function, we used Mean Squared Error (MSE). The batch size is set to 64 and the total training epoch is 40. During the training procedure, the model is evaluated using the validation subset at the end of every epoch with the purpose of saving the weights that achieved the lowest validation loss. A.1.2 IC-U-NET The optimizer adopted in IC-U-Net (Chuang et al., 2022) training is SGD with an initial learning rate of 1e-2, momentum of 0.9, and weight decay of 5e-4. The learning rate scheduler used in the training procedure is the multistep scheduler. As for the loss function, a novel ensemble loss proposed in ICU-Net is adopted. This ensemble is a simple linear combination of the Mean Squared Error (MSE) in amplitude, velocity, acceleration, and frequency components of EEG signals. Each term in the loss function has same weight. The batch size is set to 64 and the total training epoch is 200. The weight saving strategy in IC-U-Net training is the same as CLEEGN. A.1.3 1D-RESCNN The Adam optimizer with an initial learning rate of 1e-3 and the Mean Squared Error (MSE) loss function is adopted in 1D-ResCNN (Sun et al., 2020) training. Since the 1D-ResCNN is developed under one-dimensional synthesized EEG data and no explicit instruction of using multi-channel EEG provided, we trained 1D-ResCNN using two different method. For one method, we trained multiple models for each EEG channel respectively. For the other method, we viewed each multichannel EEG segment as a batch and the channel arrangement within a batch is fixed. Under our experiment, the result showed that the second method not only provides an efficient training process, but also results in a better reconstructed performance. The same weight saving strategy in CLEEGN is adopted in 1D-ResCNN training. A.1.4 DECODING PERFORMANCE EVALUATION MODEL: EEGNET: EEGNet (Lawhern et al., 2018) is a famous EEG decoding model and widely used in EEG literature. In the original EEGNet paper, they investigated their proposed model with a different number of kernels and denoted the model with F1 temporal filters and D spatial filters as EEGNet-F1,D. We use two different settings to train the two datasets used. In ERN classification, we use the EEGNet8,2 structure suggested by the EEGNet paper. As for the SSVEP classification, an experimental result showed that EEGNet-100,8 can draw the best performance. We trained and evaluated the decoding performance individually for each subject. We divided the collection of event epochs into three splits within each subject: training set, validation set, and test set with a ratio of 3:1:1. The ratio between classes remained the same in each set. The loss function is categorical cross entropy (CCE) and the Adam optimizer is adopted with a learning rate of 10\u22123 and zero weight decay. The batch size is set to 32 and the total training epoch is 200. A.2 DESCRIPTION OF DATASETS:  A.2.1 DATASET 1: FEEDBACK ERROR-RELATED NEGATIVITY (ERN): Error-related negativity (ERN) can be categorized as a kind of event-related potential (ERP), which occurs after an erroneous or abnormal event perceived by the subject. One characteristic of the feedback ERN is a relatively large negative amplitude approximately 350 ms and a positive amplitude approximately 500 ms after visual feedback triggered by the error event. In this work, we mainly use a well-studied EEG dataset from the BCI Challenge competition hosted by Kaggle to evaluate the artifact removal effectiveness. This dataset includes EEG recordings of 26 subjects (16 subjects labeled and 10 subjects unlabeled) that participated in a P300 speller task. P300 speller is a wellknown BCI system that develops a typing application through P300 response evoked potential. The ERN experiment was conducted under the assumption that the ERN occurred if the subject received incorrect prediction (feedback) from the P300 speller. The objective of the competition was to improve the P300 speller performance by implementing error correction through ERN potentials. We used the 16 subjects with labeled data of which the sampling rate is 200 Hz initially with 56 passive Ag/AgCl EEG sensors. In the interest of increasing the usability of EEG data, we applied some pre-processing procedures to each EEG recording. The EEG data were down-sampled to 128 Hz and re-referenced by the common average reference (CAR) method to eliminate common-mode noise and to zero-center the data. Each recording was band-pass filtered to 1-40 Hz through the FIR filter implemented by EEGLAB to remove DC drifting. During EEG decoding evaluation, we epoch EEG signals in [0, 1.25] second interval to obtain correct and erroneous feedback. A.2.2 DATASET 2: STEADY STATE VISUALLY EVOKED POTENTIAL (SSVEP): Steady state visually evoked potential (SSVEP) is another kind of ERP that is characterized as periodic potential induced by rapidly repetitive visual stimulation. The SSVEP is composed of several discrete frequency components, which consist of the fundamental frequency of the visual stimulus as well as its harmonics. To investigate the generalization ability of the model, we use \u201dEEG SSVEP Dataset II\u201d from Multimedia Authoring & Management using your Eyes & Mind (MAMEM). The dataset includes EEG data from 11 subjects and consists of five different frequencies (6.66, 7.50, 8.57, 10.00, and 12.00 Hz). Each subject was recorded in five sessions and each session included 25 trials (5 trials for each class). The data used a 256-channel HydroCel Geodesic Sensor Net (HCGSN) and captured the signals with a sampling rate of 250 Hz. Since there are several different bad channels in each subject\u2019s EEG recording, we preserved 20 common channels from each subject to train the CLEEGN model. Every recording was down-sampled to 125 Hz, re-referenced by the common average reference (CAR) method, and band-pass filtered to 1-40 Hz. We epoch EEG signals in [1, 5] second interval for each event recorded timestamp. The first second was discarded under the consideration of a reaction delay of the subject. A.3 INVESTIGATION OF DECODING PERFORMANCE EVALUATION: SSVEP In this work, we mainly use decoding performance as the assessment of different artifact removal methods. We provide an interpretation of the relationship between EEG quality and decoding performance. In Section 4 SSVEP result, IC-U-Net can optimize the weight with minimal MSE value among all compared artifact removal networks. However, the decoding performance is worse than CLEEGN and even the reference method (ICLabel), which implies that the average error over data points (MSE) is not the best assessment of EEG quality. In the SSVEP experiment, the fundamental frequency of the external rapidly repetitive visual stimulation and its harmonics can be an important feature in classification. Hence, higher quality in the frequency component of EEG data is required. We use the power spectrum density (PSD) to interpret the result of decoding performance. Figure 10 shows the PSD of EEG event data in each class denoised by different methods. We can see the reconstructed EEG spectrum from CLEEGN in each class is similar to the reference. As for IC-U-Net, it seems that the model can not completely reconstruct in several frequency bands (15-20Hz, 30-40Hz), which leads to the low decoding performance. The PSD result shows that there is great distortion in the power density of reconstructed data generated by 1D-ResCNN. Since 1D-ResCNN is a one-dimensional structure, we hypothesize that spatial information is important in EEG artifact removal. A.4 ADDITIONAL TABLES", "agent_review": [{"role": "user", "content": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction\n\n1 INTRODUCTION: Since the first record of human electroencephalogram (EEG) performed almost a century ago (in 1924), EEG has been one of the most widely used non-invasive neural modalities that monitors brain activity in high temporal resolution (Koike et al., 2013; Mehta & Parasuraman, 2013; Sejnowski et al., 2014). Among a variety of modalities, EEG has extensive use in the clinical assessment of neurological and psychiatric conditions, as well as in the research of neuroscience, cognitive science, psychology, and brain-computer interfacing (BCI). EEG signals measure subtle fluctuations of the electrical field driven by local neuroelectrophysiological activity of a population of neurons in the brain cortex (Cohen, 2017). While the electrodes are placed on the surface of the scalp, undesired artifacts may introduce interruption in the measurements and distort the signal of interest. Even in a well-controlled laboratory with a well-trained subject who can maximally keep the body still and relaxed, the EEG signals, unfortunately, could be contaminated by inevitable behavioral and physiological artifacts such as eye blinks, reflective muscle movements, ocular activity, cardiac activity, etc (Croft & Barry, 2000; Wallstrom et al., 2004; Romero et al., 2008). In practice, it is difficult to identify and track the sources of artifacts entirely due to their diversity and non-stationarity. Noise cancellation and artifact removal remain major issues in EEG signal processing and decoding. Currently, numerous methods have been proposed to alleviate the influence of artifacts in EEG signals. Traditional EEG denoising algorithms include filtering, regression, data separation or decomposition (Makeig et al., 1995; Islam et al., 2016; Kothe & Jung, 2016). According to previous meta-analyses on EEG artifact removal literature (Urigu\u0308en & Garcia-Zapirain, 2015; Jiang et al., 2019), independent component analysis (ICA) is especially popular. It is majorly used in 45% of EEG denoising literature. ICA-based artifact removal estimates the component activity by unmixing the EEG data in the channel domain. Through manual or automatic identification, one can exclude the artifact components and then reconstruct the EEG data through back projection based on non-artifact components (Jung et al., 2000a). The fast growth of deep learning methods has drawn state-of-the-art performances in a variety of machine learning problems (LeCun et al., 2015). Lately, deep-learning-based EEG reconstruction has drawn much attention (Leite et al., 2018; Sun et al., 2020; Lopes et al., 2021; Lee et al., 2020; Chuang et al., 2022). Although these methods can effectively remove artifacts from artificial synthetic signals, their performance in reconstructing real EEG data has not yet been validated in terms of decoding labeled EEG data. Meanwhile, the model design of existing deep-learning-based techniques for EEG reconstruction rarely takes the characteristics of EEG into account. In this work, we propose CLEEGN, a ConvoLutional neural network for EEG reconstructioN. CLEEGN is capable of subject-independent EEG construction without any training/calibration for a new subject. The contributions of this work are three-fold: \u2022 a light-weight autoencoder CNN, CLEEGN, with a subject-independent framework that facilitates plug-and-play EEG reconstruction. \u2022 CLEEGN outperforms leading online/offline methods in providing reconstructed EEG data with the best decoding performance for BCI datasets. \u2022 with a novel model design dedicated to EEG reconstruction, CLEEGN characterizes patterns of EEG interpretable and provides neuroscientific insights. 2 RELATED WORK: Current processing techniques for EEG artifact removal are highly varied based on the context where the algorithm may apply. Earlier attempts of EEG denoising assumed that the EEG signals and artifacts appear in different frequency ranges. Based on the assumption, some significant artifacts can be eliminated by the linear filtering method during the online stage (Seifzadeh et al., 2014). Despite the advantages of low computational time, linear filtering hardly removes artifacts that distribute in an overlapped frequency range of EEG signals. Another approach, adaptive filtering (Schlo\u0308gl et al., 2007), estimates artifact signals through additional EOG, EMG, ECG channels and removes these noisy signals from the recording signals by regression. Nevertheless, this approach requires additional auxiliary electrodes and raises the cost and inconvenience in practical applications. The blind source separation (BSS) method in EEG denoising was developed by assuming that the recording EEG signals are linear combinations of the signals from noise sources and the brain neurons. One of the most well-known BSS method is independent component analysis (ICA) (Jung et al., 2000a;b), which is able to separate EEG signals into independent components (ICs) (Makeig et al., 1995). Traditionally, the artifact components extracted by ICA are determined and removed through manual inspection. Recently developed ICLabel can label the ICs provenance into seven different categories: brain, eye, heart, muscle, line noise, channel noise, and other (Pion-Tonachini et al., 2019). Artifact subspace reconstruction (ASR) is another automatic approach, which is based on the principal component analysis (PCA) method (Kothe & Jung, 2016). The ASR method selects relatively noiseless periods from the multi-channel EEG data as reference based on the data distribution. After projecting all EEG data to the principal-component domain, high-variance components projected from the artifacts are detected by a cutoff parameter k. The noiseless signals are reconstructed by preserving the components without carrying artifacts and back-projected to the time domain. The ASR method has been shown capable of improving the quality of ICA decomposition (Chang et al., 2020). Recently, neural network-based methods have been proposed to remove artifacts for EEG data. A variety of network structures have been applied to the framework for removing EEG artifacts and reconstructing clean EEG. A deep convolutional autoencoder (Leite et al., 2018) can enhance the peak-signal-to-noise ratio compared to the linear filtering method via a common CNN autoencoder structure, which has been widely used on image denoising. Their work shows that it seems practicable to transform the EEG waveform through a CNN structure. Later on, a combined framework integrating Bayesian deep learning and ICA (Lee et al., 2020) used thresholding of the EEG data distribution to discard ICs classified as ocular artifacts. These methods leverage the flexibility of deep learning model design and achieve improvements in their assessments. Considering the non- stationary property in EEG data and the degradation phenomenon during training, 1D-ResCNN (Sun et al., 2020) was proposed, which adopted an Inception-Residual module in the network structure. This network is able to remove EOG, ECG, EMG on single-channel synthesis EEG data. Instead of using synthesis EEG data, IC-U-Net (Chuang et al., 2022) was created, which generated pairings of noisy and noiseless EEG data through ICA as training data. The proposed neural network is a one-dimensional adaptation of U-Net architecture trained with the ensemble of four loss functions to minimize the difference between amplitude, velocity, acceleration, and frequency components from the signals. This work showed that their reconstructed signal has higher SNR and can surely increase the number of brain components classified by ICLabel. 3 MATERIALS AND METHODS: CLEEGN Architecture and Model Training. CLEEGN is designed to map multi-channel noisy EEG into a latent space and reconstruct it into noiseless EEG signals. The architecture of CLEEGN is shown in Figure 1. The architecture of the encoder is inspired by an existing CNN model designed for EEG recognition (Wei et al., 2019) and incorporates convolutional blocks that capture spatiotemporal characteristics of EEG data. The first convolution block is used to extract spatial EEG features through a convolutional layer containing C spatial filters whose shape is (C, 1), where C is the number of electrodes of the EEG signals. The second convolution block is used to extract temporal EEG features through a convolutional layer containing NF temporal filters with shape (1, \u230afs \u00d7 0.1\u230b), where fs denotes the sample frequency of the EEG signals. As for the decoder, we design an approximately symmetric structure to the encoder with three convolutional blocks. The first convolutional block decodes the EEG feature with NF temporal kernels with shape is (1, \u230afs\u00d70.1\u230b). The second block decodes the EEG feature by a convolution layer with C spatial kernels of shape (C, 1). The last convolutional block is for projecting the feature domain back to the original time domain. To maintain the shape between the model inputs and outputs, every convolution block applies zeropadding except the first block in the encoder. The detail of the CLEEGN architecture is available in Table 1. As illustrated in Figure 1, the objective of the proposed method is to minimize the difference between noiseless signals, Y , and the model output, Y\u0302 . The recording EEG data, X , is the combination of brain signals, Y , and the signals from multiple noise sources, N . CLEEGN is a denoising autoencoder that is intended to perform artifact removal on EEG data by creating a mapping between multi-channel noisy EEG signals, X , and noiseless signals, Y . The training process of CLEEGN utilizes pairing noisy raw EEG data and noiseless reference EEG data so that the model can learn to transform noisy EEG data into reconstructed EEG data with maximal similarity to the reference data. To generate large-scale reference EEG data, we adopted automatic denoising methods, ICLabel and ASR, to remove artifacts and reconstruct clean waveforms offline. The pairing EEG recording with C channels of continuous noisy/noiseless data was further segmented into training sample pairs. The input size is (C, T ), where T is the number of time points based on the sampling rate of the EEG data in 4-second segments. The windows size and stride are set as T and 0.5T . In the context of plug-and-play EEG reconstruction, CLEEGN was trained as a subject-independent model. To perform subject-independent training and testing, we grouped subjects into k disjoint sets. During the training process, one of the sets was left out for testing. Subjects\u2019 EEG data in the left-alone set were not involved in both training and validation. A complete experiment on a single dataset would result in k different models. The artifact removal performance of a model was evaluated by using the left-alone set. The number of subjects in one set and the EEG duration available for each subject depend on the experiment setting and the dataset used. EEG Reconstruction and Evaluation. The reconstructed EEG data of CLEEGN is generated through an online reconstruction simulation. In the online stage, the system performs artifact removal on a new subject using a pre-trained CLEEGN model without any requirement of training/calibration. Figure 2 shows the online EEG reconstruction simulated in offline. The EEG data from a subject in the left-alone set would be fed into the model sequentially. The stride size of the sliding window is set to 0.5 seconds to minimize the delay in online reconstruction. The fitness of the model can be evaluated by observing the waveform visually or measuring the similarity using the mean square error (MSE) between reconstructed and noiseless EEG. Meanwhile, we propose to use the decoding performance of labeled EEG data as an objective measurement of the reconstructed EEG quality. We employed EEGNet, a compact CNN for end-to-end EEG decoding (Lawhern et al., 2018), as the classifier to decode the labeled EEG data in our study. 4 EXPERIMENTS: Data and Model Fitting. In this work, we applied two labeled EEG datasets, the BCI-ERN dataset 1 and the MAMEM-SSVEP-II dataset 2. We estimate the classification performance by calculating the score of Area Under the Curve (AUC) in ERN binary classification and Top-1 accuracy in the SSVEP task. Figure 3 is the schematic diagram of the process of decoding performance estimation using the reconstructed EEG data. The segmented EEG epochs are divided into three splits within each subject: training set, validation set, and test set. The ratio between classes remained the same in each set. We perform 20 repeated runs with shuffled data to estimate the average performance of each decoding performance evaluation. Detailed descriptions of the two datasets are available in the appendix at A.2. Figure 4 presents the loss curve and the reconstruction result at the end of three different training iterations in a sample run using the ERN dataset. Through this exploration, we can look into the course of model fitting. In the early stage of training, the model reduces the large MSE loss by smoothing the input signals. To further minimize the error between reconstruction and reference (offline denoised signal), the model compensates the intensity to better fit the reference signals. At the convergent stage of training, we can see the reconstructed waveform is very similar to the one in reference data. 1https://www.kaggle.com/c/inria-bci-challenge 2https://www.mamem.eu/results/datasets/ Types of Reference Data. As ICLabel, ASR, and the hybrid ASR-ICLabel are employed to automatically generate large-scale noiseless reference EEG data offline, it is of our interest to investigate what type of reference EEG serves as the best noiseless reference data for the CLEEGN model training. Figure 5 presents the EEG waveforms sampled from Subject 2 in the ERN dataset. The EEG time series from top to bottom represent the recording of Fp1, T7, Cz, T8, and O2 channels. The nine sub-figures visualize the comparison between the noisy raw data and the reference data reconstructed by the ICLabel, ASR with or without ICLabel when the cutoff parameter k was set to 4, 8, 16, and 32. We can see the waveform of ICLabel and the ASR-ICLabel hybrid method with 4 different cutoff parameters k are similar, and the noiseless waveform reconstructed by CLEEGN are highly correlated to the result from these 5 methods. High-amplitude ocular artifacts such as blinking and eye saccades are phenomenal in frontal channels (Fp1) and muscular artifacts featuring high-frequency (around 24-30 Hz in the \u03b2 band) are observed on the T7 channel. CLEEGN, ICA, and ASR-ICLabel hybrid methods can eliminate these kinds of artifacts. Compared to the ICLabel and ASR-ICLabel hybrid methods, ASR is unable to eliminate the high-frequency EMG artifact for this dataset. The tolerance of large amplitude EOG artifact increases with the employment of larger cutoff parameter k. Limited by the reference data generated by ASR, CLEEGN preserves the high-frequency artifact. As for the large amplitude artifacts that failed to be identified by ASR-32 and ASR-16, we can see that CLEEGN is able to mitigate those EOG artifacts. As shown in Figure 6(a), the reference data prepared by ICLabel offer the lowest MSE, i.e. the best fitness for CLEEGN training. We also compare the types of reference EEG data and their corresponding CLEEGN reconstruction results in the decoding performance of the ERN EEG dataset to assess their data quality. As illustrated in Figure 6(b), CLEEGN reconstruction draws better performances for all denoising methods. Although the performance decreases with a smaller cutoff parameter k, CLEEGN can outperform the reference data used for its training. This result suggests that our method not only removes the artifact but also preserves informative brain activity in the EEG under our cross-subject training scheme. As the ICLabel provides the reference data with the best decoding performance and fitness, we chose ICLabel as the source of reference data for further experiments in this study. Training Data Size. We explored the effect of data length per subject in the training set regarding the fitness and the decoding performance of CLEEGN. The training data were segmented from the first 1, 2, 4, 10, 20, and 30 minutes of data in each EEG recording to investigate the trade-off between training time and reconstruction performance. Figure 7(a) shows the fitness of CLEEGN to the reference data. The model trained using the first 10 minutes obtain the minimal value among all duration configurations. Though the difference in MSE between 10, 20, and 30 minutes is not noticeable, Figure 7(b) shows that using the 10-minute training data yields the best decoding performance among all settings. Interestingly, with only one-minute training data from each subject, CLEEGN can achieve comparable decoding performance to the reference data. This indicates that CLEEGN retains its performance even when each subject only contributes a short recording for training. In addition, we explore the effect of the number of subjects included in the training set on the fitness of CLEEGN model training and the decoding performance of the reconstructed EEG data. We randomly reduced the number of subjects for training from 12 to 6, 4, and 2. Since the subset of subjects may influence the performance, we tested multiple combinations and averaged the results. With the decrease in the number of subjects, the MSE value and the standard error (light span area) increase in Figure 8(a), which indicates that the generalization ability of the CLEEGN model reduces when fewer subjects are included for training. In Figure 8(b), we can observe a slight decrease in decoding performance. We consider the number of subjects as an essential factor in the performance of CLEEGN, yet it requires only a few subjects to achieve a satisfactory performance compared to the reference data. Performance Comparison. We compare the performance of CLEEGN against three baseline methods, ICLabel, 1D-ResCNN, and IC-U-Net. Except for the ICLabel that operates offline, CLEEGN and the other two neural network-based methods perform a simulated online reconstruction based on the same training process with the reference data generated by ICLabel. Results of the ERN EEG dataset are shown in Table 2. CLEEGN has the best fitness to the reference data with the minimum MSE, the highest AUC score in the decoding performance, and the least parameters. For the SSVEP EEG data, although not having the minimum MSE, CLEEGN outperforms other methods in the decoding accuracy and the model size. The evaluation across the two datasets suggests an overall superiority of our proposed CLEEGN model over other existing neural network-based methods in online reconstruction. CLEEGN even provides a better reconstruction than the offline ICLabel. These promising results indicate the usability of CLEEGN in online training/calibration-free EEG reconstruction that truly meets the need for real-world applications of EEG-based BCI. Visualization. On account of the interpretability of the CLEEGN model, we visualized its intermediate latent features by mapping onto a 2-dimensional domain based on the principal component analysis (PCA) of the noisy raw EEG data (Wold et al., 1987). Figure 9(a) shows the principal component space and the noisy EEG data channel projections. We observe that the scatters of noisy EEG data retain the spatial relationship of the actual EEG electrodes. The arrangement of frontal electrodes (prefix in Fp, P) to the posterior (prefix in P, O) are projected along the x-axis from right to left. The projection from top to bottom matches the left-side electrodes (suffixed in odd numbers), central electrode (suffixed in z), and right-side electrodes (suffixed in even numbers). The projection suggests that EEG data of adjacent channels tend to show similar waveforms. The blue dots in Figure 9(b), (c) are the projection of latent features resulting from the first and second convolutional layers, which make up the encoder design of CLEEGN. The projection of latent features from the next two convolutional layers is presented in Figure 9(d), (e). From Figure 9(b) to (e), we can see the distribution range of the latent features shrink in the encoder design and expand in the decoder design of CLEEGN. Figure 9(f) compares the distribution of noisy data and CLEEGN reconstructed data, showing that the CLEEGN reconstructed data is a more compact cluster than the noisy EEG in the PCA space. This implies that the process within CLEEGN includes a projection of the original EEG data in the upstream layer, complex temporal filtering and combination in the midstream layers, and a final projection that converts the noiseless latent features back to the channel domain. 5 CONCLUSION: In this work, we have proposed CLEEGN, a novel convolutional neural network for plug-and-play automatic EEG reconstruction. The training of CLEEGN leverages the conventional offline denoising methods, ASR and ICLabel, with automatic component classification to generate abundant noiseless EEG data. The performance of CLEEGN was objectively validated using multiple evaluations including waveform observation; reconstruction error assessment; and decoding accuracy on well-studied, labeled datasets. The experiment results suggest that, even without any calibration, CLEEGN can predominantly preserve inherent brain activity. According to the decoding performance of reconstructed EEG data, CLEEGN outperforms other neural network-based denoising methods on both ERN and SSVEP EEG decoding. From the visualization of the waveform, CLEEGN can remove artifacts from different sources and the waveform is highly correlated to the reference data. Through the visualization of model parameters and latent features, we exhibit model behavior and reveal explainable insights related to existing knowledge of neuroscience. We foresee pervasive applications of CLEEGN in prospective works of EEG decoding and analysis. A APPENDIX:  A.1 NEURAL NETWORK TRAINING CONFIGURATION:  A.1.1 CLEEGN: For training CLEEGN, we used the Adam optimizer with an initial learning rate of 1e-3 without weight decay. Besides, the exponential learning rate scheduler is applied with a gamma of 0.8. As for the loss function, we used Mean Squared Error (MSE). The batch size is set to 64 and the total training epoch is 40. During the training procedure, the model is evaluated using the validation subset at the end of every epoch with the purpose of saving the weights that achieved the lowest validation loss. A.1.2 IC-U-NET The optimizer adopted in IC-U-Net (Chuang et al., 2022) training is SGD with an initial learning rate of 1e-2, momentum of 0.9, and weight decay of 5e-4. The learning rate scheduler used in the training procedure is the multistep scheduler. As for the loss function, a novel ensemble loss proposed in ICU-Net is adopted. This ensemble is a simple linear combination of the Mean Squared Error (MSE) in amplitude, velocity, acceleration, and frequency components of EEG signals. Each term in the loss function has same weight. The batch size is set to 64 and the total training epoch is 200. The weight saving strategy in IC-U-Net training is the same as CLEEGN. A.1.3 1D-RESCNN The Adam optimizer with an initial learning rate of 1e-3 and the Mean Squared Error (MSE) loss function is adopted in 1D-ResCNN (Sun et al., 2020) training. Since the 1D-ResCNN is developed under one-dimensional synthesized EEG data and no explicit instruction of using multi-channel EEG provided, we trained 1D-ResCNN using two different method. For one method, we trained multiple models for each EEG channel respectively. For the other method, we viewed each multichannel EEG segment as a batch and the channel arrangement within a batch is fixed. Under our experiment, the result showed that the second method not only provides an efficient training process, but also results in a better reconstructed performance. The same weight saving strategy in CLEEGN is adopted in 1D-ResCNN training. A.1.4 DECODING PERFORMANCE EVALUATION MODEL: EEGNET: EEGNet (Lawhern et al., 2018) is a famous EEG decoding model and widely used in EEG literature. In the original EEGNet paper, they investigated their proposed model with a different number of kernels and denoted the model with F1 temporal filters and D spatial filters as EEGNet-F1,D. We use two different settings to train the two datasets used. In ERN classification, we use the EEGNet8,2 structure suggested by the EEGNet paper. As for the SSVEP classification, an experimental result showed that EEGNet-100,8 can draw the best performance. We trained and evaluated the decoding performance individually for each subject. We divided the collection of event epochs into three splits within each subject: training set, validation set, and test set with a ratio of 3:1:1. The ratio between classes remained the same in each set. The loss function is categorical cross entropy (CCE) and the Adam optimizer is adopted with a learning rate of 10\u22123 and zero weight decay. The batch size is set to 32 and the total training epoch is 200. A.2 DESCRIPTION OF DATASETS:  A.2.1 DATASET 1: FEEDBACK ERROR-RELATED NEGATIVITY (ERN): Error-related negativity (ERN) can be categorized as a kind of event-related potential (ERP), which occurs after an erroneous or abnormal event perceived by the subject. One characteristic of the feedback ERN is a relatively large negative amplitude approximately 350 ms and a positive amplitude approximately 500 ms after visual feedback triggered by the error event. In this work, we mainly use a well-studied EEG dataset from the BCI Challenge competition hosted by Kaggle to evaluate the artifact removal effectiveness. This dataset includes EEG recordings of 26 subjects (16 subjects labeled and 10 subjects unlabeled) that participated in a P300 speller task. P300 speller is a wellknown BCI system that develops a typing application through P300 response evoked potential. The ERN experiment was conducted under the assumption that the ERN occurred if the subject received incorrect prediction (feedback) from the P300 speller. The objective of the competition was to improve the P300 speller performance by implementing error correction through ERN potentials. We used the 16 subjects with labeled data of which the sampling rate is 200 Hz initially with 56 passive Ag/AgCl EEG sensors. In the interest of increasing the usability of EEG data, we applied some pre-processing procedures to each EEG recording. The EEG data were down-sampled to 128 Hz and re-referenced by the common average reference (CAR) method to eliminate common-mode noise and to zero-center the data. Each recording was band-pass filtered to 1-40 Hz through the FIR filter implemented by EEGLAB to remove DC drifting. During EEG decoding evaluation, we epoch EEG signals in [0, 1.25] second interval to obtain correct and erroneous feedback. A.2.2 DATASET 2: STEADY STATE VISUALLY EVOKED POTENTIAL (SSVEP): Steady state visually evoked potential (SSVEP) is another kind of ERP that is characterized as periodic potential induced by rapidly repetitive visual stimulation. The SSVEP is composed of several discrete frequency components, which consist of the fundamental frequency of the visual stimulus as well as its harmonics. To investigate the generalization ability of the model, we use \u201dEEG SSVEP Dataset II\u201d from Multimedia Authoring & Management using your Eyes & Mind (MAMEM). The dataset includes EEG data from 11 subjects and consists of five different frequencies (6.66, 7.50, 8.57, 10.00, and 12.00 Hz). Each subject was recorded in five sessions and each session included 25 trials (5 trials for each class). The data used a 256-channel HydroCel Geodesic Sensor Net (HCGSN) and captured the signals with a sampling rate of 250 Hz. Since there are several different bad channels in each subject\u2019s EEG recording, we preserved 20 common channels from each subject to train the CLEEGN model. Every recording was down-sampled to 125 Hz, re-referenced by the common average reference (CAR) method, and band-pass filtered to 1-40 Hz. We epoch EEG signals in [1, 5] second interval for each event recorded timestamp. The first second was discarded under the consideration of a reaction delay of the subject. A.3 INVESTIGATION OF DECODING PERFORMANCE EVALUATION: SSVEP In this work, we mainly use decoding performance as the assessment of different artifact removal methods. We provide an interpretation of the relationship between EEG quality and decoding performance. In Section 4 SSVEP result, IC-U-Net can optimize the weight with minimal MSE value among all compared artifact removal networks. However, the decoding performance is worse than CLEEGN and even the reference method (ICLabel), which implies that the average error over data points (MSE) is not the best assessment of EEG quality. In the SSVEP experiment, the fundamental frequency of the external rapidly repetitive visual stimulation and its harmonics can be an important feature in classification. Hence, higher quality in the frequency component of EEG data is required. We use the power spectrum density (PSD) to interpret the result of decoding performance. Figure 10 shows the PSD of EEG event data in each class denoised by different methods. We can see the reconstructed EEG spectrum from CLEEGN in each class is similar to the reference. As for IC-U-Net, it seems that the model can not completely reconstruct in several frequency bands (15-20Hz, 30-40Hz), which leads to the low decoding performance. The PSD result shows that there is great distortion in the power density of reconstructed data generated by 1D-ResCNN. Since 1D-ResCNN is a one-dimensional structure, we hypothesize that spatial information is important in EEG artifact removal. A.4 ADDITIONAL TABLES"}, {"role": "assistant", "content": "I have retrieved the ICLR 2023 Reviewer Guide. Based on the guide, the key aspects to consider are:\n\n*   **Objective of the work:** What is the goal of the paper?\n*   **Strong points:** Is the submission clear, technically correct, experimentally rigorous, reproducible, and does it present novel findings?\n*   **Weak points:** Are there weaknesses in any of the aspects listed above?\n*   **Significance of the work:** Does it contribute new knowledge and sufficient value to the community?\n\nThe review should include:\n\n*   A summary of the paper's contributions.\n*   A list of strong and weak points.\n*   A clear recommendation (accept or reject) with key reasons.\n*   Supporting arguments for the recommendation.\n*   Questions for the authors.\n*   Additional feedback to improve the paper.\n\nNow I will proceed to Step 2: Paper Context Research and Novelty Investigation.\n"}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'query': 'EEG artifact removal deep learning', 'max_results': 5}, name='ArxivTool'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'query': 'EEG reconstruction convolutional neural network', 'max_results': 5}, name='ArxivTool'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'limit': 5, 'query': 'EEG artifact removal deep learning'}, name='SemanticScholarTool'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'limit': 5, 'query': 'EEG reconstruction convolutional neural network'}, name='SemanticScholarTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Found 5 paper(s) on arXiv\",\n  \"output\": [\n    {\n      \"title\": \"Deep learning denoising for EOG artifacts removal from EEG signals\",\n      \"authors\": [\n        \"Najmeh Mashhadi\",\n        \"Abolfazl Zargari Khuzani\",\n        \"Morteza Heidari\",\n        \"Donya Khaledyan\"\n      ],\n      \"published\": \"2020-09-12T23:28:12+00:00\",\n      \"summary\": \"There are many sources of interference encountered in the\\nelectroencephalogram (EEG) recordings, specifically ocular, muscular, and\\ncardiac artifacts. Rejection of EEG artifacts is an essential process in EEG\\nanalysis since such artifacts cause many problems in EEG signals analysis. One\\nof the most challenging issues in EEG denoising processes is removing the\\nocular artifacts where Electrooculographic (EOG), and EEG signals have an\\noverlap in both frequency and time domains. In this paper, we build and train a\\ndeep learning model to deal with this challenge and remove the ocular artifacts\\neffectively. In the proposed scheme, we convert each EEG signal to an image to\\nbe fed to a U-NET model, which is a deep learning model usually used in image\\nsegmentation tasks. We proposed three different schemes and made our U-NET\\nbased models learn to purify contaminated EEG signals similar to the process\\nused in the image segmentation process. The results confirm that one of our\\nschemes can achieve a reliable and promising accuracy to reduce the Mean square\\nerror between the target signal (Pure EEGs) and the predicted signal (Purified\\nEEGs).\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2009.08809v1\"\n    },\n    {\n      \"title\": \"Embedding Decomposition for Artifacts Removal in EEG Signals\",\n      \"authors\": [\n        \"Junjie Yu\",\n        \"Chenyi Li\",\n        \"Kexin Lou\",\n        \"Chen Wei\",\n        \"Quanying Liu\"\n      ],\n      \"published\": \"2021-12-02T05:30:38+00:00\",\n      \"summary\": \"Electroencephalogram (EEG) recordings are often contaminated with artifacts.\\nVarious methods have been developed to eliminate or weaken the influence of\\nartifacts. However, most of them rely on prior experience for analysis. Here,\\nwe propose an deep learning framework to separate neural signal and artifacts\\nin the embedding space and reconstruct the denoised signal, which is called\\nDeepSeparator. DeepSeparator employs an encoder to extract and amplify the\\nfeatures in the raw EEG, a module called decomposer to extract the trend,\\ndetect and suppress artifact and a decoder to reconstruct the denoised signal.\\nBesides, DeepSeparator can extract the artifact, which largely increases the\\nmodel interpretability. The proposed method is tested with a semi-synthetic EEG\\ndataset and a real task-related EEG dataset, suggesting that DeepSeparator\\noutperforms the conventional models in both EOG and EMG artifact removal.\\nDeepSeparator can be extended to multi-channel EEG and data of any length. It\\nmay motivate future developments and application of deep learning-based EEG\\ndenoising. The code for DeepSeparator is available at\\nhttps://github.com/ncclabsustech/DeepSeparator.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2112.00989v2\"\n    },\n    {\n      \"title\": \"EEG Artifact Detection and Correction with Deep Autoencoders\",\n      \"authors\": [\n        \"David Aquilu\u00e9-Llorens\",\n        \"Aureli Soria-Frisch\"\n      ],\n      \"published\": \"2025-02-12T12:06:36+00:00\",\n      \"summary\": \"EEG signals convey important information about brain activity both in healthy\\nand pathological conditions. However, they are inherently noisy, which poses\\nsignificant challenges for accurate analysis and interpretation. Traditional\\nEEG artifact removal methods, while effective, often require extensive expert\\nintervention. This study presents LSTEEG, a novel LSTM-based autoencoder\\ndesigned for the detection and correction of artifacts in EEG signals.\\nLeveraging deep learning, particularly LSTM layers, LSTEEG captures non-linear\\ndependencies in sequential EEG data. LSTEEG demonstrates superior performance\\nin both artifact detection and correction tasks compared to other\\nstate-of-the-art convolutional autoencoders. Our methodology enhances the\\ninterpretability and utility of the autoencoder's latent space, enabling\\ndata-driven automated artefact removal in EEG its application in downstream\\ntasks. This research advances the field of efficient and accurate multi-channel\\nEEG preprocessing, and promotes the implementation and usage of automated EEG\\nanalysis pipelines for brain health applications.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2502.08686v1\"\n    },\n    {\n      \"title\": \"Removal of Ocular Artifacts in EEG Using Deep Learning\",\n      \"authors\": [\n        \"Mehmet Akif Ozdemir\",\n        \"Sumeyye Kizilisik\",\n        \"Onan Guren\"\n      ],\n      \"published\": \"2022-09-24T11:19:52+00:00\",\n      \"summary\": \"EEG signals are complex and low-frequency signals. Therefore, they are easily\\ninfluenced by external factors. EEG artifact removal is crucial in neuroscience\\nbecause artifacts have a significant impact on the results of EEG analysis. The\\nremoval of ocular artifacts is the most challenging among these artifacts. In\\nthis study, a novel ocular artifact removal method is presented by developing\\nbidirectional long-short term memory (BiLSTM)-based deep learning (DL) models.\\nWe created a benchmarking dataset to train and test proposed DL models by\\ncombining the EEGdenoiseNet and DEAP datasets. We also augmented the data by\\ncontaminating ground-truth clean EEG signals with EOG at various SNR levels.\\nThe BiLSTM network is then fed to features extracted from augmented signals\\nusing highly-localized time-frequency (TF) coefficients obtained by wavelet\\nsynchrosqueezed transform (WSST). We also compare the WSST-based DL model\\nresults with traditional TF analysis (TFA) methods namely short-time Fourier\\ntransformation (STFT) and continuous wavelet transform (CWT) as well as\\naugmented raw signals. The best average MSE value of 0.3066 was obtained by the\\nfirst time-proposed BiLSTM-based WSST-Net model. Our results demonstrated the\\nWSST-Net model significantly improves artifact removal performance compared to\\ntraditional TF and raw signal methods. Also, the proposed EOG removal approach\\nreveals that it outperforms many conventional and DL-based ocular artifact\\nremoval methods in the literature.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2209.11980v1\"\n    },\n    {\n      \"title\": \"A novel convolutional neural network model to remove muscle artifacts from EEG\",\n      \"authors\": [\n        \"Haoming Zhang\",\n        \"Chen Wei\",\n        \"Mingqi Zhao\",\n        \"Haiyan Wu\",\n        \"Quanying Liu\"\n      ],\n      \"published\": \"2020-10-22T13:33:52+00:00\",\n      \"summary\": \"The recorded electroencephalography (EEG) signals are usually contaminated by\\nmany artifacts. In recent years, deep learning models have been used for\\ndenoising of electroencephalography (EEG) data and provided comparable\\nperformance with that of traditional techniques. However, the performance of\\nthe existing networks in electromyograph (EMG) artifact removal was limited and\\nsuffered from the over-fitting problem. Here we introduce a novel convolutional\\nneural network (CNN) with gradually ascending feature dimensions and\\ndownsampling in time series for removing muscle artifacts in EEG data. Compared\\nwith other types of convolutional networks, this model largely eliminates the\\nover-fitting and significantly outperforms four benchmark networks in\\nEEGdenoiseNet. Our study suggested that the deep network architecture might\\nhelp avoid overfitting and better remove EMG artifacts in EEG.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2010.11709v2\"\n    }\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ArxivTool` with \n```json\n{\n  \"query\": \"EEG artifact removal deep learning\",\n  \"max_results\": 5\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Found 5 paper(s) on arXiv\",\n  \"output\": [\n    {\n      \"title\": \"Eeg2vec: Self-Supervised Electroencephalographic Representation Learning\",\n      \"authors\": [\n        \"Qiushi Zhu\",\n        \"Xiaoying Zhao\",\n        \"Jie Zhang\",\n        \"Yu Gu\",\n        \"Chao Weng\",\n        \"Yuchen Hu\"\n      ],\n      \"published\": \"2023-05-23T11:34:14+00:00\",\n      \"summary\": \"Recently, many efforts have been made to explore how the brain processes\\nspeech using electroencephalographic (EEG) signals, where deep learning-based\\napproaches were shown to be applicable in this field. In order to decode speech\\nsignals from EEG signals, linear networks, convolutional neural networks (CNN)\\nand long short-term memory networks are often used in a supervised manner.\\nRecording EEG-speech labeled data is rather time-consuming and laborious, while\\nunlabeled EEG data is abundantly available. Whether self-supervised methods are\\nhelpful to learn EEG representation to boost the performance of EEG\\nauditory-related tasks has not been well explored. In this work, we first\\npropose a self-supervised model based on contrastive loss and reconstruction\\nloss to learn EEG representations, and then use the obtained pre-trained model\\nas a feature extractor for downstream tasks. Second, for two considered\\ndownstream tasks, we use CNNs and Transformer networks to learn local features\\nand global features, respectively. Finally, the EEG data from other channels\\nare mixed into the chosen EEG data for augmentation. The effectiveness of our\\nmethod is verified on the EEG match-mismatch and EEG regression tasks of the\\nICASSP2023 Auditory EEG Challenge.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2305.13957v1\"\n    },\n    {\n      \"title\": \"Subject Disentanglement Neural Network for Speech Envelope Reconstruction from EEG\",\n      \"authors\": [\n        \"Li Zhang\",\n        \"Jiyao Liu\"\n      ],\n      \"published\": \"2025-01-15T10:08:07+00:00\",\n      \"summary\": \"Reconstructing speech envelopes from EEG signals is essential for exploring\\nneural mechanisms underlying speech perception. Yet, EEG variability across\\nsubjects and physiological artifacts complicate accurate reconstruction. To\\naddress this problem, we introduce Subject Disentangling Neural Network\\n(SDN-Net), which disentangles subject identity information from reconstructed\\nspeech envelopes to enhance cross-subject reconstruction accuracy. SDN-Net\\nintegrates three key components: MLA-Codec, MPN-MI, and CTA-MTDNN. The\\nMLA-Codec, a fully convolutional neural network, decodes EEG signals into\\nspeech envelopes. The CTA-MTDNN module, a multi-scale time-delay neural network\\nwith channel and temporal attention, extracts subject identity features from\\nEEG signals. Lastly, the MPN-MI module, a mutual information estimator with a\\nmulti-layer perceptron, supervises the removal of subject identity information\\nfrom the reconstructed speech envelope. Experiments on the Auditory EEG\\nDecoding Dataset demonstrate that SDN-Net achieves superior performance in\\ninner- and cross-subject speech envelope reconstruction compared to recent\\nstate-of-the-art methods.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2501.08693v1\"\n    },\n    {\n      \"title\": \"CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction\",\n      \"authors\": [\n        \"Pin-Hua Lai\",\n        \"Bo-Shan Wang\",\n        \"Wei-Chun Yang\",\n        \"Hsiang-Chieh Tsou\",\n        \"Chun-Shu Wei\"\n      ],\n      \"published\": \"2022-10-12T07:56:09+00:00\",\n      \"summary\": \"Human electroencephalography (EEG) is a brain monitoring modality that senses\\ncortical neuroelectrophysiological activity in high-temporal resolution. One of\\nthe greatest challenges posed in applications of EEG is the unstable signal\\nquality susceptible to inevitable artifacts during recordings. To date, most\\nexisting techniques for EEG artifact removal and reconstruction are applicable\\nto offline analysis solely, or require individualized training data to\\nfacilitate online reconstruction. We have proposed CLEEGN, a novel\\nconvolutional neural network for plug-and-play automatic EEG reconstruction.\\nCLEEGN is based on a subject-independent pre-trained model using existing data\\nand can operate on a new user without any further calibration. The performance\\nof CLEEGN was validated using multiple evaluations including waveform\\nobservation, reconstruction error assessment, and decoding accuracy on\\nwell-studied labeled datasets. The results of simulated online validation\\nsuggest that, even without any calibration, CLEEGN can largely preserve\\ninherent brain activity and outperforms leading online/offline artifact removal\\nmethods in the decoding accuracy of reconstructed EEG data. In addition,\\nvisualization of model parameters and latent features exhibit the model\\nbehavior and reveal explainable insights related to existing knowledge of\\nneuroscience. We foresee pervasive applications of CLEEGN in prospective works\\nof online plug-and-play EEG decoding and analysis.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2210.05988v2\"\n    },\n    {\n      \"title\": \"Decoding Taste Information in Human Brain: A Temporal and Spatial Reconstruction Data Augmentation Method Coupled with Taste EEG\",\n      \"authors\": [\n        \"Xiuxin Xia\",\n        \"Yuchao Yang\",\n        \"Yan Shi\",\n        \"Wenbo Zheng\",\n        \"Hong Men\"\n      ],\n      \"published\": \"2023-07-01T08:56:34+00:00\",\n      \"summary\": \"For humans, taste is essential for perceiving food's nutrient content or\\nharmful components. The current sensory evaluation of taste mainly relies on\\nartificial sensory evaluation and electronic tongue, but the former has strong\\nsubjectivity and poor repeatability, and the latter is not flexible enough.\\nThis work proposed a strategy for acquiring and recognizing taste\\nelectroencephalogram (EEG), aiming to decode people's objective perception of\\ntaste through taste EEG. Firstly, according to the proposed experimental\\nparadigm, the taste EEG of subjects under different taste stimulation was\\ncollected. Secondly, to avoid insufficient training of the model due to the\\nsmall number of taste EEG samples, a Temporal and Spatial Reconstruction Data\\nAugmentation (TSRDA) method was proposed, which effectively augmented the taste\\nEEG by reconstructing the taste EEG's important features in temporal and\\nspatial dimensions. Thirdly, a multi-view channel attention module was\\nintroduced into a designed convolutional neural network to extract the\\nimportant features of the augmented taste EEG. The proposed method has accuracy\\nof 99.56%, F1-score of 99.48%, and kappa of 99.38%, proving the method's\\nability to distinguish the taste EEG evoked by different taste stimuli\\nsuccessfully. In summary, combining TSRDA with taste EEG technology provides an\\nobjective and effective method for sensory evaluation of food taste.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2307.05365v1\"\n    },\n    {\n      \"title\": \"ConvConcatNet: a deep convolutional neural network to reconstruct mel spectrogram from the EEG\",\n      \"authors\": [\n        \"Xiran Xu\",\n        \"Bo Wang\",\n        \"Yujie Yan\",\n        \"Haolin Zhu\",\n        \"Zechen Zhang\",\n        \"Xihong Wu\",\n        \"Jing Chen\"\n      ],\n      \"published\": \"2024-01-10T07:15:45+00:00\",\n      \"summary\": \"To investigate the processing of speech in the brain, simple linear models\\nare commonly used to establish a relationship between brain signals and speech\\nfeatures. However, these linear models are ill-equipped to model a highly\\ndynamic and complex non-linear system like the brain. Although non-linear\\nmethods with neural networks have been developed recently, reconstructing\\nunseen stimuli from unseen subjects' EEG is still a highly challenging task.\\nThis work presents a novel method, ConvConcatNet, to reconstruct mel-specgrams\\nfrom EEG, in which the deep convolution neural network and extensive\\nconcatenation operation were combined. With our ConvConcatNet model, the\\nPearson correlation between the reconstructed and the target mel-spectrogram\\ncan achieve 0.0420, which was ranked as No.1 in the Task 2 of the Auditory EEG\\nChallenge. The codes and models to implement our work will be available on\\nGithub: https://github.com/xuxiran/ConvConcatNet\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2401.04965v1\"\n    }\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ArxivTool` with \n```json\n{\n  \"query\": \"EEG reconstruction convolutional neural network\",\n  \"max_results\": 5\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Found 5 paper(s) on Semantic Scholar\",\n  \"output\": [\n    {\n      \"title\": \"Advanced Signal Processing and Machine/Deep Learning Approaches on a Preprocessing Block for EEG Artifact Removal: A Comprehensive Review\",\n      \"authors\": [\n        \"Said Agounad\",\n        \"Ousama Tarahi\",\n        \"Mustapha Moufassih\",\n        \"Soukaina Hamou\",\n        \"Anas Mazid\"\n      ],\n      \"abstract\": null,\n      \"url\": \"https://www.semanticscholar.org/paper/303515b75925dd7294c659b71874ec4ab593b6b3\"\n    },\n    {\n      \"title\": \"AnEEG: leveraging deep learning for effective artifact removal in EEG data\",\n      \"authors\": [\n        \"Bhabesh Kalita\",\n        \"Nabamita Deb\",\n        \"Daisy Das\"\n      ],\n      \"abstract\": \"In neuroscience and clinical diagnostics, electroencephalography (EEG) is a crucial instrument for capturing neural activity. However, this signal is polluted by different artifacts like muscle activity, eye blinks, environmental interference, etc., which makes it more difficult to retrieve important information from the signal. Deep learning methods have demonstrated the potential to lower these artifacts and enhance the EEG\u2019s quality in recent years. In this work, a novel deep learning method,\u201cAnEEG\u201d is presented for eliminating artifacts from EEG signal. The quantitative matrices NMSE, RMSE, CC, SNR and SAR are calculated to confirm the effectiveness of the proposed model. Through this process, it was found that the suggested model outperformed wavelet decomposition techniques. The model achieves lower NMSE and RMSE values, which indicates better agreement with the original signal. Achieving higher CC values means stronger linear agreement with the ground truth signals. Additionally, the model shows improvements in both SNR and SAR values. Overall, this suggested approach showcases promising results in improving the quality of EEG data by utilizing deep learning.\",\n      \"url\": \"https://www.semanticscholar.org/paper/0c7db46cf5830e4f67101b8879e26a9862fba8d0\"\n    },\n    {\n      \"title\": \"EEG Artifact Removal by Bayesian Deep Learning & ICA\",\n      \"authors\": [\n        \"Sangmin S. Lee\",\n        \"Kiwon Lee\",\n        \"Guiyeom Kang\"\n      ],\n      \"abstract\": \"Artifact removal is important for EEG signal processing because artifacts adversely affect analysis results. To preserve normal EEG signal, several methods based on independent component analysis (ICA) have been studied and artifacts are removed by discarding independent components (ICs) classified as artifacts. In this study, a method using Bayesian deep learning and attention module is presented to improve performance of the classifier for ICs. Probability value is computed through the method to predict if a component is artifact and to treat ambiguous inputs. The attention module achieves increasing classification accuracy and shows the map of the region where the classifier concentrates on.\",\n      \"url\": \"https://www.semanticscholar.org/paper/5e5e25957fdcfb3675acea35687da9796f8f5fb8\"\n    },\n    {\n      \"title\": \"Denosieformer: A Transformer-Based Approach for Single-Channel EEG Artifact Removal\",\n      \"authors\": [\n        \"Junfu Chen\",\n        \"Dechang Pi\",\n        \"Xiaoyi Jiang\",\n        \"Yue Xu\",\n        \"Yang Chen\",\n        \"Xixuan Wang\"\n      ],\n      \"abstract\": \"As an essential tool in clinical medicine, brain research, and the study of neurological diseases, the electroencephalogram (EEG) is susceptible to various physiological signals, posing difficulties for subsequent research and analysis. Deep learning (DL), an end-to-end method without expert knowledge, is gaining attention in the field of EEG artifact removal. However, the existing DL methods cannot globally observe the latent relationship of EEG signals. They lack the capacity to effectively explore the feature patterns of the signals, resulting in unsatisfactory denoising performance. To address the aforementioned issues, this article proposes a transformer-based EEG denoising architecture (denoiseformer) to solve the artifact removal problem of single-channel contaminated EEG signals. First, the contaminated EEG signals are divided into multiple slices as input so that the transformer module can extract the potential pattern relationships between the slices. Then, we design a multiscale feature extraction and fusion module to obtain high-resolution features with richer and more robust information. Next, a novel attention mechanism, named slice pattern attention, is applied to extract the global information on the temporal dimension of each EEG slice. In addition, a residual variational autoencoder structure can generate superior latent codes between different EEG samples, which help the encoder to distinguish contaminated signal patterns, and hence, improve the recovery performance for artifacts. Comprehensive experiments on three types of semisimulated and one real-laboratory artifacts show our model can obtain excellent performance on two quantitative metrics (relative root-mean-square error and correlation coefficient) for artifact removal tasks in various scenarios.\",\n      \"url\": \"https://www.semanticscholar.org/paper/a47654873c25a47081576ddb5076ba4fc51617d2\"\n    },\n    {\n      \"title\": \"Advanced Ocular Artifact Removal In EEG For Enhanced Schizophrenia Diagnosis Using Wavelet Shrinkage And LSTM Deep Learning Techniques\",\n      \"authors\": [\n        \"Bh.V.V.S.R.K.K.Pavan\",\n        \"Herald Anantha\"\n      ],\n      \"abstract\": \".in Schizophrenia is a potentially life-threatening mental disease. It is distinguished by deficiencies in cognition, affect, and conduct. Countless individuals worldwide are affected by it. Accurate diagnosis can be achieved by analyzing EEG data. Researchers can non-invasively measure brain activity by using an electroencephalogram (EEG). Blinking of the eyes results in the production of an electrooculogram, which is defined by a robust electrical potential that envelops the eyes. The electrooculogram derives its name from this potential. The EEG data might become corrupted when cortical activity spreads across the scalp. Visual distortions of this nature are commonly known as ocular artifacts (OAs). The device is a commonly seen form of interference that can occur in electroencephalogram (EEG) research. Prior to processing EEG data, it is crucial to eliminate or minimize ocular artifacts (OAs). In order to eliminate the typical ocular artifacts (OAs) that occur naturally, it is essential to perform either two electrooculograms (EOGs) or a multichannel electroencephalogram (EEG). Our study has led us to develop a consistent method by combining EEMD with Fejer-Korovkin\",\n      \"url\": \"https://www.semanticscholar.org/paper/6314876b797925889b7feeee8b7e475077d6e177\"\n    }\n  ]\n}\n```\n", "metadata": {"title": "Invoking `SemanticScholarTool` with \n```json\n{\n  \"limit\": 5,\n  \"query\": \"EEG artifact removal deep learning\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Found 5 paper(s) on Semantic Scholar\",\n  \"output\": [\n    {\n      \"title\": \"CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction\",\n      \"authors\": [\n        \"Pin-Hua Lai\",\n        \"Wei Yang\",\n        \"Hsiang-Chieh Tsou\",\n        \"Chunshan Wei\"\n      ],\n      \"abstract\": \"Human electroencephalography (EEG) is a brain monitoring modality that senses cortical neuroelectrophysiological activity in high-temporal resolution. One of the greatest challenges posed in applications of EEG is the unstable signal quality susceptible to inevitable artifacts during recordings. To date, most existing techniques for EEG artifact removal and reconstruction are applicable to offline analysis solely, or require individualized training data to facilitate online reconstruction. We have proposed CLEEGN, a novel convolutional neural network for plug-and-play automatic EEG reconstruction. CLEEGN is based on a subject-independent pre-trained model using existing data and can operate on a new user without any further calibration. The performance of CLEEGN was validated using multiple evaluations including waveform observation, reconstruction error assessment, and decoding accuracy on well-studied labeled datasets. The results of simulated online validation suggest that, even without any calibration, CLEEGN can largely preserve inherent brain activity and outperforms leading online/offline artifact removal methods in the decoding accuracy of reconstructed EEG data. In addition, visualization of model parameters and latent features exhibit the model behavior and reveal explainable insights related to existing knowledge of neuroscience. We foresee pervasive applications of CLEEGN in prospective works of online plug-and-play EEG decoding and analysis.\",\n      \"url\": \"https://www.semanticscholar.org/paper/1c88fa4869591bfe6ebb1a424e08c9fab36f8010\"\n    },\n    {\n      \"title\": \"Multi-Channel EEG Emotion Recognition Based on Parallel Transformer and 3D-Convolutional Neural Network\",\n      \"authors\": [\n        \"Jie Sun\",\n        \"Xuan Wang\",\n        \"Kangning Zhao\",\n        \"Siyuan Hao\",\n        \"Tianyu Wang\"\n      ],\n      \"abstract\": \"Due to its covert and real-time properties, electroencephalography (EEG) has long been the medium of choice for emotion identification research. Currently, EEG-based emotion recognition focuses on exploiting temporal, spatial, and spatiotemporal EEG data for emotion recognition. Due to the lack of consideration of both spatial and temporal aspects of EEG data, the accuracy of EEG emotion detection algorithms employing solely spatial or temporal variables is low. In addition, approaches that use spatiotemporal properties of EEG for emotion recognition take temporal and spatial characteristics of EEG into account; however, these methods extract temporal and spatial information directly from EEG data. Since there is no reconstruction of the EEG data format, the temporal and spatial properties of the EEG data cannot be extracted efficiently. To address the aforementioned issues, this research proposes a multi-channel EEG emotion identification model based on the parallel transformer and three-dimensional convolutional neural networks (3D-CNN). First, parallel channel EEG data and position reconstruction EEG sequence data are created separately. The temporal and spatial characteristics of EEG are then retrieved using transformer and 3D-CNN models. Finally, the features of the two parallel modules are combined to form the final features for emotion recognition. On the DEAP, Dreamer, and SEED databases, the technique achieved greater accuracy in emotion recognition than other methods. It demonstrates the efficiency of the strategy described in this paper.\",\n      \"url\": \"https://www.semanticscholar.org/paper/0bf5469ca51cf71d95f53436ff9694e74d05d0ef\"\n    },\n    {\n      \"title\": \"MSFR-GCN: A Multi-Scale Feature Reconstruction Graph Convolutional Network for EEG Emotion and Cognition Recognition\",\n      \"authors\": [\n        \"Deng Pan\",\n        \"Haohao Zheng\",\n        \"Feifan Xu\",\n        \"Ouyang Yu\",\n        \"Zhe Jia\",\n        \"Chu Wang\",\n        \"Hong Zeng\"\n      ],\n      \"abstract\": \"Graph Convolutional Network (GCN) excels at EEG recognition by capturing brain connections, but previous studies neglect the important EEG feature itself. In this study, we propose MSFR-GCN, a multi-scale feature reconstruction GCN for recognizing emotion and cognition tasks. Specifically, MSFR-GCN includes the MSFR and feature-pool characteristically, with the MSFR consisting of two sub-modules, multi-scale Squeeze-and-Excitation (MSSE) and multi-scale sample re-weighting (MSSR). MSSE assigns weights to channels and frequency bands based on their separate statistical information, while MSSR assigns sample weights based on combined channel and frequency information. The feature-pool, which pools across the feature dimension, is applied after GCN to retain EEG channel information. The MSFR-GCN achieves excellent results in emotion recognition when first tested on two public datasets, SEED and SEED-IV. Than the MSFR-GCN is tested on our self-collected Emotion and Cognition EEG dataset (ECED) for both emotion and cognition classification tasks. The results show MSFR-GCN\u2019s good performance in emotion and cognition classification tasks and reveal the implicit relationship between the two, which may provide aid in the rehabilitation of people with cognitive impairments from an emotional perspective.\",\n      \"url\": \"https://www.semanticscholar.org/paper/db208be202cf8afd5bc8dbdea43c8d133deb48e7\"\n    },\n    {\n      \"title\": \"Subject Disentanglement Neural Network for Speech Envelope Reconstruction from EEG\",\n      \"authors\": [\n        \"Li Zhang\",\n        \"Jiyao Liu\",\n        \"Leiming Xie\"\n      ],\n      \"abstract\": \"Reconstructing speech envelopes from EEG signals is essential for exploring neural mechanisms underlying speech perception. Yet, EEG variability across subjects and physiological artifacts complicate accurate reconstruction. To address this problem, we introduce Subject Disentangling Neural Network (SDN-Net), which disentangles subject identity information from reconstructed speech envelopes to enhance cross-subject reconstruction accuracy. SDN-Net integrates three key components: MLA-Codec, MPN-MI, and CTA-MTDNN. The MLA-Codec, a fully convolutional neural network, decodes EEG signals into speech envelopes. The CTA-MTDNN module, a multi-scale time-delay neural network with channel and temporal attention, extracts subject identity features from EEG signals. Lastly, the MPN-MI module, a mutual information estimator with a multilayer perceptron, supervises the removal of subject identity information from the reconstructed speech envelope. Experiments on the Auditory EEG Decoding Dataset demonstrate that SDN-Net achieves superior performance in inner- and cross-subject speech envelope reconstruction compared to recent state-of-the-art methods.\",\n      \"url\": \"https://www.semanticscholar.org/paper/15fcb108167aec6d7b6a46fda2b90554277fd285\"\n    },\n    {\n      \"title\": \"Classification and Reconstruction of Biomedical Signals Based on Convolutional Neural Network\",\n      \"authors\": [\n        \"Zijiang Zhu\",\n        \"Hang Chen\",\n        \"Song Xie\",\n        \"Y. Hu\",\n        \"Jing Chang\"\n      ],\n      \"abstract\": \"The efficient biological signal processing method can effectively improve the efficiency of researchers to explore the work of life mechanism, so as to better reveal the relationship between physiological structure and function, thus promoting the generation of major biological discoveries; high-precision medical signal analysis strategy can, to a certain extent, share the pressure of doctors' clinical diagnosis and assist them to formulate more favorable plans for disease prevention and treatment, so as to alleviate patients' physical and mental pain and improve the overall health level of the society. This article in biomedical signal is very representative of the two types of signals: mammary gland molybdenum target X-ray image (mammography) and the EEG signal as the research object, combined with the deep learning field of CNN; the most representative model is two kinds of biomedical signal classification, and reconstruction methods conducted a series of research: (1) a new classification method of breast masses based on multi-layer CNN is proposed. The method includes a CNN feature representation network for breast masses and a feature decision mechanism that simulates the physician's diagnosis process. By comparing with the objective classification accuracy of other methods for the identification of benign and malignant breast masses, the method achieved the highest classification accuracy of 97.0% under different values of c and gamma, which further verified the effectiveness of the proposed method in the identification of breast masses based on molybdenum target X-ray images. (2) An EEG signal classification method based on spatiotemporal fusion CNN is proposed. This method includes a multi-channel input classification network focusing on spatial information of EEG signals, a single-channel input classification network focusing on temporal information of EEG signals, and a spatial-temporal fusion strategy. Through comparative experiments on EEG signal classification tasks, the effectiveness of the proposed method was verified from the aspects of objective classification accuracy, number of model parameters, and subjective evaluation of CNN feature representation validity. It can be seen that the method proposed in this paper not only has high accuracy, but also can be well applied to the classification and reconstruction of biomedical signals.\",\n      \"url\": \"https://www.semanticscholar.org/paper/f1ca3a48737f1c24bbc7268c95c9861d8dfb632a\"\n    }\n  ]\n}\n```\n", "metadata": {"title": "Invoking `SemanticScholarTool` with \n```json\n{\n  \"limit\": 5,\n  \"query\": \"EEG reconstruction convolutional neural network\"\n}\n```\n", "id": 3, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ArxivTool', response={'result': {'status': 'success', 'message': 'Found 5 paper(s) on arXiv', 'output': [{'title': 'Deep learning denoising for EOG artifacts removal from EEG signals', 'authors': ['Najmeh Mashhadi', 'Abolfazl Zargari Khuzani', 'Morteza Heidari', 'Donya Khaledyan'], 'published': '2020-09-12T23:28:12+00:00', 'summary': 'There are many sources of interference encountered in the\\nelectroencephalogram (EEG) recordings, specifically ocular, muscular, and\\ncardiac artifacts. Rejection of EEG artifacts is an essential process in EEG\\nanalysis since such artifacts cause many problems in EEG signals analysis. One\\nof the most challenging issues in EEG denoising processes is removing the\\nocular artifacts where Electrooculographic (EOG), and EEG signals have an\\noverlap in both frequency and time domains. In this paper, we build and train a\\ndeep learning model to deal with this challenge and remove the ocular artifacts\\neffectively. In the proposed scheme, we convert each EEG signal to an image to\\nbe fed to a U-NET model, which is a deep learning model usually used in image\\nsegmentation tasks. We proposed three different schemes and made our U-NET\\nbased models learn to purify contaminated EEG signals similar to the process\\nused in the image segmentation process. The results confirm that one of our\\nschemes can achieve a reliable and promising accuracy to reduce the Mean square\\nerror between the target signal (Pure EEGs) and the predicted signal (Purified\\nEEGs).', 'pdf_url': 'http://arxiv.org/pdf/2009.08809v1'}, {'title': 'Embedding Decomposition for Artifacts Removal in EEG Signals', 'authors': ['Junjie Yu', 'Chenyi Li', 'Kexin Lou', 'Chen Wei', 'Quanying Liu'], 'published': '2021-12-02T05:30:38+00:00', 'summary': 'Electroencephalogram (EEG) recordings are often contaminated with artifacts.\\nVarious methods have been developed to eliminate or weaken the influence of\\nartifacts. However, most of them rely on prior experience for analysis. Here,\\nwe propose an deep learning framework to separate neural signal and artifacts\\nin the embedding space and reconstruct the denoised signal, which is called\\nDeepSeparator. DeepSeparator employs an encoder to extract and amplify the\\nfeatures in the raw EEG, a module called decomposer to extract the trend,\\ndetect and suppress artifact and a decoder to reconstruct the denoised signal.\\nBesides, DeepSeparator can extract the artifact, which largely increases the\\nmodel interpretability. The proposed method is tested with a semi-synthetic EEG\\ndataset and a real task-related EEG dataset, suggesting that DeepSeparator\\noutperforms the conventional models in both EOG and EMG artifact removal.\\nDeepSeparator can be extended to multi-channel EEG and data of any length. It\\nmay motivate future developments and application of deep learning-based EEG\\ndenoising. The code for DeepSeparator is available at\\nhttps://github.com/ncclabsustech/DeepSeparator.', 'pdf_url': 'http://arxiv.org/pdf/2112.00989v2'}, {'title': 'EEG Artifact Detection and Correction with Deep Autoencoders', 'authors': ['David Aquilu\u00e9-Llorens', 'Aureli Soria-Frisch'], 'published': '2025-02-12T12:06:36+00:00', 'summary': \"EEG signals convey important information about brain activity both in healthy\\nand pathological conditions. However, they are inherently noisy, which poses\\nsignificant challenges for accurate analysis and interpretation. Traditional\\nEEG artifact removal methods, while effective, often require extensive expert\\nintervention. This study presents LSTEEG, a novel LSTM-based autoencoder\\ndesigned for the detection and correction of artifacts in EEG signals.\\nLeveraging deep learning, particularly LSTM layers, LSTEEG captures non-linear\\ndependencies in sequential EEG data. LSTEEG demonstrates superior performance\\nin both artifact detection and correction tasks compared to other\\nstate-of-the-art convolutional autoencoders. Our methodology enhances the\\ninterpretability and utility of the autoencoder's latent space, enabling\\ndata-driven automated artefact removal in EEG its application in downstream\\ntasks. This research advances the field of efficient and accurate multi-channel\\nEEG preprocessing, and promotes the implementation and usage of automated EEG\\nanalysis pipelines for brain health applications.\", 'pdf_url': 'http://arxiv.org/pdf/2502.08686v1'}, {'title': 'Removal of Ocular Artifacts in EEG Using Deep Learning', 'authors': ['Mehmet Akif Ozdemir', 'Sumeyye Kizilisik', 'Onan Guren'], 'published': '2022-09-24T11:19:52+00:00', 'summary': 'EEG signals are complex and low-frequency signals. Therefore, they are easily\\ninfluenced by external factors. EEG artifact removal is crucial in neuroscience\\nbecause artifacts have a significant impact on the results of EEG analysis. The\\nremoval of ocular artifacts is the most challenging among these artifacts. In\\nthis study, a novel ocular artifact removal method is presented by developing\\nbidirectional long-short term memory (BiLSTM)-based deep learning (DL) models.\\nWe created a benchmarking dataset to train and test proposed DL models by\\ncombining the EEGdenoiseNet and DEAP datasets. We also augmented the data by\\ncontaminating ground-truth clean EEG signals with EOG at various SNR levels.\\nThe BiLSTM network is then fed to features extracted from augmented signals\\nusing highly-localized time-frequency (TF) coefficients obtained by wavelet\\nsynchrosqueezed transform (WSST). We also compare the WSST-based DL model\\nresults with traditional TF analysis (TFA) methods namely short-time Fourier\\ntransformation (STFT) and continuous wavelet transform (CWT) as well as\\naugmented raw signals. The best average MSE value of 0.3066 was obtained by the\\nfirst time-proposed BiLSTM-based WSST-Net model. Our results demonstrated the\\nWSST-Net model significantly improves artifact removal performance compared to\\ntraditional TF and raw signal methods. Also, the proposed EOG removal approach\\nreveals that it outperforms many conventional and DL-based ocular artifact\\nremoval methods in the literature.', 'pdf_url': 'http://arxiv.org/pdf/2209.11980v1'}, {'title': 'A novel convolutional neural network model to remove muscle artifacts from EEG', 'authors': ['Haoming Zhang', 'Chen Wei', 'Mingqi Zhao', 'Haiyan Wu', 'Quanying Liu'], 'published': '2020-10-22T13:33:52+00:00', 'summary': 'The recorded electroencephalography (EEG) signals are usually contaminated by\\nmany artifacts. In recent years, deep learning models have been used for\\ndenoising of electroencephalography (EEG) data and provided comparable\\nperformance with that of traditional techniques. However, the performance of\\nthe existing networks in electromyograph (EMG) artifact removal was limited and\\nsuffered from the over-fitting problem. Here we introduce a novel convolutional\\nneural network (CNN) with gradually ascending feature dimensions and\\ndownsampling in time series for removing muscle artifacts in EEG data. Compared\\nwith other types of convolutional networks, this model largely eliminates the\\nover-fitting and significantly outperforms four benchmark networks in\\nEEGdenoiseNet. Our study suggested that the deep network architecture might\\nhelp avoid overfitting and better remove EMG artifacts in EEG.', 'pdf_url': 'http://arxiv.org/pdf/2010.11709v2'}]}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ArxivTool', response={'result': {'status': 'success', 'message': 'Found 5 paper(s) on arXiv', 'output': [{'title': 'Eeg2vec: Self-Supervised Electroencephalographic Representation Learning', 'authors': ['Qiushi Zhu', 'Xiaoying Zhao', 'Jie Zhang', 'Yu Gu', 'Chao Weng', 'Yuchen Hu'], 'published': '2023-05-23T11:34:14+00:00', 'summary': 'Recently, many efforts have been made to explore how the brain processes\\nspeech using electroencephalographic (EEG) signals, where deep learning-based\\napproaches were shown to be applicable in this field. In order to decode speech\\nsignals from EEG signals, linear networks, convolutional neural networks (CNN)\\nand long short-term memory networks are often used in a supervised manner.\\nRecording EEG-speech labeled data is rather time-consuming and laborious, while\\nunlabeled EEG data is abundantly available. Whether self-supervised methods are\\nhelpful to learn EEG representation to boost the performance of EEG\\nauditory-related tasks has not been well explored. In this work, we first\\npropose a self-supervised model based on contrastive loss and reconstruction\\nloss to learn EEG representations, and then use the obtained pre-trained model\\nas a feature extractor for downstream tasks. Second, for two considered\\ndownstream tasks, we use CNNs and Transformer networks to learn local features\\nand global features, respectively. Finally, the EEG data from other channels\\nare mixed into the chosen EEG data for augmentation. The effectiveness of our\\nmethod is verified on the EEG match-mismatch and EEG regression tasks of the\\nICASSP2023 Auditory EEG Challenge.', 'pdf_url': 'http://arxiv.org/pdf/2305.13957v1'}, {'title': 'Subject Disentanglement Neural Network for Speech Envelope Reconstruction from EEG', 'authors': ['Li Zhang', 'Jiyao Liu'], 'published': '2025-01-15T10:08:07+00:00', 'summary': 'Reconstructing speech envelopes from EEG signals is essential for exploring\\nneural mechanisms underlying speech perception. Yet, EEG variability across\\nsubjects and physiological artifacts complicate accurate reconstruction. To\\naddress this problem, we introduce Subject Disentangling Neural Network\\n(SDN-Net), which disentangles subject identity information from reconstructed\\nspeech envelopes to enhance cross-subject reconstruction accuracy. SDN-Net\\nintegrates three key components: MLA-Codec, MPN-MI, and CTA-MTDNN. The\\nMLA-Codec, a fully convolutional neural network, decodes EEG signals into\\nspeech envelopes. The CTA-MTDNN module, a multi-scale time-delay neural network\\nwith channel and temporal attention, extracts subject identity features from\\nEEG signals. Lastly, the MPN-MI module, a mutual information estimator with a\\nmulti-layer perceptron, supervises the removal of subject identity information\\nfrom the reconstructed speech envelope. Experiments on the Auditory EEG\\nDecoding Dataset demonstrate that SDN-Net achieves superior performance in\\ninner- and cross-subject speech envelope reconstruction compared to recent\\nstate-of-the-art methods.', 'pdf_url': 'http://arxiv.org/pdf/2501.08693v1'}, {'title': 'CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction', 'authors': ['Pin-Hua Lai', 'Bo-Shan Wang', 'Wei-Chun Yang', 'Hsiang-Chieh Tsou', 'Chun-Shu Wei'], 'published': '2022-10-12T07:56:09+00:00', 'summary': 'Human electroencephalography (EEG) is a brain monitoring modality that senses\\ncortical neuroelectrophysiological activity in high-temporal resolution. One of\\nthe greatest challenges posed in applications of EEG is the unstable signal\\nquality susceptible to inevitable artifacts during recordings. To date, most\\nexisting techniques for EEG artifact removal and reconstruction are applicable\\nto offline analysis solely, or require individualized training data to\\nfacilitate online reconstruction. We have proposed CLEEGN, a novel\\nconvolutional neural network for plug-and-play automatic EEG reconstruction.\\nCLEEGN is based on a subject-independent pre-trained model using existing data\\nand can operate on a new user without any further calibration. The performance\\nof CLEEGN was validated using multiple evaluations including waveform\\nobservation, reconstruction error assessment, and decoding accuracy on\\nwell-studied labeled datasets. The results of simulated online validation\\nsuggest that, even without any calibration, CLEEGN can largely preserve\\ninherent brain activity and outperforms leading online/offline artifact removal\\nmethods in the decoding accuracy of reconstructed EEG data. In addition,\\nvisualization of model parameters and latent features exhibit the model\\nbehavior and reveal explainable insights related to existing knowledge of\\nneuroscience. We foresee pervasive applications of CLEEGN in prospective works\\nof online plug-and-play EEG decoding and analysis.', 'pdf_url': 'http://arxiv.org/pdf/2210.05988v2'}, {'title': 'Decoding Taste Information in Human Brain: A Temporal and Spatial Reconstruction Data Augmentation Method Coupled with Taste EEG', 'authors': ['Xiuxin Xia', 'Yuchao Yang', 'Yan Shi', 'Wenbo Zheng', 'Hong Men'], 'published': '2023-07-01T08:56:34+00:00', 'summary': \"For humans, taste is essential for perceiving food's nutrient content or\\nharmful components. The current sensory evaluation of taste mainly relies on\\nartificial sensory evaluation and electronic tongue, but the former has strong\\nsubjectivity and poor repeatability, and the latter is not flexible enough.\\nThis work proposed a strategy for acquiring and recognizing taste\\nelectroencephalogram (EEG), aiming to decode people's objective perception of\\ntaste through taste EEG. Firstly, according to the proposed experimental\\nparadigm, the taste EEG of subjects under different taste stimulation was\\ncollected. Secondly, to avoid insufficient training of the model due to the\\nsmall number of taste EEG samples, a Temporal and Spatial Reconstruction Data\\nAugmentation (TSRDA) method was proposed, which effectively augmented the taste\\nEEG by reconstructing the taste EEG's important features in temporal and\\nspatial dimensions. Thirdly, a multi-view channel attention module was\\nintroduced into a designed convolutional neural network to extract the\\nimportant features of the augmented taste EEG. The proposed method has accuracy\\nof 99.56%, F1-score of 99.48%, and kappa of 99.38%, proving the method's\\nability to distinguish the taste EEG evoked by different taste stimuli\\nsuccessfully. In summary, combining TSRDA with taste EEG technology provides an\\nobjective and effective method for sensory evaluation of food taste.\", 'pdf_url': 'http://arxiv.org/pdf/2307.05365v1'}, {'title': 'ConvConcatNet: a deep convolutional neural network to reconstruct mel spectrogram from the EEG', 'authors': ['Xiran Xu', 'Bo Wang', 'Yujie Yan', 'Haolin Zhu', 'Zechen Zhang', 'Xihong Wu', 'Jing Chen'], 'published': '2024-01-10T07:15:45+00:00', 'summary': \"To investigate the processing of speech in the brain, simple linear models\\nare commonly used to establish a relationship between brain signals and speech\\nfeatures. However, these linear models are ill-equipped to model a highly\\ndynamic and complex non-linear system like the brain. Although non-linear\\nmethods with neural networks have been developed recently, reconstructing\\nunseen stimuli from unseen subjects' EEG is still a highly challenging task.\\nThis work presents a novel method, ConvConcatNet, to reconstruct mel-specgrams\\nfrom EEG, in which the deep convolution neural network and extensive\\nconcatenation operation were combined. With our ConvConcatNet model, the\\nPearson correlation between the reconstructed and the target mel-spectrogram\\ncan achieve 0.0420, which was ranked as No.1 in the Task 2 of the Auditory EEG\\nChallenge. The codes and models to implement our work will be available on\\nGithub: https://github.com/xuxiran/ConvConcatNet\", 'pdf_url': 'http://arxiv.org/pdf/2401.04965v1'}]}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='SemanticScholarTool', response={'result': {'status': 'success', 'message': 'Found 5 paper(s) on Semantic Scholar', 'output': [{'title': 'Advanced Signal Processing and Machine/Deep Learning Approaches on a Preprocessing Block for EEG Artifact Removal: A Comprehensive Review', 'authors': ['Said Agounad', 'Ousama Tarahi', 'Mustapha Moufassih', 'Soukaina Hamou', 'Anas Mazid'], 'abstract': None, 'url': 'https://www.semanticscholar.org/paper/303515b75925dd7294c659b71874ec4ab593b6b3'}, {'title': 'AnEEG: leveraging deep learning for effective artifact removal in EEG data', 'authors': ['Bhabesh Kalita', 'Nabamita Deb', 'Daisy Das'], 'abstract': 'In neuroscience and clinical diagnostics, electroencephalography (EEG) is a crucial instrument for capturing neural activity. However, this signal is polluted by different artifacts like muscle activity, eye blinks, environmental interference, etc., which makes it more difficult to retrieve important information from the signal. Deep learning methods have demonstrated the potential to lower these artifacts and enhance the EEG\u2019s quality in recent years. In this work, a novel deep learning method,\u201cAnEEG\u201d is presented for eliminating artifacts from EEG signal. The quantitative matrices NMSE, RMSE, CC, SNR and SAR are calculated to confirm the effectiveness of the proposed model. Through this process, it was found that the suggested model outperformed wavelet decomposition techniques. The model achieves lower NMSE and RMSE values, which indicates better agreement with the original signal. Achieving higher CC values means stronger linear agreement with the ground truth signals. Additionally, the model shows improvements in both SNR and SAR values. Overall, this suggested approach showcases promising results in improving the quality of EEG data by utilizing deep learning.', 'url': 'https://www.semanticscholar.org/paper/0c7db46cf5830e4f67101b8879e26a9862fba8d0'}, {'title': 'EEG Artifact Removal by Bayesian Deep Learning & ICA', 'authors': ['Sangmin S. Lee', 'Kiwon Lee', 'Guiyeom Kang'], 'abstract': 'Artifact removal is important for EEG signal processing because artifacts adversely affect analysis results. To preserve normal EEG signal, several methods based on independent component analysis (ICA) have been studied and artifacts are removed by discarding independent components (ICs) classified as artifacts. In this study, a method using Bayesian deep learning and attention module is presented to improve performance of the classifier for ICs. Probability value is computed through the method to predict if a component is artifact and to treat ambiguous inputs. The attention module achieves increasing classification accuracy and shows the map of the region where the classifier concentrates on.', 'url': 'https://www.semanticscholar.org/paper/5e5e25957fdcfb3675acea35687da9796f8f5fb8'}, {'title': 'Denosieformer: A Transformer-Based Approach for Single-Channel EEG Artifact Removal', 'authors': ['Junfu Chen', 'Dechang Pi', 'Xiaoyi Jiang', 'Yue Xu', 'Yang Chen', 'Xixuan Wang'], 'abstract': 'As an essential tool in clinical medicine, brain research, and the study of neurological diseases, the electroencephalogram (EEG) is susceptible to various physiological signals, posing difficulties for subsequent research and analysis. Deep learning (DL), an end-to-end method without expert knowledge, is gaining attention in the field of EEG artifact removal. However, the existing DL methods cannot globally observe the latent relationship of EEG signals. They lack the capacity to effectively explore the feature patterns of the signals, resulting in unsatisfactory denoising performance. To address the aforementioned issues, this article proposes a transformer-based EEG denoising architecture (denoiseformer) to solve the artifact removal problem of single-channel contaminated EEG signals. First, the contaminated EEG signals are divided into multiple slices as input so that the transformer module can extract the potential pattern relationships between the slices. Then, we design a multiscale feature extraction and fusion module to obtain high-resolution features with richer and more robust information. Next, a novel attention mechanism, named slice pattern attention, is applied to extract the global information on the temporal dimension of each EEG slice. In addition, a residual variational autoencoder structure can generate superior latent codes between different EEG samples, which help the encoder to distinguish contaminated signal patterns, and hence, improve the recovery performance for artifacts. Comprehensive experiments on three types of semisimulated and one real-laboratory artifacts show our model can obtain excellent performance on two quantitative metrics (relative root-mean-square error and correlation coefficient) for artifact removal tasks in various scenarios.', 'url': 'https://www.semanticscholar.org/paper/a47654873c25a47081576ddb5076ba4fc51617d2'}, {'title': 'Advanced Ocular Artifact Removal In EEG For Enhanced Schizophrenia Diagnosis Using Wavelet Shrinkage And LSTM Deep Learning Techniques', 'authors': ['Bh.V.V.S.R.K.K.Pavan', 'Herald Anantha'], 'abstract': '.in Schizophrenia is a potentially life-threatening mental disease. It is distinguished by deficiencies in cognition, affect, and conduct. Countless individuals worldwide are affected by it. Accurate diagnosis can be achieved by analyzing EEG data. Researchers can non-invasively measure brain activity by using an electroencephalogram (EEG). Blinking of the eyes results in the production of an electrooculogram, which is defined by a robust electrical potential that envelops the eyes. The electrooculogram derives its name from this potential. The EEG data might become corrupted when cortical activity spreads across the scalp. Visual distortions of this nature are commonly known as ocular artifacts (OAs). The device is a commonly seen form of interference that can occur in electroencephalogram (EEG) research. Prior to processing EEG data, it is crucial to eliminate or minimize ocular artifacts (OAs). In order to eliminate the typical ocular artifacts (OAs) that occur naturally, it is essential to perform either two electrooculograms (EOGs) or a multichannel electroencephalogram (EEG). Our study has led us to develop a consistent method by combining EEMD with Fejer-Korovkin', 'url': 'https://www.semanticscholar.org/paper/6314876b797925889b7feeee8b7e475077d6e177'}]}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='SemanticScholarTool', response={'result': {'status': 'success', 'message': 'Found 5 paper(s) on Semantic Scholar', 'output': [{'title': 'CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction', 'authors': ['Pin-Hua Lai', 'Wei Yang', 'Hsiang-Chieh Tsou', 'Chunshan Wei'], 'abstract': 'Human electroencephalography (EEG) is a brain monitoring modality that senses cortical neuroelectrophysiological activity in high-temporal resolution. One of the greatest challenges posed in applications of EEG is the unstable signal quality susceptible to inevitable artifacts during recordings. To date, most existing techniques for EEG artifact removal and reconstruction are applicable to offline analysis solely, or require individualized training data to facilitate online reconstruction. We have proposed CLEEGN, a novel convolutional neural network for plug-and-play automatic EEG reconstruction. CLEEGN is based on a subject-independent pre-trained model using existing data and can operate on a new user without any further calibration. The performance of CLEEGN was validated using multiple evaluations including waveform observation, reconstruction error assessment, and decoding accuracy on well-studied labeled datasets. The results of simulated online validation suggest that, even without any calibration, CLEEGN can largely preserve inherent brain activity and outperforms leading online/offline artifact removal methods in the decoding accuracy of reconstructed EEG data. In addition, visualization of model parameters and latent features exhibit the model behavior and reveal explainable insights related to existing knowledge of neuroscience. We foresee pervasive applications of CLEEGN in prospective works of online plug-and-play EEG decoding and analysis.', 'url': 'https://www.semanticscholar.org/paper/1c88fa4869591bfe6ebb1a424e08c9fab36f8010'}, {'title': 'Multi-Channel EEG Emotion Recognition Based on Parallel Transformer and 3D-Convolutional Neural Network', 'authors': ['Jie Sun', 'Xuan Wang', 'Kangning Zhao', 'Siyuan Hao', 'Tianyu Wang'], 'abstract': 'Due to its covert and real-time properties, electroencephalography (EEG) has long been the medium of choice for emotion identification research. Currently, EEG-based emotion recognition focuses on exploiting temporal, spatial, and spatiotemporal EEG data for emotion recognition. Due to the lack of consideration of both spatial and temporal aspects of EEG data, the accuracy of EEG emotion detection algorithms employing solely spatial or temporal variables is low. In addition, approaches that use spatiotemporal properties of EEG for emotion recognition take temporal and spatial characteristics of EEG into account; however, these methods extract temporal and spatial information directly from EEG data. Since there is no reconstruction of the EEG data format, the temporal and spatial properties of the EEG data cannot be extracted efficiently. To address the aforementioned issues, this research proposes a multi-channel EEG emotion identification model based on the parallel transformer and three-dimensional convolutional neural networks (3D-CNN). First, parallel channel EEG data and position reconstruction EEG sequence data are created separately. The temporal and spatial characteristics of EEG are then retrieved using transformer and 3D-CNN models. Finally, the features of the two parallel modules are combined to form the final features for emotion recognition. On the DEAP, Dreamer, and SEED databases, the technique achieved greater accuracy in emotion recognition than other methods. It demonstrates the efficiency of the strategy described in this paper.', 'url': 'https://www.semanticscholar.org/paper/0bf5469ca51cf71d95f53436ff9694e74d05d0ef'}, {'title': 'MSFR-GCN: A Multi-Scale Feature Reconstruction Graph Convolutional Network for EEG Emotion and Cognition Recognition', 'authors': ['Deng Pan', 'Haohao Zheng', 'Feifan Xu', 'Ouyang Yu', 'Zhe Jia', 'Chu Wang', 'Hong Zeng'], 'abstract': 'Graph Convolutional Network (GCN) excels at EEG recognition by capturing brain connections, but previous studies neglect the important EEG feature itself. In this study, we propose MSFR-GCN, a multi-scale feature reconstruction GCN for recognizing emotion and cognition tasks. Specifically, MSFR-GCN includes the MSFR and feature-pool characteristically, with the MSFR consisting of two sub-modules, multi-scale Squeeze-and-Excitation (MSSE) and multi-scale sample re-weighting (MSSR). MSSE assigns weights to channels and frequency bands based on their separate statistical information, while MSSR assigns sample weights based on combined channel and frequency information. The feature-pool, which pools across the feature dimension, is applied after GCN to retain EEG channel information. The MSFR-GCN achieves excellent results in emotion recognition when first tested on two public datasets, SEED and SEED-IV. Than the MSFR-GCN is tested on our self-collected Emotion and Cognition EEG dataset (ECED) for both emotion and cognition classification tasks. The results show MSFR-GCN\u2019s good performance in emotion and cognition classification tasks and reveal the implicit relationship between the two, which may provide aid in the rehabilitation of people with cognitive impairments from an emotional perspective.', 'url': 'https://www.semanticscholar.org/paper/db208be202cf8afd5bc8dbdea43c8d133deb48e7'}, {'title': 'Subject Disentanglement Neural Network for Speech Envelope Reconstruction from EEG', 'authors': ['Li Zhang', 'Jiyao Liu', 'Leiming Xie'], 'abstract': 'Reconstructing speech envelopes from EEG signals is essential for exploring neural mechanisms underlying speech perception. Yet, EEG variability across subjects and physiological artifacts complicate accurate reconstruction. To address this problem, we introduce Subject Disentangling Neural Network (SDN-Net), which disentangles subject identity information from reconstructed speech envelopes to enhance cross-subject reconstruction accuracy. SDN-Net integrates three key components: MLA-Codec, MPN-MI, and CTA-MTDNN. The MLA-Codec, a fully convolutional neural network, decodes EEG signals into speech envelopes. The CTA-MTDNN module, a multi-scale time-delay neural network with channel and temporal attention, extracts subject identity features from EEG signals. Lastly, the MPN-MI module, a mutual information estimator with a multilayer perceptron, supervises the removal of subject identity information from the reconstructed speech envelope. Experiments on the Auditory EEG Decoding Dataset demonstrate that SDN-Net achieves superior performance in inner- and cross-subject speech envelope reconstruction compared to recent state-of-the-art methods.', 'url': 'https://www.semanticscholar.org/paper/15fcb108167aec6d7b6a46fda2b90554277fd285'}, {'title': 'Classification and Reconstruction of Biomedical Signals Based on Convolutional Neural Network', 'authors': ['Zijiang Zhu', 'Hang Chen', 'Song Xie', 'Y. Hu', 'Jing Chang'], 'abstract': \"The efficient biological signal processing method can effectively improve the efficiency of researchers to explore the work of life mechanism, so as to better reveal the relationship between physiological structure and function, thus promoting the generation of major biological discoveries; high-precision medical signal analysis strategy can, to a certain extent, share the pressure of doctors' clinical diagnosis and assist them to formulate more favorable plans for disease prevention and treatment, so as to alleviate patients' physical and mental pain and improve the overall health level of the society. This article in biomedical signal is very representative of the two types of signals: mammary gland molybdenum target X-ray image (mammography) and the EEG signal as the research object, combined with the deep learning field of CNN; the most representative model is two kinds of biomedical signal classification, and reconstruction methods conducted a series of research: (1) a new classification method of breast masses based on multi-layer CNN is proposed. The method includes a CNN feature representation network for breast masses and a feature decision mechanism that simulates the physician's diagnosis process. By comparing with the objective classification accuracy of other methods for the identification of benign and malignant breast masses, the method achieved the highest classification accuracy of 97.0% under different values of c and gamma, which further verified the effectiveness of the proposed method in the identification of breast masses based on molybdenum target X-ray images. (2) An EEG signal classification method based on spatiotemporal fusion CNN is proposed. This method includes a multi-channel input classification network focusing on spatial information of EEG signals, a single-channel input classification network focusing on temporal information of EEG signals, and a spatial-temporal fusion strategy. Through comparative experiments on EEG signal classification tasks, the effectiveness of the proposed method was verified from the aspects of objective classification accuracy, number of model parameters, and subjective evaluation of CNN feature representation validity. It can be seen that the method proposed in this paper not only has high accuracy, but also can be well applied to the classification and reconstruction of biomedical signals.\", 'url': 'https://www.semanticscholar.org/paper/f1ca3a48737f1c24bbc7268c95c9861d8dfb632a'}]}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Okay, I have gathered information about related work. Now, I will create three reviewer agents with different expertise to evaluate the paper.\n\n*   **Agent 1: EEG Domain Expert:** Focuses on the neurophysiological plausibility and relevance of the proposed method.\n*   **Agent 2: Deep Learning Methodology Expert:** Focuses on the novelty and correctness of the deep learning architecture and training procedure.\n*   **Agent 3: BCI Application Expert:** Focuses on the practical applicability and impact of the method in Brain-Computer Interface systems.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"gemini-2.5-flash-preview-05-20\": {\n      \"description\": \"Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math\",\n      \"create_expense_cost\": 0,\n      \"invoke_expense_cost\": 0.15,\n      \"output_expense_cost\": 0.6\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'gemini-2.5-flash-preview-05-20': {'description': 'Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math', 'create_expense_cost': 0, 'invoke_expense_cost': 0.15, 'output_expense_cost': 0.6}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'Expert in EEG signal processing and neuroscience.', 'agent_name': 'EEGDomainExpert', 'base_model': 'gemini-2.5-flash-preview-05-20', 'system_prompt': 'You are an expert in EEG signal processing and neuroscience. You will evaluate the paper based on the neurophysiological plausibility and relevance of the proposed method. Assess whether the method aligns with known principles of EEG signal generation and artifact characteristics. Critically evaluate the interpretation of the results in the context of neuroscience and EEG research.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'system_prompt': 'You are an expert in deep learning methodologies. You will evaluate the paper based on the novelty and correctness of the deep learning architecture and training procedure. Assess the technical soundness of the proposed CNN architecture, loss functions, and training methodology. Evaluate the novelty and appropriateness of the chosen deep learning techniques for the EEG reconstruction task. Provide detailed feedback on the clarity and completeness of the methodological description.', 'base_model': 'gemini-2.5-flash-preview-05-20', 'description': 'Expert in deep learning methodologies.', 'agent_name': 'DLMethodologyExpert'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'Expert in Brain-Computer Interface applications.', 'base_model': 'gemini-2.5-flash-preview-05-20', 'agent_name': 'BCIApplicationExpert', 'system_prompt': 'You are an expert in Brain-Computer Interface applications. You will evaluate the paper based on the practical applicability and impact of the method in Brain-Computer Interface systems. Assess the potential of the proposed method for improving the performance and usability of BCI systems. Evaluate the experimental results in terms of their relevance to real-world BCI applications. Provide insights on the limitations and future directions of the work in the context of BCI research.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.8686143\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"Expert in EEG signal processing and neuroscience.\",\n  \"agent_name\": \"EEGDomainExpert\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"system_prompt\": \"You are an expert in EEG signal processing and neuroscience. You will evaluate the paper based on the neurophysiological plausibility and relevance of the proposed method. Assess whether the method aligns with known principles of EEG signal generation and artifact characteristics. Critically evaluate the interpretation of the results in the context of neuroscience and EEG research.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.8686143\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"system_prompt\": \"You are an expert in deep learning methodologies. You will evaluate the paper based on the novelty and correctness of the deep learning architecture and training procedure. Assess the technical soundness of the proposed CNN architecture, loss functions, and training methodology. Evaluate the novelty and appropriateness of the chosen deep learning techniques for the EEG reconstruction task. Provide detailed feedback on the clarity and completeness of the methodological description.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"description\": \"Expert in deep learning methodologies.\",\n  \"agent_name\": \"DLMethodologyExpert\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.8686143\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"Expert in Brain-Computer Interface applications.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"agent_name\": \"BCIApplicationExpert\",\n  \"system_prompt\": \"You are an expert in Brain-Computer Interface applications. You will evaluate the paper based on the practical applicability and impact of the method in Brain-Computer Interface systems. Assess the potential of the proposed method for improving the performance and usability of BCI systems. Evaluate the experimental results in terms of their relevance to real-world BCI applications. Provide insights on the limitations and future directions of the work in the context of BCI research.\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.8686143}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.8686143}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.8686143}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Please provide a detailed review of the paper, focusing on the neurophysiological plausibility and relevance of the proposed method. Assess whether the method aligns with known principles of EEG signal generation and artifact characteristics. Critically evaluate the interpretation of the results in the context of neuroscience and EEG research. The paper title is: CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction. The paper content is: Since the first record of human electroencephalogram (EEG) performed almost a century ago (in 1924), EEG has been one of the most widely used non-invasive neural modalities that monitors brain activity in high temporal resolution (Koike et al., 2013; Mehta & Parasuraman, 2013; Sejnowski et al., 2014). Among a variety of modalities, EEG has extensive use in the clinical assessment of neurological and psychiatric conditions, as well as in the research of neuroscience, cognitive science, psychology, and brain-computer interfacing (BCI). EEG signals measure subtle fluctuations of the electrical field driven by local neuroelectrophysiological activity of a population of neurons in the brain cortex (Cohen, 2017). While the electrodes are placed on the surface of the scalp, undesired artifacts may introduce interruption in the measurements and distort the signal of interest. Even in a well-controlled laboratory with a well-trained subject who can maximally keep the body still and relaxed, the EEG signals, unfortunately, could be contaminated by inevitable behavioral and physiological artifacts such as eye blinks, reflective muscle movements, ocular activity, cardiac activity, etc (Croft & Barry, 2000; Wallstrom et al., 2004; Romero et al., 2008). In practice, it is difficult to identify and track the sources of artifacts entirely due to their diversity and non-stationarity. Noise cancellation and artifact removal remain major issues in EEG signal processing and decoding. Currently, numerous methods have been proposed to alleviate the influence of artifacts in EEG signals. Traditional EEG denoising algorithms include filtering, regression, data separation or decomposition (Makeig et al., 1995; Islam et al., 2016; Kothe & Jung, 2016). According to previous meta-analyses on EEG artifact removal literature (Urigu\u0308en & Garcia-Zapirain, 2015; Jiang et al., 2019), independent component analysis (ICA) is especially popular. It is majorly used in 45% of EEG denoising literature. ICA-based artifact removal estimates the component activity by unmixing the EEG data in the channel domain. Through manual or automatic identification, one can exclude the artifact components and then reconstruct the EEG data through back projection based on non-artifact components (Jung et al., 2000a). The fast growth of deep learning methods has drawn state-of-the-art performances in a variety of machine learning problems (LeCun et al., 2015). Lately, deep-learning-based EEG reconstruction has drawn much attention (Leite et al., 2018; Sun et al., 2020; Lopes et al., 2021; Lee et al., 2020; Chuang et al., 2022). Although these methods can effectively remove artifacts from artificial synthetic signals, their performance in reconstructing real EEG data has not yet been validated in terms of decoding labeled EEG data. Meanwhile, the model design of existing deep-learning-based techniques for EEG reconstruction rarely takes the characteristics of EEG into account. In this work, we propose CLEEGN, a ConvoLutional neural network for EEG reconstructioN. CLEEGN is capable of subject-independent EEG construction without any training/calibration for a new subject. The contributions of this work are three-fold: \u2022 a light-weight autoencoder CNN, CLEEGN, with a subject-independent framework that facilitates plug-and-play EEG reconstruction. \u2022 CLEEGN outperforms leading online/offline methods in providing reconstructed EEG data with the best decoding performance for BCI datasets. \u2022 with a novel model design dedicated to EEG reconstruction, CLEEGN characterizes patterns of EEG interpretable and provides neuroscientific insights. 2 RELATED WORK: Current processing techniques for EEG artifact removal are highly varied based on the context where the algorithm may apply. Earlier attempts of EEG denoising assumed that the EEG signals and artifacts appear in different frequency ranges. Based on the assumption, some significant artifacts can be eliminated by the linear filtering method during the online stage (Seifzadeh et al., 2014). Despite the advantages of low computational time, linear filtering hardly removes artifacts that distribute in an overlapped frequency range of EEG signals. Another approach, adaptive filtering (Schlo\u0308gl et al., 2007), estimates artifact signals through additional EOG, EMG, ECG channels and removes these noisy signals from the recording signals by regression. Nevertheless, this approach requires additional auxiliary electrodes and raises the cost and inconvenience in practical applications. The blind source separation (BSS) method in EEG denoising was developed by assuming that the recording EEG signals are linear combinations of the signals from noise sources and the brain neurons. One of the most well-known BSS method is independent component analysis (ICA) (Jung et al., 2000a;b), which is able to separate EEG signals into independent components (ICs) (Makeig et al., 1995). Traditionally, the artifact components extracted by ICA are determined and removed through manual inspection. Recently developed ICLabel can label the ICs provenance into seven different categories: brain, eye, heart, muscle, line noise, channel noise, and other (Pion-Tonachini et al., 2019). Artifact subspace reconstruction (ASR) is another automatic approach, which is based on the principal component analysis (PCA) method (Kothe & Jung, 2016). The ASR method selects relatively noiseless periods from the multi-channel EEG data as reference based on the data distribution. After projecting all EEG data to the principal-component domain, high-variance components projected from the artifacts are detected by a cutoff parameter k. The noiseless signals are reconstructed by preserving the components without carrying artifacts and back-projected to the time domain. The ASR method has been shown capable of improving the quality of ICA decomposition (Chang et al., 2020). Recently, neural network-based methods have been proposed to remove artifacts for EEG data. A variety of network structures have been applied to the framework for removing EEG artifacts and reconstructing clean EEG. A deep convolutional autoencoder (Leite et al., 2018) can enhance the peak-signal-to-noise ratio compared to the linear filtering method via a common CNN autoencoder structure, which has been widely used on image denoising. Their work shows that it seems practicable to transform the EEG waveform through a CNN structure. Later on, a combined framework integrating Bayesian deep learning and ICA (Lee et al., 2020) used thresholding of the EEG data distribution to discard ICs classified as ocular artifacts. These methods leverage the flexibility of deep learning model design and achieve improvements in their assessments. Considering the non- stationary property in EEG data and the degradation phenomenon during training, 1D-ResCNN (Sun et al., 2020) was proposed, which adopted an Inception-Residual module in the network structure. This network is able to remove EOG, ECG, EMG on single-channel synthesis EEG data. Instead of using synthesis EEG data, IC-U-Net (Chuang et al., 2022) was created, which generated pairings of noisy and noiseless EEG data through ICA as training data. The proposed neural network is a one-dimensional adaptation of U-Net architecture trained with the ensemble of four loss functions to minimize the difference between amplitude, velocity, acceleration, and frequency components from the signals. This work showed that their reconstructed signal has higher SNR and can surely increase the number of brain components classified by ICLabel. 3 MATERIALS AND METHODS: CLEEGN Architecture and Model Training. CLEEGN is designed to map multi-channel noisy EEG into a latent space and reconstruct it into noiseless EEG signals. The architecture of CLEEGN is shown in Figure 1. The architecture of the encoder is inspired by an existing CNN model designed for EEG recognition (Wei et al., 2019) and incorporates convolutional blocks that capture spatiotemporal characteristics of EEG data. The first convolution block is used to extract spatial EEG features through a convolutional layer containing C spatial filters whose shape is (C, 1), where C is the number of electrodes of the EEG signals. The second convolution block is used to extract temporal EEG features through a convolutional layer containing NF temporal filters with shape (1, \u230afs \u00d7 0.1\u230b), where fs denotes the sample frequency of the EEG signals. As for the decoder, we design an approximately symmetric structure to the encoder with three convolutional blocks. The first convolutional block decodes the EEG feature with NF temporal kernels with shape is (1, \u230afs\u00d70.1\u230b). The second block decodes the EEG feature by a convolution layer with C spatial kernels of shape (C, 1). The last convolutional block is for projecting the feature domain back to the original time domain. To maintain the shape between the model inputs and outputs, every convolution block applies zeropadding except the first block in the encoder. The detail of the CLEEGN architecture is available in Table 1. As illustrated in Figure 1, the objective of the proposed method is to minimize the difference between noiseless signals, Y , and the model output, Y\u0302 . The recording EEG data, X , is the combination of brain signals, Y , and the signals from multiple noise sources, N . CLEEGN is a denoising autoencoder that is intended to perform artifact removal on EEG data by creating a mapping between multi-channel noisy EEG signals, X , and noiseless signals, Y . The training process of CLEEGN utilizes pairing noisy raw EEG data and noiseless reference EEG data so that the model can learn to transform noisy EEG data into reconstructed EEG data with maximal similarity to the reference data. To generate large-scale reference EEG data, we adopted automatic denoising methods, ICLabel and ASR, to remove artifacts and reconstruct clean waveforms offline. The pairing EEG recording with C channels of continuous noisy/noiseless data was further segmented into training sample pairs. The input size is (C, T ), where T is the number of time points based on the sampling rate of the EEG data in 4-second segments. The windows size and stride are set as T and 0.5T . In the context of plug-and-play EEG reconstruction, CLEEGN was trained as a subject-independent model. To perform subject-independent training and testing, we grouped subjects into k disjoint sets. During the training process, one of the sets was left out for testing. Subjects\u2019 EEG data in the left-alone set were not involved in both training and validation. A complete experiment on a single dataset would result in k different models. The artifact removal performance of a model was evaluated by using the left-alone set. The number of subjects in one set and the EEG duration available for each subject depend on the experiment setting and the dataset used. EEG Reconstruction and Evaluation. The reconstructed EEG data of CLEEGN is generated through an online reconstruction simulation. In the online stage, the system performs artifact removal on a new subject using a pre-trained CLEEGN model without any requirement of training/calibration. Figure 2 shows the online EEG reconstruction simulated in offline. The EEG data from a subject in the left-alone set would be fed into the model sequentially. The stride size of the sliding window is set to 0.5 seconds to minimize the delay in online reconstruction. The fitness of the model can be evaluated by observing the waveform visually or measuring the similarity using the mean square error (MSE) between reconstructed and noiseless EEG. Meanwhile, we propose to use the decoding performance of labeled EEG data as an objective measurement of the reconstructed EEG quality. We employed EEGNet, a compact CNN for end-to-end EEG decoding (Lawhern et al., 2018), as the classifier to decode the labeled EEG data in our study. 4 EXPERIMENTS: Data and Model Fitting. In this work, we applied two labeled EEG datasets, the BCI-ERN dataset 1 and the MAMEM-SSVEP-II dataset 2. We estimate the classification performance by calculating the score of Area Under the Curve (AUC) in ERN binary classification and Top-1 accuracy in the SSVEP task. Figure 3 is the schematic diagram of the process of decoding performance estimation using the reconstructed EEG data. The segmented EEG epochs are divided into three splits within each subject: training set, validation set, and test set. The ratio between classes remained the same in each set. We perform 20 repeated runs with shuffled data to estimate the average performance of each decoding performance evaluation. Detailed descriptions of the two datasets are available in the appendix at A.2. Figure 4 presents the loss curve and the reconstruction result at the end of three different training iterations in a sample run using the ERN dataset. Through this exploration, we can look into the course of model fitting. In the early stage of training, the model reduces the large MSE loss by smoothing the input signals. To further minimize the error between reconstruction and reference (offline denoised signal), the model compensates the intensity to better fit the reference signals. At the convergent stage of training, we can see the reconstructed waveform is very similar to the one in reference data. 1https://www.kaggle.com/c/inria-bci-challenge 2https://www.mamem.eu/results/datasets/ Types of Reference Data. As ICLabel, ASR, and the hybrid ASR-ICLabel are employed to automatically generate large-scale noiseless reference EEG data offline, it is of our interest to investigate what type of reference EEG serves as the best noiseless reference data for the CLEEGN model training. Figure 5 presents the EEG waveforms sampled from Subject 2 in the ERN dataset. The EEG time series from top to bottom represent the recording of Fp1, T7, Cz, T8, and O2 channels. The nine sub-figures visualize the comparison between the noisy raw data and the reference data reconstructed by the ICLabel, ASR with or without ICLabel when the cutoff parameter k was set to 4, 8, 16, and 32. We can see the waveform of ICLabel and the ASR-ICLabel hybrid method with 4 different cutoff parameters k are similar, and the noiseless waveform reconstructed by CLEEGN are highly correlated to the result from these 5 methods. High-amplitude ocular artifacts such as blinking and eye saccades are phenomenal in frontal channels (Fp1) and muscular artifacts featuring high-frequency (around 24-30 Hz in the \u03b2 band) are observed on the T7 channel. CLEEGN, ICA, and ASR-ICLabel hybrid methods can eliminate these kinds of artifacts. Compared to the ICLabel and ASR-ICLabel hybrid methods, ASR is unable to eliminate the high-frequency EMG artifact for this dataset. The tolerance of large amplitude EOG artifact increases with the employment of larger cutoff parameter k. Limited by the reference data generated by ASR, CLEEGN preserves the high-frequency artifact. As for the large amplitude artifacts that failed to be identified by ASR-32 and ASR-16, we can see that CLEEGN is able to mitigate those EOG artifacts. As shown in Figure 6(a), the reference data prepared by ICLabel offer the lowest MSE, i.e. the best fitness for CLEEGN training. We also compare the types of reference EEG data and their corresponding CLEEGN reconstruction results in the decoding performance of the ERN EEG dataset to assess their data quality. As illustrated in Figure 6(b), CLEEGN reconstruction draws better performances for all denoising methods. Although the performance decreases with a smaller cutoff parameter k, CLEEGN can outperform the reference data used for its training. This result suggests that our method not only removes the artifact but also preserves informative brain activity in the EEG under our cross-subject training scheme. As the ICLabel provides the reference data with the best decoding performance and fitness, we chose ICLabel as the source of reference data for further experiments in this study. Training Data Size. We explored the effect of data length per subject in the training set regarding the fitness and the decoding performance of CLEEGN. The training data were segmented from the first 1, 2, 4, 10, 20, and 30 minutes of data in each EEG recording to investigate the trade-off between training time and reconstruction performance. Figure 7(a) shows the fitness of CLEEGN to the reference data. The model trained using the first 10 minutes obtain the minimal value among all duration configurations. Though the difference in MSE between 10, 20, and 30 minutes is not noticeable, Figure 7(b) shows that using the 10-minute training data yields the best decoding performance among all settings. Interestingly, with only one-minute training data from each subject, CLEEGN can achieve comparable decoding performance to the reference data. This indicates that CLEEGN retains its performance even when each subject only contributes a short recording for training. In addition, we explore the effect of the number of subjects included in the training set on the fitness of CLEEGN model training and the decoding performance of the reconstructed EEG data. We randomly reduced the number of subjects for training from 12 to 6, 4, and 2. Since the subset of subjects may influence the performance, we tested multiple combinations and averaged the results. With the decrease in the number of subjects, the MSE value and the standard error (light span area) increase in Figure 8(a), which indicates that the generalization ability of the CLEEGN model reduces when fewer subjects are included for training. In Figure 8(b), we can observe a slight decrease in decoding performance. We consider the number of subjects as an essential factor in the performance of CLEEGN, yet it requires only a few subjects to achieve a satisfactory performance compared to the reference data. Performance Comparison. We compare the performance of CLEEGN against three baseline methods, ICLabel, 1D-ResCNN, and IC-U-Net. Except for the ICLabel that operates offline, CLEEGN and the other two neural network-based methods perform a simulated online reconstruction based on the same training process with the reference data generated by ICLabel. Results of the ERN EEG dataset are shown in Table 2. CLEEGN has the best fitness to the reference data with the minimum MSE, the highest AUC score in the decoding performance, and the least parameters. For the SSVEP EEG data, although not having the minimum MSE, CLEEGN outperforms other methods in the decoding accuracy and the model size. The evaluation across the two datasets suggests an overall superiority of our proposed CLEEGN model over other existing neural network-based methods in online reconstruction. CLEEGN even provides a better reconstruction than the offline ICLabel. These promising results indicate the usability of CLEEGN in online training/calibration-free EEG reconstruction that truly meets the need for real-world applications of EEG-based BCI. Visualization. On account of the interpretability of the CLEEGN model, we visualized its intermediate latent features by mapping onto a 2-dimensional domain based on the principal component analysis (PCA) of the noisy raw EEG data (Wold et al., 1987). Figure 9(a) shows the principal component space and the noisy EEG data channel projections. We observe that the scatters of noisy EEG data retain the spatial relationship of the actual EEG electrodes. The arrangement of frontal electrodes (prefix in Fp, P) to the posterior (prefix in P, O) are projected along the x-axis from right to left. The projection from top to bottom matches the left-side electrodes (suffixed in odd numbers), central electrode (suffixed in z), and right-side electrodes (suffixed in even numbers). The projection suggests that EEG data of adjacent channels tend to show similar waveforms. The blue dots in Figure 9(b), (c) are the projection of latent features resulting from the first and second convolutional layers, which make up the encoder design of CLEEGN. The projection of latent features from the next two convolutional layers is presented in Figure 9(d), (e). From Figure 9(b) to (e), we can see the distribution range of the latent features shrink in the encoder design and expand in the decoder design of CLEEGN. Figure 9(f) compares the distribution of noisy data and CLEEGN reconstructed data, showing that the CLEEGN reconstructed data is a more compact cluster than the noisy EEG in the PCA space. This implies that the process within CLEEGN includes a projection of the original EEG data in the upstream layer, complex temporal filtering and combination in the midstream layers, and a final projection that converts the noiseless latent features back to the channel domain. 5 CONCLUSION: In this work, we have proposed CLEEGN, a novel convolutional neural network for plug-and-play automatic EEG reconstruction. The training of CLEEGN leverages the conventional offline denoising methods, ASR and ICLabel, with automatic component classification to generate abundant noiseless EEG data. The performance of CLEEGN was objectively validated using multiple evaluations including waveform observation; reconstruction error assessment; and decoding accuracy on well-studied, labeled datasets. The experiment results suggest that, even without any calibration, CLEEGN can predominantly preserve inherent brain activity. According to the decoding performance of reconstructed EEG data, CLEEGN outperforms other neural network-based denoising methods on both ERN and SSVEP EEG decoding. From the visualization of the waveform, CLEEGN can remove artifacts from different sources and the waveform is highly correlated to the reference data. Through the visualization of model parameters and latent features, we exhibit model behavior and reveal explainable insights related to existing knowledge of neuroscience. We foresee pervasive applications of CLEEGN in prospective works of EEG decoding and analysis. A APPENDIX:  A.1 NEURAL NETWORK TRAINING CONFIGURATION:  A.1.1 CLEEGN: For training CLEEGN, we used the Adam optimizer with an initial learning rate of 1e-3 without weight decay. Besides, the exponential learning rate scheduler is applied with a gamma of 0.8. As for the loss function, we used Mean Squared Error (MSE). The batch size is set to 64 and the total training epoch is 40. During the training procedure, the model is evaluated using the validation subset at the end of every epoch with the purpose of saving the weights that achieved the lowest validation loss. A.1.2 IC-U-NET The optimizer adopted in IC-U-Net (Chuang et al., 2022) training is SGD with an initial learning rate of 1e-2, momentum of 0.9, and weight decay of 5e-4. The learning rate scheduler used in the training procedure is the multistep scheduler. As for the loss function, a novel ensemble loss proposed in ICU-Net is adopted. This ensemble is a simple linear combination of the Mean Squared Error (MSE) in amplitude, velocity, acceleration, and frequency components of EEG signals. Each term in the loss function has same weight. The batch size is set to 64 and the total training epoch is 200. The weight saving strategy in IC-U-Net training is the same as CLEEGN. A.1.3 1D-RESCNN The Adam optimizer with an initial learning rate of 1e-3 and the Mean Squared Error (MSE) loss function is adopted in 1D-ResCNN (Sun et al., 2020) training. Since the 1D-ResCNN is developed under one-dimensional synthesized EEG data and no explicit instruction of using multi-channel EEG provided, we trained 1D-ResCNN using two different method. For one method, we trained multiple models for each EEG channel respectively. For the other method, we viewed each multichannel EEG segment as a batch and the channel arrangement within a batch is fixed. Under our experiment, the result showed that the second method not only provides an efficient training process, but also results in a better reconstructed performance. The same weight saving strategy in CLEEGN is adopted in 1D-ResCNN training. A.1.4 DECODING PERFORMANCE EVALUATION MODEL: EEGNET: EEGNet (Lawhern et al., 2018) is a famous EEG decoding model and widely used in EEG literature. In the original EEGNet paper, they investigated their proposed model with a different number of kernels and denoted the model with F1 temporal filters and D spatial filters as EEGNet-F1,D. We use two different settings to train the two datasets used. In ERN classification, we use the EEGNet8,2 structure suggested by the EEGNet paper. As for the SSVEP classification, an experimental result showed that EEGNet-100,8 can draw the best performance. We trained and evaluated the decoding performance individually for each subject. We divided the collection of event epochs into three splits within each subject: training set, validation set, and test set with a ratio of 3:1:1. The ratio between classes remained the same in each set. The loss function is categorical cross entropy (CCE) and the Adam optimizer is adopted with a learning rate of 10\u22123 and zero weight decay. The batch size is set to 32 and the total training epoch is 200. A.2 DESCRIPTION OF DATASETS:  A.2.1 DATASET 1: FEEDBACK ERROR-RELATED NEGATIVITY (ERN): Error-related negativity (ERN) can be categorized as a kind of event-related potential (ERP), which occurs after an erroneous or abnormal event perceived by the subject. One characteristic of the feedback ERN is a relatively large negative amplitude approximately 350 ms and a positive amplitude approximately 500 ms after visual feedback triggered by the error event. In this work, we mainly use a well-studied EEG dataset from the BCI Challenge competition hosted by Kaggle to evaluate the artifact removal effectiveness. This dataset includes EEG recordings of 26 subjects (16 subjects labeled and 10 subjects unlabeled) that participated in a P300 speller task. P300 speller is a wellknown BCI system that develops a typing application through P300 response evoked potential. The ERN experiment was conducted under the assumption that the ERN occurred if the subject received incorrect prediction (feedback) from the P300 speller. The objective of the competition was to improve the P300 speller performance by implementing error correction through ERN potentials. We used the 16 subjects with labeled data of which the sampling rate is 200 Hz initially with 56 passive Ag/AgCl EEG sensors. In the interest of increasing the usability of EEG data, we applied some pre-processing procedures to each EEG recording. The EEG data were down-sampled to 128 Hz and re-referenced by the common average reference (CAR) method to eliminate common-mode noise and to zero-center the data. Each recording was band-pass filtered to 1-40 Hz through the FIR filter implemented by EEGLAB to remove DC drifting. During EEG decoding evaluation, we epoch EEG signals in [0, 1.25] second interval to obtain correct and erroneous feedback. A.2.2 DATASET 2: STEADY STATE VISUALLY EVOKED POTENTIAL (SSVEP): Steady state visually evoked potential (SSVEP) is another kind of ERP that is characterized as periodic potential induced by rapidly repetitive visual stimulation. The SSVEP is composed of several discrete frequency components, which consist of the fundamental frequency of the visual stimulus as well as its harmonics. To investigate the generalization ability of the model, we use \u201dEEG SSVEP Dataset II\u201d from Multimedia Authoring & Management using your Eyes & Mind (MAMEM). The dataset includes EEG data from 11 subjects and consists of five different frequencies (6.66, 7.50, 8.57, 10.00, and 12.00 Hz). Each subject was recorded in five sessions and each session included 25 trials (5 trials for each class). The data used a 256-channel HydroCel Geodesic Sensor Net (HCGSN) and captured the signals with a sampling rate of 250 Hz. Since there are several different bad channels in each subject\u2019s EEG recording, we preserved 20 common channels from each subject to train the CLEEGN model. Every recording was down-sampled to 125 Hz, re-referenced by the common average reference (CAR) method, and band-pass filtered to 1-40 Hz. We epoch EEG signals in [1, 5] second interval for each event recorded timestamp. The first second was discarded under the consideration of a reaction delay of the subject. A.3 INVESTIGATION OF DECODING PERFORMANCE EVALUATION: SSVEP In this work, we mainly use decoding performance as the assessment of different artifact removal methods. We provide an interpretation of the relationship between EEG quality and decoding performance. In Section 4 SSVEP result, IC-U-Net can optimize the weight with minimal MSE value among all compared artifact removal networks. However, the decoding performance is worse than CLEEGN and even the reference method (ICLabel), which implies that the average error over data points (MSE) is not the best assessment of EEG quality. In the SSVEP experiment, the fundamental frequency of the external rapidly repetitive visual stimulation and its harmonics can be an important feature in classification. Hence, higher quality in the frequency component of EEG data is required. We use the power spectrum density (PSD) to interpret the result of decoding performance. Figure 10 shows the PSD of EEG event data in each class denoised by different methods. We can see the reconstructed EEG spectrum from CLEEGN in each class is similar to the reference. As for IC-U-Net, it seems that the model can not completely reconstruct in several frequency bands (15-20Hz, 30-40Hz), which leads to the low decoding performance. The PSD result shows that there is great distortion in the power density of reconstructed data generated by 1D-ResCNN. Since 1D-ResCNN is a one-dimensional structure, we hypothesize that spatial information is important in EEG artifact removal. A.4 ADDITIONAL TABLES', 'agent_name': 'EEGDomainExpert'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The paper \\\"CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction\\\" proposes a novel deep learning architecture for automatic EEG artifact removal. This review will critically evaluate the method's neurophysiological plausibility, its alignment with known EEG principles, and the interpretation of its results within the context of neuroscience and EEG research.\\n\\n## 1. Neurophysiological Plausibility and Relevance of the Proposed Method\\n\\n**1.1 Understanding of EEG Signals and Artifacts:**\\nThe paper correctly identifies EEG as reflecting \\\"subtle fluctuations of the electrical field driven by local neuroelectrophysiological activity of a population of neurons in the brain cortex.\\\" This foundational understanding is crucial. It also accurately lists common physiological artifacts (eye blinks, muscle movements, ocular activity, cardiac activity) and acknowledges their diversity and non-stationarity, which are indeed major challenges in EEG signal processing. The problem statement is well-aligned with the neurophysiological reality of EEG recording.\\n\\n**1.2 CLEEGN Architecture and Principles:**\\nCLEEGN utilizes a convolutional autoencoder structure, which is a plausible approach for signal denoising. The design incorporates distinct spatial and temporal filtering steps, which aligns well with the multi-channel, time-series nature of EEG data.\\n\\n*   **Spatial Filters (Encoder First Block):** The use of `(C, 1)` shaped filters in the first convolutional block to \\\"extract spatial EEG features\\\" is neurophysiologically intuitive. Different neural sources and artifact sources have distinct spatial distributions across the scalp. A spatial filter can learn to identify and separate these patterns. This is akin to concepts like common average referencing, Laplacian filtering, or spatial ICA components that exploit the unique scalp topography of brain and artifact signals.\\n*   **Temporal Filters (Encoder Second Block):** The `(1, \u230afs \u00d7 0.1\u230b)` shaped filters for \\\"temporal EEG features\\\" are also plausible. EEG signals are oscillatory and dynamic, characterized by specific frequency bands and event-related potentials (ERPs) with distinct temporal profiles. Temporal convolutions can capture these patterns, learning to identify the characteristic waveforms of neural activity versus artifacts (e.g., sharp eye blinks, high-frequency EMG). A filter length of approximately 100ms can capture a significant portion of common EEG rhythms (e.g., an alpha cycle ~100ms, a theta cycle ~200ms).\\n*   **Autoencoder Symmetry:** The symmetric decoder structure is standard for autoencoders and aims to reconstruct the original signal space from the learned latent representation. Conceptually, the encoder learns a compressed, \\\"clean\\\" representation, and the decoder expands it back to the channel-time domain. This process implicitly assumes that brain activity and artifact information can be separated in this latent space, with artifacts being discarded or minimized during the encoding-decoding process.\\n\\n**1.3 The \\\"Reference Data\\\" as Ground Truth:**\\nThis is the most critical point concerning neurophysiological plausibility. CLEEGN is trained as a denoising autoencoder using pairs of \\\"noisy raw EEG data\\\" and \\\"noiseless reference EEG data.\\\" The \\\"noiseless reference\\\" is generated by *offline automatic denoising methods* (ICLabel, ASR, or their hybrid).\\n*   **Implication for Plausibility:** CLEEGN learns to mimic the output of these existing artifact removal algorithms. This means that CLEEGN's \\\"neurophysiological plausibility\\\" is directly contingent on the neurophysiological accuracy and \\\"cleanliness\\\" of its \\\"teachers\\\" (ICLabel and ASR).\\n    *   **ICLabel (based on ICA):** ICA assumes linear mixing and statistical independence of sources. While powerful for separating brain and artifact sources, ICA is not infallible. Components might sometimes blend brain and artifact activity, or brain components might be inadvertently removed if misclassified. ICLabel's automatic classification, while sophisticated, can still make errors.\\n    *   **ASR (based on PCA):** ASR targets high-variance artifacts. While effective for transient, high-amplitude artifacts (like blinks), it can potentially distort or remove genuine neural activity if that activity also exhibits high variance (e.g., large ERPs). It may also struggle with more subtle or distributed artifacts.\\n*   **Overall:** By relying on ICLabel and ASR as the source of \\\"ground truth,\\\" CLEEGN is learning to reproduce *their* version of a clean signal, including any potential biases, misclassifications, or distortions introduced by these upstream methods. Therefore, while CLEEGN offers a \\\"plug-and-play\\\" solution, its reconstructed signals are only as neurophysiologically accurate as the methods used to generate its training data. This is an inherent limitation of learning-based denoising where true \\\"ground truth\\\" clean EEG is generally unavailable.\\n\\n## 2. Alignment with Known Principles of EEG Signal Generation and Artifact Characteristics\\n\\n**2.1 Artifact Removal Mechanism:**\\nThe paper claims CLEEGN can eliminate artifacts like high-amplitude ocular artifacts (EOG) and muscular artifacts (EMG).\\n*   **EOG:** EOG artifacts typically manifest as large, slow waves, primarily in frontal electrodes, but with some spread to more posterior channels. They have distinct spatiotemporal characteristics. The spatial filters (C,1) and temporal filters (1,T) in CLEEGN are well-suited to capture these spatial and temporal signatures and suppress them. The visual examples in Figure 5 show effective EOG removal in Fp1, which aligns with expected performance for such artifacts.\\n*   **EMG:** EMG artifacts are typically high-frequency (often >20-30 Hz), irregular, and localized over muscle regions (e.g., temporal muscles for jaw clenching). The temporal filters are crucial here to distinguish these high-frequency, non-neural signals from desired brain oscillations. The visual examples in Figure 5 show effective EMG removal on T7, which is a common location for such artifacts. The comparison with ASR's inability to remove high-frequency EMG is also consistent with known limitations of ASR for sustained high-frequency noise.\\n*   **Spatial Information Importance:** The appendix's discussion on 1D-ResCNN performing poorly due to a lack of spatial information is a strong neurophysiological point. Many artifacts (and brain sources) have specific spatial distributions. A method that processes channels independently (like a true 1D CNN) will fail to leverage this crucial spatial coherence, leading to suboptimal artifact removal and potential distortion of neural signals. CLEEGN's multi-channel spatial filtering step directly addresses this principle.\\n\\n**2.2 Preservation of Neural Information:**\\nThe ultimate goal of artifact removal is to preserve the underlying brain activity.\\n*   **Functional Validation (Decoding Performance):** The use of decoding performance (AUC for ERN, Accuracy for SSVEP) as a primary metric is an excellent choice for evaluating whether task-relevant neural information is preserved. If a method improves decoding, it implies that the reconstructed signal contains more distinguishable task-related features. This is highly relevant for BCI and cognitive neuroscience research.\\n*   **Frequency Domain Integrity (PSD Analysis in Appendix A.3):** This section is particularly valuable for neurophysiological assessment.\\n    *   The SSVEP task relies on specific frequency components (fundamental and harmonics of visual stimulation). The finding that CLEEGN's reconstructed EEG spectrum is \\\"similar to the reference\\\" and that IC-U-Net and 1D-ResCNN distort specific frequency bands (e.g., 15-20Hz, 30-40Hz for IC-U-Net, general distortion for 1D-ResCNN) is crucial. Neurophysiologically, preserving the power spectral density of brain oscillations and evoked responses is paramount for accurate interpretation. Distortion in specific frequency bands would significantly compromise the utility of the data for frequency-based analyses or biomarker detection. CLEEGN's ability to maintain these components, as indicated by the PSD plots, is a strong argument for its neurophysiological utility.\\n\\n## 3. Critical Evaluation of Interpretation of Results in the Context of Neuroscience and EEG Research\\n\\n**3.1 Interpretation of \\\"Outperforming the Reference\\\":**\\nThe paper repeatedly states that \\\"CLEEGN can outperform the reference data used for its training\\\" and \\\"CLEEGN even provides a better reconstruction than the offline ICLabel\\\" in terms of decoding performance. This is a bold and significant claim.\\n*   **Possible Interpretations (Neuroscientific Context):**\\n    1.  **Learning Robust Features:** CLEEGN, by training on a large dataset of (noisy, reference) pairs, might learn more robust representations of brain activity and artifacts that generalize better than individual runs of ICLabel or ASR on unseen data. The deep learning model might implicitly \\\"average out\\\" or \\\"smooth over\\\" minor imperfections or misclassifications in the individual reference data points, leading to a more consistent and functionally relevant clean signal.\\n    2.  **Optimized for Downstream Task:** It is possible that CLEEGN, through its training, learns to optimize the signal specifically for the subsequent EEGNet classifier. It might preserve or even subtly enhance the features that EEGNet finds most discriminative, even if those features are not \\\"perfectly pure\\\" in a general neurophysiological sense. This doesn't mean it's necessarily a \\\"purer\\\" brain signal, but rather a *more useful* signal for the intended BCI/decoding task. This is still a valuable outcome for application-focused research.\\n    3.  **Limitations of MSE:** The paper acknowledges that \\\"the average error over data points (MSE) is not the best assessment of EEG quality.\\\" This is crucial. Low MSE doesn't guarantee neurophysiological fidelity. A signal that is slightly higher in MSE but preserves crucial ERP peaks or oscillatory power might be more neurophysiologically meaningful. The fact that CLEEGN has the lowest MSE *and* best decoding for ERN, but not lowest MSE for SSVEP while still having best decoding, reinforces that functional utility (decoding) is a more relevant metric than raw signal similarity (MSE) for applications.\\n\\n**3.2 Visualization for Neuroscientific Insights:**\\nThe visualization using PCA of latent features (Figure 9) is interesting.\\n*   **Interpretation:** The observation that latent features \\\"shrink in the encoder design and expand in the decoder design\\\" and that reconstructed data is a \\\"more compact cluster\\\" in PCA space is typical for autoencoders performing dimensionality reduction and denoising. It implies that the network effectively learns to filter out high-variance noise, leading to a more consistent underlying representation.\\n*   **Neuroscientific Insight Level:** While this demonstrates the network's internal workings (feature learning, denoising effect), it provides somewhat limited *direct* neuroscientific insights into *how* brain signals are separated from artifacts at a neural level. It confirms that the denoising works, but doesn't explicitly reveal novel findings about brain function or artifact generation beyond confirming the network's efficacy. However, the accompanying interpretation of spatial arrangement (frontal to posterior along x-axis, left-central-right along y-axis) in PCA space of raw EEG is a good illustration of the spatial coherence of EEG.\\n\\n**3.3 \\\"Plug-and-Play\\\" Relevance:**\\nThe \\\"plug-and-play\\\" (subject-independent, no calibration) aspect is highly relevant for real-world BCI applications and clinical settings where individual calibration is often impractical. The findings regarding minimal training data duration and number of subjects (Figure 7, 8) further support this practical utility. While this is not directly a neurophysiological insight, it enables the widespread use of EEG in scenarios where neurophysiological insights are then sought.\\n\\n## Conclusion\\n\\nCLEEGN represents a significant step towards robust, automatic, and functionally effective EEG artifact removal. Its architecture thoughtfully incorporates spatial and temporal convolutions, aligning well with the inherent characteristics of EEG signals and common artifacts. The detailed comparison with other methods and the insightful PSD analysis strongly support its ability to preserve task-relevant neural information (e.g., SSVEP frequencies) while effectively removing artifacts.\\n\\nThe primary neurophysiological limitation lies in its reliance on existing artifact removal algorithms (ICLabel, ASR) to generate its \\\"ground truth\\\" training data. This means CLEEGN learns to reproduce the *version* of clean EEG provided by these methods, inheriting any subtle biases or imperfections. Future work could aim for more direct neurophysiological validation, perhaps by using synthetic artifact models with precisely known ground truth, or by more directly evaluating the impact on specific ERP component morphology, source localization accuracy, or coherence measures, rather than solely relying on decoding performance.\\n\\nDespite this, the consistent improvement in decoding performance across different datasets, coupled with the detailed visualization of artifact removal and frequency spectrum preservation, demonstrates CLEEGN's strong functional relevance for neuroscience and BCI applications. Its \\\"plug-and-play\\\" nature makes it a highly promising tool for advancing EEG research and real-world brain-computer interfaces.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.8653268\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Please provide a detailed review of the paper, focusing on the neurophysiological plausibility and relevance of the proposed method. Assess whether the method aligns with known principles of EEG signal generation and artifact characteristics. Critically evaluate the interpretation of the results in the context of neuroscience and EEG research. The paper title is: CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction. The paper content is: Since the first record of human electroencephalogram (EEG) performed almost a century ago (in 1924), EEG has been one of the most widely used non-invasive neural modalities that monitors brain activity in high temporal resolution (Koike et al., 2013; Mehta & Parasuraman, 2013; Sejnowski et al., 2014). Among a variety of modalities, EEG has extensive use in the clinical assessment of neurological and psychiatric conditions, as well as in the research of neuroscience, cognitive science, psychology, and brain-computer interfacing (BCI). EEG signals measure subtle fluctuations of the electrical field driven by local neuroelectrophysiological activity of a population of neurons in the brain cortex (Cohen, 2017). While the electrodes are placed on the surface of the scalp, undesired artifacts may introduce interruption in the measurements and distort the signal of interest. Even in a well-controlled laboratory with a well-trained subject who can maximally keep the body still and relaxed, the EEG signals, unfortunately, could be contaminated by inevitable behavioral and physiological artifacts such as eye blinks, reflective muscle movements, ocular activity, cardiac activity, etc (Croft & Barry, 2000; Wallstrom et al., 2004; Romero et al., 2008). In practice, it is difficult to identify and track the sources of artifacts entirely due to their diversity and non-stationarity. Noise cancellation and artifact removal remain major issues in EEG signal processing and decoding. Currently, numerous methods have been proposed to alleviate the influence of artifacts in EEG signals. Traditional EEG denoising algorithms include filtering, regression, data separation or decomposition (Makeig et al., 1995; Islam et al., 2016; Kothe & Jung, 2016). According to previous meta-analyses on EEG artifact removal literature (Urigu\u0308en & Garcia-Zapirain, 2015; Jiang et al., 2019), independent component analysis (ICA) is especially popular. It is majorly used in 45% of EEG denoising literature. ICA-based artifact removal estimates the component activity by unmixing the EEG data in the channel domain. Through manual or automatic identification, one can exclude the artifact components and then reconstruct the EEG data through back projection based on non-artifact components (Jung et al., 2000a). The fast growth of deep learning methods has drawn state-of-the-art performances in a variety of machine learning problems (LeCun et al., 2015). Lately, deep-learning-based EEG reconstruction has drawn much attention (Leite et al., 2018; Sun et al., 2020; Lopes et al., 2021; Lee et al., 2020; Chuang et al., 2022). Although these methods can effectively remove artifacts from artificial synthetic signals, their performance in reconstructing real EEG data has not yet been validated in terms of decoding labeled EEG data. Meanwhile, the model design of existing deep-learning-based techniques for EEG reconstruction rarely takes the characteristics of EEG into account. In this work, we propose CLEEGN, a ConvoLutional neural network for EEG reconstructioN. CLEEGN is capable of subject-independent EEG construction without any training/calibration for a new subject. The contributions of this work are three-fold: \u2022 a light-weight autoencoder CNN, CLEEGN, with a subject-independent framework that facilitates plug-and-play EEG reconstruction. \u2022 CLEEGN outperforms leading online/offline methods in providing reconstructed EEG data with the best decoding performance for BCI datasets. \u2022 with a novel model design dedicated to EEG reconstruction, CLEEGN characterizes patterns of EEG interpretable and provides neuroscientific insights. 2 RELATED WORK: Current processing techniques for EEG artifact removal are highly varied based on the context where the algorithm may apply. Earlier attempts of EEG denoising assumed that the EEG signals and artifacts appear in different frequency ranges. Based on the assumption, some significant artifacts can be eliminated by the linear filtering method during the online stage (Seifzadeh et al., 2014). Despite the advantages of low computational time, linear filtering hardly removes artifacts that distribute in an overlapped frequency range of EEG signals. Another approach, adaptive filtering (Schlo\u0308gl et al., 2007), estimates artifact signals through additional EOG, EMG, ECG channels and removes these noisy signals from the recording signals by regression. Nevertheless, this approach requires additional auxiliary electrodes and raises the cost and inconvenience in practical applications. The blind source separation (BSS) method in EEG denoising was developed by assuming that the recording EEG signals are linear combinations of the signals from noise sources and the brain neurons. One of the most well-known BSS method is independent component analysis (ICA) (Jung et al., 2000a;b), which is able to separate EEG signals into independent components (ICs) (Makeig et al., 1995). Traditionally, the artifact components extracted by ICA are determined and removed through manual inspection. Recently developed ICLabel can label the ICs provenance into seven different categories: brain, eye, heart, muscle, line noise, channel noise, and other (Pion-Tonachini et al., 2019). Artifact subspace reconstruction (ASR) is another automatic approach, which is based on the principal component analysis (PCA) method (Kothe & Jung, 2016). The ASR method selects relatively noiseless periods from the multi-channel EEG data as reference based on the data distribution. After projecting all EEG data to the principal-component domain, high-variance components projected from the artifacts are detected by a cutoff parameter k. The noiseless signals are reconstructed by preserving the components without carrying artifacts and back-projected to the time domain. The ASR method has been shown capable of improving the quality of ICA decomposition (Chang et al., 2020). Recently, neural network-based methods have been proposed to remove artifacts for EEG data. A variety of network structures have been applied to the framework for removing EEG artifacts and reconstructing clean EEG. A deep convolutional autoencoder (Leite et al., 2018) can enhance the peak-signal-to-noise ratio compared to the linear filtering method via a common CNN autoencoder structure, which has been widely used on image denoising. Their work shows that it seems practicable to transform the EEG waveform through a CNN structure. Later on, a combined framework integrating Bayesian deep learning and ICA (Lee et al., 2020) used thresholding of the EEG data distribution to discard ICs classified as ocular artifacts. These methods leverage the flexibility of deep learning model design and achieve improvements in their assessments. Considering the non- stationary property in EEG data and the degradation phenomenon during training, 1D-ResCNN (Sun et al., 2020) was proposed, which adopted an Inception-Residual module in the network structure. This network is able to remove EOG, ECG, EMG on single-channel synthesis EEG data. Instead of using synthesis EEG data, IC-U-Net (Chuang et al., 2022) was created, which generated pairings of noisy and noiseless EEG data through ICA as training data. The proposed neural network is a one-dimensional adaptation of U-Net architecture trained with the ensemble of four loss functions to minimize the difference between amplitude, velocity, acceleration, and frequency components from the signals. This work showed that their reconstructed signal has higher SNR and can surely increase the number of brain components classified by ICLabel. 3 MATERIALS AND METHODS: CLEEGN Architecture and Model Training. CLEEGN is designed to map multi-channel noisy EEG into a latent space and reconstruct it into noiseless EEG signals. The architecture of CLEEGN is shown in Figure 1. The architecture of the encoder is inspired by an existing CNN model designed for EEG recognition (Wei et al., 2019) and incorporates convolutional blocks that capture spatiotemporal characteristics of EEG data. The first convolution block is used to extract spatial EEG features through a convolutional layer containing C spatial filters whose shape is (C, 1), where C is the number of electrodes of the EEG signals. The second convolution block is used to extract temporal EEG features through a convolutional layer containing NF temporal filters with shape (1, \u230afs \u00d7 0.1\u230b), where fs denotes the sample frequency of the EEG signals. As for the decoder, we design an approximately symmetric structure to the encoder with three convolutional blocks. The first convolutional block decodes the EEG feature with NF temporal kernels with shape is (1, \u230afs\u00d70.1\u230b). The second block decodes the EEG feature by a convolution layer with C spatial kernels of shape (C, 1). The last convolutional block is for projecting the feature domain back to the original time domain. To maintain the shape between the model inputs and outputs, every convolution block applies zeropadding except the first block in the encoder. The detail of the CLEEGN architecture is available in Table 1. As illustrated in Figure 1, the objective of the proposed method is to minimize the difference between noiseless signals, Y , and the model output, Y\u0302 . The recording EEG data, X , is the combination of brain signals, Y , and the signals from multiple noise sources, N . CLEEGN is a denoising autoencoder that is intended to perform artifact removal on EEG data by creating a mapping between multi-channel noisy EEG signals, X , and noiseless signals, Y . The training process of CLEEGN utilizes pairing noisy raw EEG data and noiseless reference EEG data so that the model can learn to transform noisy EEG data into reconstructed EEG data with maximal similarity to the reference data. To generate large-scale reference EEG data, we adopted automatic denoising methods, ICLabel and ASR, to remove artifacts and reconstruct clean waveforms offline. The pairing EEG recording with C channels of continuous noisy/noiseless data was further segmented into training sample pairs. The input size is (C, T ), where T is the number of time points based on the sampling rate of the EEG data in 4-second segments. The windows size and stride are set as T and 0.5T . In the context of plug-and-play EEG reconstruction, CLEEGN was trained as a subject-independent model. To perform subject-independent training and testing, we grouped subjects into k disjoint sets. During the training process, one of the sets was left out for testing. Subjects\u2019 EEG data in the left-alone set were not involved in both training and validation. A complete experiment on a single dataset would result in k different models. The artifact removal performance of a model was evaluated by using the left-alone set. The number of subjects in one set and the EEG duration available for each subject depend on the experiment setting and the dataset used. EEG Reconstruction and Evaluation. The reconstructed EEG data of CLEEGN is generated through an online reconstruction simulation. In the online stage, the system performs artifact removal on a new subject using a pre-trained CLEEGN model without any requirement of training/calibration. Figure 2 shows the online EEG reconstruction simulated in offline. The EEG data from a subject in the left-alone set would be fed into the model sequentially. The stride size of the sliding window is set to 0.5 seconds to minimize the delay in online reconstruction. The fitness of the model can be evaluated by observing the waveform visually or measuring the similarity using the mean square error (MSE) between reconstructed and noiseless EEG. Meanwhile, we propose to use the decoding performance of labeled EEG data as an objective measurement of the reconstructed EEG quality. We employed EEGNet, a compact CNN for end-to-end EEG decoding (Lawhern et al., 2018), as the classifier to decode the labeled EEG data in our study. 4 EXPERIMENTS: Data and Model Fitting. In this work, we applied two labeled EEG datasets, the BCI-ERN dataset 1 and the MAMEM-SSVEP-II dataset 2. We estimate the classification performance by calculating the score of Area Under the Curve (AUC) in ERN binary classification and Top-1 accuracy in the SSVEP task. Figure 3 is the schematic diagram of the process of decoding performance estimation using the reconstructed EEG data. The segmented EEG epochs are divided into three splits within each subject: training set, validation set, and test set. The ratio between classes remained the same in each set. We perform 20 repeated runs with shuffled data to estimate the average performance of each decoding performance evaluation. Detailed descriptions of the two datasets are available in the appendix at A.2. Figure 4 presents the loss curve and the reconstruction result at the end of three different training iterations in a sample run using the ERN dataset. Through this exploration, we can look into the course of model fitting. In the early stage of training, the model reduces the large MSE loss by smoothing the input signals. To further minimize the error between reconstruction and reference (offline denoised signal), the model compensates the intensity to better fit the reference signals. At the convergent stage of training, we can see the reconstructed waveform is very similar to the one in reference data. 1https://www.kaggle.com/c/inria-bci-challenge 2https://www.mamem.eu/results/datasets/ Types of Reference Data. As ICLabel, ASR, and the hybrid ASR-ICLabel are employed to automatically generate large-scale noiseless reference EEG data offline, it is of our interest to investigate what type of reference EEG serves as the best noiseless reference data for the CLEEGN model training. Figure 5 presents the EEG waveforms sampled from Subject 2 in the ERN dataset. The EEG time series from top to bottom represent the recording of Fp1, T7, Cz, T8, and O2 channels. The nine sub-figures visualize the comparison between the noisy raw data and the reference data reconstructed by the ICLabel, ASR with or without ICLabel when the cutoff parameter k was set to 4, 8, 16, and 32. We can see the waveform of ICLabel and the ASR-ICLabel hybrid method with 4 different cutoff parameters k are similar, and the noiseless waveform reconstructed by CLEEGN are highly correlated to the result from these 5 methods. High-amplitude ocular artifacts such as blinking and eye saccades are phenomenal in frontal channels (Fp1) and muscular artifacts featuring high-frequency (around 24-30 Hz in the \u03b2 band) are observed on the T7 channel. CLEEGN, ICA, and ASR-ICLabel hybrid methods can eliminate these kinds of artifacts. Compared to the ICLabel and ASR-ICLabel hybrid methods, ASR is unable to eliminate the high-frequency EMG artifact for this dataset. The tolerance of large amplitude EOG artifact increases with the employment of larger cutoff parameter k. Limited by the reference data generated by ASR, CLEEGN preserves the high-frequency artifact. As for the large amplitude artifacts that failed to be identified by ASR-32 and ASR-16, we can see that CLEEGN is able to mitigate those EOG artifacts. As shown in Figure 6(a), the reference data prepared by ICLabel offer the lowest MSE, i.e. the best fitness for CLEEGN training. We also compare the types of reference EEG data and their corresponding CLEEGN reconstruction results in the decoding performance of the ERN EEG dataset to assess their data quality. As illustrated in Figure 6(b), CLEEGN reconstruction draws better performances for all denoising methods. Although the performance decreases with a smaller cutoff parameter k, CLEEGN can outperform the reference data used for its training. This result suggests that our method not only removes the artifact but also preserves informative brain activity in the EEG under our cross-subject training scheme. As the ICLabel provides the reference data with the best decoding performance and fitness, we chose ICLabel as the source of reference data for further experiments in this study. Training Data Size. We explored the effect of data length per subject in the training set regarding the fitness and the decoding performance of CLEEGN. The training data were segmented from the first 1, 2, 4, 10, 20, and 30 minutes of data in each EEG recording to investigate the trade-off between training time and reconstruction performance. Figure 7(a) shows the fitness of CLEEGN to the reference data. The model trained using the first 10 minutes obtain the minimal value among all duration configurations. Though the difference in MSE between 10, 20, and 30 minutes is not noticeable, Figure 7(b) shows that using the 10-minute training data yields the best decoding performance among all settings. Interestingly, with only one-minute training data from each subject, CLEEGN can achieve comparable decoding performance to the reference data. This indicates that CLEEGN retains its performance even when each subject only contributes a short recording for training. In addition, we explore the effect of the number of subjects included in the training set on the fitness of CLEEGN model training and the decoding performance of the reconstructed EEG data. We randomly reduced the number of subjects for training from 12 to 6, 4, and 2. Since the subset of subjects may influence the performance, we tested multiple combinations and averaged the results. With the decrease in the number of subjects, the MSE value and the standard error (light span area) increase in Figure 8(a), which indicates that the generalization ability of the CLEEGN model reduces when fewer subjects are included for training. In Figure 8(b), we can observe a slight decrease in decoding performance. We consider the number of subjects as an essential factor in the performance of CLEEGN, yet it requires only a few subjects to achieve a satisfactory performance compared to the reference data. Performance Comparison. We compare the performance of CLEEGN against three baseline methods, ICLabel, 1D-ResCNN, and IC-U-Net. Except for the ICLabel that operates offline, CLEEGN and the other two neural network-based methods perform a simulated online reconstruction based on the same training process with the reference data generated by ICLabel. Results of the ERN EEG dataset are shown in Table 2. CLEEGN has the best fitness to the reference data with the minimum MSE, the highest AUC score in the decoding performance, and the least parameters. For the SSVEP EEG data, although not having the minimum MSE, CLEEGN outperforms other methods in the decoding accuracy and the model size. The evaluation across the two datasets suggests an overall superiority of our proposed CLEEGN model over other existing neural network-based methods in online reconstruction. CLEEGN even provides a better reconstruction than the offline ICLabel. These promising results indicate the usability of CLEEGN in online training/calibration-free EEG reconstruction that truly meets the need for real-world applications of EEG-based BCI. Visualization. On account of the interpretability of the CLEEGN model, we visualized its intermediate latent features by mapping onto a 2-dimensional domain based on the principal component analysis (PCA) of the noisy raw EEG data (Wold et al., 1987). Figure 9(a) shows the principal component space and the noisy EEG data channel projections. We observe that the scatters of noisy EEG data retain the spatial relationship of the actual EEG electrodes. The arrangement of frontal electrodes (prefix in Fp, P) to the posterior (prefix in P, O) are projected along the x-axis from right to left. The projection from top to bottom matches the left-side electrodes (suffixed in odd numbers), central electrode (suffixed in z), and right-side electrodes (suffixed in even numbers). The projection suggests that EEG data of adjacent channels tend to show similar waveforms. The blue dots in Figure 9(b), (c) are the projection of latent features resulting from the first and second convolutional layers, which make up the encoder design of CLEEGN. The projection of latent features from the next two convolutional layers is presented in Figure 9(d), (e). From Figure 9(b) to (e), we can see the distribution range of the latent features shrink in the encoder design and expand in the decoder design of CLEEGN. Figure 9(f) compares the distribution of noisy data and CLEEGN reconstructed data, showing that the CLEEGN reconstructed data is a more compact cluster than the noisy EEG in the PCA space. This implies that the process within CLEEGN includes a projection of the original EEG data in the upstream layer, complex temporal filtering and combination in the midstream layers, and a final projection that converts the noiseless latent features back to the channel domain. 5 CONCLUSION: In this work, we have proposed CLEEGN, a novel convolutional neural network for plug-and-play automatic EEG reconstruction. The training of CLEEGN leverages the conventional offline denoising methods, ASR and ICLabel, with automatic component classification to generate abundant noiseless EEG data. The performance of CLEEGN was objectively validated using multiple evaluations including waveform observation; reconstruction error assessment; and decoding accuracy on well-studied, labeled datasets. The experiment results suggest that, even without any calibration, CLEEGN can predominantly preserve inherent brain activity. According to the decoding performance of reconstructed EEG data, CLEEGN outperforms other neural network-based denoising methods on both ERN and SSVEP EEG decoding. From the visualization of the waveform, CLEEGN can remove artifacts from different sources and the waveform is highly correlated to the reference data. Through the visualization of model parameters and latent features, we exhibit model behavior and reveal explainable insights related to existing knowledge of neuroscience. We foresee pervasive applications of CLEEGN in prospective works of EEG decoding and analysis. A APPENDIX:  A.1 NEURAL NETWORK TRAINING CONFIGURATION:  A.1.1 CLEEGN: For training CLEEGN, we used the Adam optimizer with an initial learning rate of 1e-3 without weight decay. Besides, the exponential learning rate scheduler is applied with a gamma of 0.8. As for the loss function, we used Mean Squared Error (MSE). The batch size is set to 64 and the total training epoch is 40. During the training procedure, the model is evaluated using the validation subset at the end of every epoch with the purpose of saving the weights that achieved the lowest validation loss. A.1.2 IC-U-NET The optimizer adopted in IC-U-Net (Chuang et al., 2022) training is SGD with an initial learning rate of 1e-2, momentum of 0.9, and weight decay of 5e-4. The learning rate scheduler used in the training procedure is the multistep scheduler. As for the loss function, a novel ensemble loss proposed in ICU-Net is adopted. This ensemble is a simple linear combination of the Mean Squared Error (MSE) in amplitude, velocity, acceleration, and frequency components of EEG signals. Each term in the loss function has same weight. The batch size is set to 64 and the total training epoch is 200. The weight saving strategy in IC-U-Net training is the same as CLEEGN. A.1.3 1D-RESCNN The Adam optimizer with an initial learning rate of 1e-3 and the Mean Squared Error (MSE) loss function is adopted in 1D-ResCNN (Sun et al., 2020) training. Since the 1D-ResCNN is developed under one-dimensional synthesized EEG data and no explicit instruction of using multi-channel EEG provided, we trained 1D-ResCNN using two different method. For one method, we trained multiple models for each EEG channel respectively. For the other method, we viewed each multichannel EEG segment as a batch and the channel arrangement within a batch is fixed. Under our experiment, the result showed that the second method not only provides an efficient training process, but also results in a better reconstructed performance. The same weight saving strategy in CLEEGN is adopted in 1D-ResCNN training. A.1.4 DECODING PERFORMANCE EVALUATION MODEL: EEGNET: EEGNet (Lawhern et al., 2018) is a famous EEG decoding model and widely used in EEG literature. In the original EEGNet paper, they investigated their proposed model with a different number of kernels and denoted the model with F1 temporal filters and D spatial filters as EEGNet-F1,D. We use two different settings to train the two datasets used. In ERN classification, we use the EEGNet8,2 structure suggested by the EEGNet paper. As for the SSVEP classification, an experimental result showed that EEGNet-100,8 can draw the best performance. We trained and evaluated the decoding performance individually for each subject. We divided the collection of event epochs into three splits within each subject: training set, validation set, and test set with a ratio of 3:1:1. The ratio between classes remained the same in each set. The loss function is categorical cross entropy (CCE) and the Adam optimizer is adopted with a learning rate of 10\u22123 and zero weight decay. The batch size is set to 32 and the total training epoch is 200. A.2 DESCRIPTION OF DATASETS:  A.2.1 DATASET 1: FEEDBACK ERROR-RELATED NEGATIVITY (ERN): Error-related negativity (ERN) can be categorized as a kind of event-related potential (ERP), which occurs after an erroneous or abnormal event perceived by the subject. One characteristic of the feedback ERN is a relatively large negative amplitude approximately 350 ms and a positive amplitude approximately 500 ms after visual feedback triggered by the error event. In this work, we mainly use a well-studied EEG dataset from the BCI Challenge competition hosted by Kaggle to evaluate the artifact removal effectiveness. This dataset includes EEG recordings of 26 subjects (16 subjects labeled and 10 subjects unlabeled) that participated in a P300 speller task. P300 speller is a wellknown BCI system that develops a typing application through P300 response evoked potential. The ERN experiment was conducted under the assumption that the ERN occurred if the subject received incorrect prediction (feedback) from the P300 speller. The objective of the competition was to improve the P300 speller performance by implementing error correction through ERN potentials. We used the 16 subjects with labeled data of which the sampling rate is 200 Hz initially with 56 passive Ag/AgCl EEG sensors. In the interest of increasing the usability of EEG data, we applied some pre-processing procedures to each EEG recording. The EEG data were down-sampled to 128 Hz and re-referenced by the common average reference (CAR) method to eliminate common-mode noise and to zero-center the data. Each recording was band-pass filtered to 1-40 Hz through the FIR filter implemented by EEGLAB to remove DC drifting. During EEG decoding evaluation, we epoch EEG signals in [0, 1.25] second interval to obtain correct and erroneous feedback. A.2.2 DATASET 2: STEADY STATE VISUALLY EVOKED POTENTIAL (SSVEP): Steady state visually evoked potential (SSVEP) is another kind of ERP that is characterized as periodic potential induced by rapidly repetitive visual stimulation. The SSVEP is composed of several discrete frequency components, which consist of the fundamental frequency of the visual stimulus as well as its harmonics. To investigate the generalization ability of the model, we use \u201dEEG SSVEP Dataset II\u201d from Multimedia Authoring & Management using your Eyes & Mind (MAMEM). The dataset includes EEG data from 11 subjects and consists of five different frequencies (6.66, 7.50, 8.57, 10.00, and 12.00 Hz). Each subject was recorded in five sessions and each session included 25 trials (5 trials for each class). The data used a 256-channel HydroCel Geodesic Sensor Net (HCGSN) and captured the signals with a sampling rate of 250 Hz. Since there are several different bad channels in each subject\u2019s EEG recording, we preserved 20 common channels from each subject to train the CLEEGN model. Every recording was down-sampled to 125 Hz, re-referenced by the common average reference (CAR) method, and band-pass filtered to 1-40 Hz. We epoch EEG signals in [1, 5] second interval for each event recorded timestamp. The first second was discarded under the consideration of a reaction delay of the subject. A.3 INVESTIGATION OF DECODING PERFORMANCE EVALUATION: SSVEP In this work, we mainly use decoding performance as the assessment of different artifact removal methods. We provide an interpretation of the relationship between EEG quality and decoding performance. In Section 4 SSVEP result, IC-U-Net can optimize the weight with minimal MSE value among all compared artifact removal networks. However, the decoding performance is worse than CLEEGN and even the reference method (ICLabel), which implies that the average error over data points (MSE) is not the best assessment of EEG quality. In the SSVEP experiment, the fundamental frequency of the external rapidly repetitive visual stimulation and its harmonics can be an important feature in classification. Hence, higher quality in the frequency component of EEG data is required. We use the power spectrum density (PSD) to interpret the result of decoding performance. Figure 10 shows the PSD of EEG event data in each class denoised by different methods. We can see the reconstructed EEG spectrum from CLEEGN in each class is similar to the reference. As for IC-U-Net, it seems that the model can not completely reconstruct in several frequency bands (15-20Hz, 30-40Hz), which leads to the low decoding performance. The PSD result shows that there is great distortion in the power density of reconstructed data generated by 1D-ResCNN. Since 1D-ResCNN is a one-dimensional structure, we hypothesize that spatial information is important in EEG artifact removal. A.4 ADDITIONAL TABLES\",\n  \"agent_name\": \"EEGDomainExpert\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The paper \"CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction\" proposes a novel deep learning architecture for automatic EEG artifact removal. This review will critically evaluate the method\\'s neurophysiological plausibility, its alignment with known EEG principles, and the interpretation of its results within the context of neuroscience and EEG research.\\n\\n## 1. Neurophysiological Plausibility and Relevance of the Proposed Method\\n\\n**1.1 Understanding of EEG Signals and Artifacts:**\\nThe paper correctly identifies EEG as reflecting \"subtle fluctuations of the electrical field driven by local neuroelectrophysiological activity of a population of neurons in the brain cortex.\" This foundational understanding is crucial. It also accurately lists common physiological artifacts (eye blinks, muscle movements, ocular activity, cardiac activity) and acknowledges their diversity and non-stationarity, which are indeed major challenges in EEG signal processing. The problem statement is well-aligned with the neurophysiological reality of EEG recording.\\n\\n**1.2 CLEEGN Architecture and Principles:**\\nCLEEGN utilizes a convolutional autoencoder structure, which is a plausible approach for signal denoising. The design incorporates distinct spatial and temporal filtering steps, which aligns well with the multi-channel, time-series nature of EEG data.\\n\\n*   **Spatial Filters (Encoder First Block):** The use of `(C, 1)` shaped filters in the first convolutional block to \"extract spatial EEG features\" is neurophysiologically intuitive. Different neural sources and artifact sources have distinct spatial distributions across the scalp. A spatial filter can learn to identify and separate these patterns. This is akin to concepts like common average referencing, Laplacian filtering, or spatial ICA components that exploit the unique scalp topography of brain and artifact signals.\\n*   **Temporal Filters (Encoder Second Block):** The `(1, \u230afs \u00d7 0.1\u230b)` shaped filters for \"temporal EEG features\" are also plausible. EEG signals are oscillatory and dynamic, characterized by specific frequency bands and event-related potentials (ERPs) with distinct temporal profiles. Temporal convolutions can capture these patterns, learning to identify the characteristic waveforms of neural activity versus artifacts (e.g., sharp eye blinks, high-frequency EMG). A filter length of approximately 100ms can capture a significant portion of common EEG rhythms (e.g., an alpha cycle ~100ms, a theta cycle ~200ms).\\n*   **Autoencoder Symmetry:** The symmetric decoder structure is standard for autoencoders and aims to reconstruct the original signal space from the learned latent representation. Conceptually, the encoder learns a compressed, \"clean\" representation, and the decoder expands it back to the channel-time domain. This process implicitly assumes that brain activity and artifact information can be separated in this latent space, with artifacts being discarded or minimized during the encoding-decoding process.\\n\\n**1.3 The \"Reference Data\" as Ground Truth:**\\nThis is the most critical point concerning neurophysiological plausibility. CLEEGN is trained as a denoising autoencoder using pairs of \"noisy raw EEG data\" and \"noiseless reference EEG data.\" The \"noiseless reference\" is generated by *offline automatic denoising methods* (ICLabel, ASR, or their hybrid).\\n*   **Implication for Plausibility:** CLEEGN learns to mimic the output of these existing artifact removal algorithms. This means that CLEEGN\\'s \"neurophysiological plausibility\" is directly contingent on the neurophysiological accuracy and \"cleanliness\" of its \"teachers\" (ICLabel and ASR).\\n    *   **ICLabel (based on ICA):** ICA assumes linear mixing and statistical independence of sources. While powerful for separating brain and artifact sources, ICA is not infallible. Components might sometimes blend brain and artifact activity, or brain components might be inadvertently removed if misclassified. ICLabel\\'s automatic classification, while sophisticated, can still make errors.\\n    *   **ASR (based on PCA):** ASR targets high-variance artifacts. While effective for transient, high-amplitude artifacts (like blinks), it can potentially distort or remove genuine neural activity if that activity also exhibits high variance (e.g., large ERPs). It may also struggle with more subtle or distributed artifacts.\\n*   **Overall:** By relying on ICLabel and ASR as the source of \"ground truth,\" CLEEGN is learning to reproduce *their* version of a clean signal, including any potential biases, misclassifications, or distortions introduced by these upstream methods. Therefore, while CLEEGN offers a \"plug-and-play\" solution, its reconstructed signals are only as neurophysiologically accurate as the methods used to generate its training data. This is an inherent limitation of learning-based denoising where true \"ground truth\" clean EEG is generally unavailable.\\n\\n## 2. Alignment with Known Principles of EEG Signal Generation and Artifact Characteristics\\n\\n**2.1 Artifact Removal Mechanism:**\\nThe paper claims CLEEGN can eliminate artifacts like high-amplitude ocular artifacts (EOG) and muscular artifacts (EMG).\\n*   **EOG:** EOG artifacts typically manifest as large, slow waves, primarily in frontal electrodes, but with some spread to more posterior channels. They have distinct spatiotemporal characteristics. The spatial filters (C,1) and temporal filters (1,T) in CLEEGN are well-suited to capture these spatial and temporal signatures and suppress them. The visual examples in Figure 5 show effective EOG removal in Fp1, which aligns with expected performance for such artifacts.\\n*   **EMG:** EMG artifacts are typically high-frequency (often >20-30 Hz), irregular, and localized over muscle regions (e.g., temporal muscles for jaw clenching). The temporal filters are crucial here to distinguish these high-frequency, non-neural signals from desired brain oscillations. The visual examples in Figure 5 show effective EMG removal on T7, which is a common location for such artifacts. The comparison with ASR\\'s inability to remove high-frequency EMG is also consistent with known limitations of ASR for sustained high-frequency noise.\\n*   **Spatial Information Importance:** The appendix\\'s discussion on 1D-ResCNN performing poorly due to a lack of spatial information is a strong neurophysiological point. Many artifacts (and brain sources) have specific spatial distributions. A method that processes channels independently (like a true 1D CNN) will fail to leverage this crucial spatial coherence, leading to suboptimal artifact removal and potential distortion of neural signals. CLEEGN\\'s multi-channel spatial filtering step directly addresses this principle.\\n\\n**2.2 Preservation of Neural Information:**\\nThe ultimate goal of artifact removal is to preserve the underlying brain activity.\\n*   **Functional Validation (Decoding Performance):** The use of decoding performance (AUC for ERN, Accuracy for SSVEP) as a primary metric is an excellent choice for evaluating whether task-relevant neural information is preserved. If a method improves decoding, it implies that the reconstructed signal contains more distinguishable task-related features. This is highly relevant for BCI and cognitive neuroscience research.\\n*   **Frequency Domain Integrity (PSD Analysis in Appendix A.3):** This section is particularly valuable for neurophysiological assessment.\\n    *   The SSVEP task relies on specific frequency components (fundamental and harmonics of visual stimulation). The finding that CLEEGN\\'s reconstructed EEG spectrum is \"similar to the reference\" and that IC-U-Net and 1D-ResCNN distort specific frequency bands (e.g., 15-20Hz, 30-40Hz for IC-U-Net, general distortion for 1D-ResCNN) is crucial. Neurophysiologically, preserving the power spectral density of brain oscillations and evoked responses is paramount for accurate interpretation. Distortion in specific frequency bands would significantly compromise the utility of the data for frequency-based analyses or biomarker detection. CLEEGN\\'s ability to maintain these components, as indicated by the PSD plots, is a strong argument for its neurophysiological utility.\\n\\n## 3. Critical Evaluation of Interpretation of Results in the Context of Neuroscience and EEG Research\\n\\n**3.1 Interpretation of \"Outperforming the Reference\":**\\nThe paper repeatedly states that \"CLEEGN can outperform the reference data used for its training\" and \"CLEEGN even provides a better reconstruction than the offline ICLabel\" in terms of decoding performance. This is a bold and significant claim.\\n*   **Possible Interpretations (Neuroscientific Context):**\\n    1.  **Learning Robust Features:** CLEEGN, by training on a large dataset of (noisy, reference) pairs, might learn more robust representations of brain activity and artifacts that generalize better than individual runs of ICLabel or ASR on unseen data. The deep learning model might implicitly \"average out\" or \"smooth over\" minor imperfections or misclassifications in the individual reference data points, leading to a more consistent and functionally relevant clean signal.\\n    2.  **Optimized for Downstream Task:** It is possible that CLEEGN, through its training, learns to optimize the signal specifically for the subsequent EEGNet classifier. It might preserve or even subtly enhance the features that EEGNet finds most discriminative, even if those features are not \"perfectly pure\" in a general neurophysiological sense. This doesn\\'t mean it\\'s necessarily a \"purer\" brain signal, but rather a *more useful* signal for the intended BCI/decoding task. This is still a valuable outcome for application-focused research.\\n    3.  **Limitations of MSE:** The paper acknowledges that \"the average error over data points (MSE) is not the best assessment of EEG quality.\" This is crucial. Low MSE doesn\\'t guarantee neurophysiological fidelity. A signal that is slightly higher in MSE but preserves crucial ERP peaks or oscillatory power might be more neurophysiologically meaningful. The fact that CLEEGN has the lowest MSE *and* best decoding for ERN, but not lowest MSE for SSVEP while still having best decoding, reinforces that functional utility (decoding) is a more relevant metric than raw signal similarity (MSE) for applications.\\n\\n**3.2 Visualization for Neuroscientific Insights:**\\nThe visualization using PCA of latent features (Figure 9) is interesting.\\n*   **Interpretation:** The observation that latent features \"shrink in the encoder design and expand in the decoder design\" and that reconstructed data is a \"more compact cluster\" in PCA space is typical for autoencoders performing dimensionality reduction and denoising. It implies that the network effectively learns to filter out high-variance noise, leading to a more consistent underlying representation.\\n*   **Neuroscientific Insight Level:** While this demonstrates the network\\'s internal workings (feature learning, denoising effect), it provides somewhat limited *direct* neuroscientific insights into *how* brain signals are separated from artifacts at a neural level. It confirms that the denoising works, but doesn\\'t explicitly reveal novel findings about brain function or artifact generation beyond confirming the network\\'s efficacy. However, the accompanying interpretation of spatial arrangement (frontal to posterior along x-axis, left-central-right along y-axis) in PCA space of raw EEG is a good illustration of the spatial coherence of EEG.\\n\\n**3.3 \"Plug-and-Play\" Relevance:**\\nThe \"plug-and-play\" (subject-independent, no calibration) aspect is highly relevant for real-world BCI applications and clinical settings where individual calibration is often impractical. The findings regarding minimal training data duration and number of subjects (Figure 7, 8) further support this practical utility. While this is not directly a neurophysiological insight, it enables the widespread use of EEG in scenarios where neurophysiological insights are then sought.\\n\\n## Conclusion\\n\\nCLEEGN represents a significant step towards robust, automatic, and functionally effective EEG artifact removal. Its architecture thoughtfully incorporates spatial and temporal convolutions, aligning well with the inherent characteristics of EEG signals and common artifacts. The detailed comparison with other methods and the insightful PSD analysis strongly support its ability to preserve task-relevant neural information (e.g., SSVEP frequencies) while effectively removing artifacts.\\n\\nThe primary neurophysiological limitation lies in its reliance on existing artifact removal algorithms (ICLabel, ASR) to generate its \"ground truth\" training data. This means CLEEGN learns to reproduce the *version* of clean EEG provided by these methods, inheriting any subtle biases or imperfections. Future work could aim for more direct neurophysiological validation, perhaps by using synthetic artifact models with precisely known ground truth, or by more directly evaluating the impact on specific ERP component morphology, source localization accuracy, or coherence measures, rather than solely relying on decoding performance.\\n\\nDespite this, the consistent improvement in decoding performance across different datasets, coupled with the detailed visualization of artifact removal and frequency spectrum preservation, demonstrates CLEEGN\\'s strong functional relevance for neuroscience and BCI applications. Its \"plug-and-play\" nature makes it a highly promising tool for advancing EEG research and real-world brain-computer interfaces.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.8653268}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'DLMethodologyExpert', 'prompt': 'Please provide a detailed review of the paper, focusing on the novelty and correctness of the deep learning architecture and training procedure. Assess the technical soundness of the proposed CNN architecture, loss functions, and training methodology. Evaluate the novelty and appropriateness of the chosen deep learning techniques for the EEG reconstruction task. Provide detailed feedback on the clarity and completeness of the methodological description. The paper title is: CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction. The paper content is: Since the first record of human electroencephalogram (EEG) performed almost a century ago (in 1924), EEG has been one of the most widely used non-invasive neural modalities that monitors brain activity in high temporal resolution (Koike et al., 2013; Mehta & Parasuraman, 2013; Sejnowski et al., 2014). Among a variety of modalities, EEG has extensive use in the clinical assessment of neurological and psychiatric conditions, as well as in the research of neuroscience, cognitive science, psychology, and brain-computer interfacing (BCI). EEG signals measure subtle fluctuations of the electrical field driven by local neuroelectrophysiological activity of a population of neurons in the brain cortex (Cohen, 2017). While the electrodes are placed on the surface of the scalp, undesired artifacts may introduce interruption in the measurements and distort the signal of interest. Even in a well-controlled laboratory with a well-trained subject who can maximally keep the body still and relaxed, the EEG signals, unfortunately, could be contaminated by inevitable behavioral and physiological artifacts such as eye blinks, reflective muscle movements, ocular activity, cardiac activity, etc (Croft & Barry, 2000; Wallstrom et al., 2004; Romero et al., 2008). In practice, it is difficult to identify and track the sources of artifacts entirely due to their diversity and non-stationarity. Noise cancellation and artifact removal remain major issues in EEG signal processing and decoding. Currently, numerous methods have been proposed to alleviate the influence of artifacts in EEG signals. Traditional EEG denoising algorithms include filtering, regression, data separation or decomposition (Makeig et al., 1995; Islam et al., 2016; Kothe & Jung, 2016). According to previous meta-analyses on EEG artifact removal literature (Urigu\u0308en & Garcia-Zapirain, 2015; Jiang et al., 2019), independent component analysis (ICA) is especially popular. It is majorly used in 45% of EEG denoising literature. ICA-based artifact removal estimates the component activity by unmixing the EEG data in the channel domain. Through manual or automatic identification, one can exclude the artifact components and then reconstruct the EEG data through back projection based on non-artifact components (Jung et al., 2000a). The fast growth of deep learning methods has drawn state-of-the-art performances in a variety of machine learning problems (LeCun et al., 2015). Lately, deep-learning-based EEG reconstruction has drawn much attention (Leite et al., 2018; Sun et al., 2020; Lopes et al., 2021; Lee et al., 2020; Chuang et al., 2022). Although these methods can effectively remove artifacts from artificial synthetic signals, their performance in reconstructing real EEG data has not yet been validated in terms of decoding labeled EEG data. Meanwhile, the model design of existing deep-learning-based techniques for EEG reconstruction rarely takes the characteristics of EEG into account. In this work, we propose CLEEGN, a ConvoLutional neural network for EEG reconstructioN. CLEEGN is capable of subject-independent EEG construction without any training/calibration for a new subject. The contributions of this work are three-fold: \u2022 a light-weight autoencoder CNN, CLEEGN, with a subject-independent framework that facilitates plug-and-play EEG reconstruction. \u2022 CLEEGN outperforms leading online/offline methods in providing reconstructed EEG data with the best decoding performance for BCI datasets. \u2022 with a novel model design dedicated to EEG reconstruction, CLEEGN characterizes patterns of EEG interpretable and provides neuroscientific insights. 2 RELATED WORK: Current processing techniques for EEG artifact removal are highly varied based on the context where the algorithm may apply. Earlier attempts of EEG denoising assumed that the EEG signals and artifacts appear in different frequency ranges. Based on the assumption, some significant artifacts can be eliminated by the linear filtering method during the online stage (Seifzadeh et al., 2014). Despite the advantages of low computational time, linear filtering hardly removes artifacts that distribute in an overlapped frequency range of EEG signals. Another approach, adaptive filtering (Schlo\u0308gl et al., 2007), estimates artifact signals through additional EOG, EMG, ECG channels and removes these noisy signals from the recording signals by regression. Nevertheless, this approach requires additional auxiliary electrodes and raises the cost and inconvenience in practical applications. The blind source separation (BSS) method in EEG denoising was developed by assuming that the recording EEG signals are linear combinations of the signals from noise sources and the brain neurons. One of the most well-known BSS method is independent component analysis (ICA) (Jung et al., 2000a;b), which is able to separate EEG signals into independent components (ICs) (Makeig et al., 1995). Traditionally, the artifact components extracted by ICA are determined and removed through manual inspection. Recently developed ICLabel can label the ICs provenance into seven different categories: brain, eye, heart, muscle, line noise, channel noise, and other (Pion-Tonachini et al., 2019). Artifact subspace reconstruction (ASR) is another automatic approach, which is based on the principal component analysis (PCA) method (Kothe & Jung, 2016). The ASR method selects relatively noiseless periods from the multi-channel EEG data as reference based on the data distribution. After projecting all EEG data to the principal-component domain, high-variance components projected from the artifacts are detected by a cutoff parameter k. The noiseless signals are reconstructed by preserving the components without carrying artifacts and back-projected to the time domain. The ASR method has been shown capable of improving the quality of ICA decomposition (Chang et al., 2020). Recently, neural network-based methods have been proposed to remove artifacts for EEG data. A variety of network structures have been applied to the framework for removing EEG artifacts and reconstructing clean EEG. A deep convolutional autoencoder (Leite et al., 2018) can enhance the peak-signal-to-noise ratio compared to the linear filtering method via a common CNN autoencoder structure, which has been widely used on image denoising. Their work shows that it seems practicable to transform the EEG waveform through a CNN structure. Later on, a combined framework integrating Bayesian deep learning and ICA (Lee et al., 2020) used thresholding of the EEG data distribution to discard ICs classified as ocular artifacts. These methods leverage the flexibility of deep learning model design and achieve improvements in their assessments. Considering the non- stationary property in EEG data and the degradation phenomenon during training, 1D-ResCNN (Sun et al., 2020) was proposed, which adopted an Inception-Residual module in the network structure. This network is able to remove EOG, ECG, EMG on single-channel synthesis EEG data. Instead of using synthesis EEG data, IC-U-Net (Chuang et al., 2022) was created, which generated pairings of noisy and noiseless EEG data through ICA as training data. The proposed neural network is a one-dimensional adaptation of U-Net architecture trained with the ensemble of four loss functions to minimize the difference between amplitude, velocity, acceleration, and frequency components from the signals. This work showed that their reconstructed signal has higher SNR and can surely increase the number of brain components classified by ICLabel. 3 MATERIALS AND METHODS: CLEEGN Architecture and Model Training. CLEEGN is designed to map multi-channel noisy EEG into a latent space and reconstruct it into noiseless EEG signals. The architecture of CLEEGN is shown in Figure 1. The architecture of the encoder is inspired by an existing CNN model designed for EEG recognition (Wei et al., 2019) and incorporates convolutional blocks that capture spatiotemporal characteristics of EEG data. The first convolution block is used to extract spatial EEG features through a convolutional layer containing C spatial filters whose shape is (C, 1), where C is the number of electrodes of the EEG signals. The second convolution block is used to extract temporal EEG features through a convolutional layer containing NF temporal filters with shape (1, \u230afs \u00d7 0.1\u230b), where fs denotes the sample frequency of the EEG signals. As for the decoder, we design an approximately symmetric structure to the encoder with three convolutional blocks. The first convolutional block decodes the EEG feature with NF temporal kernels with shape is (1, \u230afs\u00d70.1\u230b). The second block decodes the EEG feature by a convolution layer with C spatial kernels of shape (C, 1). The last convolutional block is for projecting the feature domain back to the original time domain. To maintain the shape between the model inputs and outputs, every convolution block applies zeropadding except the first block in the encoder. The detail of the CLEEGN architecture is available in Table 1. As illustrated in Figure 1, the objective of the proposed method is to minimize the difference between noiseless signals, Y , and the model output, Y\u0302 . The recording EEG data, X , is the combination of brain signals, Y , and the signals from multiple noise sources, N . CLEEGN is a denoising autoencoder that is intended to perform artifact removal on EEG data by creating a mapping between multi-channel noisy EEG signals, X , and noiseless signals, Y . The training process of CLEEGN utilizes pairing noisy raw EEG data and noiseless reference EEG data so that the model can learn to transform noisy EEG data into reconstructed EEG data with maximal similarity to the reference data. To generate large-scale reference EEG data, we adopted automatic denoising methods, ICLabel and ASR, to remove artifacts and reconstruct clean waveforms offline. The pairing EEG recording with C channels of continuous noisy/noiseless data was further segmented into training sample pairs. The input size is (C, T ), where T is the number of time points based on the sampling rate of the EEG data in 4-second segments. The windows size and stride are set as T and 0.5T . In the context of plug-and-play EEG reconstruction, CLEEGN was trained as a subject-independent model. To perform subject-independent training and testing, we grouped subjects into k disjoint sets. During the training process, one of the sets was left out for testing. Subjects\u2019 EEG data in the left-alone set were not involved in both training and validation. A complete experiment on a single dataset would result in k different models. The artifact removal performance of a model was evaluated by using the left-alone set. The number of subjects in one set and the EEG duration available for each subject depend on the experiment setting and the dataset used. EEG Reconstruction and Evaluation. The reconstructed EEG data of CLEEGN is generated through an online reconstruction simulation. In the online stage, the system performs artifact removal on a new subject using a pre-trained CLEEGN model without any requirement of training/calibration. Figure 2 shows the online EEG reconstruction simulated in offline. The EEG data from a subject in the left-alone set would be fed into the model sequentially. The stride size of the sliding window is set to 0.5 seconds to minimize the delay in online reconstruction. The fitness of the model can be evaluated by observing the waveform visually or measuring the similarity using the mean square error (MSE) between reconstructed and noiseless EEG. Meanwhile, we propose to use the decoding performance of labeled EEG data as an objective measurement of the reconstructed EEG quality. We employed EEGNet, a compact CNN for end-to-end EEG decoding (Lawhern et al., 2018), as the classifier to decode the labeled EEG data in our study. 4 EXPERIMENTS: Data and Model Fitting. In this work, we applied two labeled EEG datasets, the BCI-ERN dataset 1 and the MAMEM-SSVEP-II dataset 2. We estimate the classification performance by calculating the score of Area Under the Curve (AUC) in ERN binary classification and Top-1 accuracy in the SSVEP task. Figure 3 is the schematic diagram of the process of decoding performance estimation using the reconstructed EEG data. The segmented EEG epochs are divided into three splits within each subject: training set, validation set, and test set. The ratio between classes remained the same in each set. We perform 20 repeated runs with shuffled data to estimate the average performance of each decoding performance evaluation. Detailed descriptions of the two datasets are available in the appendix at A.2. Figure 4 presents the loss curve and the reconstruction result at the end of three different training iterations in a sample run using the ERN dataset. Through this exploration, we can look into the course of model fitting. In the early stage of training, the model reduces the large MSE loss by smoothing the input signals. To further minimize the error between reconstruction and reference (offline denoised signal), the model compensates the intensity to better fit the reference signals. At the convergent stage of training, we can see the reconstructed waveform is very similar to the one in reference data. 1https://www.kaggle.com/c/inria-bci-challenge 2https://www.mamem.eu/results/datasets/ Types of Reference Data. As ICLabel, ASR, and the hybrid ASR-ICLabel are employed to automatically generate large-scale noiseless reference EEG data offline, it is of our interest to investigate what type of reference EEG serves as the best noiseless reference data for the CLEEGN model training. Figure 5 presents the EEG waveforms sampled from Subject 2 in the ERN dataset. The EEG time series from top to bottom represent the recording of Fp1, T7, Cz, T8, and O2 channels. The nine sub-figures visualize the comparison between the noisy raw data and the reference data reconstructed by the ICLabel, ASR with or without ICLabel when the cutoff parameter k was set to 4, 8, 16, and 32. We can see the waveform of ICLabel and the ASR-ICLabel hybrid method with 4 different cutoff parameters k are similar, and the noiseless waveform reconstructed by CLEEGN are highly correlated to the result from these 5 methods. High-amplitude ocular artifacts such as blinking and eye saccades are phenomenal in frontal channels (Fp1) and muscular artifacts featuring high-frequency (around 24-30 Hz in the \u03b2 band) are observed on the T7 channel. CLEEGN, ICA, and ASR-ICLabel hybrid methods can eliminate these kinds of artifacts. Compared to the ICLabel and ASR-ICLabel hybrid methods, ASR is unable to eliminate the high-frequency EMG artifact for this dataset. The tolerance of large amplitude EOG artifact increases with the employment of larger cutoff parameter k. Limited by the reference data generated by ASR, CLEEGN preserves the high-frequency artifact. As for the large amplitude artifacts that failed to be identified by ASR-32 and ASR-16, we can see that CLEEGN is able to mitigate those EOG artifacts. As shown in Figure 6(a), the reference data prepared by ICLabel offer the lowest MSE, i.e. the best fitness for CLEEGN training. We also compare the types of reference EEG data and their corresponding CLEEGN reconstruction results in the decoding performance of the ERN EEG dataset to assess their data quality. As illustrated in Figure 6(b), CLEEGN reconstruction draws better performances for all denoising methods. Although the performance decreases with a smaller cutoff parameter k, CLEEGN can outperform the reference data used for its training. This result suggests that our method not only removes the artifact but also preserves informative brain activity in the EEG under our cross-subject training scheme. As the ICLabel provides the reference data with the best decoding performance and fitness, we chose ICLabel as the source of reference data for further experiments in this study. Training Data Size. We explored the effect of data length per subject in the training set regarding the fitness and the decoding performance of CLEEGN. The training data were segmented from the first 1, 2, 4, 10, 20, and 30 minutes of data in each EEG recording to investigate the trade-off between training time and reconstruction performance. Figure 7(a) shows the fitness of CLEEGN to the reference data. The model trained using the first 10 minutes obtain the minimal value among all duration configurations. Though the difference in MSE between 10, 20, and 30 minutes is not noticeable, Figure 7(b) shows that using the 10-minute training data yields the best decoding performance among all settings. Interestingly, with only one-minute training data from each subject, CLEEGN can achieve comparable decoding performance to the reference data. This indicates that CLEEGN retains its performance even when each subject only contributes a short recording for training. In addition, we explore the effect of the number of subjects included in the training set on the fitness of CLEEGN model training and the decoding performance of the reconstructed EEG data. We randomly reduced the number of subjects for training from 12 to 6, 4, and 2. Since the subset of subjects may influence the performance, we tested multiple combinations and averaged the results. With the decrease in the number of subjects, the MSE value and the standard error (light span area) increase in Figure 8(a), which indicates that the generalization ability of the CLEEGN model reduces when fewer subjects are included for training. In Figure 8(b), we can observe a slight decrease in decoding performance. We consider the number of subjects as an essential factor in the performance of CLEEGN, yet it requires only a few subjects to achieve a satisfactory performance compared to the reference data. Performance Comparison. We compare the performance of CLEEGN against three baseline methods, ICLabel, 1D-ResCNN, and IC-U-Net. Except for the ICLabel that operates offline, CLEEGN and the other two neural network-based methods perform a simulated online reconstruction based on the same training process with the reference data generated by ICLabel. Results of the ERN EEG dataset are shown in Table 2. CLEEGN has the best fitness to the reference data with the minimum MSE, the highest AUC score in the decoding performance, and the least parameters. For the SSVEP EEG data, although not having the minimum MSE, CLEEGN outperforms other methods in the decoding accuracy and the model size. The evaluation across the two datasets suggests an overall superiority of our proposed CLEEGN model over other existing neural network-based methods in online reconstruction. CLEEGN even provides a better reconstruction than the offline ICLabel. These promising results indicate the usability of CLEEGN in online training/calibration-free EEG reconstruction that truly meets the need for real-world applications of EEG-based BCI. Visualization. On account of the interpretability of the CLEEGN model, we visualized its intermediate latent features by mapping onto a 2-dimensional domain based on the principal component analysis (PCA) of the noisy raw EEG data (Wold et al., 1987). Figure 9(a) shows the principal component space and the noisy EEG data channel projections. We observe that the scatters of noisy EEG data retain the spatial relationship of the actual EEG electrodes. The arrangement of frontal electrodes (prefix in Fp, P) to the posterior (prefix in P, O) are projected along the x-axis from right to left. The projection from top to bottom matches the left-side electrodes (suffixed in odd numbers), central electrode (suffixed in z), and right-side electrodes (suffixed in even numbers). The projection suggests that EEG data of adjacent channels tend to show similar waveforms. The blue dots in Figure 9(b), (c) are the projection of latent features resulting from the first and second convolutional layers, which make up the encoder design of CLEEGN. The projection of latent features from the next two convolutional layers is presented in Figure 9(d), (e). From Figure 9(b) to (e), we can see the distribution range of the latent features shrink in the encoder design and expand in the decoder design of CLEEGN. Figure 9(f) compares the distribution of noisy data and CLEEGN reconstructed data, showing that the CLEEGN reconstructed data is a more compact cluster than the noisy EEG in the PCA space. This implies that the process within CLEEGN includes a projection of the original EEG data in the upstream layer, complex temporal filtering and combination in the midstream layers, and a final projection that converts the noiseless latent features back to the channel domain. 5 CONCLUSION: In this work, we have proposed CLEEGN, a novel convolutional neural network for plug-and-play automatic EEG reconstruction. The training of CLEEGN leverages the conventional offline denoising methods, ASR and ICLabel, with automatic component classification to generate abundant noiseless EEG data. The performance of CLEEGN was objectively validated using multiple evaluations including waveform observation; reconstruction error assessment; and decoding accuracy on well-studied, labeled datasets. The experiment results suggest that, even without any calibration, CLEEGN can predominantly preserve inherent brain activity. According to the decoding performance of reconstructed EEG data, CLEEGN outperforms other neural network-based denoising methods on both ERN and SSVEP EEG decoding. From the visualization of the waveform, CLEEGN can remove artifacts from different sources and the waveform is highly correlated to the reference data. Through the visualization of model parameters and latent features, we exhibit model behavior and reveal explainable insights related to existing knowledge of neuroscience. We foresee pervasive applications of CLEEGN in prospective works of EEG decoding and analysis. A APPENDIX:  A.1 NEURAL NETWORK TRAINING CONFIGURATION:  A.1.1 CLEEGN: For training CLEEGN, we used the Adam optimizer with an initial learning rate of 1e-3 without weight decay. Besides, the exponential learning rate scheduler is applied with a gamma of 0.8. As for the loss function, we used Mean Squared Error (MSE). The batch size is set to 64 and the total training epoch is 40. During the training procedure, the model is evaluated using the validation subset at the end of every epoch with the purpose of saving the weights that achieved the lowest validation loss. A.1.2 IC-U-NET The optimizer adopted in IC-U-Net (Chuang et al., 2022) training is SGD with an initial learning rate of 1e-2, momentum of 0.9, and weight decay of 5e-4. The learning rate scheduler used in the training procedure is the multistep scheduler. As for the loss function, a novel ensemble loss proposed in ICU-Net is adopted. This ensemble is a simple linear combination of the Mean Squared Error (MSE) in amplitude, velocity, acceleration, and frequency components of EEG signals. Each term in the loss function has same weight. The batch size is set to 64 and the total training epoch is 200. The weight saving strategy in IC-U-Net training is the same as CLEEGN. A.1.3 1D-RESCNN The Adam optimizer with an initial learning rate of 1e-3 and the Mean Squared Error (MSE) loss function is adopted in 1D-ResCNN (Sun et al., 2020) training. Since the 1D-ResCNN is developed under one-dimensional synthesized EEG data and no explicit instruction of using multi-channel EEG provided, we trained 1D-ResCNN using two different method. For one method, we trained multiple models for each EEG channel respectively. For the other method, we viewed each multichannel EEG segment as a batch and the channel arrangement within a batch is fixed. Under our experiment, the result showed that the second method not only provides an efficient training process, but also results in a better reconstructed performance. The same weight saving strategy in CLEEGN is adopted in 1D-ResCNN training. A.1.4 DECODING PERFORMANCE EVALUATION MODEL: EEGNET: EEGNet (Lawhern et al., 2018) is a famous EEG decoding model and widely used in EEG literature. In the original EEGNet paper, they investigated their proposed model with a different number of kernels and denoted the model with F1 temporal filters and D spatial filters as EEGNet-F1,D. We use two different settings to train the two datasets used. In ERN classification, we use the EEGNet8,2 structure suggested by the EEGNet paper. As for the SSVEP classification, an experimental result showed that EEGNet-100,8 can draw the best performance. We trained and evaluated the decoding performance individually for each subject. We divided the collection of event epochs into three splits within each subject: training set, validation set, and test set with a ratio of 3:1:1. The ratio between classes remained the same in each set. The loss function is categorical cross entropy (CCE) and the Adam optimizer is adopted with a learning rate of 10\u22123 and zero weight decay. The batch size is set to 32 and the total training epoch is 200. A.2 DESCRIPTION OF DATASETS:  A.2.1 DATASET 1: FEEDBACK ERROR-RELATED NEGATIVITY (ERN): Error-related negativity (ERN) can be categorized as a kind of event-related potential (ERP), which occurs after an erroneous or abnormal event perceived by the subject. One characteristic of the feedback ERN is a relatively large negative amplitude approximately 350 ms and a positive amplitude approximately 500 ms after visual feedback triggered by the error event. In this work, we mainly use a well-studied EEG dataset from the BCI Challenge competition hosted by Kaggle to evaluate the artifact removal effectiveness. This dataset includes EEG recordings of 26 subjects (16 subjects labeled and 10 subjects unlabeled) that participated in a P300 speller task. P300 speller is a wellknown BCI system that develops a typing application through P300 response evoked potential. The ERN experiment was conducted under the assumption that the ERN occurred if the subject received incorrect prediction (feedback) from the P300 speller. The objective of the competition was to improve the P300 speller performance by implementing error correction through ERN potentials. We used the 16 subjects with labeled data of which the sampling rate is 200 Hz initially with 56 passive Ag/AgCl EEG sensors. In the interest of increasing the usability of EEG data, we applied some pre-processing procedures to each EEG recording. The EEG data were down-sampled to 128 Hz and re-referenced by the common average reference (CAR) method to eliminate common-mode noise and to zero-center the data. Each recording was band-pass filtered to 1-40 Hz through the FIR filter implemented by EEGLAB to remove DC drifting. During EEG decoding evaluation, we epoch EEG signals in [0, 1.25] second interval to obtain correct and erroneous feedback. A.2.2 DATASET 2: STEADY STATE VISUALLY EVOKED POTENTIAL (SSVEP): Steady state visually evoked potential (SSVEP) is another kind of ERP that is characterized as periodic potential induced by rapidly repetitive visual stimulation. The SSVEP is composed of several discrete frequency components, which consist of the fundamental frequency of the visual stimulus as well as its harmonics. To investigate the generalization ability of the model, we use \u201dEEG SSVEP Dataset II\u201d from Multimedia Authoring & Management using your Eyes & Mind (MAMEM). The dataset includes EEG data from 11 subjects and consists of five different frequencies (6.66, 7.50, 8.57, 10.00, and 12.00 Hz). Each subject was recorded in five sessions and each session included 25 trials (5 trials for each class). The data used a 256-channel HydroCel Geodesic Sensor Net (HCGSN) and captured the signals with a sampling rate of 250 Hz. Since there are several different bad channels in each subject\u2019s EEG recording, we preserved 20 common channels from each subject to train the CLEEGN model. Every recording was down-sampled to 125 Hz, re-referenced by the common average reference (CAR) method, and band-pass filtered to 1-40 Hz. We epoch EEG signals in [1, 5] second interval for each event recorded timestamp. The first second was discarded under the consideration of a reaction delay of the subject. A.3 INVESTIGATION OF DECODING PERFORMANCE EVALUATION: SSVEP In this work, we mainly use decoding performance as the assessment of different artifact removal methods. We provide an interpretation of the relationship between EEG quality and decoding performance. In Section 4 SSVEP result, IC-U-Net can optimize the weight with minimal MSE value among all compared artifact removal networks. However, the decoding performance is worse than CLEEGN and even the reference method (ICLabel), which implies that the average error over data points (MSE) is not the best assessment of EEG quality. In the SSVEP experiment, the fundamental frequency of the external rapidly repetitive visual stimulation and its harmonics can be an important feature in classification. Hence, higher quality in the frequency component of EEG data is required. We use the power spectrum density (PSD) to interpret the result of decoding performance. Figure 10 shows the PSD of EEG event data in each class denoised by different methods. We can see the reconstructed EEG spectrum from CLEEGN in each class is similar to the reference. As for IC-U-Net, it seems that the model can not completely reconstruct in several frequency bands (15-20Hz, 30-40Hz), which leads to the low decoding performance. The PSD result shows that there is great distortion in the power density of reconstructed data generated by 1D-ResCNN. Since 1D-ResCNN is a one-dimensional structure, we hypothesize that spatial information is important in EEG artifact removal. A.4 ADDITIONAL TABLES'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The paper \\\"CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction\\\" proposes a novel deep learning approach for EEG artifact removal. The authors claim several contributions, including a lightweight autoencoder CNN for subject-independent \\\"plug-and-play\\\" reconstruction, superior performance in decoding labeled EEG data, and a model design that offers neuroscientific insights. This review will assess the novelty, correctness, technical soundness, and clarity of the proposed deep learning architecture and training procedure.\\n\\n## 1. Novelty and Appropriateness of Deep Learning Techniques\\n\\nThe paper introduces CLEEGN as a convolutional autoencoder designed for multi-channel noisy EEG reconstruction.\\n\\n*   **Architectural Novelty:** The core architecture is described as a convolutional autoencoder. While autoencoders are standard for denoising tasks, the specific design choice of separating spatial and temporal feature extraction in the encoder (and symmetric decoding in the decoder) is a common and appropriate pattern for EEG signals (e.g., similar to structures found in EEGNet). The paper states it's inspired by an existing CNN model for EEG recognition, suggesting the building blocks themselves are not entirely new. However, applying this specific spatial-then-temporal convolutional block structure within an autoencoder framework for *denoising* with multi-channel EEG is a tailored adaptation. The claim of a \\\"novel model design dedicated to EEG reconstruction\\\" holds some weight due to this thoughtful adaptation of known EEG-specific convolutional patterns.\\n*   **Training Procedure Novelty:**\\n    *   **Reference Data Generation:** The use of established offline denoising methods (ICLabel, ASR) to generate \\\"noiseless\\\" reference data for training is a common and pragmatic approach in EEG denoising, as true noise-free EEG is rarely available. The authors conduct an ablation study on different reference types, which adds to the thoroughness rather than novelty.\\n    *   **Subject-Independent, Plug-and-Play Framework:** This is where the paper's primary novelty lies. Training CLEEGN in a subject-independent manner, where models are tested on entirely unseen subjects without any subject-specific calibration or fine-tuning, directly addresses a critical need for real-world BCI applications. While the concept of subject-independent models exists, demonstrating robust and superior performance in this specific \\\"plug-and-play\\\" context for EEG *denoising* is a significant contribution. The rigorous evaluation using a k-fold cross-subject validation scheme reinforces this claim.\\n*   **Appropriateness:** Convolutional Neural Networks (CNNs) are highly appropriate for EEG data due to their ability to capture local spatiotemporal patterns. The autoencoder structure is also a natural fit for denoising tasks, learning an identity mapping from noisy to clean data. The architectural choices (spatial and temporal filters) are well-aligned with the characteristics of EEG signals.\\n\\n## 2. Technical Soundness of the Proposed CNN Architecture, Loss Functions, and Training Methodology\\n\\n### 2.1. CNN Architecture (CLEEGN)\\n\\n*   **Encoder Design:** The encoder starts with a convolutional block to extract spatial EEG features (C spatial filters, shape (C, 1)), followed by another block for temporal features (NF temporal filters, shape (1, \u230afs \u00d7 0.1\u230b)). This sequential extraction of spatial and then temporal features is a technically sound approach for multi-channel time-series data like EEG. A (C,1) filter essentially performs a weighted sum across all channels at each time point, creating a new feature map that combines spatial information. A (1, temporal_length) filter then acts as a standard temporal convolution along the time axis for each of these spatial features.\\n*   **Decoder Design:** The decoder is described as \\\"approximately symmetric.\\\" This is a common and generally effective design for autoencoders, aiming to reverse the encoding process. The description implies it reverses the spatial and temporal decoding, which is logical for reconstruction.\\n*   **Padding:** The mention of \\\"zero-padding except the first block in the encoder\\\" is important. For an autoencoder aiming for reconstruction to the original input shape, \\\"same\\\" padding (which zero-padding helps achieve) is typically used. If the first encoder block uses \\\"valid\\\" convolution (no padding), it would reduce the spatial dimensions, requiring careful upsampling or padding in the decoder to match the original input size. Without \\\"Table 1\\\" being provided in the text, it's hard to definitively evaluate the exact shape transformations, but the textual description suggests an awareness of shape preservation.\\n\\n### 2.2. Loss Functions\\n\\n*   **Mean Squared Error (MSE):** The paper uses MSE as the sole loss function. While MSE is a common choice for regression and denoising tasks, it is known to sometimes lead to blurry or over-smoothed outputs. The paper acknowledges that other methods (e.g., IC-U-Net) use ensemble losses incorporating amplitude, velocity, acceleration, and frequency components. However, CLEEGN's experimental results (specifically, better decoding performance) suggest that MSE, in combination with its specific architecture and training data generation, is sufficient and perhaps even superior for preserving task-relevant information in EEG. The discussion in Appendix A.3 regarding SSVEP results and PSD analysis provides a good justification for why CLEEGN, despite not minimizing MSE the most in all cases, better preserves critical frequency components. This strengthens the argument for the choice of MSE when the ultimate goal is decoding performance.\\n\\n### 2.3. Training Methodology\\n\\n*   **Reference Data Generation:** The approach of generating large-scale reference EEG data using ICLabel and ASR is technically sound. These are widely accepted artifact removal methods, making them reasonable choices for generating pseudo-ground truth. The explicit comparison of different reference types (ICLabel, ASR, ASR-ICLabel) is a strong point, demonstrating a thorough empirical evaluation. Choosing ICLabel for further experiments based on its performance in training and decoding is a well-justified decision.\\n*   **Input Segmentation:** The use of 4-second segments with 0.5T stride for training creates overlapping windows, which can help in learning smoother transitions and reducing boundary artifacts.\\n*   **Subject-Independent Training:** The k-fold cross-validation scheme where entire subjects are held out for testing is crucial for validating the \\\"plug-and-play\\\" claim. This is a rigorous and technically sound approach to assess generalization across subjects.\\n*   **Online Reconstruction Simulation:** Evaluating the model by sequentially feeding data with a sliding window (0.5-second stride) simulates a realistic online application scenario. This is a technically sound way to demonstrate the practical utility of CLEEGN.\\n*   **Evaluation Metrics:** The use of both reconstruction error (MSE) and, more importantly, **decoding performance (AUC/Top-1 accuracy)** using a separate classifier (EEGNet) is excellent. Evaluating on decoding performance is superior because it directly assesses whether the artifact removal process preserves brain activity crucial for downstream BCI tasks. This addresses a major limitation of many previous denoising studies that only focus on signal fidelity metrics.\\n*   **Hyperparameters:** The specified training configurations for CLEEGN (Adam optimizer, learning rate schedule, MSE loss, batch size, epochs, validation-based weight saving) are standard and technically sound. The detailed configurations for comparison models (IC-U-Net, 1D-ResCNN, EEGNet) in the Appendix are commendable for reproducibility.\\n\\n## 3. Clarity and Completeness of Methodological Description\\n\\n*   **Clarity:** The methodological description is generally clear and easy to follow. The purpose of CLEEGN, its high-level architecture, and the training objectives are well-articulated. The distinction between offline reference generation and online reconstruction simulation is clearly made.\\n*   **Completeness:**\\n    *   **Architecture Details:** The paper relies on \\\"Figure 1\\\" and \\\"Table 1\\\" for detailed architectural information (e.g., specific layer types, number of filters, kernel sizes, strides, activation functions, pooling). As these figures/tables are not provided in the text, the textual description alone is **incomplete** in terms of allowing full replication or a deep technical evaluation of the exact layer-by-layer design. While the *type* of convolutions (spatial then temporal) is described, the specific dimensions, number of layers, and features are missing from the text.\\n    *   **Reference Generation:** The process of generating reference data using ICLabel and ASR is adequately explained.\\n    *   **Training Process:** The subject-independent training strategy, input segmentation, and evaluation procedure are comprehensively described.\\n    *   **Experimental Setup:** The details on datasets (ERN, SSVEP), preprocessing steps (downsampling, CAR, band-pass filtering), and EEGNet configurations for decoding are well-covered, primarily in the Appendix. This level of detail is excellent for reproducibility.\\n    *   **Loss Function Details:** The MSE loss is straightforward. The brief discussion of IC-U-Net's ensemble loss provides context for comparison.\\n\\n## Conclusion\\n\\nCLEEGN presents a technically sound and empirically validated approach for plug-and-play automatic EEG reconstruction. The core deep learning techniques\u2014a convolutional autoencoder with a specific spatial-temporal filter arrangement\u2014are appropriate for EEG data. The primary novelty lies in the rigorous subject-independent training and evaluation, demonstrating strong performance in preserving brain activity for downstream decoding tasks, which is a significant practical advancement.\\n\\nWhile the textual description of the architecture could be more complete without relying on figures/tables, the overall methodology is well-explained, and the experimental design is robust. The choice to prioritize decoding performance as an evaluation metric over mere signal fidelity is a strong point, as it directly addresses the utility of the denoised EEG in real-world BCI applications. The ablation studies on reference data types and training data size further enhance the paper's rigor. CLEEGN appears to be a promising step towards truly practical and automated EEG artifact removal systems.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.86135845\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"DLMethodologyExpert\",\n  \"prompt\": \"Please provide a detailed review of the paper, focusing on the novelty and correctness of the deep learning architecture and training procedure. Assess the technical soundness of the proposed CNN architecture, loss functions, and training methodology. Evaluate the novelty and appropriateness of the chosen deep learning techniques for the EEG reconstruction task. Provide detailed feedback on the clarity and completeness of the methodological description. The paper title is: CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction. The paper content is: Since the first record of human electroencephalogram (EEG) performed almost a century ago (in 1924), EEG has been one of the most widely used non-invasive neural modalities that monitors brain activity in high temporal resolution (Koike et al., 2013; Mehta & Parasuraman, 2013; Sejnowski et al., 2014). Among a variety of modalities, EEG has extensive use in the clinical assessment of neurological and psychiatric conditions, as well as in the research of neuroscience, cognitive science, psychology, and brain-computer interfacing (BCI). EEG signals measure subtle fluctuations of the electrical field driven by local neuroelectrophysiological activity of a population of neurons in the brain cortex (Cohen, 2017). While the electrodes are placed on the surface of the scalp, undesired artifacts may introduce interruption in the measurements and distort the signal of interest. Even in a well-controlled laboratory with a well-trained subject who can maximally keep the body still and relaxed, the EEG signals, unfortunately, could be contaminated by inevitable behavioral and physiological artifacts such as eye blinks, reflective muscle movements, ocular activity, cardiac activity, etc (Croft & Barry, 2000; Wallstrom et al., 2004; Romero et al., 2008). In practice, it is difficult to identify and track the sources of artifacts entirely due to their diversity and non-stationarity. Noise cancellation and artifact removal remain major issues in EEG signal processing and decoding. Currently, numerous methods have been proposed to alleviate the influence of artifacts in EEG signals. Traditional EEG denoising algorithms include filtering, regression, data separation or decomposition (Makeig et al., 1995; Islam et al., 2016; Kothe & Jung, 2016). According to previous meta-analyses on EEG artifact removal literature (Urigu\u0308en & Garcia-Zapirain, 2015; Jiang et al., 2019), independent component analysis (ICA) is especially popular. It is majorly used in 45% of EEG denoising literature. ICA-based artifact removal estimates the component activity by unmixing the EEG data in the channel domain. Through manual or automatic identification, one can exclude the artifact components and then reconstruct the EEG data through back projection based on non-artifact components (Jung et al., 2000a). The fast growth of deep learning methods has drawn state-of-the-art performances in a variety of machine learning problems (LeCun et al., 2015). Lately, deep-learning-based EEG reconstruction has drawn much attention (Leite et al., 2018; Sun et al., 2020; Lopes et al., 2021; Lee et al., 2020; Chuang et al., 2022). Although these methods can effectively remove artifacts from artificial synthetic signals, their performance in reconstructing real EEG data has not yet been validated in terms of decoding labeled EEG data. Meanwhile, the model design of existing deep-learning-based techniques for EEG reconstruction rarely takes the characteristics of EEG into account. In this work, we propose CLEEGN, a ConvoLutional neural network for EEG reconstructioN. CLEEGN is capable of subject-independent EEG construction without any training/calibration for a new subject. The contributions of this work are three-fold: \u2022 a light-weight autoencoder CNN, CLEEGN, with a subject-independent framework that facilitates plug-and-play EEG reconstruction. \u2022 CLEEGN outperforms leading online/offline methods in providing reconstructed EEG data with the best decoding performance for BCI datasets. \u2022 with a novel model design dedicated to EEG reconstruction, CLEEGN characterizes patterns of EEG interpretable and provides neuroscientific insights. 2 RELATED WORK: Current processing techniques for EEG artifact removal are highly varied based on the context where the algorithm may apply. Earlier attempts of EEG denoising assumed that the EEG signals and artifacts appear in different frequency ranges. Based on the assumption, some significant artifacts can be eliminated by the linear filtering method during the online stage (Seifzadeh et al., 2014). Despite the advantages of low computational time, linear filtering hardly removes artifacts that distribute in an overlapped frequency range of EEG signals. Another approach, adaptive filtering (Schlo\u0308gl et al., 2007), estimates artifact signals through additional EOG, EMG, ECG channels and removes these noisy signals from the recording signals by regression. Nevertheless, this approach requires additional auxiliary electrodes and raises the cost and inconvenience in practical applications. The blind source separation (BSS) method in EEG denoising was developed by assuming that the recording EEG signals are linear combinations of the signals from noise sources and the brain neurons. One of the most well-known BSS method is independent component analysis (ICA) (Jung et al., 2000a;b), which is able to separate EEG signals into independent components (ICs) (Makeig et al., 1995). Traditionally, the artifact components extracted by ICA are determined and removed through manual inspection. Recently developed ICLabel can label the ICs provenance into seven different categories: brain, eye, heart, muscle, line noise, channel noise, and other (Pion-Tonachini et al., 2019). Artifact subspace reconstruction (ASR) is another automatic approach, which is based on the principal component analysis (PCA) method (Kothe & Jung, 2016). The ASR method selects relatively noiseless periods from the multi-channel EEG data as reference based on the data distribution. After projecting all EEG data to the principal-component domain, high-variance components projected from the artifacts are detected by a cutoff parameter k. The noiseless signals are reconstructed by preserving the components without carrying artifacts and back-projected to the time domain. The ASR method has been shown capable of improving the quality of ICA decomposition (Chang et al., 2020). Recently, neural network-based methods have been proposed to remove artifacts for EEG data. A variety of network structures have been applied to the framework for removing EEG artifacts and reconstructing clean EEG. A deep convolutional autoencoder (Leite et al., 2018) can enhance the peak-signal-to-noise ratio compared to the linear filtering method via a common CNN autoencoder structure, which has been widely used on image denoising. Their work shows that it seems practicable to transform the EEG waveform through a CNN structure. Later on, a combined framework integrating Bayesian deep learning and ICA (Lee et al., 2020) used thresholding of the EEG data distribution to discard ICs classified as ocular artifacts. These methods leverage the flexibility of deep learning model design and achieve improvements in their assessments. Considering the non- stationary property in EEG data and the degradation phenomenon during training, 1D-ResCNN (Sun et al., 2020) was proposed, which adopted an Inception-Residual module in the network structure. This network is able to remove EOG, ECG, EMG on single-channel synthesis EEG data. Instead of using synthesis EEG data, IC-U-Net (Chuang et al., 2022) was created, which generated pairings of noisy and noiseless EEG data through ICA as training data. The proposed neural network is a one-dimensional adaptation of U-Net architecture trained with the ensemble of four loss functions to minimize the difference between amplitude, velocity, acceleration, and frequency components from the signals. This work showed that their reconstructed signal has higher SNR and can surely increase the number of brain components classified by ICLabel. 3 MATERIALS AND METHODS: CLEEGN Architecture and Model Training. CLEEGN is designed to map multi-channel noisy EEG into a latent space and reconstruct it into noiseless EEG signals. The architecture of CLEEGN is shown in Figure 1. The architecture of the encoder is inspired by an existing CNN model designed for EEG recognition (Wei et al., 2019) and incorporates convolutional blocks that capture spatiotemporal characteristics of EEG data. The first convolution block is used to extract spatial EEG features through a convolutional layer containing C spatial filters whose shape is (C, 1), where C is the number of electrodes of the EEG signals. The second convolution block is used to extract temporal EEG features through a convolutional layer containing NF temporal filters with shape (1, \u230afs \u00d7 0.1\u230b), where fs denotes the sample frequency of the EEG signals. As for the decoder, we design an approximately symmetric structure to the encoder with three convolutional blocks. The first convolutional block decodes the EEG feature with NF temporal kernels with shape is (1, \u230afs\u00d70.1\u230b). The second block decodes the EEG feature by a convolution layer with C spatial kernels of shape (C, 1). The last convolutional block is for projecting the feature domain back to the original time domain. To maintain the shape between the model inputs and outputs, every convolution block applies zeropadding except the first block in the encoder. The detail of the CLEEGN architecture is available in Table 1. As illustrated in Figure 1, the objective of the proposed method is to minimize the difference between noiseless signals, Y , and the model output, Y\u0302 . The recording EEG data, X , is the combination of brain signals, Y , and the signals from multiple noise sources, N . CLEEGN is a denoising autoencoder that is intended to perform artifact removal on EEG data by creating a mapping between multi-channel noisy EEG signals, X , and noiseless signals, Y . The training process of CLEEGN utilizes pairing noisy raw EEG data and noiseless reference EEG data so that the model can learn to transform noisy EEG data into reconstructed EEG data with maximal similarity to the reference data. To generate large-scale reference EEG data, we adopted automatic denoising methods, ICLabel and ASR, to remove artifacts and reconstruct clean waveforms offline. The pairing EEG recording with C channels of continuous noisy/noiseless data was further segmented into training sample pairs. The input size is (C, T ), where T is the number of time points based on the sampling rate of the EEG data in 4-second segments. The windows size and stride are set as T and 0.5T . In the context of plug-and-play EEG reconstruction, CLEEGN was trained as a subject-independent model. To perform subject-independent training and testing, we grouped subjects into k disjoint sets. During the training process, one of the sets was left out for testing. Subjects\u2019 EEG data in the left-alone set were not involved in both training and validation. A complete experiment on a single dataset would result in k different models. The artifact removal performance of a model was evaluated by using the left-alone set. The number of subjects in one set and the EEG duration available for each subject depend on the experiment setting and the dataset used. EEG Reconstruction and Evaluation. The reconstructed EEG data of CLEEGN is generated through an online reconstruction simulation. In the online stage, the system performs artifact removal on a new subject using a pre-trained CLEEGN model without any requirement of training/calibration. Figure 2 shows the online EEG reconstruction simulated in offline. The EEG data from a subject in the left-alone set would be fed into the model sequentially. The stride size of the sliding window is set to 0.5 seconds to minimize the delay in online reconstruction. The fitness of the model can be evaluated by observing the waveform visually or measuring the similarity using the mean square error (MSE) between reconstructed and noiseless EEG. Meanwhile, we propose to use the decoding performance of labeled EEG data as an objective measurement of the reconstructed EEG quality. We employed EEGNet, a compact CNN for end-to-end EEG decoding (Lawhern et al., 2018), as the classifier to decode the labeled EEG data in our study. 4 EXPERIMENTS: Data and Model Fitting. In this work, we applied two labeled EEG datasets, the BCI-ERN dataset 1 and the MAMEM-SSVEP-II dataset 2. We estimate the classification performance by calculating the score of Area Under the Curve (AUC) in ERN binary classification and Top-1 accuracy in the SSVEP task. Figure 3 is the schematic diagram of the process of decoding performance estimation using the reconstructed EEG data. The segmented EEG epochs are divided into three splits within each subject: training set, validation set, and test set. The ratio between classes remained the same in each set. We perform 20 repeated runs with shuffled data to estimate the average performance of each decoding performance evaluation. Detailed descriptions of the two datasets are available in the appendix at A.2. Figure 4 presents the loss curve and the reconstruction result at the end of three different training iterations in a sample run using the ERN dataset. Through this exploration, we can look into the course of model fitting. In the early stage of training, the model reduces the large MSE loss by smoothing the input signals. To further minimize the error between reconstruction and reference (offline denoised signal), the model compensates the intensity to better fit the reference signals. At the convergent stage of training, we can see the reconstructed waveform is very similar to the one in reference data. 1https://www.kaggle.com/c/inria-bci-challenge 2https://www.mamem.eu/results/datasets/ Types of Reference Data. As ICLabel, ASR, and the hybrid ASR-ICLabel are employed to automatically generate large-scale noiseless reference EEG data offline, it is of our interest to investigate what type of reference EEG serves as the best noiseless reference data for the CLEEGN model training. Figure 5 presents the EEG waveforms sampled from Subject 2 in the ERN dataset. The EEG time series from top to bottom represent the recording of Fp1, T7, Cz, T8, and O2 channels. The nine sub-figures visualize the comparison between the noisy raw data and the reference data reconstructed by the ICLabel, ASR with or without ICLabel when the cutoff parameter k was set to 4, 8, 16, and 32. We can see the waveform of ICLabel and the ASR-ICLabel hybrid method with 4 different cutoff parameters k are similar, and the noiseless waveform reconstructed by CLEEGN are highly correlated to the result from these 5 methods. High-amplitude ocular artifacts such as blinking and eye saccades are phenomenal in frontal channels (Fp1) and muscular artifacts featuring high-frequency (around 24-30 Hz in the \u03b2 band) are observed on the T7 channel. CLEEGN, ICA, and ASR-ICLabel hybrid methods can eliminate these kinds of artifacts. Compared to the ICLabel and ASR-ICLabel hybrid methods, ASR is unable to eliminate the high-frequency EMG artifact for this dataset. The tolerance of large amplitude EOG artifact increases with the employment of larger cutoff parameter k. Limited by the reference data generated by ASR, CLEEGN preserves the high-frequency artifact. As for the large amplitude artifacts that failed to be identified by ASR-32 and ASR-16, we can see that CLEEGN is able to mitigate those EOG artifacts. As shown in Figure 6(a), the reference data prepared by ICLabel offer the lowest MSE, i.e. the best fitness for CLEEGN training. We also compare the types of reference EEG data and their corresponding CLEEGN reconstruction results in the decoding performance of the ERN EEG dataset to assess their data quality. As illustrated in Figure 6(b), CLEEGN reconstruction draws better performances for all denoising methods. Although the performance decreases with a smaller cutoff parameter k, CLEEGN can outperform the reference data used for its training. This result suggests that our method not only removes the artifact but also preserves informative brain activity in the EEG under our cross-subject training scheme. As the ICLabel provides the reference data with the best decoding performance and fitness, we chose ICLabel as the source of reference data for further experiments in this study. Training Data Size. We explored the effect of data length per subject in the training set regarding the fitness and the decoding performance of CLEEGN. The training data were segmented from the first 1, 2, 4, 10, 20, and 30 minutes of data in each EEG recording to investigate the trade-off between training time and reconstruction performance. Figure 7(a) shows the fitness of CLEEGN to the reference data. The model trained using the first 10 minutes obtain the minimal value among all duration configurations. Though the difference in MSE between 10, 20, and 30 minutes is not noticeable, Figure 7(b) shows that using the 10-minute training data yields the best decoding performance among all settings. Interestingly, with only one-minute training data from each subject, CLEEGN can achieve comparable decoding performance to the reference data. This indicates that CLEEGN retains its performance even when each subject only contributes a short recording for training. In addition, we explore the effect of the number of subjects included in the training set on the fitness of CLEEGN model training and the decoding performance of the reconstructed EEG data. We randomly reduced the number of subjects for training from 12 to 6, 4, and 2. Since the subset of subjects may influence the performance, we tested multiple combinations and averaged the results. With the decrease in the number of subjects, the MSE value and the standard error (light span area) increase in Figure 8(a), which indicates that the generalization ability of the CLEEGN model reduces when fewer subjects are included for training. In Figure 8(b), we can observe a slight decrease in decoding performance. We consider the number of subjects as an essential factor in the performance of CLEEGN, yet it requires only a few subjects to achieve a satisfactory performance compared to the reference data. Performance Comparison. We compare the performance of CLEEGN against three baseline methods, ICLabel, 1D-ResCNN, and IC-U-Net. Except for the ICLabel that operates offline, CLEEGN and the other two neural network-based methods perform a simulated online reconstruction based on the same training process with the reference data generated by ICLabel. Results of the ERN EEG dataset are shown in Table 2. CLEEGN has the best fitness to the reference data with the minimum MSE, the highest AUC score in the decoding performance, and the least parameters. For the SSVEP EEG data, although not having the minimum MSE, CLEEGN outperforms other methods in the decoding accuracy and the model size. The evaluation across the two datasets suggests an overall superiority of our proposed CLEEGN model over other existing neural network-based methods in online reconstruction. CLEEGN even provides a better reconstruction than the offline ICLabel. These promising results indicate the usability of CLEEGN in online training/calibration-free EEG reconstruction that truly meets the need for real-world applications of EEG-based BCI. Visualization. On account of the interpretability of the CLEEGN model, we visualized its intermediate latent features by mapping onto a 2-dimensional domain based on the principal component analysis (PCA) of the noisy raw EEG data (Wold et al., 1987). Figure 9(a) shows the principal component space and the noisy EEG data channel projections. We observe that the scatters of noisy EEG data retain the spatial relationship of the actual EEG electrodes. The arrangement of frontal electrodes (prefix in Fp, P) to the posterior (prefix in P, O) are projected along the x-axis from right to left. The projection from top to bottom matches the left-side electrodes (suffixed in odd numbers), central electrode (suffixed in z), and right-side electrodes (suffixed in even numbers). The projection suggests that EEG data of adjacent channels tend to show similar waveforms. The blue dots in Figure 9(b), (c) are the projection of latent features resulting from the first and second convolutional layers, which make up the encoder design of CLEEGN. The projection of latent features from the next two convolutional layers is presented in Figure 9(d), (e). From Figure 9(b) to (e), we can see the distribution range of the latent features shrink in the encoder design and expand in the decoder design of CLEEGN. Figure 9(f) compares the distribution of noisy data and CLEEGN reconstructed data, showing that the CLEEGN reconstructed data is a more compact cluster than the noisy EEG in the PCA space. This implies that the process within CLEEGN includes a projection of the original EEG data in the upstream layer, complex temporal filtering and combination in the midstream layers, and a final projection that converts the noiseless latent features back to the channel domain. 5 CONCLUSION: In this work, we have proposed CLEEGN, a novel convolutional neural network for plug-and-play automatic EEG reconstruction. The training of CLEEGN leverages the conventional offline denoising methods, ASR and ICLabel, with automatic component classification to generate abundant noiseless EEG data. The performance of CLEEGN was objectively validated using multiple evaluations including waveform observation; reconstruction error assessment; and decoding accuracy on well-studied, labeled datasets. The experiment results suggest that, even without any calibration, CLEEGN can predominantly preserve inherent brain activity. According to the decoding performance of reconstructed EEG data, CLEEGN outperforms other neural network-based denoising methods on both ERN and SSVEP EEG decoding. From the visualization of the waveform, CLEEGN can remove artifacts from different sources and the waveform is highly correlated to the reference data. Through the visualization of model parameters and latent features, we exhibit model behavior and reveal explainable insights related to existing knowledge of neuroscience. We foresee pervasive applications of CLEEGN in prospective works of EEG decoding and analysis. A APPENDIX:  A.1 NEURAL NETWORK TRAINING CONFIGURATION:  A.1.1 CLEEGN: For training CLEEGN, we used the Adam optimizer with an initial learning rate of 1e-3 without weight decay. Besides, the exponential learning rate scheduler is applied with a gamma of 0.8. As for the loss function, we used Mean Squared Error (MSE). The batch size is set to 64 and the total training epoch is 40. During the training procedure, the model is evaluated using the validation subset at the end of every epoch with the purpose of saving the weights that achieved the lowest validation loss. A.1.2 IC-U-NET The optimizer adopted in IC-U-Net (Chuang et al., 2022) training is SGD with an initial learning rate of 1e-2, momentum of 0.9, and weight decay of 5e-4. The learning rate scheduler used in the training procedure is the multistep scheduler. As for the loss function, a novel ensemble loss proposed in ICU-Net is adopted. This ensemble is a simple linear combination of the Mean Squared Error (MSE) in amplitude, velocity, acceleration, and frequency components of EEG signals. Each term in the loss function has same weight. The batch size is set to 64 and the total training epoch is 200. The weight saving strategy in IC-U-Net training is the same as CLEEGN. A.1.3 1D-RESCNN The Adam optimizer with an initial learning rate of 1e-3 and the Mean Squared Error (MSE) loss function is adopted in 1D-ResCNN (Sun et al., 2020) training. Since the 1D-ResCNN is developed under one-dimensional synthesized EEG data and no explicit instruction of using multi-channel EEG provided, we trained 1D-ResCNN using two different method. For one method, we trained multiple models for each EEG channel respectively. For the other method, we viewed each multichannel EEG segment as a batch and the channel arrangement within a batch is fixed. Under our experiment, the result showed that the second method not only provides an efficient training process, but also results in a better reconstructed performance. The same weight saving strategy in CLEEGN is adopted in 1D-ResCNN training. A.1.4 DECODING PERFORMANCE EVALUATION MODEL: EEGNET: EEGNet (Lawhern et al., 2018) is a famous EEG decoding model and widely used in EEG literature. In the original EEGNet paper, they investigated their proposed model with a different number of kernels and denoted the model with F1 temporal filters and D spatial filters as EEGNet-F1,D. We use two different settings to train the two datasets used. In ERN classification, we use the EEGNet8,2 structure suggested by the EEGNet paper. As for the SSVEP classification, an experimental result showed that EEGNet-100,8 can draw the best performance. We trained and evaluated the decoding performance individually for each subject. We divided the collection of event epochs into three splits within each subject: training set, validation set, and test set with a ratio of 3:1:1. The ratio between classes remained the same in each set. The loss function is categorical cross entropy (CCE) and the Adam optimizer is adopted with a learning rate of 10\u22123 and zero weight decay. The batch size is set to 32 and the total training epoch is 200. A.2 DESCRIPTION OF DATASETS:  A.2.1 DATASET 1: FEEDBACK ERROR-RELATED NEGATIVITY (ERN): Error-related negativity (ERN) can be categorized as a kind of event-related potential (ERP), which occurs after an erroneous or abnormal event perceived by the subject. One characteristic of the feedback ERN is a relatively large negative amplitude approximately 350 ms and a positive amplitude approximately 500 ms after visual feedback triggered by the error event. In this work, we mainly use a well-studied EEG dataset from the BCI Challenge competition hosted by Kaggle to evaluate the artifact removal effectiveness. This dataset includes EEG recordings of 26 subjects (16 subjects labeled and 10 subjects unlabeled) that participated in a P300 speller task. P300 speller is a wellknown BCI system that develops a typing application through P300 response evoked potential. The ERN experiment was conducted under the assumption that the ERN occurred if the subject received incorrect prediction (feedback) from the P300 speller. The objective of the competition was to improve the P300 speller performance by implementing error correction through ERN potentials. We used the 16 subjects with labeled data of which the sampling rate is 200 Hz initially with 56 passive Ag/AgCl EEG sensors. In the interest of increasing the usability of EEG data, we applied some pre-processing procedures to each EEG recording. The EEG data were down-sampled to 128 Hz and re-referenced by the common average reference (CAR) method to eliminate common-mode noise and to zero-center the data. Each recording was band-pass filtered to 1-40 Hz through the FIR filter implemented by EEGLAB to remove DC drifting. During EEG decoding evaluation, we epoch EEG signals in [0, 1.25] second interval to obtain correct and erroneous feedback. A.2.2 DATASET 2: STEADY STATE VISUALLY EVOKED POTENTIAL (SSVEP): Steady state visually evoked potential (SSVEP) is another kind of ERP that is characterized as periodic potential induced by rapidly repetitive visual stimulation. The SSVEP is composed of several discrete frequency components, which consist of the fundamental frequency of the visual stimulus as well as its harmonics. To investigate the generalization ability of the model, we use \u201dEEG SSVEP Dataset II\u201d from Multimedia Authoring & Management using your Eyes & Mind (MAMEM). The dataset includes EEG data from 11 subjects and consists of five different frequencies (6.66, 7.50, 8.57, 10.00, and 12.00 Hz). Each subject was recorded in five sessions and each session included 25 trials (5 trials for each class). The data used a 256-channel HydroCel Geodesic Sensor Net (HCGSN) and captured the signals with a sampling rate of 250 Hz. Since there are several different bad channels in each subject\u2019s EEG recording, we preserved 20 common channels from each subject to train the CLEEGN model. Every recording was down-sampled to 125 Hz, re-referenced by the common average reference (CAR) method, and band-pass filtered to 1-40 Hz. We epoch EEG signals in [1, 5] second interval for each event recorded timestamp. The first second was discarded under the consideration of a reaction delay of the subject. A.3 INVESTIGATION OF DECODING PERFORMANCE EVALUATION: SSVEP In this work, we mainly use decoding performance as the assessment of different artifact removal methods. We provide an interpretation of the relationship between EEG quality and decoding performance. In Section 4 SSVEP result, IC-U-Net can optimize the weight with minimal MSE value among all compared artifact removal networks. However, the decoding performance is worse than CLEEGN and even the reference method (ICLabel), which implies that the average error over data points (MSE) is not the best assessment of EEG quality. In the SSVEP experiment, the fundamental frequency of the external rapidly repetitive visual stimulation and its harmonics can be an important feature in classification. Hence, higher quality in the frequency component of EEG data is required. We use the power spectrum density (PSD) to interpret the result of decoding performance. Figure 10 shows the PSD of EEG event data in each class denoised by different methods. We can see the reconstructed EEG spectrum from CLEEGN in each class is similar to the reference. As for IC-U-Net, it seems that the model can not completely reconstruct in several frequency bands (15-20Hz, 30-40Hz), which leads to the low decoding performance. The PSD result shows that there is great distortion in the power density of reconstructed data generated by 1D-ResCNN. Since 1D-ResCNN is a one-dimensional structure, we hypothesize that spatial information is important in EEG artifact removal. A.4 ADDITIONAL TABLES\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The paper \"CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction\" proposes a novel deep learning approach for EEG artifact removal. The authors claim several contributions, including a lightweight autoencoder CNN for subject-independent \"plug-and-play\" reconstruction, superior performance in decoding labeled EEG data, and a model design that offers neuroscientific insights. This review will assess the novelty, correctness, technical soundness, and clarity of the proposed deep learning architecture and training procedure.\\n\\n## 1. Novelty and Appropriateness of Deep Learning Techniques\\n\\nThe paper introduces CLEEGN as a convolutional autoencoder designed for multi-channel noisy EEG reconstruction.\\n\\n*   **Architectural Novelty:** The core architecture is described as a convolutional autoencoder. While autoencoders are standard for denoising tasks, the specific design choice of separating spatial and temporal feature extraction in the encoder (and symmetric decoding in the decoder) is a common and appropriate pattern for EEG signals (e.g., similar to structures found in EEGNet). The paper states it\\'s inspired by an existing CNN model for EEG recognition, suggesting the building blocks themselves are not entirely new. However, applying this specific spatial-then-temporal convolutional block structure within an autoencoder framework for *denoising* with multi-channel EEG is a tailored adaptation. The claim of a \"novel model design dedicated to EEG reconstruction\" holds some weight due to this thoughtful adaptation of known EEG-specific convolutional patterns.\\n*   **Training Procedure Novelty:**\\n    *   **Reference Data Generation:** The use of established offline denoising methods (ICLabel, ASR) to generate \"noiseless\" reference data for training is a common and pragmatic approach in EEG denoising, as true noise-free EEG is rarely available. The authors conduct an ablation study on different reference types, which adds to the thoroughness rather than novelty.\\n    *   **Subject-Independent, Plug-and-Play Framework:** This is where the paper\\'s primary novelty lies. Training CLEEGN in a subject-independent manner, where models are tested on entirely unseen subjects without any subject-specific calibration or fine-tuning, directly addresses a critical need for real-world BCI applications. While the concept of subject-independent models exists, demonstrating robust and superior performance in this specific \"plug-and-play\" context for EEG *denoising* is a significant contribution. The rigorous evaluation using a k-fold cross-subject validation scheme reinforces this claim.\\n*   **Appropriateness:** Convolutional Neural Networks (CNNs) are highly appropriate for EEG data due to their ability to capture local spatiotemporal patterns. The autoencoder structure is also a natural fit for denoising tasks, learning an identity mapping from noisy to clean data. The architectural choices (spatial and temporal filters) are well-aligned with the characteristics of EEG signals.\\n\\n## 2. Technical Soundness of the Proposed CNN Architecture, Loss Functions, and Training Methodology\\n\\n### 2.1. CNN Architecture (CLEEGN)\\n\\n*   **Encoder Design:** The encoder starts with a convolutional block to extract spatial EEG features (C spatial filters, shape (C, 1)), followed by another block for temporal features (NF temporal filters, shape (1, \u230afs \u00d7 0.1\u230b)). This sequential extraction of spatial and then temporal features is a technically sound approach for multi-channel time-series data like EEG. A (C,1) filter essentially performs a weighted sum across all channels at each time point, creating a new feature map that combines spatial information. A (1, temporal_length) filter then acts as a standard temporal convolution along the time axis for each of these spatial features.\\n*   **Decoder Design:** The decoder is described as \"approximately symmetric.\" This is a common and generally effective design for autoencoders, aiming to reverse the encoding process. The description implies it reverses the spatial and temporal decoding, which is logical for reconstruction.\\n*   **Padding:** The mention of \"zero-padding except the first block in the encoder\" is important. For an autoencoder aiming for reconstruction to the original input shape, \"same\" padding (which zero-padding helps achieve) is typically used. If the first encoder block uses \"valid\" convolution (no padding), it would reduce the spatial dimensions, requiring careful upsampling or padding in the decoder to match the original input size. Without \"Table 1\" being provided in the text, it\\'s hard to definitively evaluate the exact shape transformations, but the textual description suggests an awareness of shape preservation.\\n\\n### 2.2. Loss Functions\\n\\n*   **Mean Squared Error (MSE):** The paper uses MSE as the sole loss function. While MSE is a common choice for regression and denoising tasks, it is known to sometimes lead to blurry or over-smoothed outputs. The paper acknowledges that other methods (e.g., IC-U-Net) use ensemble losses incorporating amplitude, velocity, acceleration, and frequency components. However, CLEEGN\\'s experimental results (specifically, better decoding performance) suggest that MSE, in combination with its specific architecture and training data generation, is sufficient and perhaps even superior for preserving task-relevant information in EEG. The discussion in Appendix A.3 regarding SSVEP results and PSD analysis provides a good justification for why CLEEGN, despite not minimizing MSE the most in all cases, better preserves critical frequency components. This strengthens the argument for the choice of MSE when the ultimate goal is decoding performance.\\n\\n### 2.3. Training Methodology\\n\\n*   **Reference Data Generation:** The approach of generating large-scale reference EEG data using ICLabel and ASR is technically sound. These are widely accepted artifact removal methods, making them reasonable choices for generating pseudo-ground truth. The explicit comparison of different reference types (ICLabel, ASR, ASR-ICLabel) is a strong point, demonstrating a thorough empirical evaluation. Choosing ICLabel for further experiments based on its performance in training and decoding is a well-justified decision.\\n*   **Input Segmentation:** The use of 4-second segments with 0.5T stride for training creates overlapping windows, which can help in learning smoother transitions and reducing boundary artifacts.\\n*   **Subject-Independent Training:** The k-fold cross-validation scheme where entire subjects are held out for testing is crucial for validating the \"plug-and-play\" claim. This is a rigorous and technically sound approach to assess generalization across subjects.\\n*   **Online Reconstruction Simulation:** Evaluating the model by sequentially feeding data with a sliding window (0.5-second stride) simulates a realistic online application scenario. This is a technically sound way to demonstrate the practical utility of CLEEGN.\\n*   **Evaluation Metrics:** The use of both reconstruction error (MSE) and, more importantly, **decoding performance (AUC/Top-1 accuracy)** using a separate classifier (EEGNet) is excellent. Evaluating on decoding performance is superior because it directly assesses whether the artifact removal process preserves brain activity crucial for downstream BCI tasks. This addresses a major limitation of many previous denoising studies that only focus on signal fidelity metrics.\\n*   **Hyperparameters:** The specified training configurations for CLEEGN (Adam optimizer, learning rate schedule, MSE loss, batch size, epochs, validation-based weight saving) are standard and technically sound. The detailed configurations for comparison models (IC-U-Net, 1D-ResCNN, EEGNet) in the Appendix are commendable for reproducibility.\\n\\n## 3. Clarity and Completeness of Methodological Description\\n\\n*   **Clarity:** The methodological description is generally clear and easy to follow. The purpose of CLEEGN, its high-level architecture, and the training objectives are well-articulated. The distinction between offline reference generation and online reconstruction simulation is clearly made.\\n*   **Completeness:**\\n    *   **Architecture Details:** The paper relies on \"Figure 1\" and \"Table 1\" for detailed architectural information (e.g., specific layer types, number of filters, kernel sizes, strides, activation functions, pooling). As these figures/tables are not provided in the text, the textual description alone is **incomplete** in terms of allowing full replication or a deep technical evaluation of the exact layer-by-layer design. While the *type* of convolutions (spatial then temporal) is described, the specific dimensions, number of layers, and features are missing from the text.\\n    *   **Reference Generation:** The process of generating reference data using ICLabel and ASR is adequately explained.\\n    *   **Training Process:** The subject-independent training strategy, input segmentation, and evaluation procedure are comprehensively described.\\n    *   **Experimental Setup:** The details on datasets (ERN, SSVEP), preprocessing steps (downsampling, CAR, band-pass filtering), and EEGNet configurations for decoding are well-covered, primarily in the Appendix. This level of detail is excellent for reproducibility.\\n    *   **Loss Function Details:** The MSE loss is straightforward. The brief discussion of IC-U-Net\\'s ensemble loss provides context for comparison.\\n\\n## Conclusion\\n\\nCLEEGN presents a technically sound and empirically validated approach for plug-and-play automatic EEG reconstruction. The core deep learning techniques\u2014a convolutional autoencoder with a specific spatial-temporal filter arrangement\u2014are appropriate for EEG data. The primary novelty lies in the rigorous subject-independent training and evaluation, demonstrating strong performance in preserving brain activity for downstream decoding tasks, which is a significant practical advancement.\\n\\nWhile the textual description of the architecture could be more complete without relying on figures/tables, the overall methodology is well-explained, and the experimental design is robust. The choice to prioritize decoding performance as an evaluation metric over mere signal fidelity is a strong point, as it directly addresses the utility of the denoised EEG in real-world BCI applications. The ablation studies on reference data types and training data size further enhance the paper\\'s rigor. CLEEGN appears to be a promising step towards truly practical and automated EEG artifact removal systems.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.86135845}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'BCIApplicationExpert', 'prompt': 'Please provide a detailed review of the paper, focusing on the practical applicability and impact of the method in Brain-Computer Interface systems. Assess the potential of the proposed method for improving the performance and usability of BCI systems. Evaluate the experimental results in terms of their relevance to real-world BCI applications. Provide insights on the limitations and future directions of the work in the context of BCI research. The paper title is: CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction. The paper content is: Since the first record of human electroencephalogram (EEG) performed almost a century ago (in 1924), EEG has been one of the most widely used non-invasive neural modalities that monitors brain activity in high temporal resolution (Koike et al., 2013; Mehta & Parasuraman, 2013; Sejnowski et al., 2014). Among a variety of modalities, EEG has extensive use in the clinical assessment of neurological and psychiatric conditions, as well as in the research of neuroscience, cognitive science, psychology, and brain-computer interfacing (BCI). EEG signals measure subtle fluctuations of the electrical field driven by local neuroelectrophysiological activity of a population of neurons in the brain cortex (Cohen, 2017). While the electrodes are placed on the surface of the scalp, undesired artifacts may introduce interruption in the measurements and distort the signal of interest. Even in a well-controlled laboratory with a well-trained subject who can maximally keep the body still and relaxed, the EEG signals, unfortunately, could be contaminated by inevitable behavioral and physiological artifacts such as eye blinks, reflective muscle movements, ocular activity, cardiac activity, etc (Croft & Barry, 2000; Wallstrom et al., 2004; Romero et al., 2008). In practice, it is difficult to identify and track the sources of artifacts entirely due to their diversity and non-stationarity. Noise cancellation and artifact removal remain major issues in EEG signal processing and decoding. Currently, numerous methods have been proposed to alleviate the influence of artifacts in EEG signals. Traditional EEG denoising algorithms include filtering, regression, data separation or decomposition (Makeig et al., 1995; Islam et al., 2016; Kothe & Jung, 2016). According to previous meta-analyses on EEG artifact removal literature (Urigu\u0308en & Garcia-Zapirain, 2015; Jiang et al., 2019), independent component analysis (ICA) is especially popular. It is majorly used in 45% of EEG denoising literature. ICA-based artifact removal estimates the component activity by unmixing the EEG data in the channel domain. Through manual or automatic identification, one can exclude the artifact components and then reconstruct the EEG data through back projection based on non-artifact components (Jung et al., 2000a). The fast growth of deep learning methods has drawn state-of-the-art performances in a variety of machine learning problems (LeCun et al., 2015). Lately, deep-learning-based EEG reconstruction has drawn much attention (Leite et al., 2018; Sun et al., 2020; Lopes et al., 2021; Lee et al., 2020; Chuang et0al., 2022). Although these methods can effectively remove artifacts from artificial synthetic signals, their performance in reconstructing real EEG data has not yet been validated in terms of decoding labeled EEG data. Meanwhile, the model design of existing deep-learning-based techniques for EEG reconstruction rarely takes the characteristics of EEG into account. In this work, we propose CLEEGN, a ConvoLutional neural network for EEG reconstructioN. CLEEGN is capable of subject-independent EEG construction without any training/calibration for a new subject. The contributions of this work are three-fold: \u2022 a light-weight autoencoder CNN, CLEEGN, with a subject-independent framework that facilitates plug-and-play EEG reconstruction. \u2022 CLEEGN outperforms leading online/offline methods in providing reconstructed EEG data with the best decoding performance for BCI datasets. \u2022 with a novel model design dedicated to EEG reconstruction, CLEEGN characterizes patterns of EEG interpretable and provides neuroscientific insights. 2 RELATED WORK: Current processing techniques for EEG artifact removal are highly varied based on the context where the algorithm may apply. Earlier attempts of EEG denoising assumed that the EEG signals and artifacts appear in different frequency ranges. Based on the assumption, some significant artifacts can be eliminated by the linear filtering method during the online stage (Seifzadeh et al., 2014). Despite the advantages of low computational time, linear filtering hardly removes artifacts that distribute in an overlapped frequency range of EEG signals. Another approach, adaptive filtering (Schlo\u0308gl et al., 2007), estimates artifact signals through additional EOG, EMG, ECG channels and removes these noisy signals from the recording signals by regression. Nevertheless, this approach requires additional auxiliary electrodes and raises the cost and inconvenience in practical applications. The blind source separation (BSS) method in EEG denoising was developed by assuming that the recording EEG signals are linear combinations of the signals from noise sources and the brain neurons. One of the most well-known BSS method is independent component analysis (ICA) (Jung et al., 2000a;b), which is able to separate EEG signals into independent components (ICs) (Makeig et al., 1995). Traditionally, the artifact components extracted by ICA are determined and removed through manual inspection. Recently developed ICLabel can label the ICs provenance into seven different categories: brain, eye, heart, muscle, line noise, channel noise, and other (Pion-Tonachini et al., 2019). Artifact subspace reconstruction (ASR) is another automatic approach, which is based on the principal component analysis (PCA) method (Kothe & Jung, 2016). The ASR method selects relatively noiseless periods from the multi-channel EEG data as reference based on the data distribution. After projecting all EEG data to the principal-component domain, high-variance components projected from the artifacts are detected by a cutoff parameter k. The noiseless signals are reconstructed by preserving the components without carrying artifacts and back-projected to the time domain. The ASR method has been shown capable of improving the quality of ICA decomposition (Chang et al., 2020). Recently, neural network-based methods have been proposed to remove artifacts for EEG data. A variety of network structures have been applied to the framework for removing EEG artifacts and reconstructing clean EEG. A deep convolutional autoencoder (Leite et al., 2018) can enhance the peak-signal-to-noise ratio compared to the linear filtering method via a common CNN autoencoder structure, which has been widely used on image denoising. Their work shows that it seems practicable to transform the EEG waveform through a CNN structure. Later on, a combined framework integrating Bayesian deep learning and ICA (Lee et al., 2020) used thresholding of the EEG data distribution to discard ICs classified as ocular artifacts. These methods leverage the flexibility of deep learning model design and achieve improvements in their assessments. Considering the non- stationary property in EEG data and the degradation phenomenon during training, 1D-ResCNN (Sun et al., 2020) was proposed, which adopted an Inception-Residual module in the network structure. This network is able to remove EOG, ECG, EMG on single-channel synthesis EEG data. Instead of using synthesis EEG data, IC-U-Net (Chuang et al., 2022) was created, which generated pairings of noisy and noiseless EEG data through ICA as training data. The proposed neural network is a one-dimensional adaptation of U-Net architecture trained with the ensemble of four loss functions to minimize the difference between amplitude, velocity, acceleration, and frequency components from the signals. This work showed that their reconstructed signal has higher SNR and can surely increase the number of brain components classified by ICLabel. 3 MATERIALS AND METHODS: CLEEGN Architecture and Model Training. CLEEGN is designed to map multi-channel noisy EEG into a latent space and reconstruct it into noiseless EEG signals. The architecture of CLEEGN is shown in Figure 1. The architecture of the encoder is inspired by an existing CNN model designed for EEG recognition (Wei et al., 2019) and incorporates convolutional blocks that capture spatiotemporal characteristics of EEG data. The first convolution block is used to extract spatial EEG features through a convolutional layer containing C spatial filters whose shape is (C, 1), where C is the number of electrodes of the EEG signals. The second convolution block is used to extract temporal EEG features through a convolutional layer containing NF temporal filters with shape (1, \u230afs \u00d7 0.1\u230b), where fs denotes the sample frequency of the EEG signals. As for the decoder, we design an approximately symmetric structure to the encoder with three convolutional blocks. The first convolutional block decodes the EEG feature with NF temporal kernels with shape is (1, \u230afs\u00d70.1\u230b). The second block decodes the EEG feature by a convolution layer with C spatial kernels of shape (C, 1). The last convolutional block is for projecting the feature domain back to the original time domain. To maintain the shape between the model inputs and outputs, every convolution block applies zeropadding except the first block in the encoder. The detail of the CLEEGN architecture is available in Table 1. As illustrated in Figure 1, the objective of the proposed method is to minimize the difference between noiseless signals, Y , and the model output, Y\u0302 . The recording EEG data, X , is the combination of brain signals, Y , and the signals from multiple noise sources, N . CLEEGN is a denoising autoencoder that is intended to perform artifact removal on EEG data by creating a mapping between multi-channel noisy EEG signals, X , and noiseless signals, Y . The training process of CLEEGN utilizes pairing noisy raw EEG data and noiseless reference EEG data so that the model can learn to transform noisy EEG data into reconstructed EEG data with maximal similarity to the reference data. To generate large-scale reference EEG data, we adopted automatic denoising methods, ICLabel and ASR, to remove artifacts and reconstruct clean waveforms offline. The pairing EEG recording with C channels of continuous noisy/noiseless data was further segmented into training sample pairs. The input size is (C, T ), where T is the number of time points based on the sampling rate of the EEG data in 4-second segments. The windows size and stride are set as T and 0.5T . In the context of plug-and-play EEG reconstruction, CLEEGN was trained as a subject-independent model. To perform subject-independent training and testing, we grouped subjects into k disjoint sets. During the training process, one of the sets was left out for testing. Subjects\u2019 EEG data in the left-alone set were not involved in both training and validation. A complete experiment on a single dataset would result in k different models. The artifact removal performance of a model was evaluated by using the left-alone set. The number of subjects in one set and the EEG duration available for each subject depend on the experiment setting and the dataset used. EEG Reconstruction and Evaluation. The reconstructed EEG data of CLEEGN is generated through an online reconstruction simulation. In the online stage, the system performs artifact removal on a new subject using a pre-trained CLEEGN model without any requirement of training/calibration. Figure 2 shows the online EEG reconstruction simulated in offline. The EEG data from a subject in the left-alone set would be fed into the model sequentially. The stride size of the sliding window is set to 0.5 seconds to minimize the delay in online reconstruction. The fitness of the model can be evaluated by observing the waveform visually or measuring the similarity using the mean square error (MSE) between reconstructed and noiseless EEG. Meanwhile, we propose to use the decoding performance of labeled EEG data as an objective measurement of the reconstructed EEG quality. We employed EEGNet, a compact CNN for end-to-end EEG decoding (Lawhern et al., 2018), as the classifier to decode the labeled EEG data in our study. 4 EXPERIMENTS: Data and Model Fitting. In this work, we applied two labeled EEG datasets, the BCI-ERN dataset 1 and the MAMEM-SSVEP-II dataset 2. We estimate the classification performance by calculating the score of Area Under the Curve (AUC) in ERN binary classification and Top-1 accuracy in the SSVEP task. Figure 3 is the schematic diagram of the process of decoding performance estimation using the reconstructed EEG data. The segmented EEG epochs are divided into three splits within each subject: training set, validation set, and test set. The ratio between classes remained the same in each set. We perform 20 repeated runs with shuffled data to estimate the average performance of each decoding performance evaluation. Detailed descriptions of the two datasets are available in the appendix at A.2. Figure 4 presents the loss curve and the reconstruction result at the end of three different training iterations in a sample run using the ERN dataset. Through this exploration, we can look into the course of model fitting. In the early stage of training, the model reduces the large MSE loss by smoothing the input signals. To further minimize the error between reconstruction and reference (offline denoised signal), the model compensates the intensity to better fit the reference signals. At the convergent stage of training, we can see the reconstructed waveform is very similar to the one in reference data. 1https://www.kaggle.com/c/inria-bci-challenge 2https://www.mamem.eu/results/datasets/ Types of Reference Data. As ICLabel, ASR, and the hybrid ASR-ICLabel are employed to automatically generate large-scale noiseless reference EEG data offline, it is of our interest to investigate what type of reference EEG serves as the best noiseless reference data for the CLEEGN model training. Figure 5 presents the EEG waveforms sampled from Subject 2 in the ERN dataset. The EEG time series from top to bottom represent the recording of Fp1, T7, Cz, T8, and O2 channels. The nine sub-figures visualize the comparison between the noisy raw data and the reference data reconstructed by the ICLabel, ASR with or without ICLabel when the cutoff parameter k was set to 4, 8, 16, and 32. We can see the waveform of ICLabel and the ASR-ICLabel hybrid method with 4 different cutoff parameters k are similar, and the noiseless waveform reconstructed by CLEEGN are highly correlated to the result from these 5 methods. High-amplitude ocular artifacts such as blinking and eye saccades are phenomenal in frontal channels (Fp1) and muscular artifacts featuring high-frequency (around 24-30 Hz in the \u03b2 band) are observed on the T7 channel. CLEEGN, ICA, and ASR-ICLabel hybrid methods can eliminate these kinds of artifacts. Compared to the ICLabel and ASR-ICLabel hybrid methods, ASR is unable to eliminate the high-frequency EMG artifact for this dataset. The tolerance of large amplitude EOG artifact increases with the employment of larger cutoff parameter k. Limited by the reference data generated by ASR, CLEEGN preserves the high-frequency artifact. As for the large amplitude artifacts that failed to be identified by ASR-32 and ASR-16, we can see that CLEEGN is able to mitigate those EOG artifacts. As shown in Figure 6(a), the reference data prepared by ICLabel offer the lowest MSE, i.e. the best fitness for CLEEGN training. We also compare the types of reference EEG data and their corresponding CLEEGN reconstruction results in the decoding performance of the ERN EEG dataset to assess their data quality. As illustrated in Figure 6(b), CLEEGN reconstruction draws better performances for all denoising methods. Although the performance decreases with a smaller cutoff parameter k, CLEEGN can outperform the reference data used for its training. This result suggests that our method not only removes the artifact but also preserves informative brain activity in the EEG under our cross-subject training scheme. As the ICLabel provides the reference data with the best decoding performance and fitness, we chose ICLabel as the source of reference data for further experiments in this study. Training Data Size. We explored the effect of data length per subject in the training set regarding the fitness and the decoding performance of CLEEGN. The training data were segmented from the first 1, 2, 4, 10, 20, and 30 minutes of data in each EEG recording to investigate the trade-off between training time and reconstruction performance. Figure 7(a) shows the fitness of CLEEGN to the reference data. The model trained using the first 10 minutes obtain the minimal value among all duration configurations. Though the difference in MSE between 10, 20, and 30 minutes is not noticeable, Figure 7(b) shows that using the 10-minute training data yields the best decoding performance among all settings. Interestingly, with only one-minute training data from each subject, CLEEGN can achieve comparable decoding performance to the reference data. This indicates that CLEEGN retains its performance even when each subject only contributes a short recording for training. In addition, we explore the effect of the number of subjects included in the training set on the fitness of CLEEGN model training and the decoding performance of the reconstructed EEG data. We randomly reduced the number of subjects for training from 12 to 6, 4, and 2. Since the subset of subjects may influence the performance, we tested multiple combinations and averaged the results. With the decrease in the number of subjects, the MSE value and the standard error (light span area) increase in Figure 8(a), which indicates that the generalization ability of the CLEEGN model reduces when fewer subjects are included for training. In Figure 8(b), we can observe a slight decrease in decoding performance. We consider the number of subjects as an essential factor in the performance of CLEEGN, yet it requires only a few subjects to achieve a satisfactory performance compared to the reference data. Performance Comparison. We compare the performance of CLEEGN against three baseline methods, ICLabel, 1D-ResCNN, and IC-U-Net. Except for the ICLabel that operates offline, CLEEGN and the other two neural network-based methods perform a simulated online reconstruction based on the same training process with the reference data generated by ICLabel. Results of the ERN EEG dataset are shown in Table 2. CLEEGN has the best fitness to the reference data with the minimum MSE, the highest AUC score in the decoding performance, and the least parameters. For the SSVEP EEG data, although not having the minimum MSE, CLEEGN outperforms other methods in the decoding accuracy and the model size. The evaluation across the two datasets suggests an overall superiority of our proposed CLEEGN model over other existing neural network-based methods in online reconstruction. CLEEGN even provides a better reconstruction than the offline ICLabel. These promising results indicate the usability of CLEEGN in online training/calibration-free EEG reconstruction that truly meets the need for real-world applications of EEG-based BCI. Visualization. On account of the interpretability of the CLEEGN model, we visualized its intermediate latent features by mapping onto a 2-dimensional domain based on the principal component analysis (PCA) of the noisy raw EEG data (Wold et al., 1987). Figure 9(a) shows the principal component space and the noisy EEG data channel projections. We observe that the scatters of noisy EEG data retain the spatial relationship of the actual EEG electrodes. The arrangement of frontal electrodes (prefix in Fp, P) to the posterior (prefix in P, O) are projected along the x-axis from right to left. The projection from top to bottom matches the left-side electrodes (suffixed in odd numbers), central electrode (suffixed in z), and right-side electrodes (suffixed in even numbers). The projection suggests that EEG data of adjacent channels tend to show similar waveforms. The blue dots in Figure 9(b), (c) are the projection of latent features resulting from the first and second convolutional layers, which make up the encoder design of CLEEGN. The projection of latent features from the next two convolutional layers is presented in Figure 9(d), (e). From Figure 9(b) to (e), we can see the distribution range of the latent features shrink in the encoder design and expand in the decoder design of CLEEGN. Figure 9(f) compares the distribution of noisy data and CLEEGN reconstructed data, showing that the CLEEGN reconstructed data is a more compact cluster than the noisy EEG in the PCA space. This implies that the process within CLEEGN includes a projection of the original EEG data in the upstream layer, complex temporal filtering and combination in the midstream layers, and a final projection that converts the noiseless latent features back to the channel domain. 5 CONCLUSION: In this work, we have proposed CLEEGN, a novel convolutional neural network for plug-and-play automatic EEG reconstruction. The training of CLEEGN leverages the conventional offline denoising methods, ASR and ICLabel, with automatic component classification to generate abundant noiseless EEG data. The performance of CLEEGN was objectively validated using multiple evaluations including waveform observation; reconstruction error assessment; and decoding accuracy on well-studied, labeled datasets. The experiment results suggest that, even without any calibration, CLEEGN can predominantly preserve inherent brain activity. According to the decoding performance of reconstructed EEG data, CLEEGN outperforms other neural network-based denoising methods on both ERN and SSVEP EEG decoding. From the visualization of the waveform, CLEEGN can remove artifacts from different sources and the waveform is highly correlated to the reference data. Through the visualization of model parameters and latent features, we exhibit model behavior and reveal explainable insights related to existing knowledge of neuroscience. We foresee pervasive applications of CLEEGN in prospective works of EEG decoding and analysis. A APPENDIX:  A.1 NEURAL NETWORK TRAINING CONFIGURATION:  A.1.1 CLEEGN: For training CLEEGN, we used the Adam optimizer with an initial learning rate of 1e-3 without weight decay. Besides, the exponential learning rate scheduler is applied with a gamma of 0.8. As for the loss function, we used Mean Squared Error (MSE). The batch size is set to 64 and the total training epoch is 40. During the training procedure, the model is evaluated using the validation subset at the end of every epoch with the purpose of saving the weights that achieved the lowest validation loss. A.1.2 IC-U-NET The optimizer adopted in IC-U-Net (Chuang et al., 2022) training is SGD with an initial learning rate of 1e-2, momentum of 0.9, and weight decay of 5e-4. The learning rate scheduler used in the training procedure is the multistep scheduler. As for the loss function, a novel ensemble loss proposed in ICU-Net is adopted. This ensemble is a simple linear combination of the Mean Squared Error (MSE) in amplitude, velocity, acceleration, and frequency components of EEG signals. Each term in the loss function has same weight. The batch size is set to 64 and the total training epoch is 200. The weight saving strategy in IC-U-Net training is the same as CLEEGN. A.1.3 1D-RESCNN The Adam optimizer with an initial learning rate of 1e-3 and the Mean Squared Error (MSE) loss function is adopted in 1D-ResCNN (Sun et al., 2020) training. Since the 1D-ResCNN is developed under one-dimensional synthesized EEG data and no explicit instruction of using multi-channel EEG provided, we trained 1D-ResCNN using two different method. For one method, we trained multiple models for each EEG channel respectively. For the other method, we viewed each multichannel EEG segment as a batch and the channel arrangement within a batch is fixed. Under our experiment, the result showed that the second method not only provides an efficient training process, but also results in a better reconstructed performance. The same weight saving strategy in CLEEGN is adopted in 1D-ResCNN training. A.1.4 DECODING PERFORMANCE EVALUATION MODEL: EEGNET: EEGNet (Lawhern et al., 2018) is a famous EEG decoding model and widely used in EEG literature. In the original EEGNet paper, they investigated their proposed model with a different number of kernels and denoted the model with F1 temporal filters and D spatial filters as EEGNet-F1,D. We use two different settings to train the two datasets used. In ERN classification, we use the EEGNet8,2 structure suggested by the EEGNet paper. As for the SSVEP classification, an experimental result showed that EEGNet-100,8 can draw the best performance. We trained and evaluated the decoding performance individually for each subject. We divided the collection of event epochs into three splits within each subject: training set, validation set, and test set with a ratio of 3:1:1. The ratio between classes remained the same in each set. The loss function is categorical cross entropy (CCE) and the Adam optimizer is adopted with a learning rate of 10\u22123 and zero weight decay. The batch size is set to 32 and the total training epoch is 200. A.2 DESCRIPTION OF DATASETS:  A.2.1 DATASET 1: FEEDBACK ERROR-RELATED NEGATIVITY (ERN): Error-related negativity (ERN) can be categorized as a kind of event-related potential (ERP), which occurs after an erroneous or abnormal event perceived by the subject. One characteristic of the feedback ERN is a relatively large negative amplitude approximately 350 ms and a positive amplitude approximately 500 ms after visual feedback triggered by the error event. In this work, we mainly use a well-studied EEG dataset from the BCI Challenge competition hosted by Kaggle to evaluate the artifact removal effectiveness. This dataset includes EEG recordings of 26 subjects (16 subjects labeled and 10 subjects unlabeled) that participated in a P300 speller task. P300 speller is a wellknown BCI system that develops a typing application through P300 response evoked potential. The ERN experiment was conducted under the assumption that the ERN occurred if the subject received incorrect prediction (feedback) from the P300 speller. The objective of the competition was to improve the P300 speller performance by implementing error correction through ERN potentials. We used the 16 subjects with labeled data of which the sampling rate is 200 Hz initially with 56 passive Ag/AgCl EEG sensors. In the interest of increasing the usability of EEG data, we applied some pre-processing procedures to each EEG recording. The EEG data were down-sampled to 128 Hz and re-referenced by the common average reference (CAR) method to eliminate common-mode noise and to zero-center the data. Each recording was band-pass filtered to 1-40 Hz through the FIR filter implemented by EEGLAB to remove DC drifting. During EEG decoding evaluation, we epoch EEG signals in [0, 1.25] second interval to obtain correct and erroneous feedback. A.2.2 DATASET 2: STEADY STATE VISUALLY EVOKED POTENTIAL (SSVEP): Steady state visually evoked potential (SSVEP) is another kind of ERP that is characterized as periodic potential induced by rapidly repetitive visual stimulation. The SSVEP is composed of several discrete frequency components, which consist of the fundamental frequency of the visual stimulus as well as its harmonics. To investigate the generalization ability of the model, we use \u201dEEG SSVEP Dataset II\u201d from Multimedia Authoring & Management using your Eyes & Mind (MAMEM). The dataset includes EEG data from 11 subjects and consists of five different frequencies (6.66, 7.50, 8.57, 10.00, and 12.00 Hz). Each subject was recorded in five sessions and each session included 25 trials (5 trials for each class). The data used a 256-channel HydroCel Geodesic Sensor Net (HCGSN) and captured the signals with a sampling rate of 250 Hz. Since there are several different bad channels in each subject\u2019s EEG recording, we preserved 20 common channels from each subject to train the CLEEGN model. Every recording was down-sampled to 125 Hz, re-referenced by the common average reference (CAR) method, and band-pass filtered to 1-40 Hz. We epoch EEG signals in [1, 5] second interval for each event recorded timestamp. The first second was discarded under the consideration of a reaction delay of the subject. A.3 INVESTIGATION OF DECODING PERFORMANCE EVALUATION: SSVEP In this work, we mainly use decoding performance as the assessment of different artifact removal methods. We provide an interpretation of the relationship between EEG quality and decoding performance. In Section 4 SSVEP result, IC-U-Net can optimize the weight with minimal MSE value among all compared artifact removal networks. However, the decoding performance is worse than CLEEGN and even the reference method (ICLabel), which implies that the average error over data points (MSE) is not the best assessment of EEG quality. In the SSVEP experiment, the fundamental frequency of the external rapidly repetitive visual stimulation and its harmonics can be an important feature in classification. Hence, higher quality in the frequency component of EEG data is required. We use the power spectrum density (PSD) to interpret the result of decoding performance. Figure 10 shows the PSD of EEG event data in each class denoised by different methods. We can see the reconstructed EEG spectrum from CLEEGN in each class is similar to the reference. As for IC-U-Net, it seems that the model can not completely reconstruct in several frequency bands (15-20Hz, 30-40Hz), which leads to the low decoding performance. The PSD result shows that there is great distortion in the power density of reconstructed data generated by 1D-ResCNN. Since 1D-ResCNN is a one-dimensional structure, we hypothesize that spatial information is important in EEG artifact removal. A.4 ADDITIONAL TABLES'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The paper introduces CLEEGN, a convolutional neural network (CNN) designed for \\\"plug-and-play automatic EEG reconstruction\\\" by effectively removing artifacts. It claims to be subject-independent, requiring no training or calibration for new users, and aims to outperform existing methods in terms of decoding performance for BCI applications.\\n\\n## Detailed Review: CLEEGN for BCI Applications\\n\\n### 1. Practical Applicability and Impact of the Method in Brain-Computer Interface Systems\\n\\nThe practical applicability and potential impact of CLEEGN in Brain-Computer Interface (BCI) systems are exceptionally high, addressing several long-standing challenges:\\n\\n*   **Addressing the \\\"Plug-and-Play\\\" Challenge:** A major hurdle for widespread BCI adoption is the need for extensive subject-specific calibration and the susceptibility of EEG signals to noise. CLEEGN's \\\"plug-and-play\\\" capability, meaning it works subject-independently without per-user training/calibration, is a **game-changer**. This significantly reduces setup time, technical expertise required, and user burden, making BCI systems much more accessible for both clinical and consumer applications. Imagine a BCI device that works out of the box, much like any other consumer electronic device.\\n*   **Enhancing Robustness in Real-World Environments:** EEG artifacts (eye blinks, muscle movements, cardiac activity, line noise) are pervasive and highly variable, especially outside of controlled lab settings. CLEEGN aims to automatically mitigate these, leading to more robust and reliable BCI performance in naturalistic environments, where users cannot always remain perfectly still. This is crucial for at-home BCI use, neurorehabilitation, or any scenario where precise control of the user's state is difficult.\\n*   **Reduced Computational Overhead:** Described as \\\"light-weight\\\" and with minimal parameters compared to some other deep learning models (as shown in Table 2), CLEEGN holds promise for deployment on resource-constrained devices, such as wearable BCIs or embedded systems. This could facilitate the development of compact, portable BCI solutions.\\n*   **Streamlining Data Preprocessing Pipelines:** By offering an automatic and efficient artifact removal solution, CLEEGN can significantly simplify and accelerate the preprocessing stage, which is often a bottleneck in BCI research and development. This allows researchers and developers to focus more on brain state decoding and application development rather than extensive artifact handling.\\n\\n### 2. Potential for Improving the Performance and Usability of BCI Systems\\n\\nCLEEGN demonstrates significant potential for improving both the performance and usability of BCI systems:\\n\\n*   **Improved Decoding Performance:** The paper's most impactful finding is that CLEEGN-reconstructed EEG leads to *superior decoding performance* (higher AUC for ERN, higher accuracy for SSVEP) compared to raw data, other neural network methods (1D-ResCNN, IC-U-Net), and even the offline ICLabel method used as its own training reference. This is critical because the ultimate goal of artifact removal in BCI is not just clean signals, but signals that allow for better classification of brain states or intentions. By enhancing the signal-to-noise ratio in a way that preserves task-relevant information, CLEEGN directly contributes to more accurate and reliable BCI control.\\n*   **Enhanced Usability through Calibration-Free Operation:** As highlighted above, the plug-and-play nature is a massive leap in usability. Eliminating the need for lengthy calibration sessions or subject-specific modeling reduces the entry barrier for new users, especially those with severe disabilities who may struggle with complex setup procedures. It fosters a more seamless and intuitive user experience.\\n*   **Consistent Performance Across Paradigms:** The demonstration of improved decoding performance on two distinct BCI paradigms (ERN and SSVEP) suggests that CLEEGN is not limited to a specific type of brain signal or BCI application, indicating its generalizability and broad applicability.\\n*   **Reduced User Frustration and Increased Engagement:** When BCI systems are unreliable due to artifacts, users can become frustrated and disengaged. By providing cleaner, more consistent signals, CLEEGN can lead to a more stable and predictable user experience, thereby increasing user satisfaction and long-term engagement with BCI technology.\\n\\n### 3. Evaluation of Experimental Results in Terms of Relevance to Real-World BCI Applications\\n\\nThe experimental results are highly relevant to real-world BCI applications, largely due to the chosen evaluation methodology:\\n\\n*   **Focus on Decoding Performance:** Crucially, the paper evaluates artifact removal not just by signal-level metrics like MSE (though it is reported), but primarily by its impact on *decoding performance* using labeled BCI datasets. This is the gold standard for assessing the utility of any EEG processing technique in a BCI context, as the ultimate goal is to translate brain activity into commands.\\n*   **Use of Labeled BCI Datasets:** Employing standard, well-studied BCI datasets (ERN, SSVEP) ensures that the results are comparable and meaningful within the BCI community. These paradigms represent common BCI applications.\\n*   **Subject-Independent Evaluation:** The k-fold cross-validation scheme, leaving entire subjects out for testing, directly simulates a \\\"new user\\\" scenario. This is an essential validation for \\\"plug-and-play\\\" claims and directly reflects real-world BCI deployment challenges.\\n*   **Comparison to Strong Baselines:** Including ICA (ICLabel), a widely used and robust offline method, alongside other state-of-the-art deep learning methods (1D-ResCNN, IC-U-Net) provides a comprehensive performance comparison. The ability of CLEEGN to outperform even the offline ICLabel is particularly impressive and underscores its potential for practical use.\\n*   **Analysis of Reference Data Impact:** The investigation into different reference data types (ICLabel, ASR, hybrid) and the finding that ICLabel-based references yielded the best results for CLEEGN training is practical. Furthermore, the surprising result that CLEEGN *outperformed* its ICLabel reference in decoding (Figure 6b, Table 2) suggests that CLEEGN isn't merely mimicking the reference but learning a more optimal representation for BCI decoding, possibly by removing more relevant noise or preserving features better. The explanation in Appendix A.3 regarding PSD analysis (Figure 10) further supports this: CLEEGN's frequency spectrum is closer to the reference and preserves task-relevant frequencies better than competitors, indicating it cleans in a BCI-optimized manner.\\n*   **Training Data Efficiency:** The finding that satisfactory performance can be achieved with only 10 minutes of training data per subject (or even comparable performance with 1 minute) is highly relevant for building large, diverse training datasets in BCI research.\\n\\nHowever, some aspects could further strengthen the real-world relevance:\\n\\n*   **Lack of True Online Latency Measurement:** While simulated online reconstruction is shown, actual end-to-end latency measurements on live BCI hardware are missing. The 0.5-second stride implies some processing delay, which needs to be quantified for applications requiring low latency (e.g., motor imagery control).\\n*   **Hardware Implementation Details:** The paper mentions \\\"light-weight\\\" but doesn't delve into specific computational requirements (FLOPs, memory footprint) or demonstrate real-time performance on a typical BCI processing unit (e.g., Raspberry Pi, embedded DSP).\\n\\n### 4. Limitations and Future Directions of the Work in the Context of BCI Research\\n\\n#### Limitations:\\n\\n*   **Reliance on \\\"Synthetic Ground Truth\\\":** A fundamental limitation, common in artifact removal literature, is the absence of true \\\"noiseless\\\" EEG ground truth. CLEEGN is trained using data \\\"cleaned\\\" by existing offline methods (ICLabel, ASR). While the paper shows CLEEGN can *outperform* these references in decoding, there's an inherent dependency on the quality and assumptions of these underlying methods. If ICLabel or ASR misclassify artifact components or inadvertently remove neural signal, CLEEGN might learn these biases.\\n*   **Generalizability to Diverse Artifacts and Populations:** While the datasets show a range of common artifacts, EEG recordings from real-world, clinical, or highly mobile settings can contain more complex, non-stationary, or idiosyncratic artifacts (e.g., seizure activity, electrode movement artifacts, line noise in extremely noisy environments). The generalizability to such highly variable noise profiles and diverse patient populations (e.g., severe motor impairments, neurological conditions) needs further validation.\\n*   **Specific Artifact Quantification:** While visually compelling and supported by decoding performance, a more quantitative breakdown of the removal efficacy for *specific types* of artifacts (e.g., EOG regression, EMG power reduction in specific frequency bands) would provide deeper insight into CLEEGN's denoising capabilities.\\n*   **Interpretability Claims:** While visualizations of latent features are provided, the claim of providing \\\"neuroscientific insights\\\" is broad. While interesting, the current visualization doesn't directly reveal *why* certain brain patterns are preserved or how the network specifically isolates brain activity from noise in a neuroscientifically meaningful way beyond general spatial/temporal filtering.\\n*   **EEGNet as the Sole Classifier:** While EEGNet is a good choice, using a single classifier might limit the assessment of how well the cleaned data generalizes to other decoding algorithms. Different classifiers might be more sensitive or robust to certain types of residual noise or preserved features.\\n\\n#### Future Directions in the Context of BCI Research:\\n\\n*   **True Real-Time BCI Integration and Benchmarking:** The next crucial step is to integrate CLEEGN into a full, live BCI loop. This involves real-time latency measurements, computational load analysis on target BCI hardware, and demonstrating performance gains in closed-loop BCI tasks (e.g., real-time cursor control, speller accuracy).\\n*   **Exploring Alternative \\\"Ground Truth\\\" Generation:** Investigate self-supervised or unsupervised learning approaches for artifact removal that do not rely on pre-cleaned reference data from traditional methods. This could involve generative adversarial networks (GANs) or contrastive learning techniques to learn robust representations of clean EEG.\\n*   **Adaptation and Personalization:** While subject-independent is key, exploring how CLEEGN can be fine-tuned with minimal new user data (e.g., few-shot learning, meta-learning) could potentially push performance even higher for individual users who desire maximum accuracy and reliability.\\n*   **Multi-Modal Integration:** Investigate how CLEEGN could be combined with other modalities (e.g., EOG, EMG, motion sensors) if available, to provide a more comprehensive artifact suppression framework, or how it could denoise data for fusion with fMRI or MEG.\\n*   **Transfer Learning for Different BCI Paradigms:** Explore the efficacy of CLEEGN's pre-trained weights for transfer learning to other BCI paradigms (e.g., motor imagery, P300 detection for different spellers, natural speech decoding) or even clinical EEG analysis tasks (e.g., seizure detection, sleep staging).\\n*   **Quantifying Neurophysiological Preservation:** Beyond decoding performance, conduct neurophysiological analyses (e.g., ERP component analysis, spectral power analysis, connectivity analysis) on CLEEGN's output to demonstrate that artifact removal does not distort or eliminate relevant neurophysiological markers.\\n*   **Open-Source Release and Community Validation:** Making the CLEEGN code and pre-trained models publicly available would accelerate adoption and allow for independent validation and further development by the BCI research community.\\n\\nIn conclusion, CLEEGN represents a highly promising advancement for BCI systems. Its plug-and-play, subject-independent nature, coupled with its demonstrated ability to improve BCI decoding performance on real data, makes it a significant contribution towards making BCI technology more practical, accessible, and reliable for real-world applications. The identified limitations suggest clear pathways for future research to further solidify its impact.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.85636995\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"BCIApplicationExpert\",\n  \"prompt\": \"Please provide a detailed review of the paper, focusing on the practical applicability and impact of the method in Brain-Computer Interface systems. Assess the potential of the proposed method for improving the performance and usability of BCI systems. Evaluate the experimental results in terms of their relevance to real-world BCI applications. Provide insights on the limitations and future directions of the work in the context of BCI research. The paper title is: CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction. The paper content is: Since the first record of human electroencephalogram (EEG) performed almost a century ago (in 1924), EEG has been one of the most widely used non-invasive neural modalities that monitors brain activity in high temporal resolution (Koike et al., 2013; Mehta & Parasuraman, 2013; Sejnowski et al., 2014). Among a variety of modalities, EEG has extensive use in the clinical assessment of neurological and psychiatric conditions, as well as in the research of neuroscience, cognitive science, psychology, and brain-computer interfacing (BCI). EEG signals measure subtle fluctuations of the electrical field driven by local neuroelectrophysiological activity of a population of neurons in the brain cortex (Cohen, 2017). While the electrodes are placed on the surface of the scalp, undesired artifacts may introduce interruption in the measurements and distort the signal of interest. Even in a well-controlled laboratory with a well-trained subject who can maximally keep the body still and relaxed, the EEG signals, unfortunately, could be contaminated by inevitable behavioral and physiological artifacts such as eye blinks, reflective muscle movements, ocular activity, cardiac activity, etc (Croft & Barry, 2000; Wallstrom et al., 2004; Romero et al., 2008). In practice, it is difficult to identify and track the sources of artifacts entirely due to their diversity and non-stationarity. Noise cancellation and artifact removal remain major issues in EEG signal processing and decoding. Currently, numerous methods have been proposed to alleviate the influence of artifacts in EEG signals. Traditional EEG denoising algorithms include filtering, regression, data separation or decomposition (Makeig et al., 1995; Islam et al., 2016; Kothe & Jung, 2016). According to previous meta-analyses on EEG artifact removal literature (Urigu\u0308en & Garcia-Zapirain, 2015; Jiang et al., 2019), independent component analysis (ICA) is especially popular. It is majorly used in 45% of EEG denoising literature. ICA-based artifact removal estimates the component activity by unmixing the EEG data in the channel domain. Through manual or automatic identification, one can exclude the artifact components and then reconstruct the EEG data through back projection based on non-artifact components (Jung et al., 2000a). The fast growth of deep learning methods has drawn state-of-the-art performances in a variety of machine learning problems (LeCun et al., 2015). Lately, deep-learning-based EEG reconstruction has drawn much attention (Leite et al., 2018; Sun et al., 2020; Lopes et al., 2021; Lee et al., 2020; Chuang et0al., 2022). Although these methods can effectively remove artifacts from artificial synthetic signals, their performance in reconstructing real EEG data has not yet been validated in terms of decoding labeled EEG data. Meanwhile, the model design of existing deep-learning-based techniques for EEG reconstruction rarely takes the characteristics of EEG into account. In this work, we propose CLEEGN, a ConvoLutional neural network for EEG reconstructioN. CLEEGN is capable of subject-independent EEG construction without any training/calibration for a new subject. The contributions of this work are three-fold: \u2022 a light-weight autoencoder CNN, CLEEGN, with a subject-independent framework that facilitates plug-and-play EEG reconstruction. \u2022 CLEEGN outperforms leading online/offline methods in providing reconstructed EEG data with the best decoding performance for BCI datasets. \u2022 with a novel model design dedicated to EEG reconstruction, CLEEGN characterizes patterns of EEG interpretable and provides neuroscientific insights. 2 RELATED WORK: Current processing techniques for EEG artifact removal are highly varied based on the context where the algorithm may apply. Earlier attempts of EEG denoising assumed that the EEG signals and artifacts appear in different frequency ranges. Based on the assumption, some significant artifacts can be eliminated by the linear filtering method during the online stage (Seifzadeh et al., 2014). Despite the advantages of low computational time, linear filtering hardly removes artifacts that distribute in an overlapped frequency range of EEG signals. Another approach, adaptive filtering (Schlo\u0308gl et al., 2007), estimates artifact signals through additional EOG, EMG, ECG channels and removes these noisy signals from the recording signals by regression. Nevertheless, this approach requires additional auxiliary electrodes and raises the cost and inconvenience in practical applications. The blind source separation (BSS) method in EEG denoising was developed by assuming that the recording EEG signals are linear combinations of the signals from noise sources and the brain neurons. One of the most well-known BSS method is independent component analysis (ICA) (Jung et al., 2000a;b), which is able to separate EEG signals into independent components (ICs) (Makeig et al., 1995). Traditionally, the artifact components extracted by ICA are determined and removed through manual inspection. Recently developed ICLabel can label the ICs provenance into seven different categories: brain, eye, heart, muscle, line noise, channel noise, and other (Pion-Tonachini et al., 2019). Artifact subspace reconstruction (ASR) is another automatic approach, which is based on the principal component analysis (PCA) method (Kothe & Jung, 2016). The ASR method selects relatively noiseless periods from the multi-channel EEG data as reference based on the data distribution. After projecting all EEG data to the principal-component domain, high-variance components projected from the artifacts are detected by a cutoff parameter k. The noiseless signals are reconstructed by preserving the components without carrying artifacts and back-projected to the time domain. The ASR method has been shown capable of improving the quality of ICA decomposition (Chang et al., 2020). Recently, neural network-based methods have been proposed to remove artifacts for EEG data. A variety of network structures have been applied to the framework for removing EEG artifacts and reconstructing clean EEG. A deep convolutional autoencoder (Leite et al., 2018) can enhance the peak-signal-to-noise ratio compared to the linear filtering method via a common CNN autoencoder structure, which has been widely used on image denoising. Their work shows that it seems practicable to transform the EEG waveform through a CNN structure. Later on, a combined framework integrating Bayesian deep learning and ICA (Lee et al., 2020) used thresholding of the EEG data distribution to discard ICs classified as ocular artifacts. These methods leverage the flexibility of deep learning model design and achieve improvements in their assessments. Considering the non- stationary property in EEG data and the degradation phenomenon during training, 1D-ResCNN (Sun et al., 2020) was proposed, which adopted an Inception-Residual module in the network structure. This network is able to remove EOG, ECG, EMG on single-channel synthesis EEG data. Instead of using synthesis EEG data, IC-U-Net (Chuang et al., 2022) was created, which generated pairings of noisy and noiseless EEG data through ICA as training data. The proposed neural network is a one-dimensional adaptation of U-Net architecture trained with the ensemble of four loss functions to minimize the difference between amplitude, velocity, acceleration, and frequency components from the signals. This work showed that their reconstructed signal has higher SNR and can surely increase the number of brain components classified by ICLabel. 3 MATERIALS AND METHODS: CLEEGN Architecture and Model Training. CLEEGN is designed to map multi-channel noisy EEG into a latent space and reconstruct it into noiseless EEG signals. The architecture of CLEEGN is shown in Figure 1. The architecture of the encoder is inspired by an existing CNN model designed for EEG recognition (Wei et al., 2019) and incorporates convolutional blocks that capture spatiotemporal characteristics of EEG data. The first convolution block is used to extract spatial EEG features through a convolutional layer containing C spatial filters whose shape is (C, 1), where C is the number of electrodes of the EEG signals. The second convolution block is used to extract temporal EEG features through a convolutional layer containing NF temporal filters with shape (1, \u230afs \u00d7 0.1\u230b), where fs denotes the sample frequency of the EEG signals. As for the decoder, we design an approximately symmetric structure to the encoder with three convolutional blocks. The first convolutional block decodes the EEG feature with NF temporal kernels with shape is (1, \u230afs\u00d70.1\u230b). The second block decodes the EEG feature by a convolution layer with C spatial kernels of shape (C, 1). The last convolutional block is for projecting the feature domain back to the original time domain. To maintain the shape between the model inputs and outputs, every convolution block applies zeropadding except the first block in the encoder. The detail of the CLEEGN architecture is available in Table 1. As illustrated in Figure 1, the objective of the proposed method is to minimize the difference between noiseless signals, Y , and the model output, Y\u0302 . The recording EEG data, X , is the combination of brain signals, Y , and the signals from multiple noise sources, N . CLEEGN is a denoising autoencoder that is intended to perform artifact removal on EEG data by creating a mapping between multi-channel noisy EEG signals, X , and noiseless signals, Y . The training process of CLEEGN utilizes pairing noisy raw EEG data and noiseless reference EEG data so that the model can learn to transform noisy EEG data into reconstructed EEG data with maximal similarity to the reference data. To generate large-scale reference EEG data, we adopted automatic denoising methods, ICLabel and ASR, to remove artifacts and reconstruct clean waveforms offline. The pairing EEG recording with C channels of continuous noisy/noiseless data was further segmented into training sample pairs. The input size is (C, T ), where T is the number of time points based on the sampling rate of the EEG data in 4-second segments. The windows size and stride are set as T and 0.5T . In the context of plug-and-play EEG reconstruction, CLEEGN was trained as a subject-independent model. To perform subject-independent training and testing, we grouped subjects into k disjoint sets. During the training process, one of the sets was left out for testing. Subjects\u2019 EEG data in the left-alone set were not involved in both training and validation. A complete experiment on a single dataset would result in k different models. The artifact removal performance of a model was evaluated by using the left-alone set. The number of subjects in one set and the EEG duration available for each subject depend on the experiment setting and the dataset used. EEG Reconstruction and Evaluation. The reconstructed EEG data of CLEEGN is generated through an online reconstruction simulation. In the online stage, the system performs artifact removal on a new subject using a pre-trained CLEEGN model without any requirement of training/calibration. Figure 2 shows the online EEG reconstruction simulated in offline. The EEG data from a subject in the left-alone set would be fed into the model sequentially. The stride size of the sliding window is set to 0.5 seconds to minimize the delay in online reconstruction. The fitness of the model can be evaluated by observing the waveform visually or measuring the similarity using the mean square error (MSE) between reconstructed and noiseless EEG. Meanwhile, we propose to use the decoding performance of labeled EEG data as an objective measurement of the reconstructed EEG quality. We employed EEGNet, a compact CNN for end-to-end EEG decoding (Lawhern et al., 2018), as the classifier to decode the labeled EEG data in our study. 4 EXPERIMENTS: Data and Model Fitting. In this work, we applied two labeled EEG datasets, the BCI-ERN dataset 1 and the MAMEM-SSVEP-II dataset 2. We estimate the classification performance by calculating the score of Area Under the Curve (AUC) in ERN binary classification and Top-1 accuracy in the SSVEP task. Figure 3 is the schematic diagram of the process of decoding performance estimation using the reconstructed EEG data. The segmented EEG epochs are divided into three splits within each subject: training set, validation set, and test set. The ratio between classes remained the same in each set. We perform 20 repeated runs with shuffled data to estimate the average performance of each decoding performance evaluation. Detailed descriptions of the two datasets are available in the appendix at A.2. Figure 4 presents the loss curve and the reconstruction result at the end of three different training iterations in a sample run using the ERN dataset. Through this exploration, we can look into the course of model fitting. In the early stage of training, the model reduces the large MSE loss by smoothing the input signals. To further minimize the error between reconstruction and reference (offline denoised signal), the model compensates the intensity to better fit the reference signals. At the convergent stage of training, we can see the reconstructed waveform is very similar to the one in reference data. 1https://www.kaggle.com/c/inria-bci-challenge 2https://www.mamem.eu/results/datasets/ Types of Reference Data. As ICLabel, ASR, and the hybrid ASR-ICLabel are employed to automatically generate large-scale noiseless reference EEG data offline, it is of our interest to investigate what type of reference EEG serves as the best noiseless reference data for the CLEEGN model training. Figure 5 presents the EEG waveforms sampled from Subject 2 in the ERN dataset. The EEG time series from top to bottom represent the recording of Fp1, T7, Cz, T8, and O2 channels. The nine sub-figures visualize the comparison between the noisy raw data and the reference data reconstructed by the ICLabel, ASR with or without ICLabel when the cutoff parameter k was set to 4, 8, 16, and 32. We can see the waveform of ICLabel and the ASR-ICLabel hybrid method with 4 different cutoff parameters k are similar, and the noiseless waveform reconstructed by CLEEGN are highly correlated to the result from these 5 methods. High-amplitude ocular artifacts such as blinking and eye saccades are phenomenal in frontal channels (Fp1) and muscular artifacts featuring high-frequency (around 24-30 Hz in the \u03b2 band) are observed on the T7 channel. CLEEGN, ICA, and ASR-ICLabel hybrid methods can eliminate these kinds of artifacts. Compared to the ICLabel and ASR-ICLabel hybrid methods, ASR is unable to eliminate the high-frequency EMG artifact for this dataset. The tolerance of large amplitude EOG artifact increases with the employment of larger cutoff parameter k. Limited by the reference data generated by ASR, CLEEGN preserves the high-frequency artifact. As for the large amplitude artifacts that failed to be identified by ASR-32 and ASR-16, we can see that CLEEGN is able to mitigate those EOG artifacts. As shown in Figure 6(a), the reference data prepared by ICLabel offer the lowest MSE, i.e. the best fitness for CLEEGN training. We also compare the types of reference EEG data and their corresponding CLEEGN reconstruction results in the decoding performance of the ERN EEG dataset to assess their data quality. As illustrated in Figure 6(b), CLEEGN reconstruction draws better performances for all denoising methods. Although the performance decreases with a smaller cutoff parameter k, CLEEGN can outperform the reference data used for its training. This result suggests that our method not only removes the artifact but also preserves informative brain activity in the EEG under our cross-subject training scheme. As the ICLabel provides the reference data with the best decoding performance and fitness, we chose ICLabel as the source of reference data for further experiments in this study. Training Data Size. We explored the effect of data length per subject in the training set regarding the fitness and the decoding performance of CLEEGN. The training data were segmented from the first 1, 2, 4, 10, 20, and 30 minutes of data in each EEG recording to investigate the trade-off between training time and reconstruction performance. Figure 7(a) shows the fitness of CLEEGN to the reference data. The model trained using the first 10 minutes obtain the minimal value among all duration configurations. Though the difference in MSE between 10, 20, and 30 minutes is not noticeable, Figure 7(b) shows that using the 10-minute training data yields the best decoding performance among all settings. Interestingly, with only one-minute training data from each subject, CLEEGN can achieve comparable decoding performance to the reference data. This indicates that CLEEGN retains its performance even when each subject only contributes a short recording for training. In addition, we explore the effect of the number of subjects included in the training set on the fitness of CLEEGN model training and the decoding performance of the reconstructed EEG data. We randomly reduced the number of subjects for training from 12 to 6, 4, and 2. Since the subset of subjects may influence the performance, we tested multiple combinations and averaged the results. With the decrease in the number of subjects, the MSE value and the standard error (light span area) increase in Figure 8(a), which indicates that the generalization ability of the CLEEGN model reduces when fewer subjects are included for training. In Figure 8(b), we can observe a slight decrease in decoding performance. We consider the number of subjects as an essential factor in the performance of CLEEGN, yet it requires only a few subjects to achieve a satisfactory performance compared to the reference data. Performance Comparison. We compare the performance of CLEEGN against three baseline methods, ICLabel, 1D-ResCNN, and IC-U-Net. Except for the ICLabel that operates offline, CLEEGN and the other two neural network-based methods perform a simulated online reconstruction based on the same training process with the reference data generated by ICLabel. Results of the ERN EEG dataset are shown in Table 2. CLEEGN has the best fitness to the reference data with the minimum MSE, the highest AUC score in the decoding performance, and the least parameters. For the SSVEP EEG data, although not having the minimum MSE, CLEEGN outperforms other methods in the decoding accuracy and the model size. The evaluation across the two datasets suggests an overall superiority of our proposed CLEEGN model over other existing neural network-based methods in online reconstruction. CLEEGN even provides a better reconstruction than the offline ICLabel. These promising results indicate the usability of CLEEGN in online training/calibration-free EEG reconstruction that truly meets the need for real-world applications of EEG-based BCI. Visualization. On account of the interpretability of the CLEEGN model, we visualized its intermediate latent features by mapping onto a 2-dimensional domain based on the principal component analysis (PCA) of the noisy raw EEG data (Wold et al., 1987). Figure 9(a) shows the principal component space and the noisy EEG data channel projections. We observe that the scatters of noisy EEG data retain the spatial relationship of the actual EEG electrodes. The arrangement of frontal electrodes (prefix in Fp, P) to the posterior (prefix in P, O) are projected along the x-axis from right to left. The projection from top to bottom matches the left-side electrodes (suffixed in odd numbers), central electrode (suffixed in z), and right-side electrodes (suffixed in even numbers). The projection suggests that EEG data of adjacent channels tend to show similar waveforms. The blue dots in Figure 9(b), (c) are the projection of latent features resulting from the first and second convolutional layers, which make up the encoder design of CLEEGN. The projection of latent features from the next two convolutional layers is presented in Figure 9(d), (e). From Figure 9(b) to (e), we can see the distribution range of the latent features shrink in the encoder design and expand in the decoder design of CLEEGN. Figure 9(f) compares the distribution of noisy data and CLEEGN reconstructed data, showing that the CLEEGN reconstructed data is a more compact cluster than the noisy EEG in the PCA space. This implies that the process within CLEEGN includes a projection of the original EEG data in the upstream layer, complex temporal filtering and combination in the midstream layers, and a final projection that converts the noiseless latent features back to the channel domain. 5 CONCLUSION: In this work, we have proposed CLEEGN, a novel convolutional neural network for plug-and-play automatic EEG reconstruction. The training of CLEEGN leverages the conventional offline denoising methods, ASR and ICLabel, with automatic component classification to generate abundant noiseless EEG data. The performance of CLEEGN was objectively validated using multiple evaluations including waveform observation; reconstruction error assessment; and decoding accuracy on well-studied, labeled datasets. The experiment results suggest that, even without any calibration, CLEEGN can predominantly preserve inherent brain activity. According to the decoding performance of reconstructed EEG data, CLEEGN outperforms other neural network-based denoising methods on both ERN and SSVEP EEG decoding. From the visualization of the waveform, CLEEGN can remove artifacts from different sources and the waveform is highly correlated to the reference data. Through the visualization of model parameters and latent features, we exhibit model behavior and reveal explainable insights related to existing knowledge of neuroscience. We foresee pervasive applications of CLEEGN in prospective works of EEG decoding and analysis. A APPENDIX:  A.1 NEURAL NETWORK TRAINING CONFIGURATION:  A.1.1 CLEEGN: For training CLEEGN, we used the Adam optimizer with an initial learning rate of 1e-3 without weight decay. Besides, the exponential learning rate scheduler is applied with a gamma of 0.8. As for the loss function, we used Mean Squared Error (MSE). The batch size is set to 64 and the total training epoch is 40. During the training procedure, the model is evaluated using the validation subset at the end of every epoch with the purpose of saving the weights that achieved the lowest validation loss. A.1.2 IC-U-NET The optimizer adopted in IC-U-Net (Chuang et al., 2022) training is SGD with an initial learning rate of 1e-2, momentum of 0.9, and weight decay of 5e-4. The learning rate scheduler used in the training procedure is the multistep scheduler. As for the loss function, a novel ensemble loss proposed in ICU-Net is adopted. This ensemble is a simple linear combination of the Mean Squared Error (MSE) in amplitude, velocity, acceleration, and frequency components of EEG signals. Each term in the loss function has same weight. The batch size is set to 64 and the total training epoch is 200. The weight saving strategy in IC-U-Net training is the same as CLEEGN. A.1.3 1D-RESCNN The Adam optimizer with an initial learning rate of 1e-3 and the Mean Squared Error (MSE) loss function is adopted in 1D-ResCNN (Sun et al., 2020) training. Since the 1D-ResCNN is developed under one-dimensional synthesized EEG data and no explicit instruction of using multi-channel EEG provided, we trained 1D-ResCNN using two different method. For one method, we trained multiple models for each EEG channel respectively. For the other method, we viewed each multichannel EEG segment as a batch and the channel arrangement within a batch is fixed. Under our experiment, the result showed that the second method not only provides an efficient training process, but also results in a better reconstructed performance. The same weight saving strategy in CLEEGN is adopted in 1D-ResCNN training. A.1.4 DECODING PERFORMANCE EVALUATION MODEL: EEGNET: EEGNet (Lawhern et al., 2018) is a famous EEG decoding model and widely used in EEG literature. In the original EEGNet paper, they investigated their proposed model with a different number of kernels and denoted the model with F1 temporal filters and D spatial filters as EEGNet-F1,D. We use two different settings to train the two datasets used. In ERN classification, we use the EEGNet8,2 structure suggested by the EEGNet paper. As for the SSVEP classification, an experimental result showed that EEGNet-100,8 can draw the best performance. We trained and evaluated the decoding performance individually for each subject. We divided the collection of event epochs into three splits within each subject: training set, validation set, and test set with a ratio of 3:1:1. The ratio between classes remained the same in each set. The loss function is categorical cross entropy (CCE) and the Adam optimizer is adopted with a learning rate of 10\u22123 and zero weight decay. The batch size is set to 32 and the total training epoch is 200. A.2 DESCRIPTION OF DATASETS:  A.2.1 DATASET 1: FEEDBACK ERROR-RELATED NEGATIVITY (ERN): Error-related negativity (ERN) can be categorized as a kind of event-related potential (ERP), which occurs after an erroneous or abnormal event perceived by the subject. One characteristic of the feedback ERN is a relatively large negative amplitude approximately 350 ms and a positive amplitude approximately 500 ms after visual feedback triggered by the error event. In this work, we mainly use a well-studied EEG dataset from the BCI Challenge competition hosted by Kaggle to evaluate the artifact removal effectiveness. This dataset includes EEG recordings of 26 subjects (16 subjects labeled and 10 subjects unlabeled) that participated in a P300 speller task. P300 speller is a wellknown BCI system that develops a typing application through P300 response evoked potential. The ERN experiment was conducted under the assumption that the ERN occurred if the subject received incorrect prediction (feedback) from the P300 speller. The objective of the competition was to improve the P300 speller performance by implementing error correction through ERN potentials. We used the 16 subjects with labeled data of which the sampling rate is 200 Hz initially with 56 passive Ag/AgCl EEG sensors. In the interest of increasing the usability of EEG data, we applied some pre-processing procedures to each EEG recording. The EEG data were down-sampled to 128 Hz and re-referenced by the common average reference (CAR) method to eliminate common-mode noise and to zero-center the data. Each recording was band-pass filtered to 1-40 Hz through the FIR filter implemented by EEGLAB to remove DC drifting. During EEG decoding evaluation, we epoch EEG signals in [0, 1.25] second interval to obtain correct and erroneous feedback. A.2.2 DATASET 2: STEADY STATE VISUALLY EVOKED POTENTIAL (SSVEP): Steady state visually evoked potential (SSVEP) is another kind of ERP that is characterized as periodic potential induced by rapidly repetitive visual stimulation. The SSVEP is composed of several discrete frequency components, which consist of the fundamental frequency of the visual stimulus as well as its harmonics. To investigate the generalization ability of the model, we use \u201dEEG SSVEP Dataset II\u201d from Multimedia Authoring & Management using your Eyes & Mind (MAMEM). The dataset includes EEG data from 11 subjects and consists of five different frequencies (6.66, 7.50, 8.57, 10.00, and 12.00 Hz). Each subject was recorded in five sessions and each session included 25 trials (5 trials for each class). The data used a 256-channel HydroCel Geodesic Sensor Net (HCGSN) and captured the signals with a sampling rate of 250 Hz. Since there are several different bad channels in each subject\u2019s EEG recording, we preserved 20 common channels from each subject to train the CLEEGN model. Every recording was down-sampled to 125 Hz, re-referenced by the common average reference (CAR) method, and band-pass filtered to 1-40 Hz. We epoch EEG signals in [1, 5] second interval for each event recorded timestamp. The first second was discarded under the consideration of a reaction delay of the subject. A.3 INVESTIGATION OF DECODING PERFORMANCE EVALUATION: SSVEP In this work, we mainly use decoding performance as the assessment of different artifact removal methods. We provide an interpretation of the relationship between EEG quality and decoding performance. In Section 4 SSVEP result, IC-U-Net can optimize the weight with minimal MSE value among all compared artifact removal networks. However, the decoding performance is worse than CLEEGN and even the reference method (ICLabel), which implies that the average error over data points (MSE) is not the best assessment of EEG quality. In the SSVEP experiment, the fundamental frequency of the external rapidly repetitive visual stimulation and its harmonics can be an important feature in classification. Hence, higher quality in the frequency component of EEG data is required. We use the power spectrum density (PSD) to interpret the result of decoding performance. Figure 10 shows the PSD of EEG event data in each class denoised by different methods. We can see the reconstructed EEG spectrum from CLEEGN in each class is similar to the reference. As for IC-U-Net, it seems that the model can not completely reconstruct in several frequency bands (15-20Hz, 30-40Hz), which leads to the low decoding performance. The PSD result shows that there is great distortion in the power density of reconstructed data generated by 1D-ResCNN. Since 1D-ResCNN is a one-dimensional structure, we hypothesize that spatial information is important in EEG artifact removal. A.4 ADDITIONAL TABLES\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The paper introduces CLEEGN, a convolutional neural network (CNN) designed for \"plug-and-play automatic EEG reconstruction\" by effectively removing artifacts. It claims to be subject-independent, requiring no training or calibration for new users, and aims to outperform existing methods in terms of decoding performance for BCI applications.\\n\\n## Detailed Review: CLEEGN for BCI Applications\\n\\n### 1. Practical Applicability and Impact of the Method in Brain-Computer Interface Systems\\n\\nThe practical applicability and potential impact of CLEEGN in Brain-Computer Interface (BCI) systems are exceptionally high, addressing several long-standing challenges:\\n\\n*   **Addressing the \"Plug-and-Play\" Challenge:** A major hurdle for widespread BCI adoption is the need for extensive subject-specific calibration and the susceptibility of EEG signals to noise. CLEEGN\\'s \"plug-and-play\" capability, meaning it works subject-independently without per-user training/calibration, is a **game-changer**. This significantly reduces setup time, technical expertise required, and user burden, making BCI systems much more accessible for both clinical and consumer applications. Imagine a BCI device that works out of the box, much like any other consumer electronic device.\\n*   **Enhancing Robustness in Real-World Environments:** EEG artifacts (eye blinks, muscle movements, cardiac activity, line noise) are pervasive and highly variable, especially outside of controlled lab settings. CLEEGN aims to automatically mitigate these, leading to more robust and reliable BCI performance in naturalistic environments, where users cannot always remain perfectly still. This is crucial for at-home BCI use, neurorehabilitation, or any scenario where precise control of the user\\'s state is difficult.\\n*   **Reduced Computational Overhead:** Described as \"light-weight\" and with minimal parameters compared to some other deep learning models (as shown in Table 2), CLEEGN holds promise for deployment on resource-constrained devices, such as wearable BCIs or embedded systems. This could facilitate the development of compact, portable BCI solutions.\\n*   **Streamlining Data Preprocessing Pipelines:** By offering an automatic and efficient artifact removal solution, CLEEGN can significantly simplify and accelerate the preprocessing stage, which is often a bottleneck in BCI research and development. This allows researchers and developers to focus more on brain state decoding and application development rather than extensive artifact handling.\\n\\n### 2. Potential for Improving the Performance and Usability of BCI Systems\\n\\nCLEEGN demonstrates significant potential for improving both the performance and usability of BCI systems:\\n\\n*   **Improved Decoding Performance:** The paper\\'s most impactful finding is that CLEEGN-reconstructed EEG leads to *superior decoding performance* (higher AUC for ERN, higher accuracy for SSVEP) compared to raw data, other neural network methods (1D-ResCNN, IC-U-Net), and even the offline ICLabel method used as its own training reference. This is critical because the ultimate goal of artifact removal in BCI is not just clean signals, but signals that allow for better classification of brain states or intentions. By enhancing the signal-to-noise ratio in a way that preserves task-relevant information, CLEEGN directly contributes to more accurate and reliable BCI control.\\n*   **Enhanced Usability through Calibration-Free Operation:** As highlighted above, the plug-and-play nature is a massive leap in usability. Eliminating the need for lengthy calibration sessions or subject-specific modeling reduces the entry barrier for new users, especially those with severe disabilities who may struggle with complex setup procedures. It fosters a more seamless and intuitive user experience.\\n*   **Consistent Performance Across Paradigms:** The demonstration of improved decoding performance on two distinct BCI paradigms (ERN and SSVEP) suggests that CLEEGN is not limited to a specific type of brain signal or BCI application, indicating its generalizability and broad applicability.\\n*   **Reduced User Frustration and Increased Engagement:** When BCI systems are unreliable due to artifacts, users can become frustrated and disengaged. By providing cleaner, more consistent signals, CLEEGN can lead to a more stable and predictable user experience, thereby increasing user satisfaction and long-term engagement with BCI technology.\\n\\n### 3. Evaluation of Experimental Results in Terms of Relevance to Real-World BCI Applications\\n\\nThe experimental results are highly relevant to real-world BCI applications, largely due to the chosen evaluation methodology:\\n\\n*   **Focus on Decoding Performance:** Crucially, the paper evaluates artifact removal not just by signal-level metrics like MSE (though it is reported), but primarily by its impact on *decoding performance* using labeled BCI datasets. This is the gold standard for assessing the utility of any EEG processing technique in a BCI context, as the ultimate goal is to translate brain activity into commands.\\n*   **Use of Labeled BCI Datasets:** Employing standard, well-studied BCI datasets (ERN, SSVEP) ensures that the results are comparable and meaningful within the BCI community. These paradigms represent common BCI applications.\\n*   **Subject-Independent Evaluation:** The k-fold cross-validation scheme, leaving entire subjects out for testing, directly simulates a \"new user\" scenario. This is an essential validation for \"plug-and-play\" claims and directly reflects real-world BCI deployment challenges.\\n*   **Comparison to Strong Baselines:** Including ICA (ICLabel), a widely used and robust offline method, alongside other state-of-the-art deep learning methods (1D-ResCNN, IC-U-Net) provides a comprehensive performance comparison. The ability of CLEEGN to outperform even the offline ICLabel is particularly impressive and underscores its potential for practical use.\\n*   **Analysis of Reference Data Impact:** The investigation into different reference data types (ICLabel, ASR, hybrid) and the finding that ICLabel-based references yielded the best results for CLEEGN training is practical. Furthermore, the surprising result that CLEEGN *outperformed* its ICLabel reference in decoding (Figure 6b, Table 2) suggests that CLEEGN isn\\'t merely mimicking the reference but learning a more optimal representation for BCI decoding, possibly by removing more relevant noise or preserving features better. The explanation in Appendix A.3 regarding PSD analysis (Figure 10) further supports this: CLEEGN\\'s frequency spectrum is closer to the reference and preserves task-relevant frequencies better than competitors, indicating it cleans in a BCI-optimized manner.\\n*   **Training Data Efficiency:** The finding that satisfactory performance can be achieved with only 10 minutes of training data per subject (or even comparable performance with 1 minute) is highly relevant for building large, diverse training datasets in BCI research.\\n\\nHowever, some aspects could further strengthen the real-world relevance:\\n\\n*   **Lack of True Online Latency Measurement:** While simulated online reconstruction is shown, actual end-to-end latency measurements on live BCI hardware are missing. The 0.5-second stride implies some processing delay, which needs to be quantified for applications requiring low latency (e.g., motor imagery control).\\n*   **Hardware Implementation Details:** The paper mentions \"light-weight\" but doesn\\'t delve into specific computational requirements (FLOPs, memory footprint) or demonstrate real-time performance on a typical BCI processing unit (e.g., Raspberry Pi, embedded DSP).\\n\\n### 4. Limitations and Future Directions of the Work in the Context of BCI Research\\n\\n#### Limitations:\\n\\n*   **Reliance on \"Synthetic Ground Truth\":** A fundamental limitation, common in artifact removal literature, is the absence of true \"noiseless\" EEG ground truth. CLEEGN is trained using data \"cleaned\" by existing offline methods (ICLabel, ASR). While the paper shows CLEEGN can *outperform* these references in decoding, there\\'s an inherent dependency on the quality and assumptions of these underlying methods. If ICLabel or ASR misclassify artifact components or inadvertently remove neural signal, CLEEGN might learn these biases.\\n*   **Generalizability to Diverse Artifacts and Populations:** While the datasets show a range of common artifacts, EEG recordings from real-world, clinical, or highly mobile settings can contain more complex, non-stationary, or idiosyncratic artifacts (e.g., seizure activity, electrode movement artifacts, line noise in extremely noisy environments). The generalizability to such highly variable noise profiles and diverse patient populations (e.g., severe motor impairments, neurological conditions) needs further validation.\\n*   **Specific Artifact Quantification:** While visually compelling and supported by decoding performance, a more quantitative breakdown of the removal efficacy for *specific types* of artifacts (e.g., EOG regression, EMG power reduction in specific frequency bands) would provide deeper insight into CLEEGN\\'s denoising capabilities.\\n*   **Interpretability Claims:** While visualizations of latent features are provided, the claim of providing \"neuroscientific insights\" is broad. While interesting, the current visualization doesn\\'t directly reveal *why* certain brain patterns are preserved or how the network specifically isolates brain activity from noise in a neuroscientifically meaningful way beyond general spatial/temporal filtering.\\n*   **EEGNet as the Sole Classifier:** While EEGNet is a good choice, using a single classifier might limit the assessment of how well the cleaned data generalizes to other decoding algorithms. Different classifiers might be more sensitive or robust to certain types of residual noise or preserved features.\\n\\n#### Future Directions in the Context of BCI Research:\\n\\n*   **True Real-Time BCI Integration and Benchmarking:** The next crucial step is to integrate CLEEGN into a full, live BCI loop. This involves real-time latency measurements, computational load analysis on target BCI hardware, and demonstrating performance gains in closed-loop BCI tasks (e.g., real-time cursor control, speller accuracy).\\n*   **Exploring Alternative \"Ground Truth\" Generation:** Investigate self-supervised or unsupervised learning approaches for artifact removal that do not rely on pre-cleaned reference data from traditional methods. This could involve generative adversarial networks (GANs) or contrastive learning techniques to learn robust representations of clean EEG.\\n*   **Adaptation and Personalization:** While subject-independent is key, exploring how CLEEGN can be fine-tuned with minimal new user data (e.g., few-shot learning, meta-learning) could potentially push performance even higher for individual users who desire maximum accuracy and reliability.\\n*   **Multi-Modal Integration:** Investigate how CLEEGN could be combined with other modalities (e.g., EOG, EMG, motion sensors) if available, to provide a more comprehensive artifact suppression framework, or how it could denoise data for fusion with fMRI or MEG.\\n*   **Transfer Learning for Different BCI Paradigms:** Explore the efficacy of CLEEGN\\'s pre-trained weights for transfer learning to other BCI paradigms (e.g., motor imagery, P300 detection for different spellers, natural speech decoding) or even clinical EEG analysis tasks (e.g., seizure detection, sleep staging).\\n*   **Quantifying Neurophysiological Preservation:** Beyond decoding performance, conduct neurophysiological analyses (e.g., ERP component analysis, spectral power analysis, connectivity analysis) on CLEEGN\\'s output to demonstrate that artifact removal does not distort or eliminate relevant neurophysiological markers.\\n*   **Open-Source Release and Community Validation:** Making the CLEEGN code and pre-trained models publicly available would accelerate adoption and allow for independent validation and further development by the BCI research community.\\n\\nIn conclusion, CLEEGN represents a highly promising advancement for BCI systems. Its plug-and-play, subject-independent nature, coupled with its demonstrated ability to improve BCI decoding performance on real data, makes it a significant contribution towards making BCI technology more practical, accessible, and reliable for real-world applications. The identified limitations suggest clear pathways for future research to further solidify its impact.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.85636995}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Here is a summary of the agent reviews:\n\n*   **EEG Domain Expert:** The agent highlighted the neurophysiological plausibility of the method, especially the spatial and temporal filtering. The agent raised a concern about the reliance on ICLabel and ASR for generating the training data, as CLEEGN might inherit biases from these methods. The agent also emphasized the importance of the PSD analysis in validating the preservation of neural information.\n*   **Deep Learning Methodology Expert:** The agent focused on the novelty and correctness of the deep learning architecture and training procedure. The agent pointed out that the novelty lies in the subject-independent training and evaluation. The agent also noted that the description of the architecture could be more complete without relying on figures/tables.\n*   **BCI Application Expert:** The agent emphasized the practical applicability and impact of the method in BCI systems. The agent highlighted the plug-and-play capability of CLEEGN, which makes BCI systems more accessible. The agent also suggested future directions for research, such as integrating CLEEGN into a full, live BCI loop and exploring alternative \"ground truth\" generation methods.\n\nBased on the agent reviews, the paper presents a novel and technically sound approach for EEG artifact removal with high practical applicability in BCI systems. The main strength of the paper is the subject-independent training and evaluation, which makes the method readily usable in real-world scenarios. The use of decoding performance as the primary evaluation metric is also commendable. The main limitation is the reliance on existing artifact removal algorithms for generating the training data, which might introduce biases.\n\nTaking into account all the reviews, I recommend accepting the paper.\n\nFINAL DECISION: Accept\n"}], "ground_truth": "Reject", "response_history": "Here is a summary of the agent reviews:\n\n*   **EEG Domain Expert:** The agent highlighted the neurophysiological plausibility of the method, especially the spatial and temporal filtering. The agent raised a concern about the reliance on ICLabel and ASR for generating the training data, as CLEEGN might inherit biases from these methods. The agent also emphasized the importance of the PSD analysis in validating the preservation of neural information.\n*   **Deep Learning Methodology Expert:** The agent focused on the novelty and correctness of the deep learning architecture and training procedure. The agent pointed out that the novelty lies in the subject-independent training and evaluation. The agent also noted that the description of the architecture could be more complete without relying on figures/tables.\n*   **BCI Application Expert:** The agent emphasized the practical applicability and impact of the method in BCI systems. The agent highlighted the plug-and-play capability of CLEEGN, which makes BCI systems more accessible. The agent also suggested future directions for research, such as integrating CLEEGN into a full, live BCI loop and exploring alternative \"ground truth\" generation methods.\n\nBased on the agent reviews, the paper presents a novel and technically sound approach for EEG artifact removal with high practical applicability in BCI systems. The main strength of the paper is the subject-independent training and evaluation, which makes the method readily usable in real-world scenarios. The use of decoding performance as the primary evaluation metric is also commendable. The main limitation is the reliance on existing artifact removal algorithms for generating the training data, which might introduce biases.\n\nTaking into account all the reviews, I recommend accepting the paper.\n\nFINAL DECISION: Accept\n", "elapsed_time": 235.91665077209473, "reviewer_agents": {"EEGDomainExpert": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in EEG signal processing and neuroscience.", "system_prompt": "You are an expert in EEG signal processing and neuroscience. You will evaluate the paper based on the neurophysiological plausibility and relevance of the proposed method. Assess whether the method aligns with known principles of EEG signal generation and artifact characteristics. Critically evaluate the interpretation of the results in the context of neuroscience and EEG research.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "DLMethodologyExpert": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in deep learning methodologies.", "system_prompt": "You are an expert in deep learning methodologies. You will evaluate the paper based on the novelty and correctness of the deep learning architecture and training procedure. Assess the technical soundness of the proposed CNN architecture, loss functions, and training methodology. Evaluate the novelty and appropriateness of the chosen deep learning techniques for the EEG reconstruction task. Provide detailed feedback on the clarity and completeness of the methodological description.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "BCIApplicationExpert": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in Brain-Computer Interface applications.", "system_prompt": "You are an expert in Brain-Computer Interface applications. You will evaluate the paper based on the practical applicability and impact of the method in Brain-Computer Interface systems. Assess the potential of the proposed method for improving the performance and usability of BCI systems. Evaluate the experimental results in terms of their relevance to real-world BCI applications. Provide insights on the limitations and future directions of the work in the context of BCI research.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}}}
