{"paper_id": "b0JxQC7JLWh", "prompt": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation\n\n1 INTRODUCTION: Physically realizable adversarial attacks are a threat for safety-critical (semi-)autonomous systems such as self-driving cars or robots. Adversarial patches (Brown et al., 2017; Karmon et al., 2018) are the most prominent example of such an attack. Their realizability has been demonstrated repeatedly, for instance by Lee & Kolter (2019): an attacker places a printed version of an adversarial patch in the physical world to fool a deep learning system. While empirical defenses (Hayes, 2018; Naseer et al., 2019; Selvaraju et al., 2019; Wu et al., 2020) may offer robustness against known attacks, it does not provide any guarantees against unknown future attacks (Chiang et al., 2020). Thus, certified defenses for the patch threat model, which allow guaranteed robustness against all possible attacks for the given threat model, are crucial for safety-critical applications. Research on certifiable defenses against adversarial patches can be broadly categorized into certified recovery and certified detection. Certified recovery (Chiang et al., 2020; Levine & Feizi, 2020; Zhang et al., 2020; Xiang et al., 2021; Metzen & Yatsura, 2021; Lin et al., 2021; Xiang et al., 2022a; Salman et al., 2021; Chen et al., 2022) has the objective to make a correct prediction on an input even in the presence of an adversarial patch. In contrast, certified detection (McCoyd et al., 2020; Xiang & Mittal, 2021b; Han et al., 2021; Huang & Li, 2021) provides a weaker guarantee by only aiming at detecting inputs containing adversarial patches. While certified recovery is more desirable in principle, it typically comes at a high cost of reduced performance on clean data. In practice, certified detection might be preferable because it allows maintaining high clean performance. Most existing certifiable defenses against patches are focused on image classification, with the exception of DetectorGuard (Xiang & Mittal, 2021a) and ObjectSeeker (Xiang et al., 2022b) that certifiably defend against patch hiding attacks on object detectors. Moreover, existing defences are not easily applicable to arbitrary downstream models, because they assume either that the downstream model is trained explicitly for being certifiably robust (Levine & Feizi, 2020; Metzen & Yatsura, 2021), or that the model has a certain network architecture such as BagNet (Zhang et al., 2020; Metzen & Yatsura, 2021; Xiang et al., 2021) or a vision transformer (Salman et al., 2021; Huang & Li, 2021). A notable exception is PatchCleanser (Xiang et al., 2022a), which can be combined with arbitrary downstream models but is restricted to image classification. Adversarial patch attacks were also proposed for the image segmentation problem (Nesti et al., 2022), mostly for attacking CNN-based models that use a localized receptive field (Zhao et al., 2017). However, recently self-attention based vision transformers (Dosovitskiy et al., 2021) have achieved new state-of-the-art in the image segmentation task (Liu et al., 2021; Bousselham et al., 2021). Their output may become more vulnerable to adversarial patches if they manage to manipulate the global self-attention (Lovisotto et al., 2022). We demonstrate how significant parts of the segmentation output may be affected by a small patch for Swin tranfromer Liu et al. (2021) in Figure 1a. Full details on the attack are available in Appendix D. We point out that preventive certified defences are important because newly developed attacks can immediately be used to compromise safety-critical applications unless they are properly defended. In this work, we propose the novel framework DEMASKED SMOOTHING (Figure 1c) to obtain the first (to the best of our knowledge) certified defences against patch attacks on semantic segmentation models. Similarly to previous work (Levine & Feizi, 2020), we mask different parts of the input (Figure 1b) and provide guarantees with respect to every possible patch that is not larger than a certain pre-defined size. While prior work required the classification model to deal with such masked inputs, we leverage recent progress in image inpainting (Dong et al., 2022) to reconstruct the input before passing it to the downstream model. This decoupling of image demasking from the segmentation task allows us to support arbitrary downstream models. Moreover, we can leverage state of the art methods for image inpainting. We also propose different masking schemes tailored for the segmentation task that provide the dense input allowing the demasking model to understand the scene but still satisfy the guarantees with respect to the adversarial patch. We summarize our contributions as follows: \u2022 We propose DEMASKED SMOOTHING which is the first (to the best of our knowledge) certified recovery or certified detection based defence against adversarial patch attacks on semantic segmentation models (Section 4). \u2022 DEMASKED SMOOTHING can do certified detection and recovery with any off-the-shelf segmentation model without requiring finetuning or any other adaptation. \u2022 We implement DEMASKED SMOOTHING, evaluate it for different certification objectives and masking schemes (Section 5). We can certify 63% of all pixels in certified detection for a 1% patch and 46% in certified recovery for a 0.5% patch for the BEiT-B (Bao et al., 2022) segmentation model on the ADE20K Zhou et al. (2017) dataset. 2 RELATED WORK: Certified recovery. The first certified recovery defence for classification models against patches was proposed by Chiang et al. (2020), who adapted interval-bound propagation (Gowal et al., 2019) to the patch threat model. Levine & Feizi (2020) proposed De-Randomized Smoothing (DRS), which provides significant accuracy improvement when compared to Chiang et al. (2020) and scales to the ImageNet dataset. In DRS, a base classifier is trained on images where everything but a small local region is masked (ablated). At inference time, a majority vote of all specified ablations is taken as the final classification. If this vote has a large enough margin to the runner-up class, the prediction cannot be shifted by any patch that does not exceed a pre-defined size. A similar approach was adopted in Randomized Cropping (Lin et al., 2021). A general drawback of these approaches is that the classifier needs to be trained to process masked/cropped inputs, which (in contrast to our work) prohibits the usage of arbitrary pretrained models. A further line of work studies network architectures that are particularly suited for certified recovery. For instance, models with small receptive fields such as BagNets (Brendel & Bethge, 2019) have been explored, either by combining them with some fixed postprocessing (Zhang et al., 2020; Xiang et al., 2021) or by training them end-to-end for certified recovery (Metzen & Yatsura, 2021). Salman et al. (2021) propose to apply DRS to Vision Transfomers (ViTs). In contrast to the aforementioned works, our Demasked Smoothing can be applied to models with arbitrary architecture. This is a property shared with PatchCleanser (Xiang et al., 2022a), which however is limited to image classification and it is not clear how it can be extended to semantic segmentation where a class needs to be assigned to every pixel including the masked ones. Certified recovery against patches has also been extended to object detection, specifically to defend against patch hiding attacks. Two notable works in this direction are DetectorGuard (Xiang & Mittal, 2021b), an extension of PatchGuard (Xiang et al., 2021) to object detection, and ObjectSeeker (Xiang et al., 2022b). Randomized smoothing (Cohen et al., 2019) has been applied to certify semantic segmentation models against `2-norm bounded adversarial attacks (Fischer et al., 2021). However, to the best of our knowledge, no certified defence against patch attacks for semantic segmentation has been proposed so far. Certified detection. An alternative to certified recovery is certified detection. Here, an adversarial patch is allowed to change the model prediction. However, if it succeeds in doing so, there is a mechanism that detects this attack certifiably with zero false negatives. Minority Reports (McCoyd et al., 2020) was the first certified detection method against patches, which is based on sliding a mask over the input in a way that ensures that there will be one mask position that completely hides the patch. PatchGuard++ (Xiang & Mittal, 2021b) is an extension of Minority Reports where the sliding mask is not applied on the input but on the feature maps of a BagNet-type feature extractor. This reduces inference time drastically since the feature extractor needs to be executed only once per input. ScaleCert (Han et al., 2021) tries to identify \u201csuperficial important neurons\u201d, which allows pruning the network in a way that the prediction needs to be made for fewer masked inputs. Lastly, PatchVeto (Huang & Li, 2021) is a recently proposed method for certified detection that is tailored towards ViT models. It implements masking by removing certain input patches of the ViT. In this work, we propose a novel method for certified detection in the semantic segmentation task that can be used for any pretrained model. Image reconstruction. The problem of learning to reconstruct the full image from inputs where parts have been masked out was pioneered by Vincent et al. (2010). It recently attracted attention as proxy task for self-supervised pre-training, especially for the ViTs (Bao et al., 2022; He et al., 2021). Recent approaches to this problem are using Fourier convolutions (Suvorov et al., 2022) and visual transformers (Dong et al., 2022). SPG-Net (Song et al., 2018) trains a subnetwork to reconstruct the full semantic segmentation directly from the masked input as a part of the image inpainting pipeline. In this work, we use the state-of-the-art ZITS (Dong et al., 2022) inpainting method. 3 PROBLEM SETUP:  3.1 SEMANTIC SEGMENTATION: In this work, we focus on the semantic segmentation task. Let X be a set of rectangular images. Let x \u2208 X be an image with height H , width W and the number of channels C. We denote Y to be a finite label set. The goal is to find the segmentation map s \u2208 YH\u00d7W for x. For each pixel xi,j , the corresponding label si,j denotes the class of the object to which xi,j belongs. We denote S to be a set of segmentation maps and f : X \u2192 S to be a segmentation model. 3.2 THREAT MODEL: Let us consider an untargeted adversarial patch attack on a segmentation model. Consider an image x \u2208 [0, 1]H\u00d7W\u00d7C and its ground truth segmentation map s. Assume that the attacker can modify an arbitrary rectangular region of the image x which has a size of H \u2032\u00d7W \u2032. We refer to this modification as a patch. Let l \u2208 {0, 1}H\u00d7W be a binary mask that defines the patch location in the image in which ones denote the pixels belonging to the patch. Let L be a set of all possible patch locations for a given image x. Let p \u2208 [0, 1]H\u00d7W\u00d7C be the modification itself. Then we define an operator A as A(x, p, l) = (1\u2212 l) x+ l p, where is element-wise product. The operator A applies the H \u2032 \u00d7W \u2032 subregion of p defined by a binary mask l to the image x while keeping the rest of the image unchanged. We denote P := [0, 1]H\u00d7W\u00d7C \u00d7L to be a set of all possible patch configurations (p, l) that define an H \u2032\u00d7W \u2032 patch. Let s \u2208 S be the ground truth segmentation for x. Let Q(f(x), s) be some quality metric such as global pixel accuracy or mean intersection over union (mIoU). The goal of an attacker is to find (p?, l?) s. t. (p?, l?) = argmin (p, l)\u2208P Q(f(A(x, p, l)), s) 3.3 DEFENCE OBJECTIVE: In this paper, we propose certified defences against patch attacks. It means that we certify against any possible attack from P including (p?, l?). We consider two robustness objectives. Certified recovery For a pixel xi,j our goal is to verify that the following statement is true \u2200 (p, l) \u2208 P : f(A(x, p, l))i,j = f(x)i,j (1) Certified detection We consider a verification function v defined on X such that v(x) \u2208 {0, 1}H\u00d7W . If v(x)i,j = 1, then the adversarial patch attack on xi,j can be detected by applying the function v to the attacked image x\u2032 = A(x, p, l). v(x)i,j = 1\u21d2 [ \u2200 (p, l) \u2208 P : v(A(x, p, l))i,j = 1\u2192 f(A(x, p, l))i,j = f(x)i,j ] (2) v(x\u2032)i,j = 0 means an alert on pixel x\u2032i,j . However, if x \u2032 is not an adversarial example, then this is a false alert. In that case the fraction of pixels for which we return false alert is called false alert ratio (FAR). The secondary objective is to keep FAR as small as possible. Depending on the objective our goal is to certify one of the conditions 1, 2 for each pixel xi,j . This provides us an upper bound on an attacker\u2019s effectiveness under any adversarial patch attack from P . 4 DEMASKED SMOOTHING: DEMASKED SMOOTHING (Figure 1c) consists of several steps. First, we apply a predefined set of masks with specific properties to the input image to obtain a set of masked images. Then we reconstruct the masked regions of each image based on the available information with an inpainting model g. After that we apply a segmentation model f to the demasked results. Finally, we aggregate the segmentation outcomes and make a conclusion for the original image with respect to the statements (1) or (2). 4.1 INPUT MASKING: Motivation. Like in previous work (Section 2) we apply masking patterns to the input image and use predictions on masked images to aggregate the robust result. If an adversarial patch is completely masked, it has no effect on further processing. However, in semantic segmentation, we predict not a single whole-image label like in the classification task, but a separate label for each pixel. Thus, making prediction on a masked image must allow us to predict the labels also for the masked pixels. Preliminaries. Consider an image x \u2208 [0, 1]H\u00d7W\u00d7C . We define \"\u2217\" to be a special masking symbol that does not correspond to any pixel value and has the property \u2200z \u2208 R : z\u00d7\u2217 = \u2217. Please note that \u2217 needs to be different from 0 since 0 is a valid pixel value in unmasked inputs. Let m \u2208 {\u2217, 1}H\u00d7W be a mask. We call the element-wise product x m a masking of x. In a masking, a subset of pixels becomes \u2217 and the rest remains unchanged. We consider the threat model P with patches of size H \u2032 \u00d7W \u2032 (Section 3.2). To define the structure of our masks, we break m into an array B of non-intersecting blocks, each having the same size H \u2032 \u00d7W \u2032 as the adversarial patch. We index the blocks as B[q, r], 1 \u2264 q \u2264 d HH\u2032 e, 1 \u2264 r \u2264 d W W \u2032 e. We say that the block B[q, r] is visible in a mask m if \u2200(i, j) \u2208 B[q, r] : mi,j = 1 Consider an array M of K masks. We define each mask M [k] by a set of blocks that are visible in it. For certified recovery, each block is visible in exactly one mask and masked in the others. We say that a mask m is affected by a patch (p, l) if A(x, p, l) m 6= x m. We define T (M) = max(p,l)\u2208P |{m \u2208M |A(x, p, l) m 6= x m}|. That is: T (M) is the largest number of masks affected by some patch. If M is defined, we refer to the value T (M) as T for simplicity. Certified recovery. We define column masking M for which T = 2. We assign every k-th block column to be visible in the mask M [k] (Figure 2c). Any (p, l) \u2208 P can intersect at most two adjacent columns since (p, l) has the same width as a column. Thus, it can affect at most two masks (Figure 2b). A similar scheme can be proposed for the rows. Due to the block size in B, the patch (p, l) cannot intersect more than four blocks at once. We define a mask set that we call 3-mask s. t. for any four adjacent blocks two are visible in the same mask (Figures 2d). Hence, a patch for 3-mask can affect no more than 3 masks, T = 3. To achieve T = 4 any assignment of visible blocks to the masks works. We consider 4-mask that allows uniform coverage of the visible blocks in the image (Figure 2f). See details on masking schemes in Appendix B. Certified detection. We define Md to be a set of masks for certified detection (we use subscript d for distinction). Md should have the property: \u2200 (p, l) \u2208 P \u2203m \u2208Md : A(x, p, l) m = x m i. e. for every patch exists at least one mask not affected by this patch. For a patch of size H \u2032 \u00d7W \u2032 we consider K =W \u2212W \u2032 + 1 masks such that the mask Md[k] masks a column of width W \u2032 starting at the horizontal position k in the image (Figure 2h). To obtain the guarantee for the same P with a smaller K, we consider a set of strided columns of width W \u2032\u2032 \u2265W \u2032 and stride W \u2032\u2032 \u2212W \u2032 + 1 that also satisfy the condition (see the proof adapted from Xiang et al. (2022a) in Appendix A). A similar scheme can be proposed for the rows (Figure 2i). Alternatively, we could use a set of block masks of size H \u2032 \u00d7W \u2032. Then the number of masks grows quadratically with the image resolution. Hence, in the experiments we focus on the column and the row masking schemes. Let g be a demasking model, g(x m) \u2208 [0, 1]H\u00d7W\u00d7C . The goal of g is to make the reconstruction g(x m) as close as possible (in some metric) to the original image x. For a segmentation model f we define a segmentation array S(M,x, g, f), S[k] := f(g(x M [k])), 1 \u2264 k \u2264 K. 4.2 CERTIFICATION: Certified recovery. For the threat model P consider a set M of K masks. We define a function h : X \u2192 S that assigns a class to the pixel xi,j via majority voting over class predictions of each Algorithm 1 Demasked Smoothing Input: image x \u2208 [0, 1]H\u00d7W\u00d7C , patch size (H \u2032,W \u2032), certification type CT (recovery or detection), mask type MT (column, row, 3-mask, 4-mask), inpainting model g, segmentation model f Output: segmentation map h \u2208 YH\u00d7W , certification (or verification) map v \u2208 {0, 1}H\u00d7W 1: M \u2190 CreateMaskArray(H,W,H \u2032,W \u2032,CT,MT) . according to section 4.1 2: for k \u2190 1, . . . , |M | do . this loop can be paralellized 3: S[k]\u2190 f(g(x M [k])) . mask input, inpaint the masked regions, and apply segmentation 4: end for 5: if CT = \u2019recovery\u2019 then h\u2190 MajorityVote(S) . vote over the classes predicted for each pixel 6: else h\u2190 f(x) . in detection case, output clean segmentation 7: end if 8: v \u2190 AllEqual(S, h) . assign 1 for the pixels where all S[k] agree with h and 0 otherwise 9: Return h, v reconstructed segmentation in S. A class for the pixel that is predicted by the largest number of segmentations is assigned. We break the ties by assigning a class with a smaller index. Theorem 1. If the number of masks K satisfies K \u2265 2T (M) + 1 and for a pixel xi,j we have \u2200 S[k] \u2208 S : S[k]i,j = h(x)i,j (i.e. all the votes agree), then \u2200 (p, l) \u2208 P : h(A(x, p, l))i,j = h(x)i,j . See the proof in Appendix A. Certified detection. Consider Md = {Md[k]}Kk=1. For a set of demasked segmentations S we define the verification map v(x)i,j := [f(x)i,j = S[1]i,j = . . . = S[K]i,j ] i.e. the original segmentation is equal to all the other segmentations on masked-demasked inputs, including the one in which the potential patch was completely masked. Theorem 2. Assume that v(x)i,j = 1. Then \u2200 (p, l) \u2208 P : v(A(x, p, l))i,j = 1\u21d2 f(A(x, p, l))i,j = f(x)i,j See the proof in Appendix A. For a given image x the verification map v(x) is complementary to the model segmentation output f(x) that stays unchanged. Thus, there is no drop in clean performance however we may have some false positive alerts in the verification map v in the clean setting. We present the Demasked Smoothing procedure in Algorithm 1. 5 EXPERIMENTS: In this section, we evaluate DEMASKED SMOOTHING with the masking schemes proposed in Section 4, compare our approach with the direct application of Derandomized Smoothing Levine & Feizi (2020) to the segmentation task and evaluate the performance on different datasets and models. Certified recovery and certified detection provide certificates of different strength (Section 4) which are not comparable. We evaluate them separately for different patch sizes. 5.1 EXPERIMENTAL SETUP: We evaluate DEMASKED SMOOTHING on two challenging semantic segmentation datasets: ADE20K (Zhou et al., 2017) (150 classes, 2000 validation images) and COCO-Stuff-10K (Caesar et al., 2018) (171 classes, 1000 validation images). For demasking we use the ZITS Dong et al. (2022) inpainting model with the checkpoint provided in the official paper repository 1. The model was trained on Places2 (Zhou et al., 2016) dataset with images resized to 256\u00d7256. As a segmentation model f we use BEiT-B Bao et al. (2022), Swin Liu et al. (2021), PSPNet Zhao et al. (2017) and DeepLab v3 (Chen et al., 2018). We note that the first two models are based on transformers and obtain near stateof-the-art results. PSPNet and DeepLab v3 are CNN-based segmentation methods that we consider to demonstrate that DEMASKED SMOOTHING is not specific to transformer-based architectures. We 1https://github.com/DQiaole/ZITS_inpainting use the model implementations provided in the mmsegmentation framework Contributors (2020). An illustration of the image reconstruction and respective segmentation can be found in Figure 3. We run the evaluation in parallel on 5 Nvidia Tesla V100-32GB GPUs. The certification for the whole ADE20K validation set with ZITS and BEiT-B takes around 1.2 hours for certified recovery and 2 hours for certified detection (due to a larger number of masks). 5.2 EVALUATION METRICS: For both certified recovery and certified detection, we generate a standard segmentation output (without any abstention) and a corresponding certification map (Figure 4). In case of certified detection, the segmentation output remains the same as for the original segmentation model, however, there may be false alerts in the certificaton map. For the certified recovery, the output is obtained by a majority vote over the segmentations of demasked images (Section 4.2). We evaluate the mean intersection over union (mIoU) for these outputs. The certification map is obtained by assigning to each certified pixel the corresponding class from the segmentation output and assigning a special uncertified label to all non-certified pixels. For each image we evaluate the fraction of pixels which are certified and correct (coincide with the ground truth). %C is a mean of these fractions over all the images in the dataset. In semantic segmentation task, the class frequencies are usually skewed, therefore global pixel-wise accuracy alone is an insufficient metric. Matching the certification map separately for each class y \u2208 Y with the ground truth segmentation for y in the image x allows us to compute the guaranteed lower bound (cTPy(x)) on the number of true positive pixel predictions (TPy(x)) i.e. those that were correctly classified into y. If a pixel was certified with a correct class, then this prediction cannot be changed by a patch (or, alternatively, the change will be detected by the verification function v in certified detection). We consider recall Ry(x) = TPy(x) TPy(x)+FNy(x) where FNy(x) is the number of false negative predictions for y in x. Py(x) = TPy(x) + FNy(x) is the total area of y in the ground truth and does not depend on our prediction. We can evaluate certified recall cRy(x) = cTPy(x) Py(x) , a lower bound on the recall Ry(x). Total recall and certified total recall of class y in a dataset D are TRy(D) =\u2211 x\u2208D TPy(x)\u2211 x\u2208D Py(x) and cTRy(D) = \u2211 x\u2208D cTPy(x)\u2211 x\u2208D Py(x) respectively. Then, we obtain mean recall mR(D) = 1 |Y| \u2211 y\u2208Y TRy(D) and certified mean recall cmR(D) = 1 |Y| \u2211 y\u2208Y cTRy(D). Evaluating lower bounds on other popular metrics such as mean precision or mIoU this way results in vacuous upper bound since they depend on the upper bound on false positive (FP ) predictions. For the pixels that are not certified we cannot guarantee that they will not be assigned to a certain class, therefore, a non-trivial upper bound on FP is not straightforward. We leave this direction for future work. In certified detection, we additionally consider false alert ratio (FAR) which is the fraction of correctly classified pixels for which we return an alert on a clean image. Smaller FAR is preferable. Due to our threat model, certifying small objects in the scene can be difficult because they can be partially or completely covered by an adversarial patch in a way that there is not chance to recover the prediction. To provide an additional perspective on our methods, we also evaluate mR and cmR specifically for the \u201cbig\u201d classes, which occupy on average more than 20% of the images in which they appear. These are, for example, road, building, train, and sky, which are important for understanding the scene. The full list of such classes for each dataset is provided in the Appendix H. 5.3 DISCUSSION: In Table 1, we compare different masking schemes proposed in Section 4.1. Evaluation of all the models with all the masking schemes is consistent with these results and can be found in Appendix E. We see that column masking achieves better results in both certification modes. Effectiveness of column masking for classification task was also empirically observed by Levine & Feizi (2020). We attribute the effectiveness of column masking to the fact most of the images in the datasets have a clear horizont line, therefore having a visible column provides a slice of the image that intersects most of the scene background objects. In Table 2, we extend Derandomized Smoothing (DRS) proposed by Levine & Feizi (2020) for certified recovery in classification to the segmentation task and compare it to our method Direct adaptation requires training a model that is able to predict the full image segmentation from a small visible region. Since it is not clear what architectural design and training procedure would be needed for that, we consider two alternative baselines. DRS-S predicts the segmentation directly from the masked image and DRS-E uses our inpainting method to first reconstruct the image and then obtain the segmentation. See the implementation details in Appendix G. DRS with column smoothing performs poorly on the segmentation task, which emphasizes the need for specific masking schemes. In Table 3, we evaluate our method with column masking on different models. For certified detection we can certify more than 60% of the pixels with all models on ADE20K and more than 46 % on COCO10K. False alert ratio on correctly classified pixels is around 20%. In certified recovery, we certify more than 44% pixels on ADE20K and more than 28% pixels on COCO10K. Figure 5 shows how the performance of DEMASKED SMOOTHING depends on the patch size for the BEiT-B model. We see that certified detection metrics remain high even for a patch as big as 5% of the image surface and for the recovery they slowly deteriorate as we increase the patch size to 2%. Ablations with respect to inpainting can be found in Appendix F. DEMASKEDSMOOTHING illustrations procedure are provided in Appendix K. 6 CONCLUSION: In this work, we propose DEMASKED SMOOTHING, the first (up to our knowledge) certified defence framework against patch attacks on segmentation models. Due to its novel design based on masking schemes and image demasking, DEMASKED SMOOTHING is compatible with any segmentation model and can on average certify 63% of the pixel predictions for a 1% patch in the detection task and 46% against a 0.5% patch for the recovery task on the ADE20K dataset. 7 ETHICS STATEMENT: This work contributes to the field of certified defences against physically-realizable adversarial attacks. The proposed approach allows to certify robustness of safety-critical applications such as medical imaging or autonomous driving. The defence might be used to improve robustness of systems used for malicious purposes such as (semi-)autonomous weaponry or unauthorized surveillance. This danger may be mitigated e.g. by using a system of sparsely distributed patches which makes certifying the image more challenging. All activities in our organization are carbon neutral, so the experiments performed on our GPUs do not leave any carbon dioxide footprint. 8 REPRODUCIBILITY STATEMENT: We provide the details of our experimental setup in Section 5.1. We discuss the evaluation metrics and their computation in Section 5.2. A PROOFS (SECTION 4): In this section, we provide the proofs for the theorems stated in Section 4. Lemma 1. (Section 4.1) Consider an image of the size H \u00d7W . Let H \u2032 \u00d7W \u2032 be a fixed adversarial patch size. Let Md(K,L) be a set of masks where each mask is masking an H \u00d7W \u2032\u2032 vertical column, W \u2032\u2032 \u2265 W \u2032. Let the stride between the columns in two adjacent masks be W \u2032\u2032 \u2212W \u2032 + 1. Then for any location l \u2208 L of the patch, there exists a mask that covers it completely. Proof. (Adapted from the proof of Lemma 4 in PatchCleanser Xiang et al. (2022a)). Without loss of generality, we consider the first two adjacent column masks. The first one covers the columns from 1 to W \u2032\u2032. The second mask covers the columns from 1 + (W \u2032\u2032 \u2212W \u2032 + 1) =W \u2032\u2032 \u2212W \u2032 + 2 to (W \u2032\u2032 \u2212W \u2032 + 2) + (W \u2032\u2032 \u2212 1) = 2W \u2032\u2032 \u2212W \u2032 + 1 (See Figure 6). Now consider an adversarial patch of size H \u2032 \u00d7W \u2032. Let us find the smallest possible start index of this patch so that it does not get covered by the first mask. For that it should be visible at the column W \u2032\u2032 + 1 and, therefore, start at the column with index not smaller than (W \u2032\u2032 + 1)\u2212W \u2032 + 1 =W \u2032\u2032 \u2212W \u2032 + 2. However, it is the same column in which second mask starts. Therefore, given that W \u2032\u2032 \u2265W \u2032 we have that the patch is completely masked by the second mask. Then for a patch which is only partially masked by the second mask from the left we use an analogous argument to show that it is completely masked by the third mask and so on. Certified recovery. For the threat model P (Section 3) consider a set M of K masks. We define a function h : X \u2192 S that assigns a class to the pixel xi,j via majority voting over class predictions of each reconstructed segmentation in S. A class for the pixel that is predicted by the largest number of segmentations is assigned. We break the ties by assigning a class with a smaller index. Theorem 1. (Section 4.2) If the number of masks K satisfies K \u2265 2T (M) + 1 and for a pixel xi,j we have \u2200 S[k] \u2208 S : S[k]i,j = h(x)i,j i.e. all the votes agree, then \u2200 (p, l) \u2208 P : h(A(x, p, l))i,j = h(x)i,j . Proof. Assume that \u2203 (p, l) \u2208 P : h(A(x, p, l))i,j 6= h(x)i,j Let us denote x\u2032 := A(x, p, l) and S\u2032 to be the segmentation array for x\u2032. Then the class h(x)i,j did not get the majority vote for S\u2032. However, by definition of T (M) we know that (p, l) could affect at most T (M) segmentations. Since all K segmentations of S have voted for h(x)i,j , then at least K \u2212 T > K2 of them are still voting for h(x)i,j in S \u2032 meaning that h(x)i,j still has the majority vote in S\u2032. Therefore h(x\u2032)i,j = h(x)i,j Certified detection. Consider Md = {Md[k]}Kk=1. For a set of demasked segmentations S we define the verification map v(x)i,j := [f(x)i,j = S[1]i,j = . . . = S[K]i,j ] i.e. the original segmentation coincides with all the other segmentations including the one in which the potential patch was completely masked. Theorem 2. (Section 4.2) Assume that v(x)i,j = 1. Then \u2200 (p, l) \u2208 P : v(A(x, p, l))i,j = 1\u21d2 f(A(x, p, l))i,j = f(x)i,j Proof. Assume that \u2203 (p, l) \u2208 P s. t. v(A(x, p, l))i,j = 1 and f(A(x, p, l))i,j 6= f(x)i,j .Let us denote x\u2032 := A(x, p, l) and S\u2032 to be the segmentation set for x\u2032. By definition of Md, \u2203Md[k] \u2208Md s. t. Md[k] masks the patch (p, l) Hence, g(x Md[k]) = g(x\u2032 Md[k]), S[k] = f(g(x Md[k])) = f(g(x\u2032 Md[k])) = S\u2032[k], Since v(x)i,j = 1, we have f(x)i,j = S[k]i,j . Since v(x\u2032)i,j = 1, we have f(x\u2032)i,j = S\u2032[k]i,j . Thus, f(x\u2032)i,j = f(x)i,j . B DETAILED DESCRIPTION OF MASKING STRATEGIES: In this section, we provide additional details for constructing certified recovery masks proposed in Section 4.1. We define mask sets M that satisfy different values of T . We divide the image x into a set of non-intersecting blocks B of the same size as an adversarial patch, H \u2032 \u00d7W \u2032 (see Figure 7), 1 \u2264 q \u2264 dH/H \u2032e, 1 \u2264 r \u2264 dW/W \u2032e. In each mask, each of these blocks will be either masked or not masked (i. e. visible). Moreover, for each block there exists only one mask in which it is visible. For a set M of K masks we define the mapping \u00b5M : B \u2192 {1, . . . ,K}. If \u00b5(B[q, r]) = k, then B[q, r] is not masked in M [k]. Therefore, each mask M [k] is defined by a Bk \u2282 B s. t. for b \u2208 Bk \u00b5(b) = k. We define a set M that we call 3-mask for which T (M) = 3. We assign the blocks in each row to the masks as follows: \u00b5(B[1, 1]) = 1; \u00b5(B[1, 2]) = \u00b5(B[1, 3]) = 2; \u00b5(B[1, 4]) = \u00b5(B[1, 5]) = 3 and so on until we reach the end of the row. If we finish the first row with the value k, then we start the second row as follows \u00b5(B[2, 1]) = \u00b5(B[2, 2]) = k + 1; \u00b5(B[2, 3]) = \u00b5(B[2, 4]) = k + 2: . . .. If we finish the second row on n, we start the third row similarly to the first: \u00b5(B[3, 1]) = n+1; \u00b5(B[3, 2]) = \u00b5(B[3, 3]) = n + 2; . . . When we reach the number K, we start from 1 again (Figure 7d). Due to the block size, the patch cannot intersect more than four blocks at once. Our parity-alternating block sequence ensures that in any such intersection of four blocks either the top ones or the bottom ones will belong to the same masking, so at most three different maskings can be affected. We define a set M that we call 4-mask for which T (M) = 4. Due to our block size any assignment of masks will work because the patch cannot intersect more than four blocks. We consider the one that allows uniform distribution of the unmasked blocks (Figure 7g). We point out that for the described methods each masking keeps approximately 1/K of the pixels visible and the unmasked regions are uniformly distributed in the image. This means that for any masked pixel there exists an unmasked region located close enough to this pixel. It is the core difference between our masks and the ones proposed for certified classification such as block or column smoothing Levine & Feizi (2020). It was observed that the image demasking is facilitated when the visible regions are uniformly spread in the masked image He et al. (2021). C TEST-TIME INPUT CERTIFICATION: In this section, we discuss how certified recovery (Theorem 1) can be applied to guaranteed verification of the robustness on a test image. We also discuss how robustness guarantees for the test-time images can be evaluated by using a dataset of clean images such as ADE20K (Zhou et al., 2017) or COCOStuff-10K (Caesar et al., 2018). C.1 TEST-TIME CERTIFIED RECOVERY Let x\u2032 be a test-time input which can be either a clean image or an image attacked with an adversarial patch. We know that there exists a clean image x corresponding to x\u2032 which removes the patch if it is present. We have either x\u2032 = x or x\u2032 \u2208 A(x), where A(x) := {A(x, p, l) | (p, l) \u2208 P}. However, at test time we do not have access to the clean image x. Our goal is to certify that for our segmentation model h and a pixel xi,j we have h(x\u2032)i,j = h(x)i,j . We can achieve this result by applying the recovery certification (Theorem 1) to the test-time image. It allows us to verify whether \u2200 (p, l) \u2208 P : h(A(x\u2032, p, l))i,j = h(x\u2032)i,j . We also know that if x\u2032 \u2208 A(x), then x \u2208 A(x\u2032) (Figure 8a). Indeed, if x\u2032 is only different from x by one patch, then x can be be obtained from x\u2032 by removing this patch. Therefore, by obtaining the guarantee for A(x\u2032), we implicitly obtain the guarantee also for the image x even though we do not have direct access to it. We note that this test-time guarantee is only possible for certified recovery. In certified detection, we would need to evaluate the verification function v (Theorem 2) for both the clean image x and the attacked image x\u2032 to obtain the result. This cannot be done if x is implicit. C.2 ROBUSTNESS GUARANTEES EVALUATION The typical certified robust error for a given test data set (and pixel (i, j) in the segmentation case) is an estimate for EX\u223cD[ max (p,l)\u2208P 1h(A(X,p,l))i,j 6=h(X)i,j ], where D is the data generating probability measure and we assume that our test set to be an i.i.d. sample of it. This is the expected robust error (worst case over our threat model P for clean inputs) for a given pixel (i, j). Using the test sample to get an estimate of this quantity, we get a probabilistic guarantee that the corresponding pixel (i, j) of a new clean test sample x\u2032 drawn i.i.d. from P will have its whole \u201cpatch\u201d-neighborhood certified. However, more important for a practical security analysis is that we can certify a given instance, which can be even potentially adversarially perturbed. Formally, this means that for an input z \u2208 A(x), where x \u223c P is an unknown sample from P , that we guarantee \u2200(p, l) \u2208 P : h(A(z, p, l))i,j = h(z)i,j , and as x \u2208 A(z, p, l) this implies that we certify that the pixel (i, j) of the potentially manipulated image is classified the same as pixel (i, j) of the unperturbed image x. However, it is now tricky to get even a probabilistic estimate of the quantity Ex\u223cD max (p,l)\u2208P [ max (q,m)\u2208P 1h(A(A(x,p,l),q,m))i,j=h(A(x,p,l))i,j ], as the outer maximization process cannot be simply simulated by doing adversarial patch attacks on a clean test dataset. We propose a way to evaluate a guaranteed lower bound on the fraction of certified test-time inputs by using a dataset of clean images. Instead of considering a standard one-patch neighbourhood A(x) defined by our threat model (Section 3.2), we propose to consider a neighbourhood A2(x) of two independent patches (Figure 8b). A2(x) contains all the images x\u2032 \u2208 A(x) as well as their respective patch neighbourhoods A(x\u2032). Therefore, by verifying that \u2200 (p1, l1), (p2, l2) \u2208 P : h(A(A(x, p1, l1), p2, l2))i,j = h(x)i,j , we guarantee that \u2200 x\u2032 \u2208 A(x) \u2200 (p, l) \u2208 P : h(A(x\u2032, p, l))i,j = h(x \u2032)i,j . We note that corresponding reasoning could be applied to certification in `p models. Then A2(x) would correspond to doubling the radius of the -ball instead of adding a second patch. Note that Theorem 1 can be directly extended to a threat model of N patches. In the worst case each of the N patches can affect T different maskings. Therefore, we need to change the condition of Theorem 1 to K \u2265 2NT + 1. We apply the described method to evaluating the test-time certification guarantees for a toy example of a 0.1% patch in Table 4. We also illustrate how a column mask looks in this case in Figure 8. D ADVERSARIAL PATCH EXAMPLE: In this section, we demonstrate an example of a real adversarial patch for a semantic segmentation model similar to the one illustrated in the Figure 1a and show how it is handled by our certified defences. We illustrate it for the Swin (Liu et al., 2021) model on one of the images from the ADE20K (Zhou et al., 2017) dataset. D.1 PATCH OPTIMIZATION We set the patch size to 1% of the image surface. We select a fixed position for a patch on the rear window of a car (Figure 9a). For each pixel we extract a list of predicted logits corresponding to each class and apply multi-margin loss with respect to the ground truth label of the respective pixel. We use random patch initialization without restarts. As an optimizer we use projected gradient descent (PGD) with 1000 steps and initial step size of 0.01. We use cosine step size schedule and momentum for the gradient with the rate of 0.9. The optimization plot and the patch efficiency at different iterations of the PGD are illustrated in the Figure 9. D.2 CERTIFIED RECOVERY We denote the original image as x and the patched image as x\u2032. The voting-based segmentation function h (Section 4.2) provides the majority-vote prediction h(x) and the corresponding certification map which shows the pixels where all the votes agree. In Figure we see that a part of the building and the road is certified which means that this prediction cannot be affected by an adversarial patch. Figure demonstrates h(x\u2032) which correctly segments those regions in presence of an adversarial patch that fools the original model. D.3 CERTIFIED DETECTION We perform our analysis by evaluating the verification map v (Section 4.2) for the original image x and for the patched image x\u2032. We see that in v(x) a major part of the building is certified i. e. for a part of pixels xi,j that belong to the building and the road we have v(x)i,j = 1. However, v(x\u2032)i,j = 0 for those pixels. It means that we have detected that the prediction on this input is potentially affected by an adversarial patch. E ADDITIONAL EXPERIMENTS: In Tables 5 and 6, we provide additional experimental results for evaluating different masking schemes proposed in Section 4.1 on different models. F INPAINTING ABLATION STUDIES We perform ablation studies with respect to the demasking step. The results are in Table 7. Figure 12 provides additional illustrations. As can be seen from the results, our method heavily benefits from having available stronger inpainting models that allow achieving better clean and certified accuracy. We consider this property actually as a strength of our method since it will automatically benefit from future research and developments of stronger inpainting methods. For certified recovery, we also compare it to GIN Li et al. (2020) based on a generative model that we trained on ADE20K (without using style losses based on ImageNet trained VGG). The results are in Table 8. Illustrations can be found in Figure 13. G COMPARISON TO SIMPLIFIED DERANDOMIZED SMOOTHING: Derandomized Smoothing (DRS) Levine & Feizi (2020) was proposed for certified recovery, therefore in this section we focus on this task. Direct adaptation of derandomized smoothing to semantic segmentation task requires training a model that is able to predict the full image segmentation from a small visible region. Since it is not immediately clear to us what architectural design and training procedure would be needed to train such a model, we consider a simplified version of DRS that we call DRS-S. In this version, we consider an off-the-shelf semantic segmentation model and evaluate how it performs with column masking from DRS. Therefore, we do not encode the masked regions with the special \u2019NULL\u2019 value like in DRS but use black color instead. That is because an off-the-shelf model cannot work with \u2019NULL\u2019 values. We run our experiments on ADE20K dataset. We consider the DRS parameters from the recent SOTA version of Derandomized Smoothing by Salman et al. Salman et al. (2021). They use column width b = 19 and stride s = 10 for certified classification of 224x224 ImageNet images. To account for the fact that ADE20K images have larger resolution than ImageNet, we scale the parameters to column width b = 42 and stride s = 22. To make the comparison consistent with the rest of our results, we use the patch occupying 0.5% of the image. From Table 9 we can see that DRS-S performs poorly on semantic segmentation task. The reason for that is illustrated in Figure 14. Processing the column region in 14c would probably be sufficient for a classification model to classify the image into the class \"house\". But it is clearly not sufficient to reconstruct the whole segmentation map 14e as can be seen in the Figure 14g. Whether doing this would be possible with a model specifically trained to reconstruct the segmentation map from a very small visible region is an open research question (up to our knowledge). We point out that the value %C of certified and correctly classified pixels in the Table 9 is still surprisingly high for DRS-S compared to other metircs. We attribute this to the fact that the solid black regions are usually treated as a wall by the segmentation model, therefore the images are usually segmented as a wall by the DRS majority voting. And the wall is a common part of both indoor and outdoor scenes in ADE20K as can be implied from the Table 10 of \"big\" ADE20K classes. Therefore, always classifying the output as a wall provides a decent fraction of correctly classified pixels because of the skewed classes. However, to provide a better comparison with DRS, we emulate the model which is able to reconstruct the whole segmentation map from the column masking proposed in DRS. We do this by applying the demasking approach proposed in this work. We first try to reconstruct the whole image from one column and then segment it with an off-the-shelf model as we did with the masks proposed in this paper. We call this approach DRS-E and the results can be found in Table 9. H A LIST OF BIG CLASSES: In Section 5.2 we suggest another perspective on the evaluation of our DEMASKED SMOOTHING by specifically considering its performance on \u201cbig\u201d semantic classes. The object of these classes occupy on average more than 20% of the images in which they appear. Correctly segmenting these classes is important for understanding the scene. In Tables 10 and 11 we provide the full list of such classes in ADE20K (Zhou et al., 2017) and COCO-Stuff-10K (Caesar et al., 2018) respectively together with the average fraction of pixels that they occupy in the images in which they are present. We point out that for COCO-Stuff-10K some typically smaller classes such as \u201csandwich\u201d or \u201cfruit\u201d get included in the list of big classes because of the macro-scale images in which they occupy a big part of the scene. I COMPLEXITY ANALYSIS AND PARALLELIZATION: In DEMASKED SMOOTHING, we propose a set of K masks that are applied to the original image (denote the cost of applying a single masking by M ). As illustrated in Figure 1c, the masked images are demasked (denote the cost of demasking an image by D) and segmented (denote the cost of segmenting an image by S); thereupon per-mask segmentations are aggregated into a final segmentation and certification (cost of aggregation proportional to K). Asymptotically, compute grows thus with O(K(M +D + S) +K) while the cost of a standard segmentation is O(S). Thus, for large K or M +D S, real-time applicability would actually be impractical. However, we note that: 1. M +D is roughly of the same size as S for typical DL-based inpainting and segmentation models. 2. For certified recovery, we operate in a setting where K is small (K \u2208 5, 7, 9) and does not grow with the image resolution. This is unlike Derandomized Smoothing and its derivatives, where the number of masks in the recovery task grows with the image resolution (or randomized smoothing with thousands of samples per input). This small value of K benefits our the method in time-sensitive applications. For certified detection, we can adjust the number of masks for the computational speed by using strided masking as suggested in Section 4.1. 3. Morover, masking, demasking, and segmenting for different masks do not use any shared data and can thus be fully parallelized if sufficiently powerful hardware is available. Only the aggregation step requires the results of all the previous stages. However, aggregation time is small compared to the other stages. In terms of latency, a fully parallelized version of our procedure would thus have a latency proportional to O(M +D+ S +K). For small K and M +D \u2248 S, application to real-time video can be facilitated by means of parallelization. J USED DATA: In this work, we only use the datasets published under formal licenses: ADE20K (Zhou et al., 2017) and COCO-Stuff-10K (Caesar et al., 2018). To the best of our knowledge, data used in this project do not contain any personally identifiable information or offensive content. The models ZITS (Dong et al., 2022) and Swin (Liu et al., 2021) are published under Apache-2.0 license. The text of the license for PSPNet (Zhao et al., 2017) can be found here: https://github.com/hszhao/ PSPNet/blob/master/LICENSE K DEMASKED SMOOTHING VISUALIZATION: In this section, we provide additional illustrations of our method (Figures 15, 16, 17, 18). Similarly to the Table 1 we certify against a 1% patch for the detection task and against 0.5% patch for the recovery task. For each mask type we illustrate all the stages summarized in the Figure 1c. We also provide examples of certification maps for certified recovery and certified detection with different images (Figure 19, 20).", "agent_review": [{"role": "user", "content": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation\n\n1 INTRODUCTION: Physically realizable adversarial attacks are a threat for safety-critical (semi-)autonomous systems such as self-driving cars or robots. Adversarial patches (Brown et al., 2017; Karmon et al., 2018) are the most prominent example of such an attack. Their realizability has been demonstrated repeatedly, for instance by Lee & Kolter (2019): an attacker places a printed version of an adversarial patch in the physical world to fool a deep learning system. While empirical defenses (Hayes, 2018; Naseer et al., 2019; Selvaraju et al., 2019; Wu et al., 2020) may offer robustness against known attacks, it does not provide any guarantees against unknown future attacks (Chiang et al., 2020). Thus, certified defenses for the patch threat model, which allow guaranteed robustness against all possible attacks for the given threat model, are crucial for safety-critical applications. Research on certifiable defenses against adversarial patches can be broadly categorized into certified recovery and certified detection. Certified recovery (Chiang et al., 2020; Levine & Feizi, 2020; Zhang et al., 2020; Xiang et al., 2021; Metzen & Yatsura, 2021; Lin et al., 2021; Xiang et al., 2022a; Salman et al., 2021; Chen et al., 2022) has the objective to make a correct prediction on an input even in the presence of an adversarial patch. In contrast, certified detection (McCoyd et al., 2020; Xiang & Mittal, 2021b; Han et al., 2021; Huang & Li, 2021) provides a weaker guarantee by only aiming at detecting inputs containing adversarial patches. While certified recovery is more desirable in principle, it typically comes at a high cost of reduced performance on clean data. In practice, certified detection might be preferable because it allows maintaining high clean performance. Most existing certifiable defenses against patches are focused on image classification, with the exception of DetectorGuard (Xiang & Mittal, 2021a) and ObjectSeeker (Xiang et al., 2022b) that certifiably defend against patch hiding attacks on object detectors. Moreover, existing defences are not easily applicable to arbitrary downstream models, because they assume either that the downstream model is trained explicitly for being certifiably robust (Levine & Feizi, 2020; Metzen & Yatsura, 2021), or that the model has a certain network architecture such as BagNet (Zhang et al., 2020; Metzen & Yatsura, 2021; Xiang et al., 2021) or a vision transformer (Salman et al., 2021; Huang & Li, 2021). A notable exception is PatchCleanser (Xiang et al., 2022a), which can be combined with arbitrary downstream models but is restricted to image classification. Adversarial patch attacks were also proposed for the image segmentation problem (Nesti et al., 2022), mostly for attacking CNN-based models that use a localized receptive field (Zhao et al., 2017). However, recently self-attention based vision transformers (Dosovitskiy et al., 2021) have achieved new state-of-the-art in the image segmentation task (Liu et al., 2021; Bousselham et al., 2021). Their output may become more vulnerable to adversarial patches if they manage to manipulate the global self-attention (Lovisotto et al., 2022). We demonstrate how significant parts of the segmentation output may be affected by a small patch for Swin tranfromer Liu et al. (2021) in Figure 1a. Full details on the attack are available in Appendix D. We point out that preventive certified defences are important because newly developed attacks can immediately be used to compromise safety-critical applications unless they are properly defended. In this work, we propose the novel framework DEMASKED SMOOTHING (Figure 1c) to obtain the first (to the best of our knowledge) certified defences against patch attacks on semantic segmentation models. Similarly to previous work (Levine & Feizi, 2020), we mask different parts of the input (Figure 1b) and provide guarantees with respect to every possible patch that is not larger than a certain pre-defined size. While prior work required the classification model to deal with such masked inputs, we leverage recent progress in image inpainting (Dong et al., 2022) to reconstruct the input before passing it to the downstream model. This decoupling of image demasking from the segmentation task allows us to support arbitrary downstream models. Moreover, we can leverage state of the art methods for image inpainting. We also propose different masking schemes tailored for the segmentation task that provide the dense input allowing the demasking model to understand the scene but still satisfy the guarantees with respect to the adversarial patch. We summarize our contributions as follows: \u2022 We propose DEMASKED SMOOTHING which is the first (to the best of our knowledge) certified recovery or certified detection based defence against adversarial patch attacks on semantic segmentation models (Section 4). \u2022 DEMASKED SMOOTHING can do certified detection and recovery with any off-the-shelf segmentation model without requiring finetuning or any other adaptation. \u2022 We implement DEMASKED SMOOTHING, evaluate it for different certification objectives and masking schemes (Section 5). We can certify 63% of all pixels in certified detection for a 1% patch and 46% in certified recovery for a 0.5% patch for the BEiT-B (Bao et al., 2022) segmentation model on the ADE20K Zhou et al. (2017) dataset. 2 RELATED WORK: Certified recovery. The first certified recovery defence for classification models against patches was proposed by Chiang et al. (2020), who adapted interval-bound propagation (Gowal et al., 2019) to the patch threat model. Levine & Feizi (2020) proposed De-Randomized Smoothing (DRS), which provides significant accuracy improvement when compared to Chiang et al. (2020) and scales to the ImageNet dataset. In DRS, a base classifier is trained on images where everything but a small local region is masked (ablated). At inference time, a majority vote of all specified ablations is taken as the final classification. If this vote has a large enough margin to the runner-up class, the prediction cannot be shifted by any patch that does not exceed a pre-defined size. A similar approach was adopted in Randomized Cropping (Lin et al., 2021). A general drawback of these approaches is that the classifier needs to be trained to process masked/cropped inputs, which (in contrast to our work) prohibits the usage of arbitrary pretrained models. A further line of work studies network architectures that are particularly suited for certified recovery. For instance, models with small receptive fields such as BagNets (Brendel & Bethge, 2019) have been explored, either by combining them with some fixed postprocessing (Zhang et al., 2020; Xiang et al., 2021) or by training them end-to-end for certified recovery (Metzen & Yatsura, 2021). Salman et al. (2021) propose to apply DRS to Vision Transfomers (ViTs). In contrast to the aforementioned works, our Demasked Smoothing can be applied to models with arbitrary architecture. This is a property shared with PatchCleanser (Xiang et al., 2022a), which however is limited to image classification and it is not clear how it can be extended to semantic segmentation where a class needs to be assigned to every pixel including the masked ones. Certified recovery against patches has also been extended to object detection, specifically to defend against patch hiding attacks. Two notable works in this direction are DetectorGuard (Xiang & Mittal, 2021b), an extension of PatchGuard (Xiang et al., 2021) to object detection, and ObjectSeeker (Xiang et al., 2022b). Randomized smoothing (Cohen et al., 2019) has been applied to certify semantic segmentation models against `2-norm bounded adversarial attacks (Fischer et al., 2021). However, to the best of our knowledge, no certified defence against patch attacks for semantic segmentation has been proposed so far. Certified detection. An alternative to certified recovery is certified detection. Here, an adversarial patch is allowed to change the model prediction. However, if it succeeds in doing so, there is a mechanism that detects this attack certifiably with zero false negatives. Minority Reports (McCoyd et al., 2020) was the first certified detection method against patches, which is based on sliding a mask over the input in a way that ensures that there will be one mask position that completely hides the patch. PatchGuard++ (Xiang & Mittal, 2021b) is an extension of Minority Reports where the sliding mask is not applied on the input but on the feature maps of a BagNet-type feature extractor. This reduces inference time drastically since the feature extractor needs to be executed only once per input. ScaleCert (Han et al., 2021) tries to identify \u201csuperficial important neurons\u201d, which allows pruning the network in a way that the prediction needs to be made for fewer masked inputs. Lastly, PatchVeto (Huang & Li, 2021) is a recently proposed method for certified detection that is tailored towards ViT models. It implements masking by removing certain input patches of the ViT. In this work, we propose a novel method for certified detection in the semantic segmentation task that can be used for any pretrained model. Image reconstruction. The problem of learning to reconstruct the full image from inputs where parts have been masked out was pioneered by Vincent et al. (2010). It recently attracted attention as proxy task for self-supervised pre-training, especially for the ViTs (Bao et al., 2022; He et al., 2021). Recent approaches to this problem are using Fourier convolutions (Suvorov et al., 2022) and visual transformers (Dong et al., 2022). SPG-Net (Song et al., 2018) trains a subnetwork to reconstruct the full semantic segmentation directly from the masked input as a part of the image inpainting pipeline. In this work, we use the state-of-the-art ZITS (Dong et al., 2022) inpainting method. 3 PROBLEM SETUP:  3.1 SEMANTIC SEGMENTATION: In this work, we focus on the semantic segmentation task. Let X be a set of rectangular images. Let x \u2208 X be an image with height H , width W and the number of channels C. We denote Y to be a finite label set. The goal is to find the segmentation map s \u2208 YH\u00d7W for x. For each pixel xi,j , the corresponding label si,j denotes the class of the object to which xi,j belongs. We denote S to be a set of segmentation maps and f : X \u2192 S to be a segmentation model. 3.2 THREAT MODEL: Let us consider an untargeted adversarial patch attack on a segmentation model. Consider an image x \u2208 [0, 1]H\u00d7W\u00d7C and its ground truth segmentation map s. Assume that the attacker can modify an arbitrary rectangular region of the image x which has a size of H \u2032\u00d7W \u2032. We refer to this modification as a patch. Let l \u2208 {0, 1}H\u00d7W be a binary mask that defines the patch location in the image in which ones denote the pixels belonging to the patch. Let L be a set of all possible patch locations for a given image x. Let p \u2208 [0, 1]H\u00d7W\u00d7C be the modification itself. Then we define an operator A as A(x, p, l) = (1\u2212 l) x+ l p, where is element-wise product. The operator A applies the H \u2032 \u00d7W \u2032 subregion of p defined by a binary mask l to the image x while keeping the rest of the image unchanged. We denote P := [0, 1]H\u00d7W\u00d7C \u00d7L to be a set of all possible patch configurations (p, l) that define an H \u2032\u00d7W \u2032 patch. Let s \u2208 S be the ground truth segmentation for x. Let Q(f(x), s) be some quality metric such as global pixel accuracy or mean intersection over union (mIoU). The goal of an attacker is to find (p?, l?) s. t. (p?, l?) = argmin (p, l)\u2208P Q(f(A(x, p, l)), s) 3.3 DEFENCE OBJECTIVE: In this paper, we propose certified defences against patch attacks. It means that we certify against any possible attack from P including (p?, l?). We consider two robustness objectives. Certified recovery For a pixel xi,j our goal is to verify that the following statement is true \u2200 (p, l) \u2208 P : f(A(x, p, l))i,j = f(x)i,j (1) Certified detection We consider a verification function v defined on X such that v(x) \u2208 {0, 1}H\u00d7W . If v(x)i,j = 1, then the adversarial patch attack on xi,j can be detected by applying the function v to the attacked image x\u2032 = A(x, p, l). v(x)i,j = 1\u21d2 [ \u2200 (p, l) \u2208 P : v(A(x, p, l))i,j = 1\u2192 f(A(x, p, l))i,j = f(x)i,j ] (2) v(x\u2032)i,j = 0 means an alert on pixel x\u2032i,j . However, if x \u2032 is not an adversarial example, then this is a false alert. In that case the fraction of pixels for which we return false alert is called false alert ratio (FAR). The secondary objective is to keep FAR as small as possible. Depending on the objective our goal is to certify one of the conditions 1, 2 for each pixel xi,j . This provides us an upper bound on an attacker\u2019s effectiveness under any adversarial patch attack from P . 4 DEMASKED SMOOTHING: DEMASKED SMOOTHING (Figure 1c) consists of several steps. First, we apply a predefined set of masks with specific properties to the input image to obtain a set of masked images. Then we reconstruct the masked regions of each image based on the available information with an inpainting model g. After that we apply a segmentation model f to the demasked results. Finally, we aggregate the segmentation outcomes and make a conclusion for the original image with respect to the statements (1) or (2). 4.1 INPUT MASKING: Motivation. Like in previous work (Section 2) we apply masking patterns to the input image and use predictions on masked images to aggregate the robust result. If an adversarial patch is completely masked, it has no effect on further processing. However, in semantic segmentation, we predict not a single whole-image label like in the classification task, but a separate label for each pixel. Thus, making prediction on a masked image must allow us to predict the labels also for the masked pixels. Preliminaries. Consider an image x \u2208 [0, 1]H\u00d7W\u00d7C . We define \"\u2217\" to be a special masking symbol that does not correspond to any pixel value and has the property \u2200z \u2208 R : z\u00d7\u2217 = \u2217. Please note that \u2217 needs to be different from 0 since 0 is a valid pixel value in unmasked inputs. Let m \u2208 {\u2217, 1}H\u00d7W be a mask. We call the element-wise product x m a masking of x. In a masking, a subset of pixels becomes \u2217 and the rest remains unchanged. We consider the threat model P with patches of size H \u2032 \u00d7W \u2032 (Section 3.2). To define the structure of our masks, we break m into an array B of non-intersecting blocks, each having the same size H \u2032 \u00d7W \u2032 as the adversarial patch. We index the blocks as B[q, r], 1 \u2264 q \u2264 d HH\u2032 e, 1 \u2264 r \u2264 d W W \u2032 e. We say that the block B[q, r] is visible in a mask m if \u2200(i, j) \u2208 B[q, r] : mi,j = 1 Consider an array M of K masks. We define each mask M [k] by a set of blocks that are visible in it. For certified recovery, each block is visible in exactly one mask and masked in the others. We say that a mask m is affected by a patch (p, l) if A(x, p, l) m 6= x m. We define T (M) = max(p,l)\u2208P |{m \u2208M |A(x, p, l) m 6= x m}|. That is: T (M) is the largest number of masks affected by some patch. If M is defined, we refer to the value T (M) as T for simplicity. Certified recovery. We define column masking M for which T = 2. We assign every k-th block column to be visible in the mask M [k] (Figure 2c). Any (p, l) \u2208 P can intersect at most two adjacent columns since (p, l) has the same width as a column. Thus, it can affect at most two masks (Figure 2b). A similar scheme can be proposed for the rows. Due to the block size in B, the patch (p, l) cannot intersect more than four blocks at once. We define a mask set that we call 3-mask s. t. for any four adjacent blocks two are visible in the same mask (Figures 2d). Hence, a patch for 3-mask can affect no more than 3 masks, T = 3. To achieve T = 4 any assignment of visible blocks to the masks works. We consider 4-mask that allows uniform coverage of the visible blocks in the image (Figure 2f). See details on masking schemes in Appendix B. Certified detection. We define Md to be a set of masks for certified detection (we use subscript d for distinction). Md should have the property: \u2200 (p, l) \u2208 P \u2203m \u2208Md : A(x, p, l) m = x m i. e. for every patch exists at least one mask not affected by this patch. For a patch of size H \u2032 \u00d7W \u2032 we consider K =W \u2212W \u2032 + 1 masks such that the mask Md[k] masks a column of width W \u2032 starting at the horizontal position k in the image (Figure 2h). To obtain the guarantee for the same P with a smaller K, we consider a set of strided columns of width W \u2032\u2032 \u2265W \u2032 and stride W \u2032\u2032 \u2212W \u2032 + 1 that also satisfy the condition (see the proof adapted from Xiang et al. (2022a) in Appendix A). A similar scheme can be proposed for the rows (Figure 2i). Alternatively, we could use a set of block masks of size H \u2032 \u00d7W \u2032. Then the number of masks grows quadratically with the image resolution. Hence, in the experiments we focus on the column and the row masking schemes. Let g be a demasking model, g(x m) \u2208 [0, 1]H\u00d7W\u00d7C . The goal of g is to make the reconstruction g(x m) as close as possible (in some metric) to the original image x. For a segmentation model f we define a segmentation array S(M,x, g, f), S[k] := f(g(x M [k])), 1 \u2264 k \u2264 K. 4.2 CERTIFICATION: Certified recovery. For the threat model P consider a set M of K masks. We define a function h : X \u2192 S that assigns a class to the pixel xi,j via majority voting over class predictions of each Algorithm 1 Demasked Smoothing Input: image x \u2208 [0, 1]H\u00d7W\u00d7C , patch size (H \u2032,W \u2032), certification type CT (recovery or detection), mask type MT (column, row, 3-mask, 4-mask), inpainting model g, segmentation model f Output: segmentation map h \u2208 YH\u00d7W , certification (or verification) map v \u2208 {0, 1}H\u00d7W 1: M \u2190 CreateMaskArray(H,W,H \u2032,W \u2032,CT,MT) . according to section 4.1 2: for k \u2190 1, . . . , |M | do . this loop can be paralellized 3: S[k]\u2190 f(g(x M [k])) . mask input, inpaint the masked regions, and apply segmentation 4: end for 5: if CT = \u2019recovery\u2019 then h\u2190 MajorityVote(S) . vote over the classes predicted for each pixel 6: else h\u2190 f(x) . in detection case, output clean segmentation 7: end if 8: v \u2190 AllEqual(S, h) . assign 1 for the pixels where all S[k] agree with h and 0 otherwise 9: Return h, v reconstructed segmentation in S. A class for the pixel that is predicted by the largest number of segmentations is assigned. We break the ties by assigning a class with a smaller index. Theorem 1. If the number of masks K satisfies K \u2265 2T (M) + 1 and for a pixel xi,j we have \u2200 S[k] \u2208 S : S[k]i,j = h(x)i,j (i.e. all the votes agree), then \u2200 (p, l) \u2208 P : h(A(x, p, l))i,j = h(x)i,j . See the proof in Appendix A. Certified detection. Consider Md = {Md[k]}Kk=1. For a set of demasked segmentations S we define the verification map v(x)i,j := [f(x)i,j = S[1]i,j = . . . = S[K]i,j ] i.e. the original segmentation is equal to all the other segmentations on masked-demasked inputs, including the one in which the potential patch was completely masked. Theorem 2. Assume that v(x)i,j = 1. Then \u2200 (p, l) \u2208 P : v(A(x, p, l))i,j = 1\u21d2 f(A(x, p, l))i,j = f(x)i,j See the proof in Appendix A. For a given image x the verification map v(x) is complementary to the model segmentation output f(x) that stays unchanged. Thus, there is no drop in clean performance however we may have some false positive alerts in the verification map v in the clean setting. We present the Demasked Smoothing procedure in Algorithm 1. 5 EXPERIMENTS: In this section, we evaluate DEMASKED SMOOTHING with the masking schemes proposed in Section 4, compare our approach with the direct application of Derandomized Smoothing Levine & Feizi (2020) to the segmentation task and evaluate the performance on different datasets and models. Certified recovery and certified detection provide certificates of different strength (Section 4) which are not comparable. We evaluate them separately for different patch sizes. 5.1 EXPERIMENTAL SETUP: We evaluate DEMASKED SMOOTHING on two challenging semantic segmentation datasets: ADE20K (Zhou et al., 2017) (150 classes, 2000 validation images) and COCO-Stuff-10K (Caesar et al., 2018) (171 classes, 1000 validation images). For demasking we use the ZITS Dong et al. (2022) inpainting model with the checkpoint provided in the official paper repository 1. The model was trained on Places2 (Zhou et al., 2016) dataset with images resized to 256\u00d7256. As a segmentation model f we use BEiT-B Bao et al. (2022), Swin Liu et al. (2021), PSPNet Zhao et al. (2017) and DeepLab v3 (Chen et al., 2018). We note that the first two models are based on transformers and obtain near stateof-the-art results. PSPNet and DeepLab v3 are CNN-based segmentation methods that we consider to demonstrate that DEMASKED SMOOTHING is not specific to transformer-based architectures. We 1https://github.com/DQiaole/ZITS_inpainting use the model implementations provided in the mmsegmentation framework Contributors (2020). An illustration of the image reconstruction and respective segmentation can be found in Figure 3. We run the evaluation in parallel on 5 Nvidia Tesla V100-32GB GPUs. The certification for the whole ADE20K validation set with ZITS and BEiT-B takes around 1.2 hours for certified recovery and 2 hours for certified detection (due to a larger number of masks). 5.2 EVALUATION METRICS: For both certified recovery and certified detection, we generate a standard segmentation output (without any abstention) and a corresponding certification map (Figure 4). In case of certified detection, the segmentation output remains the same as for the original segmentation model, however, there may be false alerts in the certificaton map. For the certified recovery, the output is obtained by a majority vote over the segmentations of demasked images (Section 4.2). We evaluate the mean intersection over union (mIoU) for these outputs. The certification map is obtained by assigning to each certified pixel the corresponding class from the segmentation output and assigning a special uncertified label to all non-certified pixels. For each image we evaluate the fraction of pixels which are certified and correct (coincide with the ground truth). %C is a mean of these fractions over all the images in the dataset. In semantic segmentation task, the class frequencies are usually skewed, therefore global pixel-wise accuracy alone is an insufficient metric. Matching the certification map separately for each class y \u2208 Y with the ground truth segmentation for y in the image x allows us to compute the guaranteed lower bound (cTPy(x)) on the number of true positive pixel predictions (TPy(x)) i.e. those that were correctly classified into y. If a pixel was certified with a correct class, then this prediction cannot be changed by a patch (or, alternatively, the change will be detected by the verification function v in certified detection). We consider recall Ry(x) = TPy(x) TPy(x)+FNy(x) where FNy(x) is the number of false negative predictions for y in x. Py(x) = TPy(x) + FNy(x) is the total area of y in the ground truth and does not depend on our prediction. We can evaluate certified recall cRy(x) = cTPy(x) Py(x) , a lower bound on the recall Ry(x). Total recall and certified total recall of class y in a dataset D are TRy(D) =\u2211 x\u2208D TPy(x)\u2211 x\u2208D Py(x) and cTRy(D) = \u2211 x\u2208D cTPy(x)\u2211 x\u2208D Py(x) respectively. Then, we obtain mean recall mR(D) = 1 |Y| \u2211 y\u2208Y TRy(D) and certified mean recall cmR(D) = 1 |Y| \u2211 y\u2208Y cTRy(D). Evaluating lower bounds on other popular metrics such as mean precision or mIoU this way results in vacuous upper bound since they depend on the upper bound on false positive (FP ) predictions. For the pixels that are not certified we cannot guarantee that they will not be assigned to a certain class, therefore, a non-trivial upper bound on FP is not straightforward. We leave this direction for future work. In certified detection, we additionally consider false alert ratio (FAR) which is the fraction of correctly classified pixels for which we return an alert on a clean image. Smaller FAR is preferable. Due to our threat model, certifying small objects in the scene can be difficult because they can be partially or completely covered by an adversarial patch in a way that there is not chance to recover the prediction. To provide an additional perspective on our methods, we also evaluate mR and cmR specifically for the \u201cbig\u201d classes, which occupy on average more than 20% of the images in which they appear. These are, for example, road, building, train, and sky, which are important for understanding the scene. The full list of such classes for each dataset is provided in the Appendix H. 5.3 DISCUSSION: In Table 1, we compare different masking schemes proposed in Section 4.1. Evaluation of all the models with all the masking schemes is consistent with these results and can be found in Appendix E. We see that column masking achieves better results in both certification modes. Effectiveness of column masking for classification task was also empirically observed by Levine & Feizi (2020). We attribute the effectiveness of column masking to the fact most of the images in the datasets have a clear horizont line, therefore having a visible column provides a slice of the image that intersects most of the scene background objects. In Table 2, we extend Derandomized Smoothing (DRS) proposed by Levine & Feizi (2020) for certified recovery in classification to the segmentation task and compare it to our method Direct adaptation requires training a model that is able to predict the full image segmentation from a small visible region. Since it is not clear what architectural design and training procedure would be needed for that, we consider two alternative baselines. DRS-S predicts the segmentation directly from the masked image and DRS-E uses our inpainting method to first reconstruct the image and then obtain the segmentation. See the implementation details in Appendix G. DRS with column smoothing performs poorly on the segmentation task, which emphasizes the need for specific masking schemes. In Table 3, we evaluate our method with column masking on different models. For certified detection we can certify more than 60% of the pixels with all models on ADE20K and more than 46 % on COCO10K. False alert ratio on correctly classified pixels is around 20%. In certified recovery, we certify more than 44% pixels on ADE20K and more than 28% pixels on COCO10K. Figure 5 shows how the performance of DEMASKED SMOOTHING depends on the patch size for the BEiT-B model. We see that certified detection metrics remain high even for a patch as big as 5% of the image surface and for the recovery they slowly deteriorate as we increase the patch size to 2%. Ablations with respect to inpainting can be found in Appendix F. DEMASKEDSMOOTHING illustrations procedure are provided in Appendix K. 6 CONCLUSION: In this work, we propose DEMASKED SMOOTHING, the first (up to our knowledge) certified defence framework against patch attacks on segmentation models. Due to its novel design based on masking schemes and image demasking, DEMASKED SMOOTHING is compatible with any segmentation model and can on average certify 63% of the pixel predictions for a 1% patch in the detection task and 46% against a 0.5% patch for the recovery task on the ADE20K dataset. 7 ETHICS STATEMENT: This work contributes to the field of certified defences against physically-realizable adversarial attacks. The proposed approach allows to certify robustness of safety-critical applications such as medical imaging or autonomous driving. The defence might be used to improve robustness of systems used for malicious purposes such as (semi-)autonomous weaponry or unauthorized surveillance. This danger may be mitigated e.g. by using a system of sparsely distributed patches which makes certifying the image more challenging. All activities in our organization are carbon neutral, so the experiments performed on our GPUs do not leave any carbon dioxide footprint. 8 REPRODUCIBILITY STATEMENT: We provide the details of our experimental setup in Section 5.1. We discuss the evaluation metrics and their computation in Section 5.2. A PROOFS (SECTION 4): In this section, we provide the proofs for the theorems stated in Section 4. Lemma 1. (Section 4.1) Consider an image of the size H \u00d7W . Let H \u2032 \u00d7W \u2032 be a fixed adversarial patch size. Let Md(K,L) be a set of masks where each mask is masking an H \u00d7W \u2032\u2032 vertical column, W \u2032\u2032 \u2265 W \u2032. Let the stride between the columns in two adjacent masks be W \u2032\u2032 \u2212W \u2032 + 1. Then for any location l \u2208 L of the patch, there exists a mask that covers it completely. Proof. (Adapted from the proof of Lemma 4 in PatchCleanser Xiang et al. (2022a)). Without loss of generality, we consider the first two adjacent column masks. The first one covers the columns from 1 to W \u2032\u2032. The second mask covers the columns from 1 + (W \u2032\u2032 \u2212W \u2032 + 1) =W \u2032\u2032 \u2212W \u2032 + 2 to (W \u2032\u2032 \u2212W \u2032 + 2) + (W \u2032\u2032 \u2212 1) = 2W \u2032\u2032 \u2212W \u2032 + 1 (See Figure 6). Now consider an adversarial patch of size H \u2032 \u00d7W \u2032. Let us find the smallest possible start index of this patch so that it does not get covered by the first mask. For that it should be visible at the column W \u2032\u2032 + 1 and, therefore, start at the column with index not smaller than (W \u2032\u2032 + 1)\u2212W \u2032 + 1 =W \u2032\u2032 \u2212W \u2032 + 2. However, it is the same column in which second mask starts. Therefore, given that W \u2032\u2032 \u2265W \u2032 we have that the patch is completely masked by the second mask. Then for a patch which is only partially masked by the second mask from the left we use an analogous argument to show that it is completely masked by the third mask and so on. Certified recovery. For the threat model P (Section 3) consider a set M of K masks. We define a function h : X \u2192 S that assigns a class to the pixel xi,j via majority voting over class predictions of each reconstructed segmentation in S. A class for the pixel that is predicted by the largest number of segmentations is assigned. We break the ties by assigning a class with a smaller index. Theorem 1. (Section 4.2) If the number of masks K satisfies K \u2265 2T (M) + 1 and for a pixel xi,j we have \u2200 S[k] \u2208 S : S[k]i,j = h(x)i,j i.e. all the votes agree, then \u2200 (p, l) \u2208 P : h(A(x, p, l))i,j = h(x)i,j . Proof. Assume that \u2203 (p, l) \u2208 P : h(A(x, p, l))i,j 6= h(x)i,j Let us denote x\u2032 := A(x, p, l) and S\u2032 to be the segmentation array for x\u2032. Then the class h(x)i,j did not get the majority vote for S\u2032. However, by definition of T (M) we know that (p, l) could affect at most T (M) segmentations. Since all K segmentations of S have voted for h(x)i,j , then at least K \u2212 T > K2 of them are still voting for h(x)i,j in S \u2032 meaning that h(x)i,j still has the majority vote in S\u2032. Therefore h(x\u2032)i,j = h(x)i,j Certified detection. Consider Md = {Md[k]}Kk=1. For a set of demasked segmentations S we define the verification map v(x)i,j := [f(x)i,j = S[1]i,j = . . . = S[K]i,j ] i.e. the original segmentation coincides with all the other segmentations including the one in which the potential patch was completely masked. Theorem 2. (Section 4.2) Assume that v(x)i,j = 1. Then \u2200 (p, l) \u2208 P : v(A(x, p, l))i,j = 1\u21d2 f(A(x, p, l))i,j = f(x)i,j Proof. Assume that \u2203 (p, l) \u2208 P s. t. v(A(x, p, l))i,j = 1 and f(A(x, p, l))i,j 6= f(x)i,j .Let us denote x\u2032 := A(x, p, l) and S\u2032 to be the segmentation set for x\u2032. By definition of Md, \u2203Md[k] \u2208Md s. t. Md[k] masks the patch (p, l) Hence, g(x Md[k]) = g(x\u2032 Md[k]), S[k] = f(g(x Md[k])) = f(g(x\u2032 Md[k])) = S\u2032[k], Since v(x)i,j = 1, we have f(x)i,j = S[k]i,j . Since v(x\u2032)i,j = 1, we have f(x\u2032)i,j = S\u2032[k]i,j . Thus, f(x\u2032)i,j = f(x)i,j . B DETAILED DESCRIPTION OF MASKING STRATEGIES: In this section, we provide additional details for constructing certified recovery masks proposed in Section 4.1. We define mask sets M that satisfy different values of T . We divide the image x into a set of non-intersecting blocks B of the same size as an adversarial patch, H \u2032 \u00d7W \u2032 (see Figure 7), 1 \u2264 q \u2264 dH/H \u2032e, 1 \u2264 r \u2264 dW/W \u2032e. In each mask, each of these blocks will be either masked or not masked (i. e. visible). Moreover, for each block there exists only one mask in which it is visible. For a set M of K masks we define the mapping \u00b5M : B \u2192 {1, . . . ,K}. If \u00b5(B[q, r]) = k, then B[q, r] is not masked in M [k]. Therefore, each mask M [k] is defined by a Bk \u2282 B s. t. for b \u2208 Bk \u00b5(b) = k. We define a set M that we call 3-mask for which T (M) = 3. We assign the blocks in each row to the masks as follows: \u00b5(B[1, 1]) = 1; \u00b5(B[1, 2]) = \u00b5(B[1, 3]) = 2; \u00b5(B[1, 4]) = \u00b5(B[1, 5]) = 3 and so on until we reach the end of the row. If we finish the first row with the value k, then we start the second row as follows \u00b5(B[2, 1]) = \u00b5(B[2, 2]) = k + 1; \u00b5(B[2, 3]) = \u00b5(B[2, 4]) = k + 2: . . .. If we finish the second row on n, we start the third row similarly to the first: \u00b5(B[3, 1]) = n+1; \u00b5(B[3, 2]) = \u00b5(B[3, 3]) = n + 2; . . . When we reach the number K, we start from 1 again (Figure 7d). Due to the block size, the patch cannot intersect more than four blocks at once. Our parity-alternating block sequence ensures that in any such intersection of four blocks either the top ones or the bottom ones will belong to the same masking, so at most three different maskings can be affected. We define a set M that we call 4-mask for which T (M) = 4. Due to our block size any assignment of masks will work because the patch cannot intersect more than four blocks. We consider the one that allows uniform distribution of the unmasked blocks (Figure 7g). We point out that for the described methods each masking keeps approximately 1/K of the pixels visible and the unmasked regions are uniformly distributed in the image. This means that for any masked pixel there exists an unmasked region located close enough to this pixel. It is the core difference between our masks and the ones proposed for certified classification such as block or column smoothing Levine & Feizi (2020). It was observed that the image demasking is facilitated when the visible regions are uniformly spread in the masked image He et al. (2021). C TEST-TIME INPUT CERTIFICATION: In this section, we discuss how certified recovery (Theorem 1) can be applied to guaranteed verification of the robustness on a test image. We also discuss how robustness guarantees for the test-time images can be evaluated by using a dataset of clean images such as ADE20K (Zhou et al., 2017) or COCOStuff-10K (Caesar et al., 2018). C.1 TEST-TIME CERTIFIED RECOVERY Let x\u2032 be a test-time input which can be either a clean image or an image attacked with an adversarial patch. We know that there exists a clean image x corresponding to x\u2032 which removes the patch if it is present. We have either x\u2032 = x or x\u2032 \u2208 A(x), where A(x) := {A(x, p, l) | (p, l) \u2208 P}. However, at test time we do not have access to the clean image x. Our goal is to certify that for our segmentation model h and a pixel xi,j we have h(x\u2032)i,j = h(x)i,j . We can achieve this result by applying the recovery certification (Theorem 1) to the test-time image. It allows us to verify whether \u2200 (p, l) \u2208 P : h(A(x\u2032, p, l))i,j = h(x\u2032)i,j . We also know that if x\u2032 \u2208 A(x), then x \u2208 A(x\u2032) (Figure 8a). Indeed, if x\u2032 is only different from x by one patch, then x can be be obtained from x\u2032 by removing this patch. Therefore, by obtaining the guarantee for A(x\u2032), we implicitly obtain the guarantee also for the image x even though we do not have direct access to it. We note that this test-time guarantee is only possible for certified recovery. In certified detection, we would need to evaluate the verification function v (Theorem 2) for both the clean image x and the attacked image x\u2032 to obtain the result. This cannot be done if x is implicit. C.2 ROBUSTNESS GUARANTEES EVALUATION The typical certified robust error for a given test data set (and pixel (i, j) in the segmentation case) is an estimate for EX\u223cD[ max (p,l)\u2208P 1h(A(X,p,l))i,j 6=h(X)i,j ], where D is the data generating probability measure and we assume that our test set to be an i.i.d. sample of it. This is the expected robust error (worst case over our threat model P for clean inputs) for a given pixel (i, j). Using the test sample to get an estimate of this quantity, we get a probabilistic guarantee that the corresponding pixel (i, j) of a new clean test sample x\u2032 drawn i.i.d. from P will have its whole \u201cpatch\u201d-neighborhood certified. However, more important for a practical security analysis is that we can certify a given instance, which can be even potentially adversarially perturbed. Formally, this means that for an input z \u2208 A(x), where x \u223c P is an unknown sample from P , that we guarantee \u2200(p, l) \u2208 P : h(A(z, p, l))i,j = h(z)i,j , and as x \u2208 A(z, p, l) this implies that we certify that the pixel (i, j) of the potentially manipulated image is classified the same as pixel (i, j) of the unperturbed image x. However, it is now tricky to get even a probabilistic estimate of the quantity Ex\u223cD max (p,l)\u2208P [ max (q,m)\u2208P 1h(A(A(x,p,l),q,m))i,j=h(A(x,p,l))i,j ], as the outer maximization process cannot be simply simulated by doing adversarial patch attacks on a clean test dataset. We propose a way to evaluate a guaranteed lower bound on the fraction of certified test-time inputs by using a dataset of clean images. Instead of considering a standard one-patch neighbourhood A(x) defined by our threat model (Section 3.2), we propose to consider a neighbourhood A2(x) of two independent patches (Figure 8b). A2(x) contains all the images x\u2032 \u2208 A(x) as well as their respective patch neighbourhoods A(x\u2032). Therefore, by verifying that \u2200 (p1, l1), (p2, l2) \u2208 P : h(A(A(x, p1, l1), p2, l2))i,j = h(x)i,j , we guarantee that \u2200 x\u2032 \u2208 A(x) \u2200 (p, l) \u2208 P : h(A(x\u2032, p, l))i,j = h(x \u2032)i,j . We note that corresponding reasoning could be applied to certification in `p models. Then A2(x) would correspond to doubling the radius of the -ball instead of adding a second patch. Note that Theorem 1 can be directly extended to a threat model of N patches. In the worst case each of the N patches can affect T different maskings. Therefore, we need to change the condition of Theorem 1 to K \u2265 2NT + 1. We apply the described method to evaluating the test-time certification guarantees for a toy example of a 0.1% patch in Table 4. We also illustrate how a column mask looks in this case in Figure 8. D ADVERSARIAL PATCH EXAMPLE: In this section, we demonstrate an example of a real adversarial patch for a semantic segmentation model similar to the one illustrated in the Figure 1a and show how it is handled by our certified defences. We illustrate it for the Swin (Liu et al., 2021) model on one of the images from the ADE20K (Zhou et al., 2017) dataset. D.1 PATCH OPTIMIZATION We set the patch size to 1% of the image surface. We select a fixed position for a patch on the rear window of a car (Figure 9a). For each pixel we extract a list of predicted logits corresponding to each class and apply multi-margin loss with respect to the ground truth label of the respective pixel. We use random patch initialization without restarts. As an optimizer we use projected gradient descent (PGD) with 1000 steps and initial step size of 0.01. We use cosine step size schedule and momentum for the gradient with the rate of 0.9. The optimization plot and the patch efficiency at different iterations of the PGD are illustrated in the Figure 9. D.2 CERTIFIED RECOVERY We denote the original image as x and the patched image as x\u2032. The voting-based segmentation function h (Section 4.2) provides the majority-vote prediction h(x) and the corresponding certification map which shows the pixels where all the votes agree. In Figure we see that a part of the building and the road is certified which means that this prediction cannot be affected by an adversarial patch. Figure demonstrates h(x\u2032) which correctly segments those regions in presence of an adversarial patch that fools the original model. D.3 CERTIFIED DETECTION We perform our analysis by evaluating the verification map v (Section 4.2) for the original image x and for the patched image x\u2032. We see that in v(x) a major part of the building is certified i. e. for a part of pixels xi,j that belong to the building and the road we have v(x)i,j = 1. However, v(x\u2032)i,j = 0 for those pixels. It means that we have detected that the prediction on this input is potentially affected by an adversarial patch. E ADDITIONAL EXPERIMENTS: In Tables 5 and 6, we provide additional experimental results for evaluating different masking schemes proposed in Section 4.1 on different models. F INPAINTING ABLATION STUDIES We perform ablation studies with respect to the demasking step. The results are in Table 7. Figure 12 provides additional illustrations. As can be seen from the results, our method heavily benefits from having available stronger inpainting models that allow achieving better clean and certified accuracy. We consider this property actually as a strength of our method since it will automatically benefit from future research and developments of stronger inpainting methods. For certified recovery, we also compare it to GIN Li et al. (2020) based on a generative model that we trained on ADE20K (without using style losses based on ImageNet trained VGG). The results are in Table 8. Illustrations can be found in Figure 13. G COMPARISON TO SIMPLIFIED DERANDOMIZED SMOOTHING: Derandomized Smoothing (DRS) Levine & Feizi (2020) was proposed for certified recovery, therefore in this section we focus on this task. Direct adaptation of derandomized smoothing to semantic segmentation task requires training a model that is able to predict the full image segmentation from a small visible region. Since it is not immediately clear to us what architectural design and training procedure would be needed to train such a model, we consider a simplified version of DRS that we call DRS-S. In this version, we consider an off-the-shelf semantic segmentation model and evaluate how it performs with column masking from DRS. Therefore, we do not encode the masked regions with the special \u2019NULL\u2019 value like in DRS but use black color instead. That is because an off-the-shelf model cannot work with \u2019NULL\u2019 values. We run our experiments on ADE20K dataset. We consider the DRS parameters from the recent SOTA version of Derandomized Smoothing by Salman et al. Salman et al. (2021). They use column width b = 19 and stride s = 10 for certified classification of 224x224 ImageNet images. To account for the fact that ADE20K images have larger resolution than ImageNet, we scale the parameters to column width b = 42 and stride s = 22. To make the comparison consistent with the rest of our results, we use the patch occupying 0.5% of the image. From Table 9 we can see that DRS-S performs poorly on semantic segmentation task. The reason for that is illustrated in Figure 14. Processing the column region in 14c would probably be sufficient for a classification model to classify the image into the class \"house\". But it is clearly not sufficient to reconstruct the whole segmentation map 14e as can be seen in the Figure 14g. Whether doing this would be possible with a model specifically trained to reconstruct the segmentation map from a very small visible region is an open research question (up to our knowledge). We point out that the value %C of certified and correctly classified pixels in the Table 9 is still surprisingly high for DRS-S compared to other metircs. We attribute this to the fact that the solid black regions are usually treated as a wall by the segmentation model, therefore the images are usually segmented as a wall by the DRS majority voting. And the wall is a common part of both indoor and outdoor scenes in ADE20K as can be implied from the Table 10 of \"big\" ADE20K classes. Therefore, always classifying the output as a wall provides a decent fraction of correctly classified pixels because of the skewed classes. However, to provide a better comparison with DRS, we emulate the model which is able to reconstruct the whole segmentation map from the column masking proposed in DRS. We do this by applying the demasking approach proposed in this work. We first try to reconstruct the whole image from one column and then segment it with an off-the-shelf model as we did with the masks proposed in this paper. We call this approach DRS-E and the results can be found in Table 9. H A LIST OF BIG CLASSES: In Section 5.2 we suggest another perspective on the evaluation of our DEMASKED SMOOTHING by specifically considering its performance on \u201cbig\u201d semantic classes. The object of these classes occupy on average more than 20% of the images in which they appear. Correctly segmenting these classes is important for understanding the scene. In Tables 10 and 11 we provide the full list of such classes in ADE20K (Zhou et al., 2017) and COCO-Stuff-10K (Caesar et al., 2018) respectively together with the average fraction of pixels that they occupy in the images in which they are present. We point out that for COCO-Stuff-10K some typically smaller classes such as \u201csandwich\u201d or \u201cfruit\u201d get included in the list of big classes because of the macro-scale images in which they occupy a big part of the scene. I COMPLEXITY ANALYSIS AND PARALLELIZATION: In DEMASKED SMOOTHING, we propose a set of K masks that are applied to the original image (denote the cost of applying a single masking by M ). As illustrated in Figure 1c, the masked images are demasked (denote the cost of demasking an image by D) and segmented (denote the cost of segmenting an image by S); thereupon per-mask segmentations are aggregated into a final segmentation and certification (cost of aggregation proportional to K). Asymptotically, compute grows thus with O(K(M +D + S) +K) while the cost of a standard segmentation is O(S). Thus, for large K or M +D S, real-time applicability would actually be impractical. However, we note that: 1. M +D is roughly of the same size as S for typical DL-based inpainting and segmentation models. 2. For certified recovery, we operate in a setting where K is small (K \u2208 5, 7, 9) and does not grow with the image resolution. This is unlike Derandomized Smoothing and its derivatives, where the number of masks in the recovery task grows with the image resolution (or randomized smoothing with thousands of samples per input). This small value of K benefits our the method in time-sensitive applications. For certified detection, we can adjust the number of masks for the computational speed by using strided masking as suggested in Section 4.1. 3. Morover, masking, demasking, and segmenting for different masks do not use any shared data and can thus be fully parallelized if sufficiently powerful hardware is available. Only the aggregation step requires the results of all the previous stages. However, aggregation time is small compared to the other stages. In terms of latency, a fully parallelized version of our procedure would thus have a latency proportional to O(M +D+ S +K). For small K and M +D \u2248 S, application to real-time video can be facilitated by means of parallelization. J USED DATA: In this work, we only use the datasets published under formal licenses: ADE20K (Zhou et al., 2017) and COCO-Stuff-10K (Caesar et al., 2018). To the best of our knowledge, data used in this project do not contain any personally identifiable information or offensive content. The models ZITS (Dong et al., 2022) and Swin (Liu et al., 2021) are published under Apache-2.0 license. The text of the license for PSPNet (Zhao et al., 2017) can be found here: https://github.com/hszhao/ PSPNet/blob/master/LICENSE K DEMASKED SMOOTHING VISUALIZATION: In this section, we provide additional illustrations of our method (Figures 15, 16, 17, 18). Similarly to the Table 1 we certify against a 1% patch for the detection task and against 0.5% patch for the recovery task. For each mask type we illustrate all the stages summarized in the Figure 1c. We also provide examples of certification maps for certified recovery and certified detection with different images (Figure 19, 20)."}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetBudget'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'directory': '.'}, name='ListFiles'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Budget retrieved successfully\",\n  \"output\": {\n    \"total_resource_budget\": 100,\n    \"current_resource_usage\": 0,\n    \"current_remaining_resource_budget\": 100,\n    \"total_expense_budget\": 10000,\n    \"current_expense\": 0.14715504999999998,\n    \"current_remaining_expense_budget\": 9999.85284495\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetBudget` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Files listed successfully\",\n  \"error\": null,\n  \"output\": [\n    \"favicon.ico\",\n    \"requirements.txt\",\n    \"HASHIRU_LOGO.png\",\n    \"HASHIRU_ARCH.png\",\n    \"deleteAgents.py\",\n    \"CITATION.cff\",\n    \"tests\",\n    \"__pycache__\",\n    \"start.py\",\n    \"README.md\",\n    \"results\",\n    \"HASHIRU.png\",\n    \".gitignore\",\n    \".env\",\n    \"static\",\n    \".gitattributes\",\n    \"app.py\",\n    \"HASHIRU_2.png\",\n    \".git\",\n    \"src\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ListFiles` with \n```json\n{\n  \"directory\": \".\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetBudget', response={'result': {'status': 'success', 'message': 'Budget retrieved successfully', 'output': {'total_resource_budget': 100, 'current_resource_usage': 0, 'current_remaining_resource_budget': 100, 'total_expense_budget': 10000, 'current_expense': 0.14715504999999998, 'current_remaining_expense_budget': 9999.85284495}}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ListFiles', response={'result': {'status': 'success', 'message': 'Files listed successfully', 'error': None, 'output': ['favicon.ico', 'requirements.txt', 'HASHIRU_LOGO.png', 'HASHIRU_ARCH.png', 'deleteAgents.py', 'CITATION.cff', 'tests', '__pycache__', 'start.py', 'README.md', 'results', 'HASHIRU.png', '.gitignore', '.env', 'static', '.gitattributes', 'app.py', 'HASHIRU_2.png', '.git', 'src']}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'website': 'iclr.cc', 'query': 'ICLR 2023 acceptance rate'}, name='GoogleSearchTool'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'website': 'iclr.cc', 'query': 'ICLR 2023 review criteria'}, name='GoogleSearchTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": [\n    \"https://media.iclr.cc/Conferences/ICLR2023/ICLR2023-Fact_Sheet.pdf\",\n    \"https://media.iclr.cc/Conferences/ICLR2024/ICLR2024-Fact_Sheet.pdf\",\n    \"https://blog.iclr.cc/category/iclr-2023/\",\n    \"https://iclr.cc/virtual/2023/papers.html\",\n    \"https://iclr.cc/\",\n    \"https://blog.iclr.cc/2022/12/21/announcing-the-accepted-workshops-at-iclr-2023/\",\n    \"https://blog.iclr.cc/2023/11/02/tiny-papers-strike-back-2023-reflections-and-2024-announcement/\",\n    \"https://media.iclr.cc/Conferences/ICLR2024/ICLR2024_Press_Release.pdf\",\n    \"https://media.iclr.cc/Conferences/ICLR2025/ICLR2025_Fact_Sheet.pdf\",\n    \"https://iclr.cc/Conferences/2023/Pricing\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `GoogleSearchTool` with \n```json\n{\n  \"website\": \"iclr.cc\",\n  \"query\": \"ICLR 2023 acceptance rate\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": [\n    \"https://iclr.cc/Conferences/2023/ReviewerGuide\",\n    \"https://blog.iclr.cc/2023/04/12/ethics-review-process-for-iclr-2023/\",\n    \"https://blog.iclr.cc/category/iclr-2023/\",\n    \"https://blog.iclr.cc/tag/reviewing/\",\n    \"https://blog.iclr.cc/tag/awards/\",\n    \"https://blog.iclr.cc/2023/04/05/announcing-notable-reviewers-and-area-chairs-at-iclr-2023/\",\n    \"https://iclr.cc/Conferences/2023/CallForTinyPapers\",\n    \"https://iclr.cc/Conferences/2025/FAQ\",\n    \"https://iclr.cc/Conferences/2019/Reviewer_Guidelines\",\n    \"https://iclr.cc/Conferences/2025/ACGuide\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `GoogleSearchTool` with \n```json\n{\n  \"website\": \"iclr.cc\",\n  \"query\": \"ICLR 2023 review criteria\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GoogleSearchTool', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': ['https://media.iclr.cc/Conferences/ICLR2023/ICLR2023-Fact_Sheet.pdf', 'https://media.iclr.cc/Conferences/ICLR2024/ICLR2024-Fact_Sheet.pdf', 'https://blog.iclr.cc/category/iclr-2023/', 'https://iclr.cc/virtual/2023/papers.html', 'https://iclr.cc/', 'https://blog.iclr.cc/2022/12/21/announcing-the-accepted-workshops-at-iclr-2023/', 'https://blog.iclr.cc/2023/11/02/tiny-papers-strike-back-2023-reflections-and-2024-announcement/', 'https://media.iclr.cc/Conferences/ICLR2024/ICLR2024_Press_Release.pdf', 'https://media.iclr.cc/Conferences/ICLR2025/ICLR2025_Fact_Sheet.pdf', 'https://iclr.cc/Conferences/2023/Pricing']}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GoogleSearchTool', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': ['https://iclr.cc/Conferences/2023/ReviewerGuide', 'https://blog.iclr.cc/2023/04/12/ethics-review-process-for-iclr-2023/', 'https://blog.iclr.cc/category/iclr-2023/', 'https://blog.iclr.cc/tag/reviewing/', 'https://blog.iclr.cc/tag/awards/', 'https://blog.iclr.cc/2023/04/05/announcing-notable-reviewers-and-area-chairs-at-iclr-2023/', 'https://iclr.cc/Conferences/2023/CallForTinyPapers', 'https://iclr.cc/Conferences/2025/FAQ', 'https://iclr.cc/Conferences/2019/Reviewer_Guidelines', 'https://iclr.cc/Conferences/2025/ACGuide']}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'max_results': 5, 'query': 'certified defense adversarial patch semantic segmentation'}, name='ArxivTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Found 5 paper(s) on arXiv\",\n  \"output\": [\n    {\n      \"title\": \"Towards Practical Certifiable Patch Defense with Vision Transformer\",\n      \"authors\": [\n        \"Zhaoyu Chen\",\n        \"Bo Li\",\n        \"Jianghe Xu\",\n        \"Shuang Wu\",\n        \"Shouhong Ding\",\n        \"Wenqiang Zhang\"\n      ],\n      \"published\": \"2022-03-16T10:39:18+00:00\",\n      \"summary\": \"Patch attacks, one of the most threatening forms of physical attack in\\nadversarial examples, can lead networks to induce misclassification by\\nmodifying pixels arbitrarily in a continuous region. Certifiable patch defense\\ncan guarantee robustness that the classifier is not affected by patch attacks.\\nExisting certifiable patch defenses sacrifice the clean accuracy of classifiers\\nand only obtain a low certified accuracy on toy datasets. Furthermore, the\\nclean and certified accuracy of these methods is still significantly lower than\\nthe accuracy of normal classification networks, which limits their application\\nin practice. To move towards a practical certifiable patch defense, we\\nintroduce Vision Transformer (ViT) into the framework of Derandomized Smoothing\\n(DS). Specifically, we propose a progressive smoothed image modeling task to\\ntrain Vision Transformer, which can capture the more discriminable local\\ncontext of an image while preserving the global semantic information. For\\nefficient inference and deployment in the real world, we innovatively\\nreconstruct the global self-attention structure of the original ViT into\\nisolated band unit self-attention. On ImageNet, under 2% area patch attacks our\\nmethod achieves 41.70% certified accuracy, a nearly 1-fold increase over the\\nprevious best method (26.00%). Simultaneously, our method achieves 78.58% clean\\naccuracy, which is quite close to the normal ResNet-101 accuracy. Extensive\\nexperiments show that our method obtains state-of-the-art clean and certified\\naccuracy with inferring efficiently on CIFAR-10 and ImageNet.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2203.08519v1\"\n    },\n    {\n      \"title\": \"Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation\",\n      \"authors\": [\n        \"Maksym Yatsura\",\n        \"Kaspar Sakmann\",\n        \"N. Grace Hua\",\n        \"Matthias Hein\",\n        \"Jan Hendrik Metzen\"\n      ],\n      \"published\": \"2022-09-13T13:24:22+00:00\",\n      \"summary\": \"Adversarial patch attacks are an emerging security threat for real world deep\\nlearning applications. We present Demasked Smoothing, the first approach (up to\\nour knowledge) to certify the robustness of semantic segmentation models\\nagainst this threat model. Previous work on certifiably defending against patch\\nattacks has mostly focused on image classification task and often required\\nchanges in the model architecture and additional training which is undesirable\\nand computationally expensive. In Demasked Smoothing, any segmentation model\\ncan be applied without particular training, fine-tuning, or restriction of the\\narchitecture. Using different masking strategies, Demasked Smoothing can be\\napplied both for certified detection and certified recovery. In extensive\\nexperiments we show that Demasked Smoothing can on average certify 64% of the\\npixel predictions for a 1% patch in the detection task and 48% against a 0.5%\\npatch for the recovery task on the ADE20K dataset.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2209.05980v2\"\n    },\n    {\n      \"title\": \"Architecture-agnostic Iterative Black-box Certified Defense against Adversarial Patches\",\n      \"authors\": [\n        \"Di Yang\",\n        \"Yihao Huang\",\n        \"Qing Guo\",\n        \"Felix Juefei-Xu\",\n        \"Ming Hu\",\n        \"Yang Liu\",\n        \"Geguang Pu\"\n      ],\n      \"published\": \"2023-05-18T12:43:04+00:00\",\n      \"summary\": \"The adversarial patch attack aims to fool image classifiers within a bounded,\\ncontiguous region of arbitrary changes, posing a real threat to computer vision\\nsystems (e.g., autonomous driving, content moderation, biometric\\nauthentication, medical imaging) in the physical world. To address this problem\\nin a trustworthy way, proposals have been made for certified patch defenses\\nthat ensure the robustness of classification models and prevent future patch\\nattacks from breaching the defense. State-of-the-art certified defenses can be\\ncompatible with any model architecture, as well as achieve high clean and\\ncertified accuracy. Although the methods are adaptive to arbitrary patch\\npositions, they inevitably need to access the size of the adversarial patch,\\nwhich is unreasonable and impractical in real-world attack scenarios. To\\nimprove the feasibility of the architecture-agnostic certified defense in a\\nblack-box setting (i.e., position and size of the patch are both unknown), we\\npropose a novel two-stage Iterative Black-box Certified Defense method, termed\\nIBCD.In the first stage, it estimates the patch size in a search-based manner\\nby evaluating the size relationship between the patch and mask with pixel\\nmasking. In the second stage, the accuracy results are calculated by the\\nexisting white-box certified defense methods with the estimated patch size. The\\nexperiments conducted on two popular model architectures and two datasets\\nverify the effectiveness and efficiency of IBCD.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2305.10929v1\"\n    },\n    {\n      \"title\": \"ScaleCert: Scalable Certified Defense against Adversarial Patches with Sparse Superficial Layers\",\n      \"authors\": [\n        \"Husheng Han\",\n        \"Kaidi Xu\",\n        \"Xing Hu\",\n        \"Xiaobing Chen\",\n        \"Ling Liang\",\n        \"Zidong Du\",\n        \"Qi Guo\",\n        \"Yanzhi Wang\",\n        \"Yunji Chen\"\n      ],\n      \"published\": \"2021-10-27T02:05:00+00:00\",\n      \"summary\": \"Adversarial patch attacks that craft the pixels in a confined region of the\\ninput images show their powerful attack effectiveness in physical environments\\neven with noises or deformations. Existing certified defenses towards\\nadversarial patch attacks work well on small images like MNIST and CIFAR-10\\ndatasets, but achieve very poor certified accuracy on higher-resolution images\\nlike ImageNet. It is urgent to design both robust and effective defenses\\nagainst such a practical and harmful attack in industry-level larger images. In\\nthis work, we propose the certified defense methodology that achieves high\\nprovable robustness for high-resolution images and largely improves the\\npracticality for real adoption of the certified defense. The basic insight of\\nour work is that the adversarial patch intends to leverage localized\\nsuperficial important neurons (SIN) to manipulate the prediction results.\\nHence, we leverage the SIN-based DNN compression techniques to significantly\\nimprove the certified accuracy, by reducing the adversarial region searching\\noverhead and filtering the prediction noises. Our experimental results show\\nthat the certified accuracy is increased from 36.3% (the state-of-the-art\\ncertified detection) to 60.4% on the ImageNet dataset, largely pushing the\\ncertified defenses for practical use.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2110.14120v2\"\n    },\n    {\n      \"title\": \"Certified Defenses for Adversarial Patches\",\n      \"authors\": [\n        \"Ping-Yeh Chiang\",\n        \"Renkun Ni\",\n        \"Ahmed Abdelkader\",\n        \"Chen Zhu\",\n        \"Christoph Studer\",\n        \"Tom Goldstein\"\n      ],\n      \"published\": \"2020-03-14T19:57:31+00:00\",\n      \"summary\": \"Adversarial patch attacks are among one of the most practical threat models\\nagainst real-world computer vision systems. This paper studies certified and\\nempirical defenses against patch attacks. We begin with a set of experiments\\nshowing that most existing defenses, which work by pre-processing input images\\nto mitigate adversarial patches, are easily broken by simple white-box\\nadversaries. Motivated by this finding, we propose the first certified defense\\nagainst patch attacks, and propose faster methods for its training.\\nFurthermore, we experiment with different patch shapes for testing, obtaining\\nsurprisingly good robustness transfer across shapes, and present preliminary\\nresults on certified defense against sparse attacks. Our complete\\nimplementation can be found on:\\nhttps://github.com/Ping-C/certifiedpatchdefense.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2003.06693v2\"\n    }\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ArxivTool` with \n```json\n{\n  \"max_results\": 5,\n  \"query\": \"certified defense adversarial patch semantic segmentation\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ArxivTool', response={'result': {'status': 'success', 'message': 'Found 5 paper(s) on arXiv', 'output': [{'title': 'Towards Practical Certifiable Patch Defense with Vision Transformer', 'authors': ['Zhaoyu Chen', 'Bo Li', 'Jianghe Xu', 'Shuang Wu', 'Shouhong Ding', 'Wenqiang Zhang'], 'published': '2022-03-16T10:39:18+00:00', 'summary': 'Patch attacks, one of the most threatening forms of physical attack in\\nadversarial examples, can lead networks to induce misclassification by\\nmodifying pixels arbitrarily in a continuous region. Certifiable patch defense\\ncan guarantee robustness that the classifier is not affected by patch attacks.\\nExisting certifiable patch defenses sacrifice the clean accuracy of classifiers\\nand only obtain a low certified accuracy on toy datasets. Furthermore, the\\nclean and certified accuracy of these methods is still significantly lower than\\nthe accuracy of normal classification networks, which limits their application\\nin practice. To move towards a practical certifiable patch defense, we\\nintroduce Vision Transformer (ViT) into the framework of Derandomized Smoothing\\n(DS). Specifically, we propose a progressive smoothed image modeling task to\\ntrain Vision Transformer, which can capture the more discriminable local\\ncontext of an image while preserving the global semantic information. For\\nefficient inference and deployment in the real world, we innovatively\\nreconstruct the global self-attention structure of the original ViT into\\nisolated band unit self-attention. On ImageNet, under 2% area patch attacks our\\nmethod achieves 41.70% certified accuracy, a nearly 1-fold increase over the\\nprevious best method (26.00%). Simultaneously, our method achieves 78.58% clean\\naccuracy, which is quite close to the normal ResNet-101 accuracy. Extensive\\nexperiments show that our method obtains state-of-the-art clean and certified\\naccuracy with inferring efficiently on CIFAR-10 and ImageNet.', 'pdf_url': 'http://arxiv.org/pdf/2203.08519v1'}, {'title': 'Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation', 'authors': ['Maksym Yatsura', 'Kaspar Sakmann', 'N. Grace Hua', 'Matthias Hein', 'Jan Hendrik Metzen'], 'published': '2022-09-13T13:24:22+00:00', 'summary': 'Adversarial patch attacks are an emerging security threat for real world deep\\nlearning applications. We present Demasked Smoothing, the first approach (up to\\nour knowledge) to certify the robustness of semantic segmentation models\\nagainst this threat model. Previous work on certifiably defending against patch\\nattacks has mostly focused on image classification task and often required\\nchanges in the model architecture and additional training which is undesirable\\nand computationally expensive. In Demasked Smoothing, any segmentation model\\ncan be applied without particular training, fine-tuning, or restriction of the\\narchitecture. Using different masking strategies, Demasked Smoothing can be\\napplied both for certified detection and certified recovery. In extensive\\nexperiments we show that Demasked Smoothing can on average certify 64% of the\\npixel predictions for a 1% patch in the detection task and 48% against a 0.5%\\npatch for the recovery task on the ADE20K dataset.', 'pdf_url': 'http://arxiv.org/pdf/2209.05980v2'}, {'title': 'Architecture-agnostic Iterative Black-box Certified Defense against Adversarial Patches', 'authors': ['Di Yang', 'Yihao Huang', 'Qing Guo', 'Felix Juefei-Xu', 'Ming Hu', 'Yang Liu', 'Geguang Pu'], 'published': '2023-05-18T12:43:04+00:00', 'summary': 'The adversarial patch attack aims to fool image classifiers within a bounded,\\ncontiguous region of arbitrary changes, posing a real threat to computer vision\\nsystems (e.g., autonomous driving, content moderation, biometric\\nauthentication, medical imaging) in the physical world. To address this problem\\nin a trustworthy way, proposals have been made for certified patch defenses\\nthat ensure the robustness of classification models and prevent future patch\\nattacks from breaching the defense. State-of-the-art certified defenses can be\\ncompatible with any model architecture, as well as achieve high clean and\\ncertified accuracy. Although the methods are adaptive to arbitrary patch\\npositions, they inevitably need to access the size of the adversarial patch,\\nwhich is unreasonable and impractical in real-world attack scenarios. To\\nimprove the feasibility of the architecture-agnostic certified defense in a\\nblack-box setting (i.e., position and size of the patch are both unknown), we\\npropose a novel two-stage Iterative Black-box Certified Defense method, termed\\nIBCD.In the first stage, it estimates the patch size in a search-based manner\\nby evaluating the size relationship between the patch and mask with pixel\\nmasking. In the second stage, the accuracy results are calculated by the\\nexisting white-box certified defense methods with the estimated patch size. The\\nexperiments conducted on two popular model architectures and two datasets\\nverify the effectiveness and efficiency of IBCD.', 'pdf_url': 'http://arxiv.org/pdf/2305.10929v1'}, {'title': 'ScaleCert: Scalable Certified Defense against Adversarial Patches with Sparse Superficial Layers', 'authors': ['Husheng Han', 'Kaidi Xu', 'Xing Hu', 'Xiaobing Chen', 'Ling Liang', 'Zidong Du', 'Qi Guo', 'Yanzhi Wang', 'Yunji Chen'], 'published': '2021-10-27T02:05:00+00:00', 'summary': 'Adversarial patch attacks that craft the pixels in a confined region of the\\ninput images show their powerful attack effectiveness in physical environments\\neven with noises or deformations. Existing certified defenses towards\\nadversarial patch attacks work well on small images like MNIST and CIFAR-10\\ndatasets, but achieve very poor certified accuracy on higher-resolution images\\nlike ImageNet. It is urgent to design both robust and effective defenses\\nagainst such a practical and harmful attack in industry-level larger images. In\\nthis work, we propose the certified defense methodology that achieves high\\nprovable robustness for high-resolution images and largely improves the\\npracticality for real adoption of the certified defense. The basic insight of\\nour work is that the adversarial patch intends to leverage localized\\nsuperficial important neurons (SIN) to manipulate the prediction results.\\nHence, we leverage the SIN-based DNN compression techniques to significantly\\nimprove the certified accuracy, by reducing the adversarial region searching\\noverhead and filtering the prediction noises. Our experimental results show\\nthat the certified accuracy is increased from 36.3% (the state-of-the-art\\ncertified detection) to 60.4% on the ImageNet dataset, largely pushing the\\ncertified defenses for practical use.', 'pdf_url': 'http://arxiv.org/pdf/2110.14120v2'}, {'title': 'Certified Defenses for Adversarial Patches', 'authors': ['Ping-Yeh Chiang', 'Renkun Ni', 'Ahmed Abdelkader', 'Chen Zhu', 'Christoph Studer', 'Tom Goldstein'], 'published': '2020-03-14T19:57:31+00:00', 'summary': 'Adversarial patch attacks are among one of the most practical threat models\\nagainst real-world computer vision systems. This paper studies certified and\\nempirical defenses against patch attacks. We begin with a set of experiments\\nshowing that most existing defenses, which work by pre-processing input images\\nto mitigate adversarial patches, are easily broken by simple white-box\\nadversaries. Motivated by this finding, we propose the first certified defense\\nagainst patch attacks, and propose faster methods for its training.\\nFurthermore, we experiment with different patch shapes for testing, obtaining\\nsurprisingly good robustness transfer across shapes, and present preliminary\\nresults on certified defense against sparse attacks. Our complete\\nimplementation can be found on:\\nhttps://github.com/Ping-C/certifiedpatchdefense.', 'pdf_url': 'http://arxiv.org/pdf/2003.06693v2'}]}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "The paper \"Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation\" proposes a novel framework called DEMASKED SMOOTHING to achieve certified defenses against patch attacks on semantic segmentation models.\n\nThe search results indicate that this paper has already been published on ArXiv with essentially the same title and abstract. I will proceed with the review, but will be mindful of the publication date.\n\nBased on the paper's content, I will create three reviewer agents with the following specializations:\n\n1.  **Segmentation Expert (SegExpert):** Focuses on the semantic segmentation aspects, evaluating the suitability of the proposed method for this task, the choice of datasets, and the evaluation metrics.\n2.  **Certified Defense Specialist (CertDefensePro):** Specializes in certified defenses, particularly against adversarial attacks. This agent will assess the rigor of the certification claims, the threat model, and the comparisons to existing certified defenses.\n3.  **Image Inpainting Analyst (InpaintAnalyst):** Concentrates on the image inpainting component, evaluating the choice of inpainting model, its integration with the overall framework, and its impact on the final performance.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"gemini-2.5-flash-preview-05-20\": {\n      \"description\": \"Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math\",\n      \"create_expense_cost\": 0,\n      \"invoke_expense_cost\": 0.15,\n      \"output_expense_cost\": 0.6\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'gemini-2.5-flash-preview-05-20': {'description': 'Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math', 'create_expense_cost': 0, 'invoke_expense_cost': 0.15, 'output_expense_cost': 0.6}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'SegExpert', 'description': 'Expert in semantic segmentation, focusing on the suitability of the method for the task, datasets, and evaluation metrics.', 'base_model': 'gemini-2.5-flash-preview-05-20', 'system_prompt': 'You are a highly skilled researcher specializing in semantic segmentation. You have extensive knowledge of various segmentation techniques, datasets, and evaluation metrics. Your task is to evaluate the provided paper, focusing on the following aspects:\\n\\n*   Suitability of Demasked Smoothing for semantic segmentation.\\n*   Appropriateness of the chosen datasets (ADE20K, COCO-Stuff-10K) for evaluating the proposed defense.\\n*   Correctness and relevance of the evaluation metrics used (mIoU, %C, mR, cmR, FAR).\\n*   Comparison to other segmentation methods.\\n*   Overall contribution to the field of semantic segmentation.\\n\\nProvide a detailed assessment of these aspects, highlighting strengths and weaknesses, and suggesting potential improvements. Your review should be rigorous and constructive.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gemini-2.5-flash-preview-05-20', 'description': 'Specialist in certified defenses against adversarial attacks, assessing the rigor of certification claims and the threat model.', 'agent_name': 'CertDefensePro', 'system_prompt': 'You are an expert in certified defenses against adversarial attacks. You have a deep understanding of different certification techniques, threat models, and robustness guarantees. Your task is to evaluate the provided paper, focusing on the following aspects:\\n\\n*   Rigor of the certification claims made in the paper.\\n*   Appropriateness and realism of the threat model considered (adversarial patches).\\n*   Soundness of the proofs for the certified recovery and certified detection theorems.\\n*   Comparison to existing certified defenses against adversarial attacks.\\n*   Limitations of the proposed approach in terms of scalability, performance, or applicability.\\n\\nProvide a thorough and critical assessment of these aspects, identifying any potential flaws or weaknesses in the certification arguments. Your review should be technically sound and provide actionable feedback.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gemini-2.5-flash-preview-05-20', 'agent_name': 'InpaintAnalyst', 'system_prompt': 'You are a specialist in image inpainting techniques. You have a strong understanding of various inpainting models, their strengths and weaknesses, and their impact on downstream tasks. Your task is to evaluate the provided paper, focusing on the following aspects:\\n\\n*   Justification for the choice of the ZITS inpainting model.\\n*   Impact of the inpainting quality on the performance of the Demasked Smoothing framework.\\n*   Potential limitations of using a fixed inpainting model trained on a specific dataset (Places2).\\n*   Alternative inpainting techniques that could be explored.\\n*   Analysis of the computational cost and efficiency of the inpainting step.\\n\\nProvide a detailed analysis of these aspects, suggesting potential improvements or alternative approaches to the inpainting component. Your review should be insightful and contribute to a better understanding of the role of inpainting in the overall defense mechanism.', 'description': 'Concentrates on the image inpainting component, evaluating the choice of inpainting model and its impact on performance.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.84705895\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"agent_name\": \"SegExpert\",\n  \"description\": \"Expert in semantic segmentation, focusing on the suitability of the method for the task, datasets, and evaluation metrics.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"system_prompt\": \"You are a highly skilled researcher specializing in semantic segmentation. You have extensive knowledge of various segmentation techniques, datasets, and evaluation metrics. Your task is to evaluate the provided paper, focusing on the following aspects:\\n\\n*   Suitability of Demasked Smoothing for semantic segmentation.\\n*   Appropriateness of the chosen datasets (ADE20K, COCO-Stuff-10K) for evaluating the proposed defense.\\n*   Correctness and relevance of the evaluation metrics used (mIoU, %C, mR, cmR, FAR).\\n*   Comparison to other segmentation methods.\\n*   Overall contribution to the field of semantic segmentation.\\n\\nProvide a detailed assessment of these aspects, highlighting strengths and weaknesses, and suggesting potential improvements. Your review should be rigorous and constructive.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.84705895\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"description\": \"Specialist in certified defenses against adversarial attacks, assessing the rigor of certification claims and the threat model.\",\n  \"agent_name\": \"CertDefensePro\",\n  \"system_prompt\": \"You are an expert in certified defenses against adversarial attacks. You have a deep understanding of different certification techniques, threat models, and robustness guarantees. Your task is to evaluate the provided paper, focusing on the following aspects:\\n\\n*   Rigor of the certification claims made in the paper.\\n*   Appropriateness and realism of the threat model considered (adversarial patches).\\n*   Soundness of the proofs for the certified recovery and certified detection theorems.\\n*   Comparison to existing certified defenses against adversarial attacks.\\n*   Limitations of the proposed approach in terms of scalability, performance, or applicability.\\n\\nProvide a thorough and critical assessment of these aspects, identifying any potential flaws or weaknesses in the certification arguments. Your review should be technically sound and provide actionable feedback.\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.84705895\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"agent_name\": \"InpaintAnalyst\",\n  \"system_prompt\": \"You are a specialist in image inpainting techniques. You have a strong understanding of various inpainting models, their strengths and weaknesses, and their impact on downstream tasks. Your task is to evaluate the provided paper, focusing on the following aspects:\\n\\n*   Justification for the choice of the ZITS inpainting model.\\n*   Impact of the inpainting quality on the performance of the Demasked Smoothing framework.\\n*   Potential limitations of using a fixed inpainting model trained on a specific dataset (Places2).\\n*   Alternative inpainting techniques that could be explored.\\n*   Analysis of the computational cost and efficiency of the inpainting step.\\n\\nProvide a detailed analysis of these aspects, suggesting potential improvements or alternative approaches to the inpainting component. Your review should be insightful and contribute to a better understanding of the role of inpainting in the overall defense mechanism.\",\n  \"description\": \"Concentrates on the image inpainting component, evaluating the choice of inpainting model and its impact on performance.\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.84705895}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.84705895}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.84705895}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'SegExpert', 'prompt': \"Evaluate the paper 'Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation' focusing on the semantic segmentation aspects, datasets, and evaluation metrics. Provide a detailed assessment, highlighting strengths and weaknesses, and suggesting potential improvements.\"}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Evaluate the paper 'Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation' focusing on the rigor of the certification claims, the threat model, and comparisons to existing certified defenses. Provide a thorough and critical assessment, identifying potential flaws or weaknesses.\", 'agent_name': 'CertDefensePro'}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Evaluate the paper 'Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation' focusing on the image inpainting component, the choice of inpainting model, and its impact on performance. Provide a detailed analysis, suggesting potential improvements or alternative approaches.\", 'agent_name': 'InpaintAnalyst'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The paper \\\"Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation\\\" presents a novel approach to provide certified robustness against adversarial patch attacks for semantic segmentation models. This is a significant area of research, as robust perception is crucial for safety-critical applications. As a researcher specializing in semantic segmentation, I will evaluate the paper based on the specified criteria.\\n\\n## Detailed Assessment\\n\\n### 1. Suitability of Demasked Smoothing for Semantic Segmentation\\n\\n**Strengths:**\\n*   **Pixel-wise Certification:** This is the most crucial and innovative aspect. Semantic segmentation inherently requires per-pixel predictions, and Demasked Smoothing (DS) is adapted to provide *pixel-level* certification. This means the defense can provably guarantee that individual pixel classifications remain correct within a certain adversarial patch radius, which is a more granular and meaningful guarantee than image-level robustness.\\n*   **Targeted for Patch Attacks:** Adversarial patch attacks are a practical and realistic threat model, especially in real-world scenarios (e.g., a sticker on a stop sign). Demasked Smoothing, with its masking strategy, is well-suited to handle localized perturbations by effectively hiding or diluting the patch's influence across multiple masked samples.\\n*   **Principled Certification:** The method leverages the strong theoretical foundations of randomized smoothing, adapting it to the segmentation task. This ensures that the robustness claims are not merely empirical but come with mathematical guarantees, quantified by a confidence level (e.g., 1-$\\\\alpha$).\\n*   **Generality:** The Demasked Smoothing framework appears to be largely agnostic to the underlying segmentation architecture (e.g., FCN, DeepLab, PSPNet), making it broadly applicable.\\n\\n**Weaknesses/Challenges:**\\n*   **Computational Cost:** This is arguably the biggest weakness. Certified defenses based on randomized smoothing inherently require a large number of forward passes (N samples per pixel/image) for prediction and even more for certification. For high-resolution images and complex segmentation models, this can lead to prohibitively high inference times, limiting practical deployment in real-time applications.\\n*   **Clean Accuracy Trade-off:** As is common with certified defenses, there is likely a trade-off between certified robustness and clean (unattacked) accuracy. While the paper should demonstrate reasonable mIoU, it's critical to understand the magnitude of this drop and if it remains competitive. The process of masking inherently removes context, which can degrade performance for a model trained on unmasked images.\\n*   **Loss of Context:** By randomly masking portions of the input, the segmentation model might lose crucial contextual information necessary for accurate pixel-wise classification, especially for ambiguous regions or objects that rely heavily on global scene understanding. This could lead to a lower percentage of certifiable pixels or lower certified accuracy.\\n*   **Type of Perturbation:** While effective against patches, Demasked Smoothing might not be suitable for other types of adversarial attacks (e.g., global L-infinity or L-0 perturbations) unless adapted with different sampling/masking strategies.\\n\\n**Potential Improvements:**\\n*   **Efficiency Enhancements:** Investigate methods to reduce the number of samples required, such as adaptive sampling techniques or more efficient aggregation strategies (e.g., knowledge distillation to a more robust, faster model).\\n*   **Context Preservation:** Explore masking strategies that are more \\\"context-aware\\\" or less disruptive to global scene understanding, perhaps by using larger, overlapping masks or multi-scale masking.\\n*   **Hardware Acceleration:** Discuss the potential for parallelization and GPU optimization to mitigate the computational burden.\\n\\n### 2. Appropriateness of the Chosen Datasets (ADE20K, COCO-Stuff-10K)\\n\\n**Strengths:**\\n*   **Industry Benchmarks:** Both ADE20K and COCO-Stuff-10K are widely recognized and highly respected benchmarks in semantic segmentation research. Using these datasets ensures that the results are comparable to state-of-the-art methods and are relevant to the broader segmentation community.\\n*   **Diversity:**\\n    *   **ADE20K:** Features a vast number of classes (150) and diverse indoor/outdoor scenes, making it excellent for evaluating generalization across a wide range of object and scene categories. It's a challenging dataset due to its complexity.\\n    *   **COCO-Stuff-10K:** Complements ADE20K by focusing on \\\"stuff\\\" categories (e.g., sky, road, grass) which often present different challenges than \\\"things\\\" (e.g., people, cars). It helps assess the defense's performance on textured and amorphous regions.\\n*   **Scale:** ADE20K is a large-scale dataset, providing sufficient data for training complex segmentation models and evaluating robustness rigorously. COCO-Stuff-10K, while smaller, provides a good additional testbed.\\n\\n**Weaknesses/Gaps:**\\n*   **Computational Burden:** While appropriate, the sheer size and resolution of images in ADE20K will exacerbate the computational challenges of Demasked Smoothing. The paper should explicitly address how the computational cost scales with image resolution and dataset size.\\n*   **Real-world Scenarios:** While diverse, these datasets might not fully capture the specific nuances of adversarial patches in highly specialized real-world scenarios (e.g., specific lighting conditions, very small or distant objects). However, this is a general limitation of most academic datasets.\\n\\n**Potential Improvements:**\\n*   **Explicit Justification:** A more detailed justification in the paper for choosing these specific two datasets (e.g., ADE20K for scene understanding, COCO-Stuff-10K for stuff-centric analysis) would strengthen the experimental setup.\\n*   **Computational Breakdown:** Provide a clear breakdown of the computational cost (e.g., average inference time per image for certification) for each dataset to give readers a practical understanding of the method's feasibility.\\n\\n### 3. Correctness and Relevance of the Evaluation Metrics\\n\\nThe choice of metrics is excellent and highly relevant for evaluating a certified defense against adversarial patches in semantic segmentation.\\n\\n**Metrics Used:**\\n*   **mIoU (Mean Intersection over Union):**\\n    *   **Relevance:** The standard and most critical metric for evaluating the overall clean performance of a semantic segmentation model. It measures the overlap between predicted and ground truth masks. Essential for understanding the baseline utility of the segmented output.\\n    *   **Correctness:** Correctly calculated as the average IoU across all classes.\\n*   **%C (Percentage of Certified Pixels):**\\n    *   **Relevance:** A primary metric for certified defenses. It quantifies the proportion of pixels for which the defense can provide a provable guarantee of correctness under a specified perturbation radius. This directly measures the \\\"coverage\\\" of the certification.\\n    *   **Correctness:** Should be clearly defined regarding the confidence level ($\\\\alpha$) and the specific patch radius being certified against.\\n*   **mR (Mean Robustness Radius):**\\n    *   **Relevance:** Measures the *strength* of the certification. For each pixel, it indicates the maximum adversarial patch radius for which the defense can still guarantee the correct classification. Averaging this across all pixels gives a holistic view of the defense's robustness capability.\\n    *   **Correctness:** Defined as the largest certified patch radius.\\n*   **cmR (Correct Mean Robustness Radius):**\\n    *   **Relevance:** This is a crucial refinement of mR. It considers only those pixels that are *correctly classified* by the robust classifier (i.e., the certified label matches the ground truth). This is important because certifying an *incorrect* prediction is not useful. cmR focuses on the *useful* robustness guarantees.\\n    *   **Correctness:** Averages the robustness radius only for pixels whose certified class matches the ground truth.\\n*   **FAR (False Alarm Rate):**\\n    *   **Relevance:** Paramount for *certified* defenses. It measures how often the defense *claims* a pixel is robustly classified, but it actually *fails* under an adversarial attack within the certified radius. For a truly certified defense, FAR should be extremely low (ideally zero), indicating the trustworthiness of the theoretical guarantees.\\n    *   **Correctness:** Should be calculated as the proportion of certified pixels that are later found to be incorrect under attack.\\n\\n**Overall Assessment:**\\n*   **Strengths:** The combination of these metrics provides a comprehensive evaluation of the proposed certified defense. They cover clean performance, certification coverage, strength of certification, and the soundness/reliability of the certification. This is a very strong set of metrics for this type of work.\\n*   **Weaknesses/Gaps:**\\n    *   While %C, mR, and cmR are excellent, it would be beneficial to see how these metrics vary with different confidence levels ($\\\\alpha$).\\n    *   Visualizations of certified regions on example images (e.g., highlighting pixels certified for certain radii vs. those not certified) would significantly aid in understanding the defense's behavior.\\n    *   Explicitly discuss the trade-off curve between clean mIoU and %C or cmR. This is a critical aspect of certified defenses.\\n\\n**Potential Improvements:**\\n*   Include visualizations of certified vs. uncertified regions on representative images.\\n*   Present results (e.g., %C, cmR) across a range of $\\\\alpha$ values to show the confidence-robustness trade-off.\\n\\n### 4. Comparison to Other Segmentation Methods\\n\\n**Expected Comparisons (and their relevance):**\\n*   **Undefended Baseline (e.g., standard FCN, DeepLabv3+):** Essential to demonstrate the vulnerability of current state-of-the-art segmentation models to adversarial patch attacks and to highlight the necessity of a defense.\\n*   **Empirical Defenses (e.g., Adversarial Training for segmentation):** This is a critical comparison. Adversarial Training (AT) is the most common empirical defense. The paper should compare the certified robustness of DS against the empirical robustness of AT. DS is expected to provide *provable* guarantees (low FAR, non-zero cmR), while AT only offers empirical robustness that can be broken by stronger, unseen attacks. This comparison highlights the core advantage of certification.\\n*   **Other Certified Defenses for Segmentation (if any):** If there are other efforts to provide certified robustness for segmentation (even if partial or for different attack models), a comparison is vital to position the work. If it's truly the first of its kind for patch attacks on segmentation, then that claim should be strongly emphasized.\\n*   **Randomized Smoothing for Classification (adapted):** While Demasked Smoothing is a variant, it's worth briefly discussing how it differs from simply applying standard noise-based randomized smoothing pixel-wise, and why masking is more appropriate for patch attacks.\\n\\n**Strengths of the Comparison:**\\n*   Comparing against undefended models clearly quantifies the problem being addressed.\\n*   Comparing against empirical defenses like AT is crucial for demonstrating the unique value proposition of *certified* robustness.\\n\\n**Weaknesses/Gaps:**\\n*   **Computational Overhead:** A significant gap if not thoroughly discussed. The computational cost of DS is likely orders of magnitude higher than undefended or even AT models. This needs to be quantitatively compared to provide a realistic assessment of deployability.\\n*   **Attack Strength for AT:** For a fair comparison with AT, the paper should specify the strength and type of adversarial attacks used during AT (e.g., PGD iterations, epsilon) as this affects the empirical robustness achieved.\\n*   **Direct Certified Segmentation Comparison:** If no other *certified* segmentation defense exists, the paper should clearly state this as a novel contribution, rather than just implicitly assuming it.\\n\\n**Potential Improvements:**\\n*   Provide a clear table comparing inference times and computational resources required for DS versus undefended and AT baselines.\\n*   Discuss the specific adversarial training setup (e.g., loss function, attack type, $\\\\epsilon$) used for the AT baseline to ensure reproducibility and fair comparison.\\n\\n### 5. Overall Contribution to the Field of Semantic Segmentation\\n\\n**Strengths:**\\n*   **Pioneering Work:** This paper represents a significant step forward by bringing the concept of *certified robustness* to the domain of semantic segmentation, specifically for challenging adversarial patch attacks. Providing provable guarantees for pixel-level predictions is a substantial methodological contribution.\\n*   **Addresses a Critical Gap:** Current semantic segmentation models are known to be vulnerable to adversarial attacks. This work addresses a critical gap by offering a theoretically sound defense mechanism, which is vital for deploying AI in safety-critical applications like autonomous driving, medical imaging, and surveillance.\\n*   **Methodological Innovation:** The adaptation of Demasked Smoothing for pixel-wise certification in segmentation, along with the definition of relevant evaluation metrics (%C, mR, cmR), is a valuable methodological contribution that opens new avenues for research in robust segmentation.\\n*   **Sets a New Standard:** By introducing certified robustness, the paper implicitly raises the bar for future research in robust semantic segmentation, moving beyond purely empirical defenses.\\n\\n**Weaknesses/Limitations (Overall):**\\n*   **Practicality due to Cost:** The high computational cost remains a major hurdle for widespread practical adoption, especially for real-time applications. Future work would need to significantly improve efficiency.\\n*   **Scope of Attack:** The defense is specifically tailored for patch attacks. While important, the generalizability to other common adversarial perturbation types (e.g., global noise) might be limited without further adaptations.\\n*   **Trade-off Clarity:** While the paper will likely show the trade-off, a deeper discussion on strategies to mitigate the accuracy-robustness trade-off and improve certified accuracy for clean images would be beneficial.\\n\\n**Overall Impression:**\\nThe paper makes a substantial and timely contribution to the field of semantic segmentation. By being among the first (if not *the* first) to offer certified robustness against adversarial patch attacks for pixel-level predictions, it pushes the boundaries of reliable and trustworthy AI for computer vision. While computational efficiency remains a significant challenge, the theoretical rigor and the novelty of the pixel-wise certification are highly commendable. This work is likely to inspire further research into developing more efficient and universally robust segmentation models.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.84417405\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"SegExpert\",\n  \"prompt\": \"Evaluate the paper 'Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation' focusing on the semantic segmentation aspects, datasets, and evaluation metrics. Provide a detailed assessment, highlighting strengths and weaknesses, and suggesting potential improvements.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"This paper proposes a novel certified defense mechanism against adversarial patch attacks specifically for semantic segmentation models. It extends the well-known randomized smoothing (RS) technique to provide pixel-wise robustness guarantees, offering both \\\"certified recovery\\\" for pixels outside the patch and \\\"certified detection\\\" for attacks within the patch region.\\n\\nHere's a thorough and critical assessment of the paper:\\n\\n---\\n\\n### 1. Rigor of Certification Claims\\n\\nThe paper makes two distinct certification claims: Certified Recovery and Certified Detection.\\n\\n**1.1. Certified Recovery (Theorem 1)**\\n*   **Claim:** For any pixel $p$ that is *outside* the adversarial patch $\\\\mathcal{P}$, if its ground-truth label is $c$, then the proposed smoothed classifier $\\\\hat{f}$ will predict label $c$ with a probability $P_c > 0.5$, given a certain noise level $\\\\sigma$. This implies that the pixel $p$ is robust to the adversarial patch, as long as the patch does not *contain* $p$.\\n*   **Mechanism:** The core idea for certified recovery is to apply randomized smoothing *only to the pixels outside the potential patch region*. For a given pixel $p$, its prediction is certified if $p$ is known to be outside the adversarial patch. The method effectively trains a segmentation model that, for each pixel, aggregates votes from multiple noisy versions of the input image.\\n*   **Soundness:** The proof leverages the established theoretical guarantees of randomized smoothing (e.g., Cohen et al., 2019). The crucial insight is that if a pixel $p$ is *outside* the patch, its true label is undisturbed by the patch. The Gaussian noise added for smoothing does not change the patch's fixed location relative to the image. Therefore, for any pixel $p \\\\notin \\\\mathcal{P}$, the prediction mechanism is essentially operating on a clean image corrupted by Gaussian noise, and the standard RS guarantees for robustness against $L_2$ perturbations (which Gaussian noise effectively simulates) apply.\\n*   **Limitations in Scope:** The rigor is high *within its stated scope*. However, it's crucial to understand what it *doesn't* certify. It does **not** certify the predictions for pixels *within* the potential patch region. This is a fundamental limitation: if the patch is designed to attack a specific region, this method explicitly gives up on certifying that region. The certified region shrinks as the potential patch size increases. In a worst-case scenario where the patch can be extremely large, the certified \\\"recovery\\\" area might become negligible.\\n\\n**1.2. Certified Detection (Theorem 2)**\\n*   **Claim:** If an adversarial patch is present within a region $R$ and causes a misclassification of a pixel $p \\\\in R$, the method can detect this attack with a certain probability $P_d$, provided some conditions on the difference in smoothed probabilities inside and outside the region $R$.\\n*   **Mechanism:** This is framed as a statistical hypothesis testing problem. The method compares the distribution of predictions (or the confidence scores) within the suspected patch region against those outside it. A significant statistical difference triggers a detection.\\n*   **Soundness:** The proof relies on Chernoff bounds or similar concentration inequalities to establish a lower bound on the probability of detecting an attack. This is a standard approach for statistical detection systems. The notion of certifying a *probability* of detection, given that an attack causes a specific change, is a valid and rigorous form of certification.\\n*   **Limitations in Scope:**\\n    *   **Probabilistic Guarantee:** It's a probabilistic guarantee for detection, not a deterministic one. There's always a trade-off between the probability of detection ($P_d$) and the probability of false positives (incorrectly flagging a clean input as an attack). The paper characterizes $P_d$ but implicitly relies on the user setting appropriate thresholds for practical use.\\n    *   **Requires Change:** It detects attacks that *successfully* cause a misclassification within the patch region and *sufficiently* perturb the local prediction distribution. If an attack is very subtle or causes changes below the detection threshold, it might not be detected.\\n    *   **Statistical nature:** Like all statistical tests, the effectiveness depends on the magnitude of the effect caused by the patch. A small, inconspicuous patch might be harder to detect reliably without increasing false alarms.\\n\\n**Overall Rigor:** The mathematical framework is sound, drawing appropriately from established randomized smoothing theory and statistical hypothesis testing. The claims are precisely stated, distinguishing what is and isn't certified. The novelty lies in the *application* and *adaptation* of these concepts to the challenging problem of patch attacks on segmentation.\\n\\n---\\n\\n### 2. Appropriateness and Realism of the Threat Model (Adversarial Patches)\\n\\n*   **Threat Model:** The paper considers a single, rectangular adversarial patch of a fixed size and shape ($S \\\\times S$ pixels), which can be placed anywhere on the input image. The pixel values within the patch can be arbitrary.\\n*   **Realism:**\\n    *   **High Realism:** Adversarial patches are one of the most physically realistic and actionable types of adversarial attacks. Unlike $L_p$-norm bounded perturbations (which assume global, small changes to every pixel), patches can be printed as stickers or placed as physical objects in the environment, making them highly relevant for real-world applications like autonomous driving or surveillance.\\n    *   **Placement \\\"Anywhere\\\":** The ability to place the patch anywhere on the image significantly increases the challenge for defenses, mirroring real-world scenarios.\\n    *   **Arbitrary Pixel Values:** This makes the attacker very powerful within the patch region, which is also realistic.\\n*   **Appropriateness for Segmentation:** For semantic segmentation, which is a pixel-wise prediction task, a localized patch attack is particularly challenging because it can dramatically alter the predicted class of all pixels *within* its boundary, potentially causing a small object to disappear or a large area to be misclassified. This makes it an entirely appropriate and relevant threat model for the task.\\n*   **Limitations within the Threat Model:**\\n    *   **Single Patch:** The model considers only a single adversarial patch. Real-world scenarios might involve multiple patches. Certifying against multiple patches would be significantly more complex.\\n    *   **Fixed Size/Shape:** While common in research, assuming a fixed rectangular shape and size might be a slight idealization. Real-world patches could be irregular or of varying sizes. However, this is a reasonable simplification for initial certified defenses.\\n    *   **Crucial Implication for Recovery:** While the threat model allows the patch to be *anywhere*, the *certified recovery* mechanism critically relies on the defender either knowing *where the patch is not* or having to *scan all possible patch locations*. If the defender does not know the patch's location, to guarantee recovery for *all* pixels outside the patch, the defender needs to apply the certification process across a grid of all possible patch locations. This quickly becomes computationally prohibitive, limiting the practical applicability of the \\\"certified recovery for pixels outside the patch\\\" claim unless the set of possible patch locations is severely restricted or known. The paper acknowledges this by stating they certify for a *given* candidate patch location. This is a vital distinction: the defense isn't certified to recover *regardless* of patch location, but rather to recover *if* the patch is *not* at the certified pixel's location.\\n\\n---\\n\\n### 3. Soundness of Proofs for Certified Recovery and Certified Detection\\n\\nBoth proofs build upon the well-established mathematical foundation of randomized smoothing.\\n\\n**3.1. Certified Recovery (Theorem 1):**\\n*   **Proof Logic:** The proof for Theorem 1 correctly applies the principles of randomized smoothing. For a pixel $p$ outside the patch region, the patch itself does not alter the ground truth label of that pixel. When Gaussian noise is added for smoothing, the patch remains in its fixed location. Therefore, the effect on pixel $p$ is primarily due to the Gaussian noise, and the standard robustness guarantees derived from RS (e.g., Cohen et al.'s bound on the $L_2$ radius) apply directly to the prediction of $p$.\\n*   **Assumptions:** The critical assumption is that the adversarial patch is a *fixed, deterministic perturbation* on the input image, and the Gaussian noise is *added on top* of this possibly perturbed image. This is a standard and valid assumption in certified robustness literature.\\n*   **Soundness:** The proof is sound, provided the underlying randomized smoothing theorems it relies upon are sound. The adaptation to segmentation (pixel-wise classification) is straightforward as each pixel's classification can be treated as an independent task for certification.\\n\\n**3.2. Certified Detection (Theorem 2):**\\n*   **Proof Logic:** The proof for Theorem 2 relies on statistical hypothesis testing. It defines a test statistic based on the difference in vote counts (or similar aggregated predictions) between the region under suspicion (the potential patch region) and an unaffected reference region. If this difference exceeds a calculated threshold, an attack is detected. The derivation of the threshold and the probability of detection $P_d$ (and implicit false positive rates) typically involves concentration inequalities like Chernoff bounds or Hoeffding's inequality, which are standard tools in robust statistics.\\n*   **Assumptions:** The proof assumes that a successful attack *will* significantly alter the distribution of votes within the patch region such that it becomes statistically distinguishable from the votes outside. It also implicitly assumes that the \\\"clean\\\" regions outside the patch provide a reliable baseline.\\n*   **Soundness:** The statistical methodology is sound for establishing probabilistic guarantees. The specific bounds on $P_d$ are derived mathematically based on the distributions of votes. The challenge in practice is always the trade-off between sensitivity (detecting true attacks) and specificity (avoiding false alarms), which depends heavily on the chosen threshold and the statistical separability created by the patch. The proof itself establishes what *can* be certified given a certain statistical difference.\\n\\n---\\n\\n### 4. Comparison to Existing Certified Defenses\\n\\nThe paper successfully positions itself within the landscape of certified defenses, highlighting its unique contributions:\\n\\n*   **Extension of Randomized Smoothing (RS):** The paper correctly identifies RS as its foundation. Previous RS work, primarily Cohen et al. (2019) and Salman et al. (2019), focused on *image classification* and typically against $L_p$ norm-bounded perturbations. This paper extends RS to a more complex task (semantic segmentation) and a different, more realistic threat model (adversarial patches).\\n*   **Certified Patch Robustness (Classification):** Some prior works, like Garg et al. (2020) and Levine et al. (2020), have addressed certified robustness against adversarial patches, but predominantly for *image classification*. This paper is a direct and logical extension of that line of work from classification to pixel-level segmentation. The challenges are distinct due to the output space and the localized nature of the attack affecting only a subset of the pixels.\\n*   **Certified Semantic Segmentation (L_p attacks):** While there's emerging work on certified robustness for semantic segmentation (e.g., Jia et al., 2021; Deng et al., 2022), most of these focus on $L_p$ bounded perturbations, which are less realistic for physical attacks than patches. The challenge of patches on segmentation is distinct because the attack is localized and can have arbitrary magnitude within the patch, directly changing a subset of pixel labels.\\n*   **Key Differentiation:** The paper's novelty lies in being one of the first, if not the first, to offer *certified guarantees* against *adversarial patch attacks* specifically for *semantic segmentation*. It successfully adapts the RS framework to handle the pixel-wise output and the localized nature of the patch, by providing separate guarantees for regions inside and outside the patch. This is a significant contribution to the field.\\n\\n---\\n\\n### 5. Limitations of the Proposed Approach in Terms of Scalability, Performance, or Applicability\\n\\n*   **Scalability & Performance (Major Limitation):**\\n    *   **Inference Time:** Randomized smoothing is inherently computationally expensive due to the Monte Carlo sampling process, requiring numerous forward passes (e.g., thousands) through the base classifier for each prediction. For semantic segmentation, this is applied pixel-wise or region-wise.\\n    *   **Patch Location Dependency:** The \\\"certified recovery\\\" for pixels outside the patch requires the defender to either *know* the patch's location or *scan* all possible patch locations. If the patch can be anywhere in an $H \\\\times W$ image, a naive scan would require running the smoothed prediction (each requiring many forward passes) for *every possible top-left corner* of the patch. This quickly becomes computationally prohibitive for real-time applications (e.g., $O(HW \\\\cdot N_{samples} \\\\cdot \\\\text{inference_time_of_base_model})$). The paper's experiments show inference times in minutes/hours for a single image, making it impractical for most real-world deployments. This is the most significant practical limitation.\\n    *   **Training Time:** Training a robust base classifier (though not explicitly detailed, usually a prerequisite for effective RS) can also be resource-intensive.\\n\\n*   **Applicability:**\\n    *   **Known Patch Parameters:** The method assumes the attacker's patch size and shape are known to the defender during inference to define the \\\"potential patch region\\\" for both recovery and detection. In real-world scenarios, these might vary.\\n    *   **Single Patch:** The current framework only considers a single adversarial patch. Multiple patches, which are also realistic, would drastically increase complexity and likely break the current certification methodology.\\n    *   **Scope of Certified Recovery:** As mentioned, certified recovery only applies to pixels *outside* the adversarial patch. If the patch covers a large portion of the image (e.g., a large billboard in an autonomous driving scene), the \\\"certified\\\" area of the image for recovery becomes small, potentially limiting its utility.\\n    *   **Detection Trade-offs:** While certified detection provides probabilistic guarantees, the practical deployment requires careful tuning of thresholds to manage false positive rates. A highly sensitive detector might trigger too many false alarms, while a less sensitive one might miss real attacks.\\n    *   **Practicality of \\\"Scanning\\\":** The paper hints at scanning for candidate patch locations. This is a crucial detail for applicability. Without an efficient mechanism to propose or locate potential patch regions, the approach has limited utility in open-world settings where the patch can be anywhere. The current approach assumes a *given* patch location or pre-selected candidates for certification.\\n\\n---\\n\\n### Conclusion\\n\\nThe paper makes a valuable theoretical contribution by being among the first to offer certified robustness guarantees against adversarial patch attacks for semantic segmentation. Its adaptation of randomized smoothing for both \\\"certified recovery\\\" (outside the patch) and \\\"certified detection\\\" (inside the patch) is technically sound and rigorously proven based on established statistical and certified robustness theories. The chosen threat model of adversarial patches is highly realistic and appropriate for the task.\\n\\nHowever, the approach suffers from significant practical limitations, primarily in terms of **scalability and performance**. The inherent computational cost of randomized smoothing, compounded by the need to potentially scan numerous possible patch locations for comprehensive certification, makes it currently impractical for real-time or large-scale deployment. Furthermore, the assumption of known patch parameters (size, shape) and the limitation to a single patch constrain its applicability in fully unconstrained real-world environments.\\n\\n**Actionable Feedback:**\\n1.  **Address Scalability:** Future work should focus on significantly improving the inference efficiency. This could involve:\\n    *   Exploring techniques to reduce the number of samples needed for RS (e.g., tighter bounds, adaptive sampling).\\n    *   Investigating methods for efficient *patch localization* or *region proposal* that are less computationally intensive than exhaustive scanning.\\n    *   Considering more efficient architectures for the base classifier or leveraging sparsity/redundancy in segmentation outputs.\\n2.  **Generalize Threat Model:** Explore extensions to multiple patches or patches with unknown/irregular shapes. This would push the boundaries of certified defenses.\\n3.  **Clarify Practical Deployment:** Explicitly discuss strategies for handling the \\\"patch can be anywhere\\\" problem for recovery in a practical setting, beyond just \\\"given candidate locations.\\\" This is crucial for its adoption.\\n4.  **False Positive Analysis:** While the paper focuses on detection probability given an attack, a more thorough analysis of false positive rates would be beneficial for practical deployment of the detection module.\\n\\nDespite its current practical limitations, this paper represents an important step forward in certified defenses against a highly relevant adversarial threat model for an increasingly critical perception task.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.84263625\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Evaluate the paper 'Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation' focusing on the rigor of the certification claims, the threat model, and comparisons to existing certified defenses. Provide a thorough and critical assessment, identifying potential flaws or weaknesses.\",\n  \"agent_name\": \"CertDefensePro\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The paper \\\"Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation\\\" proposes Demasked Smoothing, a novel certified defense mechanism against adversarial patch attacks. A core component of this framework is the image inpainting step, which aims to remove the adversarial patch and restore the underlying clean image content. As an image inpainting specialist, I will evaluate this component in detail.\\n\\n---\\n\\n### Evaluation of the Inpainting Component in Demasked Smoothing\\n\\nThe inpainting module is critical for the success of Demasked Smoothing. Its role is to reconstruct the original, unperturbed image content in the region where the adversarial patch was located. The quality and efficiency of this reconstruction directly impact the defense's effectiveness and practical applicability.\\n\\n#### 1. Justification for the Choice of the ZITS Inpainting Model\\n\\nThe paper states that ZITS (Zero-shot Inpainting via Textual Semantics, often referring to a specific GAN-based model trained for general inpainting) was chosen because it is \\\"a state-of-the-art model that performs well for this task.\\\"\\n\\n*   **Strengths of ZITS (or similar GAN-based models for inpainting):**\\n    *   **General Purpose Realism:** ZITS, being a GAN-based model, is adept at generating visually plausible textures and objects, often making the inpainted region blend seamlessly with the surrounding context. This is crucial for *visual fidelity*.\\n    *   **Efficiency (Relative):** Compared to more recent diffusion models, GANs generally offer faster inference times once trained, requiring a single forward pass through the generator network.\\n    *   **Zero-shot Capability:** The \\\"Zero-shot\\\" aspect implies it can handle various mask shapes and image content without specific fine-tuning for each scenario, which is beneficial given the varying nature of adversarial patches.\\n\\n*   **Critique of Justification:**\\n    *   **Lack of Specificity for Adversarial Patches:** While ZITS is \\\"state-of-the-art\\\" for general inpainting, the paper doesn't thoroughly justify *why* it's specifically optimal for *adversarial patches*. Adversarial patches often contain highly unusual, non-natural patterns designed to mislead. General inpainting models are typically trained on *naturalistic missing regions* (e.g., random masks, object removal). How well ZITS generalizes to removing highly structured, abstract, or colorful adversarial patterns is not explicitly evaluated.\\n    *   **\\\"Performs Well\\\":** This is a subjective claim without a comparative analysis against other inpainting methods in the context of adversarial defense. Did they experiment with others and find ZITS superior? This is unclear.\\n    *   **Semantic vs. Visual Fidelity:** For a *semantic segmentation* task, semantic consistency is paramount. ZITS, like many GAN-based inpainters, prioritizes visual plausibility. It might hallucinate a visually coherent but semantically incorrect object (e.g., a tree branch where a road sign should be), which could still lead to missegmentation even if the image \\\"looks\\\" good.\\n\\n#### 2. Impact of Inpainting Quality on Demasked Smoothing Performance\\n\\nThe quality of the inpainting is paramount and directly influences the effectiveness of the Demasked Smoothing framework.\\n\\n*   **Direct Impact on Accuracy:**\\n    *   **Clean Accuracy (Base Model):** If the inpainting process introduces artifacts or incorrect content, even when no adversarial patch is present (e.g., when simulating patch removal on a clean image), the segmentation model will produce errors. Poor inpainting can degrade the \\\"clean\\\" performance of the defense.\\n    *   **Certified Accuracy:** The core idea of Demasked Smoothing is that by averaging predictions from multiple inpainted versions of the image, the noise introduced by the patch is smoothed out, revealing the true underlying class. If inpainting quality is poor, the \\\"signal\\\" (correct content) might not be reliably recovered, or worse, \\\"noise\\\" (inpainting errors) might be consistently introduced, leading to incorrect aggregation and reduced certified accuracy.\\n\\n*   **Impact on Robustness:**\\n    *   **Misleading Hallucinations:** An inpainter might hallucinate content that is visually convincing but semantically incorrect for the segmentation task. For instance, inpainting a missing road section with a grass texture. This would fool the segmentation model into misclassifying the region even after the patch is removed.\\n    *   **Residual Information:** If the inpainter fails to completely erase the adversarial signal (e.g., leaving subtle textures or color biases from the patch), the segmentation model might still be influenced by this residual information, undermining the defense.\\n    *   **Variance in Predictions:** In Demasked Smoothing, the aggregation step relies on the inpainted images converging towards the true scene. If inpainting quality is inconsistent or poor across different samples (e.g., due to different random seeds for inpainting or varying patch sizes/locations), the variance in predictions will remain high, making robust certification difficult or impossible.\\n\\n#### 3. Potential Limitations of Using a Fixed Inpainting Model Trained on a Specific Dataset (Places2)\\n\\nThe use of a pre-trained ZITS model trained on Places2 presents several limitations:\\n\\n*   **Domain Shift:** Places2 is a very general scene dataset containing diverse indoor and outdoor environments. However, many semantic segmentation benchmarks, such as Cityscapes, focus on specific domains like urban street scenes.\\n    *   *Problem:* An inpainter trained on general scenes might not have learned to plausibly fill in missing regions with objects specific to urban environments (e.g., traffic lights, specific car models, complex road markings). It might generate generic \\\"natural\\\" looking textures that are incorrect for the target domain, even if visually appealing.\\n    *   *Example:* Inpainting a hole where a car was present might result in a generic patch of grass or sky from Places2, rather than a continuation of the road or sidewalk.\\n*   **Semantic Consistency Challenge:** Places2 training primarily focuses on visual coherence. The model is not explicitly trained to maintain *semantic consistency* that is critical for segmentation tasks. The generated content might look \\\"real\\\" but make no semantic sense in the context of the segmentation task (e.g., inpainting a building facade with generic landscape textures).\\n*   **Generalization to Adversarial Patterns:** As mentioned, Places2 does not contain adversarial patterns. The inpainter has only seen natural missing regions. Its ability to effectively \\\"erase\\\" and replace highly structured, non-natural, and potentially high-frequency adversarial patterns is an assumption, not a guaranteed outcome. Some adversarial patterns might confuse the inpainter, leading to \\\"stitching\\\" artifacts or incomplete removal.\\n*   **Lack of Task-Specific Optimization:** The inpainting model is optimized for general image restoration, not specifically for improving downstream semantic segmentation accuracy after patch removal. This means it may not prioritize the features most important for segmentation.\\n\\n#### 4. Alternative Inpainting Techniques That Could Be Explored\\n\\nGiven the limitations, several alternative or complementary inpainting approaches could be explored to enhance the Demasked Smoothing framework:\\n\\n*   **1. Domain-Specific Fine-tuning/Training:**\\n    *   **Approach:** Fine-tune an existing state-of-the-art inpainting model (like ZITS or a diffusion model) on synthetic patches/masks applied to images from the *target segmentation dataset* (e.g., Cityscapes).\\n    *   **Benefit:** This would address the domain shift problem directly, allowing the inpainter to learn to generate content consistent with the specific scene types and objects encountered during semantic segmentation.\\n*   **2. Semantic/Layout-Guided Inpainting:**\\n    *   **Approach:** Utilize inpainting models that can take additional semantic information (e.g., rough segmentation masks of surrounding objects, object labels) as input to guide the generation process. Examples include models built upon SPADE (Semantic Image Synthesis with SPADE) or more recent diffusion models with conditioning mechanisms.\\n    *   **Benefit:** Ensures that the inpainted content is not just visually plausible but also *semantically consistent* with the surrounding scene, leading to more accurate segmentation results. For example, if the inpainter knows a missing region is part of a road, it will generate road texture, not a random patch of sky.\\n*   **3. Diffusion Models for Inpainting:**\\n    *   **Approach:** Explore the use of latent diffusion models (e.g., Stable Diffusion's inpainting pipeline, DALL-E 2, Imagen).\\n    *   **Benefit:** Diffusion models often generate higher fidelity and more diverse results than GANs, with a better understanding of composition and context. They might be more robust to arbitrary patch patterns and produce more natural, semantically coherent fills.\\n    *   **Drawback:** Inference is typically significantly slower than GANs, as it involves an iterative denoising process.\\n*   **4. Perceptual Loss / Task-Aware Inpainting:**\\n    *   **Approach:** Instead of just optimizing for image quality (e.g., L1/L2, GAN loss), incorporate a loss term that encourages the inpainted image to yield correct predictions from the downstream segmentation model. This would require end-to-end fine-tuning or a more complex training strategy.\\n    *   **Benefit:** Explicitly optimizes the inpainting process for the ultimate goal of accurate segmentation, rather than just visual fidelity.\\n*   **5. Multi-Inpainter Ensemble:**\\n    *   **Approach:** Instead of relying on a single inpainter, use an ensemble of different inpainting models or different runs of the same model with varying parameters/seeds. Aggregate their outputs (e.g., by averaging inpainted pixels or feeding all to the smoothing process).\\n    *   **Benefit:** Could potentially improve robustness and reduce the impact of individual inpainter failures.\\n\\n#### 5. Analysis of Computational Cost and Efficiency of the Inpainting Step\\n\\nThe inpainting step adds significant computational overhead to the Demasked Smoothing framework, especially for certified defenses that require multiple inferences.\\n\\n*   **Inference Time:**\\n    *   **ZITS (GAN):** While faster than diffusion models, GAN inference is not instantaneous. For a high-resolution image (e.g., 1024x512, common in Cityscapes), a single inpainting pass on a GPU can take tens to hundreds of milliseconds.\\n    *   **Comparison to Segmentation Model:** The inpainting model itself (ZITS is often a large UNet-like generator) can be computationally as intensive, or even more so, than the target segmentation model (e.g., a ResNet-based FCN or DeepLabV3).\\n*   **Overall Framework Impact:**\\n    *   **Demasked Smoothing Certification:** The certification process in Demasked Smoothing requires sampling multiple random masks and running the *inpainting process* followed by the *segmentation model* for each sample. If $N$ samples are required for certification, the inpainting cost is multiplied by $N$.\\n    *   **Bottleneck:** Inpainting often becomes the bottleneck for the entire defense mechanism, especially when a high number of samples are needed for high confidence certification.\\n    *   **Real-time Applications:** For real-time applications like autonomous driving, this added latency is prohibitive. Even for offline batch processing, the computational resources required can be substantial.\\n*   **Hardware Requirements:** Inpainting models, especially those based on deep generative networks, are memory-intensive and require powerful GPUs for efficient inference. This increases the deployment cost.\\n\\n**Suggestions for Improvement regarding Cost:**\\n\\n*   **Model Pruning/Quantization:** Apply techniques like pruning, quantization, or knowledge distillation to the inpainting model to reduce its size and accelerate inference, potentially at a small cost to quality.\\n*   **Conditional Inpainting for Fewer Samples:** If the inpainter is more accurate and semantically consistent, fewer samples might be needed for reliable certification, indirectly reducing overall computational cost.\\n*   **Asynchronous Processing:** If the defense is integrated into a larger system, inpainting could potentially be parallelized or run asynchronously, but this doesn't reduce the core computational load per image.\\n*   **Smaller Models for Specific Patch Sizes:** If typical patch sizes are small, a smaller, more specialized inpainting model might suffice, reducing computation.\\n\\n---\\n\\n### Conclusion and Overall Assessment\\n\\nThe inpainting component is the backbone of Demasked Smoothing. Its quality directly determines the defense's ability to restore the original signal and enable accurate segmentation. While the choice of ZITS provides a good baseline for general image realism and acceptable inference speed for a GAN, the paper's justification is somewhat superficial, and key limitations arise from its training domain (Places2) and its general-purpose nature versus the specific semantic needs of segmentation.\\n\\n**Key Takeaways:**\\n\\n1.  **Inpainting Quality is Critical:** Poor inpainting leads to degraded clean accuracy and reduced certified robustness.\\n2.  **Domain Mismatch is a Major Concern:** Using a general-purpose inpainter trained on Places2 for urban semantic segmentation is a significant limitation that likely impacts certified accuracy due to semantic inconsistencies.\\n3.  **Computational Cost is High:** The inpainting step adds substantial overhead, making the certified defense computationally expensive, especially for real-time applications or large datasets.\\n\\n**Overall Recommendations:**\\n\\n*   **Prioritize Semantic Consistency:** Future work should explore inpainting models specifically designed or fine-tuned for semantic consistency relevant to the target segmentation task and dataset (e.g., Cityscapes). This is arguably more important than purely visual fidelity for this application.\\n*   **Investigate Domain-Adapted or Semantic-Guided Inpainting:** Experiment with fine-tuning ZITS on Cityscapes (with synthetic patches) or exploring models like diffusion models or semantic-guided inpainting that explicitly leverage segmentation information during the inpainting process.\\n*   **Benchmark Inpainting Performance:** Conduct a thorough comparative study of different inpainting models specifically in the context of adversarial patch removal and its impact on *segmentation metrics*, not just visual quality.\\n*   **Address Computational Efficiency:** Explore methods to accelerate inpainting inference (e.g., model compression, more efficient architectures, or investigating if simpler inpainters suffice for this specific task) to make the certified defense more practical.\\n\\nBy refining the inpainting component, particularly by addressing the domain shift and prioritizing semantic accuracy, Demasked Smoothing could achieve even higher certified accuracy and become a more robust and practical defense against adversarial patch attacks on semantic segmentation.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.84139095\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Evaluate the paper 'Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation' focusing on the image inpainting component, the choice of inpainting model, and its impact on performance. Provide a detailed analysis, suggesting potential improvements or alternative approaches.\",\n  \"agent_name\": \"InpaintAnalyst\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The paper \"Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation\" presents a novel approach to provide certified robustness against adversarial patch attacks for semantic segmentation models. This is a significant area of research, as robust perception is crucial for safety-critical applications. As a researcher specializing in semantic segmentation, I will evaluate the paper based on the specified criteria.\\n\\n## Detailed Assessment\\n\\n### 1. Suitability of Demasked Smoothing for Semantic Segmentation\\n\\n**Strengths:**\\n*   **Pixel-wise Certification:** This is the most crucial and innovative aspect. Semantic segmentation inherently requires per-pixel predictions, and Demasked Smoothing (DS) is adapted to provide *pixel-level* certification. This means the defense can provably guarantee that individual pixel classifications remain correct within a certain adversarial patch radius, which is a more granular and meaningful guarantee than image-level robustness.\\n*   **Targeted for Patch Attacks:** Adversarial patch attacks are a practical and realistic threat model, especially in real-world scenarios (e.g., a sticker on a stop sign). Demasked Smoothing, with its masking strategy, is well-suited to handle localized perturbations by effectively hiding or diluting the patch\\'s influence across multiple masked samples.\\n*   **Principled Certification:** The method leverages the strong theoretical foundations of randomized smoothing, adapting it to the segmentation task. This ensures that the robustness claims are not merely empirical but come with mathematical guarantees, quantified by a confidence level (e.g., 1-$\\\\alpha$).\\n*   **Generality:** The Demasked Smoothing framework appears to be largely agnostic to the underlying segmentation architecture (e.g., FCN, DeepLab, PSPNet), making it broadly applicable.\\n\\n**Weaknesses/Challenges:**\\n*   **Computational Cost:** This is arguably the biggest weakness. Certified defenses based on randomized smoothing inherently require a large number of forward passes (N samples per pixel/image) for prediction and even more for certification. For high-resolution images and complex segmentation models, this can lead to prohibitively high inference times, limiting practical deployment in real-time applications.\\n*   **Clean Accuracy Trade-off:** As is common with certified defenses, there is likely a trade-off between certified robustness and clean (unattacked) accuracy. While the paper should demonstrate reasonable mIoU, it\\'s critical to understand the magnitude of this drop and if it remains competitive. The process of masking inherently removes context, which can degrade performance for a model trained on unmasked images.\\n*   **Loss of Context:** By randomly masking portions of the input, the segmentation model might lose crucial contextual information necessary for accurate pixel-wise classification, especially for ambiguous regions or objects that rely heavily on global scene understanding. This could lead to a lower percentage of certifiable pixels or lower certified accuracy.\\n*   **Type of Perturbation:** While effective against patches, Demasked Smoothing might not be suitable for other types of adversarial attacks (e.g., global L-infinity or L-0 perturbations) unless adapted with different sampling/masking strategies.\\n\\n**Potential Improvements:**\\n*   **Efficiency Enhancements:** Investigate methods to reduce the number of samples required, such as adaptive sampling techniques or more efficient aggregation strategies (e.g., knowledge distillation to a more robust, faster model).\\n*   **Context Preservation:** Explore masking strategies that are more \"context-aware\" or less disruptive to global scene understanding, perhaps by using larger, overlapping masks or multi-scale masking.\\n*   **Hardware Acceleration:** Discuss the potential for parallelization and GPU optimization to mitigate the computational burden.\\n\\n### 2. Appropriateness of the Chosen Datasets (ADE20K, COCO-Stuff-10K)\\n\\n**Strengths:**\\n*   **Industry Benchmarks:** Both ADE20K and COCO-Stuff-10K are widely recognized and highly respected benchmarks in semantic segmentation research. Using these datasets ensures that the results are comparable to state-of-the-art methods and are relevant to the broader segmentation community.\\n*   **Diversity:**\\n    *   **ADE20K:** Features a vast number of classes (150) and diverse indoor/outdoor scenes, making it excellent for evaluating generalization across a wide range of object and scene categories. It\\'s a challenging dataset due to its complexity.\\n    *   **COCO-Stuff-10K:** Complements ADE20K by focusing on \"stuff\" categories (e.g., sky, road, grass) which often present different challenges than \"things\" (e.g., people, cars). It helps assess the defense\\'s performance on textured and amorphous regions.\\n*   **Scale:** ADE20K is a large-scale dataset, providing sufficient data for training complex segmentation models and evaluating robustness rigorously. COCO-Stuff-10K, while smaller, provides a good additional testbed.\\n\\n**Weaknesses/Gaps:**\\n*   **Computational Burden:** While appropriate, the sheer size and resolution of images in ADE20K will exacerbate the computational challenges of Demasked Smoothing. The paper should explicitly address how the computational cost scales with image resolution and dataset size.\\n*   **Real-world Scenarios:** While diverse, these datasets might not fully capture the specific nuances of adversarial patches in highly specialized real-world scenarios (e.g., specific lighting conditions, very small or distant objects). However, this is a general limitation of most academic datasets.\\n\\n**Potential Improvements:**\\n*   **Explicit Justification:** A more detailed justification in the paper for choosing these specific two datasets (e.g., ADE20K for scene understanding, COCO-Stuff-10K for stuff-centric analysis) would strengthen the experimental setup.\\n*   **Computational Breakdown:** Provide a clear breakdown of the computational cost (e.g., average inference time per image for certification) for each dataset to give readers a practical understanding of the method\\'s feasibility.\\n\\n### 3. Correctness and Relevance of the Evaluation Metrics\\n\\nThe choice of metrics is excellent and highly relevant for evaluating a certified defense against adversarial patches in semantic segmentation.\\n\\n**Metrics Used:**\\n*   **mIoU (Mean Intersection over Union):**\\n    *   **Relevance:** The standard and most critical metric for evaluating the overall clean performance of a semantic segmentation model. It measures the overlap between predicted and ground truth masks. Essential for understanding the baseline utility of the segmented output.\\n    *   **Correctness:** Correctly calculated as the average IoU across all classes.\\n*   **%C (Percentage of Certified Pixels):**\\n    *   **Relevance:** A primary metric for certified defenses. It quantifies the proportion of pixels for which the defense can provide a provable guarantee of correctness under a specified perturbation radius. This directly measures the \"coverage\" of the certification.\\n    *   **Correctness:** Should be clearly defined regarding the confidence level ($\\\\alpha$) and the specific patch radius being certified against.\\n*   **mR (Mean Robustness Radius):**\\n    *   **Relevance:** Measures the *strength* of the certification. For each pixel, it indicates the maximum adversarial patch radius for which the defense can still guarantee the correct classification. Averaging this across all pixels gives a holistic view of the defense\\'s robustness capability.\\n    *   **Correctness:** Defined as the largest certified patch radius.\\n*   **cmR (Correct Mean Robustness Radius):**\\n    *   **Relevance:** This is a crucial refinement of mR. It considers only those pixels that are *correctly classified* by the robust classifier (i.e., the certified label matches the ground truth). This is important because certifying an *incorrect* prediction is not useful. cmR focuses on the *useful* robustness guarantees.\\n    *   **Correctness:** Averages the robustness radius only for pixels whose certified class matches the ground truth.\\n*   **FAR (False Alarm Rate):**\\n    *   **Relevance:** Paramount for *certified* defenses. It measures how often the defense *claims* a pixel is robustly classified, but it actually *fails* under an adversarial attack within the certified radius. For a truly certified defense, FAR should be extremely low (ideally zero), indicating the trustworthiness of the theoretical guarantees.\\n    *   **Correctness:** Should be calculated as the proportion of certified pixels that are later found to be incorrect under attack.\\n\\n**Overall Assessment:**\\n*   **Strengths:** The combination of these metrics provides a comprehensive evaluation of the proposed certified defense. They cover clean performance, certification coverage, strength of certification, and the soundness/reliability of the certification. This is a very strong set of metrics for this type of work.\\n*   **Weaknesses/Gaps:**\\n    *   While %C, mR, and cmR are excellent, it would be beneficial to see how these metrics vary with different confidence levels ($\\\\alpha$).\\n    *   Visualizations of certified regions on example images (e.g., highlighting pixels certified for certain radii vs. those not certified) would significantly aid in understanding the defense\\'s behavior.\\n    *   Explicitly discuss the trade-off curve between clean mIoU and %C or cmR. This is a critical aspect of certified defenses.\\n\\n**Potential Improvements:**\\n*   Include visualizations of certified vs. uncertified regions on representative images.\\n*   Present results (e.g., %C, cmR) across a range of $\\\\alpha$ values to show the confidence-robustness trade-off.\\n\\n### 4. Comparison to Other Segmentation Methods\\n\\n**Expected Comparisons (and their relevance):**\\n*   **Undefended Baseline (e.g., standard FCN, DeepLabv3+):** Essential to demonstrate the vulnerability of current state-of-the-art segmentation models to adversarial patch attacks and to highlight the necessity of a defense.\\n*   **Empirical Defenses (e.g., Adversarial Training for segmentation):** This is a critical comparison. Adversarial Training (AT) is the most common empirical defense. The paper should compare the certified robustness of DS against the empirical robustness of AT. DS is expected to provide *provable* guarantees (low FAR, non-zero cmR), while AT only offers empirical robustness that can be broken by stronger, unseen attacks. This comparison highlights the core advantage of certification.\\n*   **Other Certified Defenses for Segmentation (if any):** If there are other efforts to provide certified robustness for segmentation (even if partial or for different attack models), a comparison is vital to position the work. If it\\'s truly the first of its kind for patch attacks on segmentation, then that claim should be strongly emphasized.\\n*   **Randomized Smoothing for Classification (adapted):** While Demasked Smoothing is a variant, it\\'s worth briefly discussing how it differs from simply applying standard noise-based randomized smoothing pixel-wise, and why masking is more appropriate for patch attacks.\\n\\n**Strengths of the Comparison:**\\n*   Comparing against undefended models clearly quantifies the problem being addressed.\\n*   Comparing against empirical defenses like AT is crucial for demonstrating the unique value proposition of *certified* robustness.\\n\\n**Weaknesses/Gaps:**\\n*   **Computational Overhead:** A significant gap if not thoroughly discussed. The computational cost of DS is likely orders of magnitude higher than undefended or even AT models. This needs to be quantitatively compared to provide a realistic assessment of deployability.\\n*   **Attack Strength for AT:** For a fair comparison with AT, the paper should specify the strength and type of adversarial attacks used during AT (e.g., PGD iterations, epsilon) as this affects the empirical robustness achieved.\\n*   **Direct Certified Segmentation Comparison:** If no other *certified* segmentation defense exists, the paper should clearly state this as a novel contribution, rather than just implicitly assuming it.\\n\\n**Potential Improvements:**\\n*   Provide a clear table comparing inference times and computational resources required for DS versus undefended and AT baselines.\\n*   Discuss the specific adversarial training setup (e.g., loss function, attack type, $\\\\epsilon$) used for the AT baseline to ensure reproducibility and fair comparison.\\n\\n### 5. Overall Contribution to the Field of Semantic Segmentation\\n\\n**Strengths:**\\n*   **Pioneering Work:** This paper represents a significant step forward by bringing the concept of *certified robustness* to the domain of semantic segmentation, specifically for challenging adversarial patch attacks. Providing provable guarantees for pixel-level predictions is a substantial methodological contribution.\\n*   **Addresses a Critical Gap:** Current semantic segmentation models are known to be vulnerable to adversarial attacks. This work addresses a critical gap by offering a theoretically sound defense mechanism, which is vital for deploying AI in safety-critical applications like autonomous driving, medical imaging, and surveillance.\\n*   **Methodological Innovation:** The adaptation of Demasked Smoothing for pixel-wise certification in segmentation, along with the definition of relevant evaluation metrics (%C, mR, cmR), is a valuable methodological contribution that opens new avenues for research in robust segmentation.\\n*   **Sets a New Standard:** By introducing certified robustness, the paper implicitly raises the bar for future research in robust semantic segmentation, moving beyond purely empirical defenses.\\n\\n**Weaknesses/Limitations (Overall):**\\n*   **Practicality due to Cost:** The high computational cost remains a major hurdle for widespread practical adoption, especially for real-time applications. Future work would need to significantly improve efficiency.\\n*   **Scope of Attack:** The defense is specifically tailored for patch attacks. While important, the generalizability to other common adversarial perturbation types (e.g., global noise) might be limited without further adaptations.\\n*   **Trade-off Clarity:** While the paper will likely show the trade-off, a deeper discussion on strategies to mitigate the accuracy-robustness trade-off and improve certified accuracy for clean images would be beneficial.\\n\\n**Overall Impression:**\\nThe paper makes a substantial and timely contribution to the field of semantic segmentation. By being among the first (if not *the* first) to offer certified robustness against adversarial patch attacks for pixel-level predictions, it pushes the boundaries of reliable and trustworthy AI for computer vision. While computational efficiency remains a significant challenge, the theoretical rigor and the novelty of the pixel-wise certification are highly commendable. This work is likely to inspire further research into developing more efficient and universally robust segmentation models.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.84417405}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'This paper proposes a novel certified defense mechanism against adversarial patch attacks specifically for semantic segmentation models. It extends the well-known randomized smoothing (RS) technique to provide pixel-wise robustness guarantees, offering both \"certified recovery\" for pixels outside the patch and \"certified detection\" for attacks within the patch region.\\n\\nHere\\'s a thorough and critical assessment of the paper:\\n\\n---\\n\\n### 1. Rigor of Certification Claims\\n\\nThe paper makes two distinct certification claims: Certified Recovery and Certified Detection.\\n\\n**1.1. Certified Recovery (Theorem 1)**\\n*   **Claim:** For any pixel $p$ that is *outside* the adversarial patch $\\\\mathcal{P}$, if its ground-truth label is $c$, then the proposed smoothed classifier $\\\\hat{f}$ will predict label $c$ with a probability $P_c > 0.5$, given a certain noise level $\\\\sigma$. This implies that the pixel $p$ is robust to the adversarial patch, as long as the patch does not *contain* $p$.\\n*   **Mechanism:** The core idea for certified recovery is to apply randomized smoothing *only to the pixels outside the potential patch region*. For a given pixel $p$, its prediction is certified if $p$ is known to be outside the adversarial patch. The method effectively trains a segmentation model that, for each pixel, aggregates votes from multiple noisy versions of the input image.\\n*   **Soundness:** The proof leverages the established theoretical guarantees of randomized smoothing (e.g., Cohen et al., 2019). The crucial insight is that if a pixel $p$ is *outside* the patch, its true label is undisturbed by the patch. The Gaussian noise added for smoothing does not change the patch\\'s fixed location relative to the image. Therefore, for any pixel $p \\\\notin \\\\mathcal{P}$, the prediction mechanism is essentially operating on a clean image corrupted by Gaussian noise, and the standard RS guarantees for robustness against $L_2$ perturbations (which Gaussian noise effectively simulates) apply.\\n*   **Limitations in Scope:** The rigor is high *within its stated scope*. However, it\\'s crucial to understand what it *doesn\\'t* certify. It does **not** certify the predictions for pixels *within* the potential patch region. This is a fundamental limitation: if the patch is designed to attack a specific region, this method explicitly gives up on certifying that region. The certified region shrinks as the potential patch size increases. In a worst-case scenario where the patch can be extremely large, the certified \"recovery\" area might become negligible.\\n\\n**1.2. Certified Detection (Theorem 2)**\\n*   **Claim:** If an adversarial patch is present within a region $R$ and causes a misclassification of a pixel $p \\\\in R$, the method can detect this attack with a certain probability $P_d$, provided some conditions on the difference in smoothed probabilities inside and outside the region $R$.\\n*   **Mechanism:** This is framed as a statistical hypothesis testing problem. The method compares the distribution of predictions (or the confidence scores) within the suspected patch region against those outside it. A significant statistical difference triggers a detection.\\n*   **Soundness:** The proof relies on Chernoff bounds or similar concentration inequalities to establish a lower bound on the probability of detecting an attack. This is a standard approach for statistical detection systems. The notion of certifying a *probability* of detection, given that an attack causes a specific change, is a valid and rigorous form of certification.\\n*   **Limitations in Scope:**\\n    *   **Probabilistic Guarantee:** It\\'s a probabilistic guarantee for detection, not a deterministic one. There\\'s always a trade-off between the probability of detection ($P_d$) and the probability of false positives (incorrectly flagging a clean input as an attack). The paper characterizes $P_d$ but implicitly relies on the user setting appropriate thresholds for practical use.\\n    *   **Requires Change:** It detects attacks that *successfully* cause a misclassification within the patch region and *sufficiently* perturb the local prediction distribution. If an attack is very subtle or causes changes below the detection threshold, it might not be detected.\\n    *   **Statistical nature:** Like all statistical tests, the effectiveness depends on the magnitude of the effect caused by the patch. A small, inconspicuous patch might be harder to detect reliably without increasing false alarms.\\n\\n**Overall Rigor:** The mathematical framework is sound, drawing appropriately from established randomized smoothing theory and statistical hypothesis testing. The claims are precisely stated, distinguishing what is and isn\\'t certified. The novelty lies in the *application* and *adaptation* of these concepts to the challenging problem of patch attacks on segmentation.\\n\\n---\\n\\n### 2. Appropriateness and Realism of the Threat Model (Adversarial Patches)\\n\\n*   **Threat Model:** The paper considers a single, rectangular adversarial patch of a fixed size and shape ($S \\\\times S$ pixels), which can be placed anywhere on the input image. The pixel values within the patch can be arbitrary.\\n*   **Realism:**\\n    *   **High Realism:** Adversarial patches are one of the most physically realistic and actionable types of adversarial attacks. Unlike $L_p$-norm bounded perturbations (which assume global, small changes to every pixel), patches can be printed as stickers or placed as physical objects in the environment, making them highly relevant for real-world applications like autonomous driving or surveillance.\\n    *   **Placement \"Anywhere\":** The ability to place the patch anywhere on the image significantly increases the challenge for defenses, mirroring real-world scenarios.\\n    *   **Arbitrary Pixel Values:** This makes the attacker very powerful within the patch region, which is also realistic.\\n*   **Appropriateness for Segmentation:** For semantic segmentation, which is a pixel-wise prediction task, a localized patch attack is particularly challenging because it can dramatically alter the predicted class of all pixels *within* its boundary, potentially causing a small object to disappear or a large area to be misclassified. This makes it an entirely appropriate and relevant threat model for the task.\\n*   **Limitations within the Threat Model:**\\n    *   **Single Patch:** The model considers only a single adversarial patch. Real-world scenarios might involve multiple patches. Certifying against multiple patches would be significantly more complex.\\n    *   **Fixed Size/Shape:** While common in research, assuming a fixed rectangular shape and size might be a slight idealization. Real-world patches could be irregular or of varying sizes. However, this is a reasonable simplification for initial certified defenses.\\n    *   **Crucial Implication for Recovery:** While the threat model allows the patch to be *anywhere*, the *certified recovery* mechanism critically relies on the defender either knowing *where the patch is not* or having to *scan all possible patch locations*. If the defender does not know the patch\\'s location, to guarantee recovery for *all* pixels outside the patch, the defender needs to apply the certification process across a grid of all possible patch locations. This quickly becomes computationally prohibitive, limiting the practical applicability of the \"certified recovery for pixels outside the patch\" claim unless the set of possible patch locations is severely restricted or known. The paper acknowledges this by stating they certify for a *given* candidate patch location. This is a vital distinction: the defense isn\\'t certified to recover *regardless* of patch location, but rather to recover *if* the patch is *not* at the certified pixel\\'s location.\\n\\n---\\n\\n### 3. Soundness of Proofs for Certified Recovery and Certified Detection\\n\\nBoth proofs build upon the well-established mathematical foundation of randomized smoothing.\\n\\n**3.1. Certified Recovery (Theorem 1):**\\n*   **Proof Logic:** The proof for Theorem 1 correctly applies the principles of randomized smoothing. For a pixel $p$ outside the patch region, the patch itself does not alter the ground truth label of that pixel. When Gaussian noise is added for smoothing, the patch remains in its fixed location. Therefore, the effect on pixel $p$ is primarily due to the Gaussian noise, and the standard robustness guarantees derived from RS (e.g., Cohen et al.\\'s bound on the $L_2$ radius) apply directly to the prediction of $p$.\\n*   **Assumptions:** The critical assumption is that the adversarial patch is a *fixed, deterministic perturbation* on the input image, and the Gaussian noise is *added on top* of this possibly perturbed image. This is a standard and valid assumption in certified robustness literature.\\n*   **Soundness:** The proof is sound, provided the underlying randomized smoothing theorems it relies upon are sound. The adaptation to segmentation (pixel-wise classification) is straightforward as each pixel\\'s classification can be treated as an independent task for certification.\\n\\n**3.2. Certified Detection (Theorem 2):**\\n*   **Proof Logic:** The proof for Theorem 2 relies on statistical hypothesis testing. It defines a test statistic based on the difference in vote counts (or similar aggregated predictions) between the region under suspicion (the potential patch region) and an unaffected reference region. If this difference exceeds a calculated threshold, an attack is detected. The derivation of the threshold and the probability of detection $P_d$ (and implicit false positive rates) typically involves concentration inequalities like Chernoff bounds or Hoeffding\\'s inequality, which are standard tools in robust statistics.\\n*   **Assumptions:** The proof assumes that a successful attack *will* significantly alter the distribution of votes within the patch region such that it becomes statistically distinguishable from the votes outside. It also implicitly assumes that the \"clean\" regions outside the patch provide a reliable baseline.\\n*   **Soundness:** The statistical methodology is sound for establishing probabilistic guarantees. The specific bounds on $P_d$ are derived mathematically based on the distributions of votes. The challenge in practice is always the trade-off between sensitivity (detecting true attacks) and specificity (avoiding false alarms), which depends heavily on the chosen threshold and the statistical separability created by the patch. The proof itself establishes what *can* be certified given a certain statistical difference.\\n\\n---\\n\\n### 4. Comparison to Existing Certified Defenses\\n\\nThe paper successfully positions itself within the landscape of certified defenses, highlighting its unique contributions:\\n\\n*   **Extension of Randomized Smoothing (RS):** The paper correctly identifies RS as its foundation. Previous RS work, primarily Cohen et al. (2019) and Salman et al. (2019), focused on *image classification* and typically against $L_p$ norm-bounded perturbations. This paper extends RS to a more complex task (semantic segmentation) and a different, more realistic threat model (adversarial patches).\\n*   **Certified Patch Robustness (Classification):** Some prior works, like Garg et al. (2020) and Levine et al. (2020), have addressed certified robustness against adversarial patches, but predominantly for *image classification*. This paper is a direct and logical extension of that line of work from classification to pixel-level segmentation. The challenges are distinct due to the output space and the localized nature of the attack affecting only a subset of the pixels.\\n*   **Certified Semantic Segmentation (L_p attacks):** While there\\'s emerging work on certified robustness for semantic segmentation (e.g., Jia et al., 2021; Deng et al., 2022), most of these focus on $L_p$ bounded perturbations, which are less realistic for physical attacks than patches. The challenge of patches on segmentation is distinct because the attack is localized and can have arbitrary magnitude within the patch, directly changing a subset of pixel labels.\\n*   **Key Differentiation:** The paper\\'s novelty lies in being one of the first, if not the first, to offer *certified guarantees* against *adversarial patch attacks* specifically for *semantic segmentation*. It successfully adapts the RS framework to handle the pixel-wise output and the localized nature of the patch, by providing separate guarantees for regions inside and outside the patch. This is a significant contribution to the field.\\n\\n---\\n\\n### 5. Limitations of the Proposed Approach in Terms of Scalability, Performance, or Applicability\\n\\n*   **Scalability & Performance (Major Limitation):**\\n    *   **Inference Time:** Randomized smoothing is inherently computationally expensive due to the Monte Carlo sampling process, requiring numerous forward passes (e.g., thousands) through the base classifier for each prediction. For semantic segmentation, this is applied pixel-wise or region-wise.\\n    *   **Patch Location Dependency:** The \"certified recovery\" for pixels outside the patch requires the defender to either *know* the patch\\'s location or *scan* all possible patch locations. If the patch can be anywhere in an $H \\\\times W$ image, a naive scan would require running the smoothed prediction (each requiring many forward passes) for *every possible top-left corner* of the patch. This quickly becomes computationally prohibitive for real-time applications (e.g., $O(HW \\\\cdot N_{samples} \\\\cdot \\\\text{inference_time_of_base_model})$). The paper\\'s experiments show inference times in minutes/hours for a single image, making it impractical for most real-world deployments. This is the most significant practical limitation.\\n    *   **Training Time:** Training a robust base classifier (though not explicitly detailed, usually a prerequisite for effective RS) can also be resource-intensive.\\n\\n*   **Applicability:**\\n    *   **Known Patch Parameters:** The method assumes the attacker\\'s patch size and shape are known to the defender during inference to define the \"potential patch region\" for both recovery and detection. In real-world scenarios, these might vary.\\n    *   **Single Patch:** The current framework only considers a single adversarial patch. Multiple patches, which are also realistic, would drastically increase complexity and likely break the current certification methodology.\\n    *   **Scope of Certified Recovery:** As mentioned, certified recovery only applies to pixels *outside* the adversarial patch. If the patch covers a large portion of the image (e.g., a large billboard in an autonomous driving scene), the \"certified\" area of the image for recovery becomes small, potentially limiting its utility.\\n    *   **Detection Trade-offs:** While certified detection provides probabilistic guarantees, the practical deployment requires careful tuning of thresholds to manage false positive rates. A highly sensitive detector might trigger too many false alarms, while a less sensitive one might miss real attacks.\\n    *   **Practicality of \"Scanning\":** The paper hints at scanning for candidate patch locations. This is a crucial detail for applicability. Without an efficient mechanism to propose or locate potential patch regions, the approach has limited utility in open-world settings where the patch can be anywhere. The current approach assumes a *given* patch location or pre-selected candidates for certification.\\n\\n---\\n\\n### Conclusion\\n\\nThe paper makes a valuable theoretical contribution by being among the first to offer certified robustness guarantees against adversarial patch attacks for semantic segmentation. Its adaptation of randomized smoothing for both \"certified recovery\" (outside the patch) and \"certified detection\" (inside the patch) is technically sound and rigorously proven based on established statistical and certified robustness theories. The chosen threat model of adversarial patches is highly realistic and appropriate for the task.\\n\\nHowever, the approach suffers from significant practical limitations, primarily in terms of **scalability and performance**. The inherent computational cost of randomized smoothing, compounded by the need to potentially scan numerous possible patch locations for comprehensive certification, makes it currently impractical for real-time or large-scale deployment. Furthermore, the assumption of known patch parameters (size, shape) and the limitation to a single patch constrain its applicability in fully unconstrained real-world environments.\\n\\n**Actionable Feedback:**\\n1.  **Address Scalability:** Future work should focus on significantly improving the inference efficiency. This could involve:\\n    *   Exploring techniques to reduce the number of samples needed for RS (e.g., tighter bounds, adaptive sampling).\\n    *   Investigating methods for efficient *patch localization* or *region proposal* that are less computationally intensive than exhaustive scanning.\\n    *   Considering more efficient architectures for the base classifier or leveraging sparsity/redundancy in segmentation outputs.\\n2.  **Generalize Threat Model:** Explore extensions to multiple patches or patches with unknown/irregular shapes. This would push the boundaries of certified defenses.\\n3.  **Clarify Practical Deployment:** Explicitly discuss strategies for handling the \"patch can be anywhere\" problem for recovery in a practical setting, beyond just \"given candidate locations.\" This is crucial for its adoption.\\n4.  **False Positive Analysis:** While the paper focuses on detection probability given an attack, a more thorough analysis of false positive rates would be beneficial for practical deployment of the detection module.\\n\\nDespite its current practical limitations, this paper represents an important step forward in certified defenses against a highly relevant adversarial threat model for an increasingly critical perception task.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.84263625}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The paper \"Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation\" proposes Demasked Smoothing, a novel certified defense mechanism against adversarial patch attacks. A core component of this framework is the image inpainting step, which aims to remove the adversarial patch and restore the underlying clean image content. As an image inpainting specialist, I will evaluate this component in detail.\\n\\n---\\n\\n### Evaluation of the Inpainting Component in Demasked Smoothing\\n\\nThe inpainting module is critical for the success of Demasked Smoothing. Its role is to reconstruct the original, unperturbed image content in the region where the adversarial patch was located. The quality and efficiency of this reconstruction directly impact the defense\\'s effectiveness and practical applicability.\\n\\n#### 1. Justification for the Choice of the ZITS Inpainting Model\\n\\nThe paper states that ZITS (Zero-shot Inpainting via Textual Semantics, often referring to a specific GAN-based model trained for general inpainting) was chosen because it is \"a state-of-the-art model that performs well for this task.\"\\n\\n*   **Strengths of ZITS (or similar GAN-based models for inpainting):**\\n    *   **General Purpose Realism:** ZITS, being a GAN-based model, is adept at generating visually plausible textures and objects, often making the inpainted region blend seamlessly with the surrounding context. This is crucial for *visual fidelity*.\\n    *   **Efficiency (Relative):** Compared to more recent diffusion models, GANs generally offer faster inference times once trained, requiring a single forward pass through the generator network.\\n    *   **Zero-shot Capability:** The \"Zero-shot\" aspect implies it can handle various mask shapes and image content without specific fine-tuning for each scenario, which is beneficial given the varying nature of adversarial patches.\\n\\n*   **Critique of Justification:**\\n    *   **Lack of Specificity for Adversarial Patches:** While ZITS is \"state-of-the-art\" for general inpainting, the paper doesn\\'t thoroughly justify *why* it\\'s specifically optimal for *adversarial patches*. Adversarial patches often contain highly unusual, non-natural patterns designed to mislead. General inpainting models are typically trained on *naturalistic missing regions* (e.g., random masks, object removal). How well ZITS generalizes to removing highly structured, abstract, or colorful adversarial patterns is not explicitly evaluated.\\n    *   **\"Performs Well\":** This is a subjective claim without a comparative analysis against other inpainting methods in the context of adversarial defense. Did they experiment with others and find ZITS superior? This is unclear.\\n    *   **Semantic vs. Visual Fidelity:** For a *semantic segmentation* task, semantic consistency is paramount. ZITS, like many GAN-based inpainters, prioritizes visual plausibility. It might hallucinate a visually coherent but semantically incorrect object (e.g., a tree branch where a road sign should be), which could still lead to missegmentation even if the image \"looks\" good.\\n\\n#### 2. Impact of Inpainting Quality on Demasked Smoothing Performance\\n\\nThe quality of the inpainting is paramount and directly influences the effectiveness of the Demasked Smoothing framework.\\n\\n*   **Direct Impact on Accuracy:**\\n    *   **Clean Accuracy (Base Model):** If the inpainting process introduces artifacts or incorrect content, even when no adversarial patch is present (e.g., when simulating patch removal on a clean image), the segmentation model will produce errors. Poor inpainting can degrade the \"clean\" performance of the defense.\\n    *   **Certified Accuracy:** The core idea of Demasked Smoothing is that by averaging predictions from multiple inpainted versions of the image, the noise introduced by the patch is smoothed out, revealing the true underlying class. If inpainting quality is poor, the \"signal\" (correct content) might not be reliably recovered, or worse, \"noise\" (inpainting errors) might be consistently introduced, leading to incorrect aggregation and reduced certified accuracy.\\n\\n*   **Impact on Robustness:**\\n    *   **Misleading Hallucinations:** An inpainter might hallucinate content that is visually convincing but semantically incorrect for the segmentation task. For instance, inpainting a missing road section with a grass texture. This would fool the segmentation model into misclassifying the region even after the patch is removed.\\n    *   **Residual Information:** If the inpainter fails to completely erase the adversarial signal (e.g., leaving subtle textures or color biases from the patch), the segmentation model might still be influenced by this residual information, undermining the defense.\\n    *   **Variance in Predictions:** In Demasked Smoothing, the aggregation step relies on the inpainted images converging towards the true scene. If inpainting quality is inconsistent or poor across different samples (e.g., due to different random seeds for inpainting or varying patch sizes/locations), the variance in predictions will remain high, making robust certification difficult or impossible.\\n\\n#### 3. Potential Limitations of Using a Fixed Inpainting Model Trained on a Specific Dataset (Places2)\\n\\nThe use of a pre-trained ZITS model trained on Places2 presents several limitations:\\n\\n*   **Domain Shift:** Places2 is a very general scene dataset containing diverse indoor and outdoor environments. However, many semantic segmentation benchmarks, such as Cityscapes, focus on specific domains like urban street scenes.\\n    *   *Problem:* An inpainter trained on general scenes might not have learned to plausibly fill in missing regions with objects specific to urban environments (e.g., traffic lights, specific car models, complex road markings). It might generate generic \"natural\" looking textures that are incorrect for the target domain, even if visually appealing.\\n    *   *Example:* Inpainting a hole where a car was present might result in a generic patch of grass or sky from Places2, rather than a continuation of the road or sidewalk.\\n*   **Semantic Consistency Challenge:** Places2 training primarily focuses on visual coherence. The model is not explicitly trained to maintain *semantic consistency* that is critical for segmentation tasks. The generated content might look \"real\" but make no semantic sense in the context of the segmentation task (e.g., inpainting a building facade with generic landscape textures).\\n*   **Generalization to Adversarial Patterns:** As mentioned, Places2 does not contain adversarial patterns. The inpainter has only seen natural missing regions. Its ability to effectively \"erase\" and replace highly structured, non-natural, and potentially high-frequency adversarial patterns is an assumption, not a guaranteed outcome. Some adversarial patterns might confuse the inpainter, leading to \"stitching\" artifacts or incomplete removal.\\n*   **Lack of Task-Specific Optimization:** The inpainting model is optimized for general image restoration, not specifically for improving downstream semantic segmentation accuracy after patch removal. This means it may not prioritize the features most important for segmentation.\\n\\n#### 4. Alternative Inpainting Techniques That Could Be Explored\\n\\nGiven the limitations, several alternative or complementary inpainting approaches could be explored to enhance the Demasked Smoothing framework:\\n\\n*   **1. Domain-Specific Fine-tuning/Training:**\\n    *   **Approach:** Fine-tune an existing state-of-the-art inpainting model (like ZITS or a diffusion model) on synthetic patches/masks applied to images from the *target segmentation dataset* (e.g., Cityscapes).\\n    *   **Benefit:** This would address the domain shift problem directly, allowing the inpainter to learn to generate content consistent with the specific scene types and objects encountered during semantic segmentation.\\n*   **2. Semantic/Layout-Guided Inpainting:**\\n    *   **Approach:** Utilize inpainting models that can take additional semantic information (e.g., rough segmentation masks of surrounding objects, object labels) as input to guide the generation process. Examples include models built upon SPADE (Semantic Image Synthesis with SPADE) or more recent diffusion models with conditioning mechanisms.\\n    *   **Benefit:** Ensures that the inpainted content is not just visually plausible but also *semantically consistent* with the surrounding scene, leading to more accurate segmentation results. For example, if the inpainter knows a missing region is part of a road, it will generate road texture, not a random patch of sky.\\n*   **3. Diffusion Models for Inpainting:**\\n    *   **Approach:** Explore the use of latent diffusion models (e.g., Stable Diffusion\\'s inpainting pipeline, DALL-E 2, Imagen).\\n    *   **Benefit:** Diffusion models often generate higher fidelity and more diverse results than GANs, with a better understanding of composition and context. They might be more robust to arbitrary patch patterns and produce more natural, semantically coherent fills.\\n    *   **Drawback:** Inference is typically significantly slower than GANs, as it involves an iterative denoising process.\\n*   **4. Perceptual Loss / Task-Aware Inpainting:**\\n    *   **Approach:** Instead of just optimizing for image quality (e.g., L1/L2, GAN loss), incorporate a loss term that encourages the inpainted image to yield correct predictions from the downstream segmentation model. This would require end-to-end fine-tuning or a more complex training strategy.\\n    *   **Benefit:** Explicitly optimizes the inpainting process for the ultimate goal of accurate segmentation, rather than just visual fidelity.\\n*   **5. Multi-Inpainter Ensemble:**\\n    *   **Approach:** Instead of relying on a single inpainter, use an ensemble of different inpainting models or different runs of the same model with varying parameters/seeds. Aggregate their outputs (e.g., by averaging inpainted pixels or feeding all to the smoothing process).\\n    *   **Benefit:** Could potentially improve robustness and reduce the impact of individual inpainter failures.\\n\\n#### 5. Analysis of Computational Cost and Efficiency of the Inpainting Step\\n\\nThe inpainting step adds significant computational overhead to the Demasked Smoothing framework, especially for certified defenses that require multiple inferences.\\n\\n*   **Inference Time:**\\n    *   **ZITS (GAN):** While faster than diffusion models, GAN inference is not instantaneous. For a high-resolution image (e.g., 1024x512, common in Cityscapes), a single inpainting pass on a GPU can take tens to hundreds of milliseconds.\\n    *   **Comparison to Segmentation Model:** The inpainting model itself (ZITS is often a large UNet-like generator) can be computationally as intensive, or even more so, than the target segmentation model (e.g., a ResNet-based FCN or DeepLabV3).\\n*   **Overall Framework Impact:**\\n    *   **Demasked Smoothing Certification:** The certification process in Demasked Smoothing requires sampling multiple random masks and running the *inpainting process* followed by the *segmentation model* for each sample. If $N$ samples are required for certification, the inpainting cost is multiplied by $N$.\\n    *   **Bottleneck:** Inpainting often becomes the bottleneck for the entire defense mechanism, especially when a high number of samples are needed for high confidence certification.\\n    *   **Real-time Applications:** For real-time applications like autonomous driving, this added latency is prohibitive. Even for offline batch processing, the computational resources required can be substantial.\\n*   **Hardware Requirements:** Inpainting models, especially those based on deep generative networks, are memory-intensive and require powerful GPUs for efficient inference. This increases the deployment cost.\\n\\n**Suggestions for Improvement regarding Cost:**\\n\\n*   **Model Pruning/Quantization:** Apply techniques like pruning, quantization, or knowledge distillation to the inpainting model to reduce its size and accelerate inference, potentially at a small cost to quality.\\n*   **Conditional Inpainting for Fewer Samples:** If the inpainter is more accurate and semantically consistent, fewer samples might be needed for reliable certification, indirectly reducing overall computational cost.\\n*   **Asynchronous Processing:** If the defense is integrated into a larger system, inpainting could potentially be parallelized or run asynchronously, but this doesn\\'t reduce the core computational load per image.\\n*   **Smaller Models for Specific Patch Sizes:** If typical patch sizes are small, a smaller, more specialized inpainting model might suffice, reducing computation.\\n\\n---\\n\\n### Conclusion and Overall Assessment\\n\\nThe inpainting component is the backbone of Demasked Smoothing. Its quality directly determines the defense\\'s ability to restore the original signal and enable accurate segmentation. While the choice of ZITS provides a good baseline for general image realism and acceptable inference speed for a GAN, the paper\\'s justification is somewhat superficial, and key limitations arise from its training domain (Places2) and its general-purpose nature versus the specific semantic needs of segmentation.\\n\\n**Key Takeaways:**\\n\\n1.  **Inpainting Quality is Critical:** Poor inpainting leads to degraded clean accuracy and reduced certified robustness.\\n2.  **Domain Mismatch is a Major Concern:** Using a general-purpose inpainter trained on Places2 for urban semantic segmentation is a significant limitation that likely impacts certified accuracy due to semantic inconsistencies.\\n3.  **Computational Cost is High:** The inpainting step adds substantial overhead, making the certified defense computationally expensive, especially for real-time applications or large datasets.\\n\\n**Overall Recommendations:**\\n\\n*   **Prioritize Semantic Consistency:** Future work should explore inpainting models specifically designed or fine-tuned for semantic consistency relevant to the target segmentation task and dataset (e.g., Cityscapes). This is arguably more important than purely visual fidelity for this application.\\n*   **Investigate Domain-Adapted or Semantic-Guided Inpainting:** Experiment with fine-tuning ZITS on Cityscapes (with synthetic patches) or exploring models like diffusion models or semantic-guided inpainting that explicitly leverage segmentation information during the inpainting process.\\n*   **Benchmark Inpainting Performance:** Conduct a thorough comparative study of different inpainting models specifically in the context of adversarial patch removal and its impact on *segmentation metrics*, not just visual quality.\\n*   **Address Computational Efficiency:** Explore methods to accelerate inpainting inference (e.g., model compression, more efficient architectures, or investigating if simpler inpainters suffice for this specific task) to make the certified defense more practical.\\n\\nBy refining the inpainting component, particularly by addressing the domain shift and prioritizing semantic accuracy, Demasked Smoothing could achieve even higher certified accuracy and become a more robust and practical defense against adversarial patch attacks on semantic segmentation.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.84139095}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "The agents have provided comprehensive reviews of the paper from their specialized perspectives. Here's a synthesis of their assessments:\n\n*   **SegExpert:** Highlights the novelty of pixel-wise certification for semantic segmentation and the suitability of the chosen datasets. However, it raises concerns about the computational cost and the trade-off between clean accuracy and robustness.\n*   **CertDefensePro:** Validates the rigor of the certification claims and the realism of the threat model. It points out the limitations of the approach in terms of scalability and the assumption of known patch parameters.\n*   **InpaintAnalyst:** Emphasizes the critical role of the inpainting component and the potential limitations of using a fixed inpainting model trained on a specific dataset. It suggests exploring domain-specific fine-tuning and alternative inpainting techniques.\n\nOverall, the paper presents a novel and technically sound approach to certified defense against adversarial patch attacks on semantic segmentation. The reviewers agree that the work makes a significant contribution to the field by addressing a critical gap in robust perception. However, they also identify several limitations, particularly regarding computational cost and the need for further refinement of the inpainting component.\n\nConsidering the novelty of the approach and the potential for future improvements, I recommend accepting the paper.\n\nFINAL DECISION: Accept\n"}], "ground_truth": "Accept: poster", "response_history": "The agents have provided comprehensive reviews of the paper from their specialized perspectives. Here's a synthesis of their assessments:\n\n*   **SegExpert:** Highlights the novelty of pixel-wise certification for semantic segmentation and the suitability of the chosen datasets. However, it raises concerns about the computational cost and the trade-off between clean accuracy and robustness.\n*   **CertDefensePro:** Validates the rigor of the certification claims and the realism of the threat model. It points out the limitations of the approach in terms of scalability and the assumption of known patch parameters.\n*   **InpaintAnalyst:** Emphasizes the critical role of the inpainting component and the potential limitations of using a fixed inpainting model trained on a specific dataset. It suggests exploring domain-specific fine-tuning and alternative inpainting techniques.\n\nOverall, the paper presents a novel and technically sound approach to certified defense against adversarial patch attacks on semantic segmentation. The reviewers agree that the work makes a significant contribution to the field by addressing a critical gap in robust perception. However, they also identify several limitations, particularly regarding computational cost and the need for further refinement of the inpainting component.\n\nConsidering the novelty of the approach and the potential for future improvements, I recommend accepting the paper.\n\nFINAL DECISION: Accept\n", "elapsed_time": 144.75290322303772, "reviewer_agents": {"SegExpert": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in semantic segmentation, focusing on the suitability of the method for the task, datasets, and evaluation metrics.", "system_prompt": "You are a highly skilled researcher specializing in semantic segmentation. You have extensive knowledge of various segmentation techniques, datasets, and evaluation metrics. Your task is to evaluate the provided paper, focusing on the following aspects:\n\n*   Suitability of Demasked Smoothing for semantic segmentation.\n*   Appropriateness of the chosen datasets (ADE20K, COCO-Stuff-10K) for evaluating the proposed defense.\n*   Correctness and relevance of the evaluation metrics used (mIoU, %C, mR, cmR, FAR).\n*   Comparison to other segmentation methods.\n*   Overall contribution to the field of semantic segmentation.\n\nProvide a detailed assessment of these aspects, highlighting strengths and weaknesses, and suggesting potential improvements. Your review should be rigorous and constructive.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "CertDefensePro": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Specialist in certified defenses against adversarial attacks, assessing the rigor of certification claims and the threat model.", "system_prompt": "You are an expert in certified defenses against adversarial attacks. You have a deep understanding of different certification techniques, threat models, and robustness guarantees. Your task is to evaluate the provided paper, focusing on the following aspects:\n\n*   Rigor of the certification claims made in the paper.\n*   Appropriateness and realism of the threat model considered (adversarial patches).\n*   Soundness of the proofs for the certified recovery and certified detection theorems.\n*   Comparison to existing certified defenses against adversarial attacks.\n*   Limitations of the proposed approach in terms of scalability, performance, or applicability.\n\nProvide a thorough and critical assessment of these aspects, identifying any potential flaws or weaknesses in the certification arguments. Your review should be technically sound and provide actionable feedback.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "InpaintAnalyst": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Concentrates on the image inpainting component, evaluating the choice of inpainting model and its impact on performance.", "system_prompt": "You are a specialist in image inpainting techniques. You have a strong understanding of various inpainting models, their strengths and weaknesses, and their impact on downstream tasks. Your task is to evaluate the provided paper, focusing on the following aspects:\n\n*   Justification for the choice of the ZITS inpainting model.\n*   Impact of the inpainting quality on the performance of the Demasked Smoothing framework.\n*   Potential limitations of using a fixed inpainting model trained on a specific dataset (Places2).\n*   Alternative inpainting techniques that could be explored.\n*   Analysis of the computational cost and efficiency of the inpainting step.\n\nProvide a detailed analysis of these aspects, suggesting potential improvements or alternative approaches to the inpainting component. Your review should be insightful and contribute to a better understanding of the role of inpainting in the overall defense mechanism.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}}}
