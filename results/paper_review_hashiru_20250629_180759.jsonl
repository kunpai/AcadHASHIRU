{"paper_id": "YrmoVzxBLSa", "prompt": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Momentum Diminishes the Effect of Spectral Bias in Physics-Informed Neural Networks\n\n1 INTRODUCTION: Physics-informed neural networks (PINNs) have been proposed as alternatives to traditional numerical partial differential equations (PDEs) solvers (Raissi et al., 2019; 2020; Sirignano & Spiliopoulos, 2018; Tripathy & Bilionis, 2018). In PINNs, a PDE which describes the physical domain knowledge of a problem is added as a regularization term to an empirical loss function. Although PINNs have shown remarkable performance in solving a wide range of problems in science and engineering (Cai et al., 2022; Kharazmi et al., 2019; Sun et al., 2020; Kissas et al., 2020; Tartakovsky et al., 2018), regardless of the simplicity of a PDE itself, they often fail to converge to accurate solutions when the target function contains high-frequency features (Krishnapriyan et al., 2021; Wang et al., 2021). This phenomenon known as the spectral bias exists in even the simplest linear PDEs (Wang et al., 2021; Moseley et al., 2021; Krishnapriyan et al., 2021). The spectral bias is not limited to PINNs, Rahaman et al. (2019) empirically and Cao et al. (2019) theoretically showed that all fully-connected feed-forward neural networks (NNs) exhibit spectral bias, as they are biased to learn low-frequency modes that correspond to less complex functions (Rahaman et al., 2019). Recently, to better understand the dynamics of the spectral bias of PINNs, by assuming an infinitelywide network Wang et al. (2022) employed neural tangent kernel (NTK) theory (Jacot et al., 2018) to analyze the convergence of the networks. By examining the gradient flow of PINNs during training, they showed that a strong spectral bias exhibits under the stochastic gradient descent (SGD) optimization process. To tackle the issue of spectral bias, they proposed to assign a weight to each term of the loss function and dynamically update it. Although the results showed some improvements, as the frequency of the target function increased, their proposed PINN still failed to converge to solutions of PDEs. Moreover, as assigning weights can result in indefinite kernels, the training process could become extremely unstable. Some studies proposed an alternative approach in which instead of modifying the loss function terms, a high-frequency PDE is solved in a few successive steps. In these methods, it is assumed that the optimal solution of low-frequency PDEs is close to the optimal solution of high-frequency PDEs. Hence, instead of randomly initializing weights they are being initialized using the optimal solution of low-frequency PDEs. Moseley et al. (2021) implemented a finite element approach where PINNs were trained to learn basis functions over several small, overlapping subdomains. Similarly, Krishnapriyan et al. (2021) proposed a learning method based on learning the solution over small successive chunks of time. Moreover, they proposed another sequential learning scheme in which the model was gradually trained on target functions with lower frequencies, and, at each step, the optimized weights were used as the warm initialization for higher-frequency target functions. In a similar approach, Huang & Alkhalifah (2021) proposed to use the pre-trained models from lowfrequency functions and to increase the size of the network (neuron splitting) as the frequency of the target function is increased. Although these methods showed good performance on some PDEs, as the frequency terms became larger, the process became much slower as the required time steps would significantly grow. In this work, we study the spectral bias of PINNs from an optimization perspective. Existing studies only have focused on effect of the vanilla SGD (Wang et al., 2022) or they are limited to some week empirical evidence indicating that Adam might learn high feature faster (Taylor et al., 2022). We prove that an infinitely-wide PINN under the vanilla SGD optimization process will converge to the solution. However, for high-frequency modes, the learning rate needs to become very small, which makes the convergence extremely slow, and hence not possible in practice. Moreover, we prove that for infinitely-wide networks, using the SGD with momentum (SGDM) optimizer can reduce the effect of spectral bias in the networks, while significantly outperforming vanilla SGD. We also investigate why the Adam optimizer can also accelerate the optimization process while decreases the effect of spectral bias in PINNs. To the best of our knowledge this is the first time that the gradient flow of the output of PINNs under the SGDM, and Adam are being analyzed, and their relation to solving spectral bias is discussed. Finally, our extensive numerical experiments on sufficiently wide networks confirm our theoretical findings. 2 PRELIMINARIES:  2.1 PINNS GENERAL FORM: The general form of a well-posed PDE on a bounded domain (\u2126 \u2282 Rd) is defined as: D[u](x) = f(x), x \u2208 \u2126 u(x) = g(x), x \u2208 \u2202\u2126 (1) where D is a differential operator and u(x) is the solution (of the PDE), in which x = (x1, x2, . . . , xd). Note that for time-dependent equations, t = (t1, t2, . . . , td) are viewed as additional coordinates within x. Hence, the initial condition is viewed as a special type of Dirichlet boundary condition and included in the second term. Using PINNs, the solution of Eq. 1 can be approximated as u(x,w) by minimizing the following loss function: L(w) := 1 Nb Nb\u2211 i=1 (u(xib,w)\u2212 g(xib)) 2 \ufe38 \ufe37\ufe37 \ufe38 Lb(w) + 1 Nr Nr\u2211 i=1 (D[u](xir,w)\u2212 f(xir)) 2 \ufe38 \ufe37\ufe37 \ufe38 Lr(w) (2) where {xib} Nb i=1 and {xir} Nr i=1 are boundary and collocation points respectively, and w describes the neural network parameters. Lb(w) corresponds to the mean squared error of the boundary (and initial) condition data points, and Lr(w) encapsulates the physics of the problem using the randomly selected collocation points. Similar to all other NNs, minimizing the loss function L(w) results in finding the optimal solution for u(x,w). 2.2 INFINITELY-WIDE NEURAL NETWORKS: A fully-connected infinitely-wide NN with L hidden layers can be written as Jacot et al. (2018): u0(x) = x uh(x) = 1 N \u0398h \u00b7 gh + bh gh(x) = \u03c3(\u0398h\u22121 \u00b7 uh\u22121) + bh\u22121 where \u0398h and bh are respectively the weight matrices and the bias vectors in the layers h = 1, . . . , L, N is the width of the layer, and \u03c3(\u00b7) is a \u03b2-smooth activation function (e.g. tanh(\u00b7)). The final output of the NN is written as: u(x,w) = 1 N \u0398L \u00b7 gL + bL where w = (\u03980,b0, . . . ,\u0398L,bL). At each time step t, we can determine the change of the output with respect to the input, which defines a NTK: Kt(x,x\u2019) = \u2207wu(x,w(t)) \u22a4\u2207wu(x\u2032,w(t)). It is worth noting that the NTK is associated with the model and is completely independent of the choice of optimization algorithms or the loss function. Liu et al. (2020) showed that for a fully-connected NN with a linear output layer, the spectral norm of its Hessian satisfies \u2225H(w)\u2225 = O( 1\u221a N ). Consequently, as the width become larger, the norm of the Hessian becomes smaller: limN\u21920\u2225H(w)\u2225 = 0. Thus, one major consequence of dealing with infinitely-wide fully-connected NNs is that if the last layer of the network is linear, the NTK becomes static (not changing over iterations), and the output of the network can be linearized (Liu et al., 2020): ulint (w) \u2248 u(w)|w0 + (w \u2212 w0)\u2207u(w)|w0 . 3 THEORETICAL RESULTS:  3.1 CONVERGENCE OF GRADIENT DESCENT IN THE PRESENCE OF HIGH-FREQUENCY FEATURES: Generally, the optimization problems corresponding to over-parametrized systems, even on a local scale, are non-convex (Liu et al., 2022). A loss function L(w) of a \u00b5-uniformly conditioned NN satisfies the \u00b5-PL\u2217 condition on a set S \u2282 Rm if: \u2225\u2207w(L(w))\u22252 \u2265 \u00b5L(w) for all w \u2208 S (3) where \u00b5 is the lower bound of the tangent kernel K(w) of the NN. It has been shown that infinitelywide networks satisfy the \u00b5-PL\u2217 condition, a variant of the Polyak-Lojasiewicz condition, and as a result the (stochastic) gradient descent (SGD/GD) optimization algorithms will converge to the optimal solution (Liu et al., 2022). The following proposition makes use of the \u00b5-PL\u2217 condition to provide a convergence analysis for an infinitely-wide PINN with a loss function as in Eq. 2 optimized with SGD (Appendix A). Proposition 1. Let \u03bbbmax and \u03bbrmax , respectively, be the largest eigenvalues of the Hessians \u22072Lb(wt) and \u22072Lr(wt). Consider an infinitely-wide PINN optimized with the following update rule: wt+1 = wt \u2212 \u03b7\u2207L(wt), where \u03b7 is a constant learning rate. Then, provided \u03b7 = O(1/(\u03bbbmax +\u03bbrmax)), the \u00b5-PL\u2217 condition is satisfied and the PINN will converge. Although Proposition A provides a convergence guarantee for an infinitely wide-wide PINN, for PDEs exhibiting stiff dynamics (those with high frequency modes), the eigenvalues (of the Hessian) dictating convergence are often very large (Wang et al., 2021). For example, take the onedimensional Poisson equation: \u22022u \u2202x2 = s(x), x \u2208 \u2126 (4) u(x) = b(x), x \u2208 \u2202\u2126 with u(x) = sin(Cx). The norm of the Hessian H(wt) is of order O(C4) (Appendix B). Thus, as C grows so does the bound on the eigenvalues of H(wt), indicating that at least one of \u03bbbmax or \u03bbrmax will be large. Consequently, the learning rate must be prohibitively small to guarantee convergence. Therefore SGD is unusable in practice. 3.2 WIDE PINN OPTIMIZATION USING SGD WITH MOMENTUM: In order to accelerate learning, we investigate the training dynamics of PINNs under SGD with momentum (SGDM) (Du, 2019): wt+1 = wt + \u03b1(wt \u2212 wt\u22121)\u2212 \u03b7\u2207wL(w), (5) where \u03b1 and \u03b7 are fixed rates.Specifically, we analyze the gradient flow of the update rule in Eq. 5 by leveraging the notion of the NTK. The following theorem (proof in Appendix C) reveals that SGDM has a different convergence behavior from SGD: Theorem 1. For an infinitely-wide PINN, the gradient flow of SGDM is: m [ u\u0308(xb,w(t)) D[u\u0308](xr,w(t)) ] = \u2212\u00b5 [ u\u0307(xb,w(t)) D[u\u0307](xr,w(t)) ] \u2212 K [ u(xb,w(t))\u2212 g(xb) D[u](xr,w(t))\u2212 h(xr) ] that is analogous to a point mass m undergoing a damped harmonic oscillation in a viscous medium with a friction coefficient of \u00b5(\u03b1) that is function of \u03b1, Furthermore, K is defined as: K = [ Kbb Krb Kbr Krr ] , where: Kbb(x,x\u2032) = \u2207wu(w, x) \u22a4\u2207wu(w, x\u2032) Kbr(x,x\u2032) = \u2207wu(w, x) \u22a4\u2207wD[u](w, x\u2032) Krr(x,x\u2032) = \u2207wD[u](w, x \u2032) \u22a4\u2207wD[u](w, x\u2032) are three NTKs associated with the boundary and residual terms. Moreover, let \u03b3 = \u00b5/2m, \u03bai be the i-th eigenvalue of K, and \u03ba\u2032i = \u03bai m . Then, the solutions to the gradient flow are of the form: A1e \u03bbi1 t +A2e \u03bbi2 t \u03bbi1,2 = \u2212\u03b3 \u00b1 \u221a \u03b32 \u2212 \u03ba\u2032i (6) where A1 and A2 are constants. Examining the analogous gradient flow for the vanilla SGD, where the training error decays at the rate e\u2212\u03bait (Wang et al., 2021). That is, the eigenvalues of the kernel indicate how fast the training error of SGD decreases. It has been observed that eigenvectors of larger eigenvalues of the kernel matrix K correspond to lower frequency modes of the target function (Rahaman et al., 2019; Basri et al., 2020). Clearly, under vanilla SGD the high-frequency components are extremely slow to converge (as they correspond to smaller eigenvalues) (Rahaman et al., 2019; Basri et al., 2020), thus PINNs under vanilla SGD suffer from spectral bias (Wang et al., 2022). Once we add momentum, the decay rate analysis becomes more involved as Eq. 7 yields three different cases of solutions. Each of the three cases is analogous to one of the solutions of a damped harmonic oscillator Qian (1999); Arya (Fig. 1): \u2022 Under-damped: Imaginary roots (\u03b32 < \u03ba\u2032i) \u2022 Critically-damped: Real and equal roots (\u03b32 = \u03ba\u2032i) \u2022 Over-damped: Real roots (\u03b32 > \u03ba\u2032i). Under-damped case As \u221a \u03b32 \u2212 \u03ba\u2032i has imaginary roots, Eq. 7 can be rewritten as: Ae\u2212\u03b3t cos (\u03c91t+ \u03d5) where \u03c91 = \u221a \u03ba\u2032i \u2212 \u03b32, and A and \u03d5 are two constants corresponding to the amplitude and the phase of the damped oscillation. In physics, this solution corresponds to an oscillatory motion in which the amplitude is decaying exponentially. Critically-damped case The general solution of the critically-damped case can be written as: (B1 +B2t)e \u2212\u03b3t where B1 and B2 are constants. As the oscillation motion is not present, the decaying rate for this case is much faster (Fig. 1). Over-damped case Lastly, in the over-damped case the general solution is simplified as: e\u2212\u03b3t ( C1e \u03c92t+ C2e \u2212\u03c92t ) where \u03c92 = \u221a \u03b32 \u2212 \u03ba\u2032i and C1 and C2 are constants. Similar to the critically-damped case, the above equation states a fast decay. Thus, depending on |\u03bai|, the absolute value of the eigenvalues of the kernel matrix K, the dynamics of the training error corresponding to different frequency components can differ. For larger eigenvalues, the training error corresponds to an under-damped solution, in which the amplitude of an oscillatory motion is decaying exponentially, whereas for smaller eigenvalues the correspondence is with an over-damped or critically-damped oscillation, with a much faster decay. Thus, when using SGDM instead of vanilla SGD, the training process for high-frequency components of the target (corresponding to the smaller eigenvalues) will decay fast (undergoing the over-damped or critically-damped motions) while the low-frequency components (corresponding to the larger eigenvalues) will decay at a slow rate. As a consequence, the effect of spectral bias will be less prominent (compared to vanilla SGD). To provide visualizations, in Appendix E using a small and a large eigenvalue, the decay dynamics of SGDM and the vanilla SGD are plotted. Furthermore, for Poisson\u2019s equation, the evolution of the estimated solutions in time (epochs) for both SGDM and vanilla SGD are plotted. 3.3 ANALYSIS OF ADAM FOR BAND-LIMITED FUNCTIONS: Adam is a commonly-used optimizer which can be interpreted as SGDM adapted for variance (Kingma & Ba, 2014; Balles & Hennig, 2018). Thus, it has the same properties as SGDM related to the diminishing of the spectral bias because of the momentum term (see Appendix D for more details). However, because of the adaptive learning rate, it can be expected to be even faster than SGDM. Here, we briefly provide more insights into the general behavior of Adam. Optimizing with Adam, we define: g(w) = \u2211 x\u2208X \u2207\u2113(w; x)/M , where \u2113 = Lb,r, M is the batch size and X is the batch. Furthermore, the pointwise variance of g(w) is written as \u03c3 = var g(w). We will use subscripts to denote entries, so that \u03c3i is i-th entry of \u03c3. With \u2207L being the true gradient, at each epoch, Adam updates wi with a magnitude inversely proportional to an estimator of \u03c32i /\u2207L2i (Balles & Hennig, 2018). The goal of the training process is to find a weight w which minimizes the loss L across all collocation and boundary points from Eq. 2. The role that variance adaptation plays in this speedup is readily seen when the solution to a PDE is (or is well-approximated by) a band-limited function. A band-limited function on a finite set of frequencies k has a form of u(x) = \u2211 k\u2208K \u03b1k exp(2\u03c0ikx). When using SGD, the solutions eventually satisfy the following bound with high probability (Basri et al., 2019; Arora et al., 2019): L \u2a85 \u221a 2\u03c0 \u2211 k\u2208K \u03b1 2 kk 2 N . This indicates that a network optimized with SGD will learn better approximations of u(x) if lower frequencies k are present (especially when compared to another solution with otherwise identical amplitudes \u03b1k). Conversely, if u(x) is primarily high-frequency then L has very weak bounds. Since our NN is infinitely-wide it satisfies the \u00b5-PL\u2217 condition \u2225\u2207L\u22252 \u2265 \u00b5L of Eq. 3. This implies similarly weak bounds on \u2225\u2207L\u2225 and thus \u2207Li (since \u00b5 is constant). Furthermore, convergence requires \u2207Li \u2248 0 for all i, so if any \u2207Li \u226b 0 the network has yet to converge. However, this is precisely the situation in which Adam accelerates toward the solution fastest as its updates are largest when \u03c32i /\u2207L2i close to zero. That is, in the presence of high-frequency features resulting in poor bounds on the loss if optimizing with SGD, Adam would instead benefit from accelerated convergence. Combined with the inclusion of momentum as discussed earlier, this may provide the framework for it outperforming even SGDM. Our numerical experiments confirm that for sufficiently wide NNs both SGDM and Adam can converge to desirable solutions, but that Adam is even faster than SGDM (see Section 4). 4 NUMERICAL EXPERIMENTS: In the previous Section, we proved that for infinitely-wide networks SGDM and Adam can diminish the effect of spectral bias in theory. Here, we show that in practice, for sufficiently wide networks, Adam and SGDM can significantly diminish the effect of spectral bias. Using Poisson\u2019s equation, the transport function, and the reaction-diffusion problem we provide results from our numerical experiments. Experiments for the reaction-diffusion problem are presented in Appendix G.2. 4.1 POISSON\u2019S EQUATION: Poisson\u2019s equation is a well-known elliptic PDE in physics. For example, in electromagnetism, the solution to Poisson\u2019s equation is the potential field of a given electric charge. In Eq. 4 the general form of the one-dimensional Poisson\u2019s equation was presented. Here, we write Poisson\u2019s equation for a specific source function and a specific boundary condition: f(x) = \u2212C2 sin(Cx), x \u2208 [0, 1] g(x) = 0, x = 0, 1 The defined loss function as well as the analytical solution of Poisson\u2019s equation are shown in Appendix F. To facilitate later comparisons with Wang et al. (2022) we adopted their network architecture of one hidden layer of width 500, Nr = 100 and Nb = 100. Of note, Wang et al. (2022) had shown that during the training process, the NTK of networks with a width of 500 practically stayed constant. Our numerical experiments confirmed that for a relatively small value of C = 5\u03c0 (where the effect of the spectral bias is not significant), after 55000 epochs, models trained via all three algorithms could accurately estimate the solution (Fig. 2; top panel). The relative error \u2225(u\u2212 u\u0302)/u\u0302\u2225 for vanilla SGD was on the order of 10\u22122. The relative error using SGDM and Adam were respectively on the order of 10\u22123 and 10\u22124 (Fig. 3a). As the parameters of a PDE increase, its solution contains higher frequency modes, and as a result, convergence becomes more challenging. When C = 10\u03c0, after 55000 epochs, the solution obtained from training the network with vanilla SGD was far from the exact solution (Fig. 2; bottom panel), and it exhibited a relative error larger than 10\u22121 (Fig. 3b). Meanwhile, the trained models with SGDM and Adam had relative errors of 10\u22122 and 10\u22124, respectively (Fig. 3b). It is also of interest to investigate the behavior of the training loss function and its decay rate. For the low-frequency case (C = 5\u03c0), the training loss via Adam had a faster convergence, however, all three networks converged after about 30000 epochs (Fig. 4, left panel). For the high-frequency case (C = 10\u03c0), the training loss via SGD had a much slower decay rate, and after 35000 epochs it did not converge. On the other hand, the training loss for both SGDM and Adam could converge, and the model trained via Adam converged after about 25000 epochs (Fig. 4, right panel). Of note, using vanilla SGD and training the model for 180000 epochs resulted in a small relative error on the order of 10\u22122 (Fig. G.1). This confirmed our discussion presented in Section 3.1 that vanilla SGD will converge under large parameters, though due to the presence of high-frequency features it is extremely slow. 4.2 TRANSPORT EQUATION: The transport equation is a hyperbolic PDE that models the concentration of a substance flowing in a fluid. Here, we focus on a one-dimensional transport equation: \u2202u \u2202t + \u03b2 \u2202u \u2202x = 0, x \u2208 \u2126, T \u2208 [0, 1] g(x) = u(x, 0), x \u2208 \u2126 where \u03b2 is a control parameter (independent of x and t). To facilitate later comparisons with Krishnapriyan et al. (2021), we used the same network architectures as their study: Nb = 100, Nr = 1500, and a 4-layer network. We also chose the boundary and initial conditions to be u(x, 0) = sin(x) and u(0, t) = u(2\u03c0, t). The defined loss function as well as the analytical solution of the transport function are shown in Appendix F. Similar to Poisson\u2019s equation, for small \u03b2 values, the models trained via vanilla SGD, SGDM, and Adam could all easily converge to the solution, and had small relative errors. However, for \u03b2 = 20, after 125000 epochs the model trained with vanilla SGD still failed to find the solution, and the averaged relative error stood at a large value (on the order of 100). However, the model trained via SGDM after 55000 epochs could converge to the solution and the estimated solution had the relative error on the order of 10\u22122. The estimated solution from training the model via Adam, after only 15000 epochs, had a small relative error also on the order of 10\u22122. The exact solution, the estimated solutions (based on the three opti5izers), and the absolute difference between the exact and estimated solutions are shown in Fig. 5. 4.3 COMPARISON WITH OTHER METHODS: Here, we compare the solutions for Poisson\u2019s equation and the transport function (from a wide network trained with Adam) with the solutions based on the curriculum learning (hereafter C-learning) Krishnapriyan et al. (2021) and the adaptive weight approach (hereafter AW) presented in Wang et al. (2022). Of note, C-learning used the Limited-memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (L-BFGS) algorithm that mimics the second-order optimization, as Krishnapriyan et al. (2021) reported that, for their approach, using other optimization algorithms unperformed compared to LBFGS. To reproduce the results we used the code respectively presented in Krishnapriyan et al. (2021) and Wang et al. (2022). All networks were trained with the same architecture introduced in Section (4). We tested a 1D Poisson\u2019s equation with different values of C. The comparison between the relative errors of the estimated solutions is plotted in Fig. 6a. For C \u2265 8\u03c0, both AW and C-learning exhibited errors of the order of 10\u22121. The errors using Adam were at most 10\u22123. For the 1D transport function with initial condition sin2(x) and for different values of \u03b2 \u2208 [2, 20], the relative error of the estimated solutions using the mentioned methods are plotted in Fig. 6b. As the values of \u03b2 become larger, the estimated solutions from C-learning and AW became less accurate. C-learning for \u03b2 \u2265 15 and AW for \u03b2 \u2265 10 had relative errors of the estimated solutions larger than 10\u22121. Furthermore, using the C-learning methodology, in order to solve the transport function for \u03b2 \u2265 5, we had to train models for \u03b2 = 9, 11, 13, 15, 17, 18, 19, 20 such that the optimized weights for the previous \u03b2 values were used to warm initialize the model for the next beta value. Of note, further comparisons with other choices of initial condition are presented in Appendix G.3. To further evaluate the effect that the optimization algorithms have on the spectral bias, we compared the rate of training loss decay for wide SGDM, wide Adam, AW, and C-learning. An experiment with Poisson\u2019s equation with C = 10\u03c0 showed the training loss of the networks based on Adam had much faster decay than AW (see Fig. 6d). For the transport function with \u03b2 = 20, the training loss under Adam converged much faster than the loss under AW, C-learning, and SGDM Fig. 6c. Another experiment with Poisson\u2019s equation when C = 10\u03c0 showed the training loss of the network based on Adam had a faster decay than AW (see Fig. 6d). Although C-learning also showed a fast training loss convergence, the relative error of the estimate from C-learning was large (see Fig. 6b). This is not surprising, as the loss landscape for PINNsof PDEs with high-frequency modes is complex Krishnapriyan et al. (2021), and they contain many saddle points which attract second-order optimization algorithms Dauphin et al. (2014). Our observation is consistent with (Markidis, 2021), where they also observed this downside of L-BFGS and recommended using Adam for at least the first few epochs. It is also important to mention that, unlike the other methods, the weight initialization for C-learning was not random. Instead, it was based on the optimal solution achieved for lower frequency modes, which requires extra training. 5 CONCLUSION: In the present study, through the lens of NTKs, we examined the dynamics of training PINNs via SGDM. We also showed that under SGDM the convergence rate of low-frequency features becomes slower (analogous to under-damped oscillation) and many high-frequency features undergo much faster conversion dynamics (analogous to over-damped or critically-damped oscillations). Thus, the effect of spectral bias becomes less prominent. Moreover, we discussed how training a PINN via Adam can even further accelerate convergence, and showed it can be much faster than SGDM. Although we analyzed the theoretical dynamics of convergence by assuming an infinitely-wide network, our experiments confirmed the estimated solutions obtained from the trained models via SGDM and Adam had high accuracy, using finite and practical widths. A CONVERGENCE OF WIDE PINNS: Proposition A. Let \u03bbbmax and \u03bbrmax , respectively, be the largest eigenvalues of the Hessians \u22072Lb(wt) and \u22072Lr(wt). Consider an infinitely-wide PINN optimized with the following update rule: wt+1 = wt \u2212 \u03b7\u2207L(wt), where \u03b7 is a constant learning rate. Then, provided \u03b7 = O(1/(\u03bbbmax +\u03bbrmax)), the \u00b5-PL\u2217 condition is satisfied and the PINN will converge. Proof. The loss function at time t+ 1 can be written as: L(wt+1) \u2248 L(wt) + (wt+1 \u2212 wt)\u22a4\u2207L(wt) + 1 2 (wt+1 \u2212 wt)\u22a4H(wt)(wt+1 \u2212 wt) where H(wt) = \u22072Lb(wt) + \u22072Lr(wt) (Wang et al., 2021). Following the approach of (Wang et al., 2021), let Q be an orthogonal matrix diagonalizing H(wt) and v = L(wt)/\u2225L(wt)\u2225. With y = Qv, we have the following: L(wt+1) \u2248 L(wt)\u2212 \u03b7\u2225\u2207L(wt)\u22252 + \u03b72 2 \u2225\u2207L(wt)\u22252 ( Nb\u2211 i \u03bbbiyi 2 + Nr\u2211 i \u03bbriyi 2 ) \u2a85 L(wt)\u2212 \u03b7\u2225\u2207L(wt)\u22252 + \u03b72 2 \u2225\u2207L(wt)\u22252(\u03bbbmax + \u03bbrmax) where the \u03bbbi and \u03bbri are the respective eigenvalues of the Hessians of Lb and Lr ordered nondecreasingly, and the summation is taken over the components of y. From here, fixing any bound B such that \u03bbbmax + \u03bbrmax \u2264 B, we obtain the following inequality: L(wt+1) \u2a85 L(wt)\u2212 \u03b7\u2225\u2207L(wt)\u22252 ( 1\u2212 \u03b7B 2 ) Witness that if \u03b7 = 1/B, we can further simplify this to: L(wt+1) \u2a85 L(wt)\u2212 \u03b7\u2225\u2207L(wt)\u22252 Since our NN satisfies the \u00b5-PL\u2217 condition from Eq. 3 due to its width, we therefore have: L(wt+1) \u2264 (1\u2212 \u03b7\u00b5)L(wt), which concludes the proof. B HESSIAN FOR POISSON\u2019S EQUATION: Proposition B. Let H be the Hessian of the loss function of an infinitely-wide PINN for the onedimensional Poisson equation defined by Eq. 4 with u(x) = sin(Cx). Then, we have H = O(C4). Proof. The Hessian of loss on the collocation points is calculated following the methods introduced in Wang et al. (2021), where the gradient of the loss function is: \u2202Lr \u2202w = \u2202 \u222b 1 0 (\u2202 2uw \u2202x2 \u2212 \u22022u \u2202x2 ) 2dx \u2202w . Here, w \u2208 w, u(x) is the target solution (admitting some parameter C), and uw(x) is the NN approximation of the output. Assuming that the approximation is a good representation of the actual solution, it can be written as uw(x) = u(x)\u03f5w(x), where \u03f5w(x) is a smooth function taking values in [0, 1], such that |\u03f5w(x)\u22121| < \u03b4 for some \u03b4 > 0 and \u2225\u2202 k\u03f5w(x) \u2202xk \u2225 \u2264 \u03b4, where we have the L\u221e norm. The Hessian of the loss function will therefore be: \u22022Lr \u2202w2 = \u22022 \u222b 1 0 (\u2202 2uw \u2202x2 \u2212 \u22022u \u2202x2 ) 2 \u2202w2 = \u2202I \u2202w where I = \u2202Lr\u2202w , and can be calculated to be: I = 2 \u22022uw \u2202w\u2202x ( \u22022uw \u2202x2 \u2212 \u2202 2u \u2202x2 ) \u2223\u2223\u22231 0 \u2212 \u2202uw \u2202w ( \u22023uw \u2202x3 \u2212 \u2202 3u \u2202x3 ) \u2223\u2223\u22231 0 + \u222b 1 0 \u2202uw \u2202w ( \u22024uw \u2202x4 \u2212 \u2202 4u \u2202x4 ) dx. The above equation contains 3 terms, which we call I1, I2, and I3 respectively. Note that \u2202I\u2202w = \u2202I1 \u2202w + \u2202I2 \u2202w + \u2202I3 \u2202w . We calculate these terms as follows: I1 = 2 \u22022uw \u2202w\u2202x ( \u22022uw \u2202x2 \u2212 \u2202 2u \u2202x2 ) \u2223\u2223\u22231 0 \u2202I1 \u2202w = 2 \u2202( \u2202 2uw \u2202w\u2202x ) \u2202w ( \u22022uw \u2202x \u2212 \u2202 2u \u2202x2 ) ( \u22022uw \u2202x2 \u2212 \u2202 2u \u2202x2 ) \u2223\u2223\u22231 0 = 2 ( \u2202 \u2202(u\u2032(x)\u03f5w(x)+u(x)\u03f5 \u2032 w(x)) \u2202w \u2202w )( \u22022uw \u2202x2 \u2212 \u2202 2u \u2202x2 ) \u2223\u2223\u22231 0 = 2 ( \u2202(u\u2032(x)\u2202\u03f5w(x)\u2202w + u(x) \u2202\u03f5\u2032w(x) \u2202w ) \u2202w )( \u22022uw \u2202x2 \u2212 \u2202 2u \u2202x2 ) \u2223\u2223\u22231 0 = 2 ( u\u2032(x) \u22022\u03f5w(x) \u2202w2 + u(x) \u22022\u03f5\u2032w(x) \u2202w2 )( \u22022uw \u2202x2 \u2212 \u2202 2u \u2202x2 ) \u2223\u2223\u22231 0 . Note that |u(x)| \u2264 1 and by the chain rule |u\u2032(x)| \u2264 C. An application of the triangle inequality then yields: \u2223\u2223\u2223\u2223u\u2032(x)\u22022\u03f5w(x)\u2202w2 + u(x)\u22022\u03f5\u2032w(x)\u2202w2 \u2223\u2223\u2223\u2223 \u2264 C \u2225\u2225\u2225\u2225\u22022\u03f5w(x)\u2202w2 \u2225\u2225\u2225\u2225+ \u2225\u2225\u2225\u2225\u22022\u03f5\u2032w(x)\u2202w2 \u2225\u2225\u2225\u2225 . Using the assumptions on \u03f5w(x) we get:\u2223\u2223\u2223\u2223\u22022uw\u2202x2 \u2212 \u22022u\u2202x2 \u2223\u2223\u2223\u2223 \u2264 (C2 + 2)\u03b4. Combing both of these, we see \u2202I1\u2202w = O(C 3). Similarly, \u2202I2\u2202w = O(C 3) and \u2202I3\u2202w = O(C 4), which concludes the proof. C GRADIENT FLOW FOR SGD WITH MOMENTUM: Theorem C. For an infinitely-wide PINN, the gradient flow of SGDM is: m [ u\u0308(xb,w(t)) D[u\u0308](xr,w(t)) ] = \u2212\u00b5 [ u\u0307(xb,w(t)) D[u\u0307](xr,w(t)) ] \u2212 K [ u(xb,w(t))\u2212 g(xb) D[u](xr,w(t))\u2212 h(xr) ] that is analogous to a point mass m undergoing a damped harmonic oscillation in a viscous medium with a friction coefficient of \u00b5(\u03b1) that is function of \u03b1, Furthermore, K is defined as: K = [ Kbb Krb Kbr Krr ] , where: Kbb(x,x\u2032) = \u2207wu(w, x) \u22a4\u2207wu(w, x\u2032) Kbr(x,x\u2032) = \u2207wu(w, x) \u22a4\u2207wD[u](w, x\u2032) Krr(x,x\u2032) = \u2207wD[u](w, x \u2032) \u22a4\u2207wD[u](w, x\u2032) are three NTKs associated with the boundary and residual terms. Moreover, let \u03b3 = \u00b5/2m, \u03bai be the i-th eigenvalue of K, and \u03ba\u2032i = \u03bai m . Then, the solutions to the gradient flow are of the form: A1e \u03bbi1 t +A2e \u03bbi2 t \u03bbi1,2 = \u2212\u03b3 \u00b1 \u221a \u03b32 \u2212 \u03ba\u2032i (7) where A1 and A2 are constants. Proof. Recall that as the NN becomes wider, the norm of the Hessian becomes smaller, such that in the limit as N \u2192 \u221e the norm of Hessian becomes 0. One immediate consequence of small Hessian for a NN is that its output can be estimated by a linear function Lee et al. (2019). The output of a NN can thus be replaced by its first-order Taylor expansion: ulint (w) \u2248 u(w)|w0 + (w \u2212 w0)\u2207u(w)|w0 . The update rule for SGDM can be written as Du (2019): wt+1 = wt + \u03b1(wt \u2212 wt\u22121)\u2212 \u03b7\u2207wL(w). The discrete updates to the output of NN become (see Appendix C.1): ulint+1 = u lin t + \u03b1(u lin t \u2212 ulint\u22121)\u2212 \u03b7\u2207wL(w)\u2207wu(w)|w0 In the rest of this section, for simplicity, we will drop the \u201clin\u201d term. The dynamics of SGDM are analogous to the equation of motion of a point mass m undergoing a damped harmonic oscillation (Appendix C.2): mu\u0308+ \u00b5u\u0307\u2212\u2207wL(w)f(u) = 0 where f(u) is a linear function of u, and \u00b5 is the friction coefficient that is related to the momentum term in SGDM as such: \u03b1 = mm+\u00b5\u2206t Qian (1999). Thus, the gradient flow of ut and D[u](x,w(t)) can be written as: mu\u0308(xb,w(t)) =\u2212 \u00b5u\u0307(xb,w(t))\u2212 Kbb(x,x\u2032)(w)(u(xr,w(t))\u2212 g(xb)) \u2212 Krb(x,x\u2032)(w)(D[u](xr,w(t))\u2212 h(xr)) mD[u\u0308](xr,w(t)) =\u2212 \u00b5D[u\u0307](xr,w(t))\u2212 Kbr(x,x\u2032)(w)(u(xb,w(t))\u2212 g(xb)) \u2212 Krr(x,x\u2032)(w)(D[u](xr,w(t))\u2212 h(xr)). (C.1) As mentioned earlier, in wide NNs if the last layer of the network is linear then the tanget kernels are static. We write Eq. C.1 in matrix form as follows: m [ u\u0308(xb,w(t)) D[u\u0308](xr,w(t)) ] = \u2212\u00b5 [ u\u0307(xb,w(t)) D[u\u0307](xr,w(t)) ] \u2212 K [ u(xb,w(t))\u2212 g(xb) D[u](xr,w(t))\u2212 h(xr) ] (C.2) As K is a positive semi-definite matrix Wang et al. (2021) Eq. C.2 can be viewed as a set of independent differential equations, each one corresponding to an eigenvalue \u03bbi of the kernel. These give rise to the individual general solutions of the form: A1e \u03bbi1 t +A2e \u03bbi2 t \u03bbi1,2 = \u2212\u03b3 \u00b1 \u221a \u03b32 \u2212 \u03ba\u2032i where A1 and A2 are constants, \u03b3 = \u00b5/2, and \u03ba\u2032i = \u03bai m . Of note, \u00b5 and m are set by the user, which in turn define the value of momentum. The choice of values of m and \u00b5 will determine the rate of decay of the above equation. C.1 LINEAR NN UPDATES The Taylor expansions of the outputs at time steps t+ 1 and t are: ulint+1(w) = u(w)|w0 + (wt+1 \u2212 w0)\u2207u(w)|w0 ulint (w) = u(w)|w0 + (wt \u2212 w0)\u2207u(w)|w0 . Thus, the difference between the outputs in the interval is: ulint+1(w)\u2212 ulint (w) = \u2207u(w)|w0(wt+1 \u2212 wt) = \u2207u(w)|w0(\u03b1(wt \u2212 wt\u22121)\u2212 \u03b7\u2207L(wt)) where we used the update rule of SGD with momentum within the equation. Similarly, we have: ulint (w)\u2212 ulint\u22121(w) = \u2207u(w)|w0(wt \u2212 wt\u22121) and: ulint+1(w)\u2212 ulint (w) = \u2207u(w)|w0 ( \u03b1 ( ulint (w)\u2212 ulint\u22121(w) \u2207u(w)|w0 ) \u2212 \u03b7\u2207L(wt) ) . Thus: ulint+1(w) = u lin t (w)\u2212 \u03b7\u2207L(wt) \u00b7 \u2207u(w)|w0 \u2212 \u03b1(ulint (w)\u2212 ulint\u22121(w)). C.2 RELATION BETWEEN SGDM AND DAMPED OSCILLATION The dynamics of a point mass m undergoing a damped harmonic oscillation with a friction coefficient of \u00b5 are given by: mw\u0308 + \u00b5w\u0307 = \u2212\u2207wL(w) (3) where \u2207wL(w) is the force field. Qian (1999) showed that Eq. 3 in its discrete format can be written as: m wt+\u2206t + wt\u2212\u2206t \u2212 2wt \u2206t2 + \u00b5 wt+\u2206t \u2212 wt \u2206t = \u2212\u2207wL(w). After some algebraic simplifications, the above equation becomes: wt+\u2206t \u2212 wt = m m+ \u00b5\u2206t (wt \u2212 wt\u2212\u2206t)\u2212 \u2206t2 m+ \u00b5\u2206t \u2207wL(w). Clearly, mm+\u00b5\u2206t is equivalent to the momentum term in SGDM, and \u2206t2 m+\u00b5\u2206t can be treated as the learning term. The dynamics of the output of a wide network (Appendix C.1) under SGDM are similar. D RELATION BETWEEN ADAM AND DAMPED OSCILLATION: It has been observed that Adam yields similar convergence rates to SGDM (Kingma & Ba, 2014). Here, we demonstrate that Adam also has dynamics equivalent to a damped oscillator, similar to SGDM (Appendix C.2). First, let us briefly recall the traditional update rule for Adam: \u2206wt = \u2212\u03b7\u221a \u03bd\u0302t + \u03f5 m\u0302t where mt and \u03bdt are defined as: m\u0302t = mt 1\u2212 \u03b21t+1 , mt = \u03b21mt\u22121 + (1\u2212 \u03b21)gt \u03bd\u0302t = \u03bdt 1\u2212 \u03b22t+1 , \u03bdt = \u03b22\u03bdt\u22121 + (1\u2212 \u03b22)gt2 g(t) = \u2212\u2207wL(w). Proposition D. The update rule for Adam can be written as: wt+1 = wt \u2212 P (t)(wt \u2212 wt\u22121)\u2212Q(t)\u2207wL(wt). where: P (t) = \u03b21( \u221a \u03bdt\u22121 + \u03f5)(1\u2212 \u03b21(t\u2212 1)) ( \u221a \u03bdt + \u03f5)(1\u2212 \u03b21t) Q(t) = \u03b7(1\u2212 \u03b21t) ( \u221a \u03bdt + \u03f5)(1\u2212 \u03b21t) . Proof. It is straightforward to show that the update rule can be written as: wt+1 = wt \u2212 \u03b7\u03b21mt\u22121 ( \u221a \u03bdt + \u03f5)(1\u2212 \u03b21t) \u2212 \u03b7(1\u2212 \u03b21) ( \u221a \u03bdt + \u03f5)(1\u2212 \u03b21t) \u2207wL(wt). Inserting wt = wt\u22121 \u2212 \u03b7\u221a\u03bd\u0302t+\u03f5m\u0302t into the above equation shows: wt+1 = wt\u2212 \u03b21( \u221a \u03bdt\u22121 + \u03f5)(1\u2212 \u03b21(t\u2212 1)) ( \u221a \u03bdt + \u03f5)(1\u2212 \u03b21t) (wt\u2212wt\u22121)\u2212 \u03b7(1\u2212 \u03b21t) ( \u221a \u03bdt + \u03f5)(1\u2212 \u03b21t) \u2207wL(wt). (D.1) By the definition of P (t) and Q(t) we can rewrite Eq. D.1 as: wt+1 = wt \u2212 P (t)(wt \u2212 wt\u22121)\u2212Q(t)\u2207wL(wt) (D.2) as claimed. Clearly, Eq. D.2 has the same format of the SGDM update rule. Thus, the weight updates for Adam follows the same dynamics as SGDM, that of oscillatory motion of a point mass under friction (from Appendix C.2). However, based on the updated value of the second momentum of the gradient of loss function, both the momentum and the learning rate can be updated by Adam at each iteration. Thus, the convergence to the solution by Adam is much faster compared to SGDM. E DECAY COMPARISON BETWEEN SGDM AND SGD: Here, the dynamics of the error decay of SGDM and vanilla SGD under a large eigenvalue (\u03bb1 = 103) and a relatively small eigenvalue (\u03bb2 = 10\u22125) are plotted. For vanilla SGD, the training error decays at a rate of e\u2212\u03bbit. As shown in Fig. E.1a, for the \u03bb2 the training error decays slowly, however is very fast for \u03bb1 (almost immediately dropping to zero). Thus, components of the target function that correspond to the large eigenvalues will be learned much faster. These are the eigenvalues corresponding to lower frequencies in the target function, so clearly the network suffers from spectral bias under SGD. In contrast, learning under SGDM, the training error for \u03bb1 decays following under-damped oscillation (the red curve in Fig. E.1b), while the training error for \u03bb2 follows over-damped oscillation (the blue curve in Fig. E.1a). The training error of both eigenvalues, the large \u03bb1 and small \u03bb2, decay at approximately the same time. The components of the target function that correspond to the larger eigenvalue are thus not learned any faster, so the effect of spectral bias is minimized. Figure E.1: The comparison of the error decay for a small and a large eigenvalue, a) learning decay during SGD optimization process, b) learning decay during SGDD optimization process. Knowing this fundamental difference between SGDM and SGD, it is of interest to investigate how the solutions for higher-frequency target functions evolve in time under SGD and SGDM. Thus, for the Poisson equation when C = 15\u03c0, we have plotted the solutions at epochs 2000, 10000, 20000, 30000, 40000, and 50000 (SGD: Fig. E.2 and SGDM: Fig. E.3). The NN used was fully-connected, and had 2 hidden layers with 500 neurons at each layer. The evolution of the solutions given by SGD is quite interesting. Wang et al. (2022) showed that in Poisson\u2019s equation, the eigenvalues of the boundary kernel are much smaller than the eigenvalues of the PDE kernel. Thus, the former represent the higher frequencies, and they will be learned slower under SGD. This clearly reflected in the evolution. At epoch 10000, the solution has the correct sinusoidal shape, however it is vertically shifted and clearly cannot satisfy the boundary conditions. As the training continues in time, the solution becomes less vertically shifted and closer to the boundary condition values. However, the learning process is slow, and even at epoch 50000 the estimation is far from the actual solution. Meanwhile, SGDM is much faster as shown in Fig. E.3. At epoch 10000 the solution already has the correct sinusoidal shape and satisfies one end of the initial conditions. By epoch 20000 the estimated solution has a low error on the order of 10\u22122. Figure E.2: The solution of the Poisson equation, when C = 15\u03c0 in different stages of the learning process via SGD. a) 2000 epochs, b) 10000 epochs, c) 20000 epochs, d) 30000 epochs, e) 40000 epochs, and f) 50000 epochs. These observations are very insightful as they reveal a significant difference between PINNs and regular neural networks. Perhaps to explain the difference, it is useful to investigate the evolution of the solution via SGD for a completely data-driven case. We trained a fully-connected forward neural network with the same architecture as the above PINN. The training data-set contained 3000 synthetic data points generated based on the solution of Poisson\u2019s equation, and we used the mean squared error loss function. The solutions after 5000 and 50000 epochs are shown in Fig. E.4. As the fully-connected network has no physics-informed regularization term, it has difficulties estimating the correct solution. Despite this, it still exhibits spectral bias, as the estimated solutions at both 5000 and 50000 epochs represent low-frequency sinusoidal forms. However, unlike a PINN there is no vertical shift in the solutions. Clearly, the physics-informed regularization term helps significantly to resolve the classical spectral bias (dealing with high-order frequency target functions), learning the correct sinusoidal shape faster. In return, the solutions are worse at satisfying the boundary condition values. This is because the eigenvalues of the boundary kernel represent high-frequency modes. F LOSS FUNCTION OF EQUATIONS: F.1 LOSS FUNCTION FOR POISSON\u2019S EQUATION For Poisson\u2019s equation: f(x) = \u2212C2 sin(Cx), x \u2208 [0, 1] g(x) = 0, x = 0, 1 Figure E.3: The solution of the Poisson equation, when C = 15\u03c0 in different stages of the learning process via SGDM. a) 2000 epochs, b) 10000 epochs, c) 20000 epochs, d) 30000 epochs, e) 40000 epochs, and f) 50000 epochs. Figure E.4: The completely data-driven solution of the Poisson equation based on SGD optimization process. a) after 5000 epochs, b) after 50000 epochs. u(x) = sin(Cx) is used as the exact solution. The corresponding loss function is written as: L(w) := 1 Nb Nb\u2211 i=1 ( u\u0302(xb i,w)\u2212 g(xbi) )2 + 1 Nr Nr\u2211 i=1 ( \u22022u\u0302 \u2202x2 (xr i,w)\u2212 f(xri) )2 where u\u0302 is the output of the network. F.2 LOSS FUNCTION FOR THE TRANSPORT FUNCTION Using the methods of characteristics (Evans, 2010), the transport function has a well-defined analytical solution: u(x, t) = g(x \u2212 \u03b2t). This is used as the exact solution. The corresponding loss function is written as: L(w) := 1 Nb Nb\u2211 i=1 ( u\u0302(xb i, tb i,w)\u2212 g(xbi) )2 + 1 Nr Nr\u2211 i=1 ( \u2202u\u0302(xr i, tb i,w) \u2202t + \u03b2 \u2202u\u0302(xr i, tb i,w) \u2202x )2 where u\u0302 is the output of the network. G EXPERIMENT PLOTS: G.1 POISSON EQUATION Figure G.1: The estimation of the solution of the Poisson\u2019s equation for C = 15\u03c0, using the vanilla SGD algorithm after 180000 epochs. G.2 REACTION-DIFFUSION EQUATION A reaction-diffusion equation contains a reaction and a diffusion term. Its general form is written as: \u2202u \u2202t = \u03bd\u2206u+ f(u) (G.1) where u(x, t) is the solution describing the density/concentration of a substance, \u2206 is the Laplace operator, \u03bd is a diffusion coefficient, and f(u) is a smooth function describing processes that change the present state of u (for example, birth, death or a chemical reaction). Here, we assume a onedimensional equation, where f(u) = \u03c1u(1\u2212 u). For \u03c1 independent of x and t, and with the defined f(u), Eq. G.1, can be solved analytically (Evans, 2010). To estimate the solution using PINNs, the loss function for the 1D reaction-diffusion PDE is written as: L(w) := 1 Nb Nb\u2211 i=1 ( u\u0302(xib, t i b,w)\u2212 g(xib) )2 + 1 Nr Nr\u2211 i=1 ( \u2202u\u0302 \u2202t (xir, t i b,w)\u2212 \u03bd \u22022u\u0302 \u2202x2 (xir, t i b,w)\u2212 \u03c1u\u0302(xir, tib)(1\u2212 u\u0302(xir, tib)) )2 . For consistency with Krishnapriyan et al. (2021), we used Nr = 1000, and Nb = 100. We also chose the initial and boundary conditions u(x, 0) = exp\u2212 (x\u2212\u03c0) 2 \u03c02/2 and u(0, t) = u(2\u03c0, t) respectively. Similar to the previous experiments, for larger choices of \u03bd and \u03c1 the model trained via vanilla SGD (after 85000 epochs) had difficulty converging to the solution (Fig. G.3, left panel). However, models trained via SGDM and Adam (after 45000 epochs) could provide solutions with low error. The plots of estimated and exact solutions for the three algorithms when \u03bd = 3 and \u03c1 = 5 are shown in Fig. G.2. Figure G.2: 1D reaction-diffusion equation for \u03bd = 3 and \u03c1 = 5. The solutions are obtained by training a 4-layer network with width=500 at each layer. Top panels: The exact (analytical) solution. Middle panels: The estimated solution. Bottom panels: The absolute difference between the exact and estimated solutions. (a) Vanilla SGD, (b) SGDM, (c) Adam. Figure G.3: Left Panel: The training losses of the network, when \u03bd = 9 and \u03c1 = 7), via AW, Clearning, wide SGDM, and wide Adam are plotted. Right Panel: The relative error of the estimated solutions for different values of \u03bd and \u03c1 are plotted. We further compared the solutions of the wide Adam, C-learning, and AW. Similar to the previous experiments, for higher-frequency modes Adam showed superior results. The relative errors of the estimated solutions for different values of \u03bd and \u03c1 were computed (Fig. 6 (left panel)). Furthermore, when \u03bd = 9 and \u03c1 = 7), we observed that the loss function under SGDM and Adam had much faster decay than under AW. But, the loss under L-BFGS decayed as fast as under Adam. We reasoned as to why in Section 4.3. G.3 TRANSPORT FUNCTION: FURTHER COMPARISON WITH DIFFERENT INITIAL CONDITIONS We implemented different initial conditions for the transport function and ran several experiments. For the initial condition of tanh(x), when \u03b2 \u2265 6 both C-learning and AW estimated solutions of showed large relative errors on the order of 10\u22121. However, the estimated solution of a wide network trained via Adam had an acceptable relative error on the order of 10\u22122 (Fig. G.4). The estimated solutions for all three optimizers when \u03b2 = 6 are shown in Fig. G.5. All networks were trained using a 4-layer network, and AW and Adam both had a width of 500 neurons. The estimated solutions are based on training the model for 85000 epochs. Figure G.4: The relative errors of the estimated solutions of 1D transport function, for different values of \u03b2 using the curriculum learning method (red dashed line), and our approach (blue dashed line) are plotted. Figure G.5: The estimated solutions of the transport function based on the initial condition of tanh(x) when \u03b2 = 6: a) The exact solution b) Estimated solution of a wide network trained via Adam c) Estimated solution of C-learning d) Estimated solution of AW For the initial condition of sin(x) cos(x), C-learning had small estimated errors (on the order of 10\u22122) for \u03b2 \u2264 20, however for higher values of \u03b2 the error grew larger (Fig. G.6). AW had errors on the order of 10\u22122 for \u03b2 \u2264 10, however the error began to grow for larger values of \u03b2. Adam had errors on the order of 10\u22122 even up to \u03b2 \u2264 25 (Fig. G.6). The estimated solutions based on C-learning, AW, and wide Adam for \u03b2 = 25 are shown in Fig. G.7. All networks were trained using a 4-layer network. The estimated solutions are based on training the model for 85000 epochs. Figure G.6: The relative errors of the estimated solutions of 1D transport function, based on the initial condition of sin(x) cos(x), for different values of \u03b2 using the curriculum learning method (red dashed line), and our approach (blue dashed line) are plotted. Figure G.7: The estimation of the solution of the transport function based on the initial condition of sin(x) cos(x) when \u03b2 = 25: a) The exact solution b) Estimated solution of a wide network trained via Adam c) Estimated solution of C-learning d) Estimated solution of AW", "agent_review": [{"role": "user", "content": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Momentum Diminishes the Effect of Spectral Bias in Physics-Informed Neural Networks\n\n1 INTRODUCTION: Physics-informed neural networks (PINNs) have been proposed as alternatives to traditional numerical partial differential equations (PDEs) solvers (Raissi et al., 2019; 2020; Sirignano & Spiliopoulos, 2018; Tripathy & Bilionis, 2018). In PINNs, a PDE which describes the physical domain knowledge of a problem is added as a regularization term to an empirical loss function. Although PINNs have shown remarkable performance in solving a wide range of problems in science and engineering (Cai et al., 2022; Kharazmi et al., 2019; Sun et al., 2020; Kissas et al., 2020; Tartakovsky et al., 2018), regardless of the simplicity of a PDE itself, they often fail to converge to accurate solutions when the target function contains high-frequency features (Krishnapriyan et al., 2021; Wang et al., 2021). This phenomenon known as the spectral bias exists in even the simplest linear PDEs (Wang et al., 2021; Moseley et al., 2021; Krishnapriyan et al., 2021). The spectral bias is not limited to PINNs, Rahaman et al. (2019) empirically and Cao et al. (2019) theoretically showed that all fully-connected feed-forward neural networks (NNs) exhibit spectral bias, as they are biased to learn low-frequency modes that correspond to less complex functions (Rahaman et al., 2019). Recently, to better understand the dynamics of the spectral bias of PINNs, by assuming an infinitelywide network Wang et al. (2022) employed neural tangent kernel (NTK) theory (Jacot et al., 2018) to analyze the convergence of the networks. By examining the gradient flow of PINNs during training, they showed that a strong spectral bias exhibits under the stochastic gradient descent (SGD) optimization process. To tackle the issue of spectral bias, they proposed to assign a weight to each term of the loss function and dynamically update it. Although the results showed some improvements, as the frequency of the target function increased, their proposed PINN still failed to converge to solutions of PDEs. Moreover, as assigning weights can result in indefinite kernels, the training process could become extremely unstable. Some studies proposed an alternative approach in which instead of modifying the loss function terms, a high-frequency PDE is solved in a few successive steps. In these methods, it is assumed that the optimal solution of low-frequency PDEs is close to the optimal solution of high-frequency PDEs. Hence, instead of randomly initializing weights they are being initialized using the optimal solution of low-frequency PDEs. Moseley et al. (2021) implemented a finite element approach where PINNs were trained to learn basis functions over several small, overlapping subdomains. Similarly, Krishnapriyan et al. (2021) proposed a learning method based on learning the solution over small successive chunks of time. Moreover, they proposed another sequential learning scheme in which the model was gradually trained on target functions with lower frequencies, and, at each step, the optimized weights were used as the warm initialization for higher-frequency target functions. In a similar approach, Huang & Alkhalifah (2021) proposed to use the pre-trained models from lowfrequency functions and to increase the size of the network (neuron splitting) as the frequency of the target function is increased. Although these methods showed good performance on some PDEs, as the frequency terms became larger, the process became much slower as the required time steps would significantly grow. In this work, we study the spectral bias of PINNs from an optimization perspective. Existing studies only have focused on effect of the vanilla SGD (Wang et al., 2022) or they are limited to some week empirical evidence indicating that Adam might learn high feature faster (Taylor et al., 2022). We prove that an infinitely-wide PINN under the vanilla SGD optimization process will converge to the solution. However, for high-frequency modes, the learning rate needs to become very small, which makes the convergence extremely slow, and hence not possible in practice. Moreover, we prove that for infinitely-wide networks, using the SGD with momentum (SGDM) optimizer can reduce the effect of spectral bias in the networks, while significantly outperforming vanilla SGD. We also investigate why the Adam optimizer can also accelerate the optimization process while decreases the effect of spectral bias in PINNs. To the best of our knowledge this is the first time that the gradient flow of the output of PINNs under the SGDM, and Adam are being analyzed, and their relation to solving spectral bias is discussed. Finally, our extensive numerical experiments on sufficiently wide networks confirm our theoretical findings. 2 PRELIMINARIES:  2.1 PINNS GENERAL FORM: The general form of a well-posed PDE on a bounded domain (\u2126 \u2282 Rd) is defined as: D[u](x) = f(x), x \u2208 \u2126 u(x) = g(x), x \u2208 \u2202\u2126 (1) where D is a differential operator and u(x) is the solution (of the PDE), in which x = (x1, x2, . . . , xd). Note that for time-dependent equations, t = (t1, t2, . . . , td) are viewed as additional coordinates within x. Hence, the initial condition is viewed as a special type of Dirichlet boundary condition and included in the second term. Using PINNs, the solution of Eq. 1 can be approximated as u(x,w) by minimizing the following loss function: L(w) := 1 Nb Nb\u2211 i=1 (u(xib,w)\u2212 g(xib)) 2 \ufe38 \ufe37\ufe37 \ufe38 Lb(w) + 1 Nr Nr\u2211 i=1 (D[u](xir,w)\u2212 f(xir)) 2 \ufe38 \ufe37\ufe37 \ufe38 Lr(w) (2) where {xib} Nb i=1 and {xir} Nr i=1 are boundary and collocation points respectively, and w describes the neural network parameters. Lb(w) corresponds to the mean squared error of the boundary (and initial) condition data points, and Lr(w) encapsulates the physics of the problem using the randomly selected collocation points. Similar to all other NNs, minimizing the loss function L(w) results in finding the optimal solution for u(x,w). 2.2 INFINITELY-WIDE NEURAL NETWORKS: A fully-connected infinitely-wide NN with L hidden layers can be written as Jacot et al. (2018): u0(x) = x uh(x) = 1 N \u0398h \u00b7 gh + bh gh(x) = \u03c3(\u0398h\u22121 \u00b7 uh\u22121) + bh\u22121 where \u0398h and bh are respectively the weight matrices and the bias vectors in the layers h = 1, . . . , L, N is the width of the layer, and \u03c3(\u00b7) is a \u03b2-smooth activation function (e.g. tanh(\u00b7)). The final output of the NN is written as: u(x,w) = 1 N \u0398L \u00b7 gL + bL where w = (\u03980,b0, . . . ,\u0398L,bL). At each time step t, we can determine the change of the output with respect to the input, which defines a NTK: Kt(x,x\u2019) = \u2207wu(x,w(t)) \u22a4\u2207wu(x\u2032,w(t)). It is worth noting that the NTK is associated with the model and is completely independent of the choice of optimization algorithms or the loss function. Liu et al. (2020) showed that for a fully-connected NN with a linear output layer, the spectral norm of its Hessian satisfies \u2225H(w)\u2225 = O( 1\u221a N ). Consequently, as the width become larger, the norm of the Hessian becomes smaller: limN\u21920\u2225H(w)\u2225 = 0. Thus, one major consequence of dealing with infinitely-wide fully-connected NNs is that if the last layer of the network is linear, the NTK becomes static (not changing over iterations), and the output of the network can be linearized (Liu et al., 2020): ulint (w) \u2248 u(w)|w0 + (w \u2212 w0)\u2207u(w)|w0 . 3 THEORETICAL RESULTS:  3.1 CONVERGENCE OF GRADIENT DESCENT IN THE PRESENCE OF HIGH-FREQUENCY FEATURES: Generally, the optimization problems corresponding to over-parametrized systems, even on a local scale, are non-convex (Liu et al., 2022). A loss function L(w) of a \u00b5-uniformly conditioned NN satisfies the \u00b5-PL\u2217 condition on a set S \u2282 Rm if: \u2225\u2207w(L(w))\u22252 \u2265 \u00b5L(w) for all w \u2208 S (3) where \u00b5 is the lower bound of the tangent kernel K(w) of the NN. It has been shown that infinitelywide networks satisfy the \u00b5-PL\u2217 condition, a variant of the Polyak-Lojasiewicz condition, and as a result the (stochastic) gradient descent (SGD/GD) optimization algorithms will converge to the optimal solution (Liu et al., 2022). The following proposition makes use of the \u00b5-PL\u2217 condition to provide a convergence analysis for an infinitely-wide PINN with a loss function as in Eq. 2 optimized with SGD (Appendix A). Proposition 1. Let \u03bbbmax and \u03bbrmax , respectively, be the largest eigenvalues of the Hessians \u22072Lb(wt) and \u22072Lr(wt). Consider an infinitely-wide PINN optimized with the following update rule: wt+1 = wt \u2212 \u03b7\u2207L(wt), where \u03b7 is a constant learning rate. Then, provided \u03b7 = O(1/(\u03bbbmax +\u03bbrmax)), the \u00b5-PL\u2217 condition is satisfied and the PINN will converge. Although Proposition A provides a convergence guarantee for an infinitely wide-wide PINN, for PDEs exhibiting stiff dynamics (those with high frequency modes), the eigenvalues (of the Hessian) dictating convergence are often very large (Wang et al., 2021). For example, take the onedimensional Poisson equation: \u22022u \u2202x2 = s(x), x \u2208 \u2126 (4) u(x) = b(x), x \u2208 \u2202\u2126 with u(x) = sin(Cx). The norm of the Hessian H(wt) is of order O(C4) (Appendix B). Thus, as C grows so does the bound on the eigenvalues of H(wt), indicating that at least one of \u03bbbmax or \u03bbrmax will be large. Consequently, the learning rate must be prohibitively small to guarantee convergence. Therefore SGD is unusable in practice. 3.2 WIDE PINN OPTIMIZATION USING SGD WITH MOMENTUM: In order to accelerate learning, we investigate the training dynamics of PINNs under SGD with momentum (SGDM) (Du, 2019): wt+1 = wt + \u03b1(wt \u2212 wt\u22121)\u2212 \u03b7\u2207wL(w), (5) where \u03b1 and \u03b7 are fixed rates.Specifically, we analyze the gradient flow of the update rule in Eq. 5 by leveraging the notion of the NTK. The following theorem (proof in Appendix C) reveals that SGDM has a different convergence behavior from SGD: Theorem 1. For an infinitely-wide PINN, the gradient flow of SGDM is: m [ u\u0308(xb,w(t)) D[u\u0308](xr,w(t)) ] = \u2212\u00b5 [ u\u0307(xb,w(t)) D[u\u0307](xr,w(t)) ] \u2212 K [ u(xb,w(t))\u2212 g(xb) D[u](xr,w(t))\u2212 h(xr) ] that is analogous to a point mass m undergoing a damped harmonic oscillation in a viscous medium with a friction coefficient of \u00b5(\u03b1) that is function of \u03b1, Furthermore, K is defined as: K = [ Kbb Krb Kbr Krr ] , where: Kbb(x,x\u2032) = \u2207wu(w, x) \u22a4\u2207wu(w, x\u2032) Kbr(x,x\u2032) = \u2207wu(w, x) \u22a4\u2207wD[u](w, x\u2032) Krr(x,x\u2032) = \u2207wD[u](w, x \u2032) \u22a4\u2207wD[u](w, x\u2032) are three NTKs associated with the boundary and residual terms. Moreover, let \u03b3 = \u00b5/2m, \u03bai be the i-th eigenvalue of K, and \u03ba\u2032i = \u03bai m . Then, the solutions to the gradient flow are of the form: A1e \u03bbi1 t +A2e \u03bbi2 t \u03bbi1,2 = \u2212\u03b3 \u00b1 \u221a \u03b32 \u2212 \u03ba\u2032i (6) where A1 and A2 are constants. Examining the analogous gradient flow for the vanilla SGD, where the training error decays at the rate e\u2212\u03bait (Wang et al., 2021). That is, the eigenvalues of the kernel indicate how fast the training error of SGD decreases. It has been observed that eigenvectors of larger eigenvalues of the kernel matrix K correspond to lower frequency modes of the target function (Rahaman et al., 2019; Basri et al., 2020). Clearly, under vanilla SGD the high-frequency components are extremely slow to converge (as they correspond to smaller eigenvalues) (Rahaman et al., 2019; Basri et al., 2020), thus PINNs under vanilla SGD suffer from spectral bias (Wang et al., 2022). Once we add momentum, the decay rate analysis becomes more involved as Eq. 7 yields three different cases of solutions. Each of the three cases is analogous to one of the solutions of a damped harmonic oscillator Qian (1999); Arya (Fig. 1): \u2022 Under-damped: Imaginary roots (\u03b32 < \u03ba\u2032i) \u2022 Critically-damped: Real and equal roots (\u03b32 = \u03ba\u2032i) \u2022 Over-damped: Real roots (\u03b32 > \u03ba\u2032i). Under-damped case As \u221a \u03b32 \u2212 \u03ba\u2032i has imaginary roots, Eq. 7 can be rewritten as: Ae\u2212\u03b3t cos (\u03c91t+ \u03d5) where \u03c91 = \u221a \u03ba\u2032i \u2212 \u03b32, and A and \u03d5 are two constants corresponding to the amplitude and the phase of the damped oscillation. In physics, this solution corresponds to an oscillatory motion in which the amplitude is decaying exponentially. Critically-damped case The general solution of the critically-damped case can be written as: (B1 +B2t)e \u2212\u03b3t where B1 and B2 are constants. As the oscillation motion is not present, the decaying rate for this case is much faster (Fig. 1). Over-damped case Lastly, in the over-damped case the general solution is simplified as: e\u2212\u03b3t ( C1e \u03c92t+ C2e \u2212\u03c92t ) where \u03c92 = \u221a \u03b32 \u2212 \u03ba\u2032i and C1 and C2 are constants. Similar to the critically-damped case, the above equation states a fast decay. Thus, depending on |\u03bai|, the absolute value of the eigenvalues of the kernel matrix K, the dynamics of the training error corresponding to different frequency components can differ. For larger eigenvalues, the training error corresponds to an under-damped solution, in which the amplitude of an oscillatory motion is decaying exponentially, whereas for smaller eigenvalues the correspondence is with an over-damped or critically-damped oscillation, with a much faster decay. Thus, when using SGDM instead of vanilla SGD, the training process for high-frequency components of the target (corresponding to the smaller eigenvalues) will decay fast (undergoing the over-damped or critically-damped motions) while the low-frequency components (corresponding to the larger eigenvalues) will decay at a slow rate. As a consequence, the effect of spectral bias will be less prominent (compared to vanilla SGD). To provide visualizations, in Appendix E using a small and a large eigenvalue, the decay dynamics of SGDM and the vanilla SGD are plotted. Furthermore, for Poisson\u2019s equation, the evolution of the estimated solutions in time (epochs) for both SGDM and vanilla SGD are plotted. 3.3 ANALYSIS OF ADAM FOR BAND-LIMITED FUNCTIONS: Adam is a commonly-used optimizer which can be interpreted as SGDM adapted for variance (Kingma & Ba, 2014; Balles & Hennig, 2018). Thus, it has the same properties as SGDM related to the diminishing of the spectral bias because of the momentum term (see Appendix D for more details). However, because of the adaptive learning rate, it can be expected to be even faster than SGDM. Here, we briefly provide more insights into the general behavior of Adam. Optimizing with Adam, we define: g(w) = \u2211 x\u2208X \u2207\u2113(w; x)/M , where \u2113 = Lb,r, M is the batch size and X is the batch. Furthermore, the pointwise variance of g(w) is written as \u03c3 = var g(w). We will use subscripts to denote entries, so that \u03c3i is i-th entry of \u03c3. With \u2207L being the true gradient, at each epoch, Adam updates wi with a magnitude inversely proportional to an estimator of \u03c32i /\u2207L2i (Balles & Hennig, 2018). The goal of the training process is to find a weight w which minimizes the loss L across all collocation and boundary points from Eq. 2. The role that variance adaptation plays in this speedup is readily seen when the solution to a PDE is (or is well-approximated by) a band-limited function. A band-limited function on a finite set of frequencies k has a form of u(x) = \u2211 k\u2208K \u03b1k exp(2\u03c0ikx). When using SGD, the solutions eventually satisfy the following bound with high probability (Basri et al., 2019; Arora et al., 2019): L \u2a85 \u221a 2\u03c0 \u2211 k\u2208K \u03b1 2 kk 2 N . This indicates that a network optimized with SGD will learn better approximations of u(x) if lower frequencies k are present (especially when compared to another solution with otherwise identical amplitudes \u03b1k). Conversely, if u(x) is primarily high-frequency then L has very weak bounds. Since our NN is infinitely-wide it satisfies the \u00b5-PL\u2217 condition \u2225\u2207L\u22252 \u2265 \u00b5L of Eq. 3. This implies similarly weak bounds on \u2225\u2207L\u2225 and thus \u2207Li (since \u00b5 is constant). Furthermore, convergence requires \u2207Li \u2248 0 for all i, so if any \u2207Li \u226b 0 the network has yet to converge. However, this is precisely the situation in which Adam accelerates toward the solution fastest as its updates are largest when \u03c32i /\u2207L2i close to zero. That is, in the presence of high-frequency features resulting in poor bounds on the loss if optimizing with SGD, Adam would instead benefit from accelerated convergence. Combined with the inclusion of momentum as discussed earlier, this may provide the framework for it outperforming even SGDM. Our numerical experiments confirm that for sufficiently wide NNs both SGDM and Adam can converge to desirable solutions, but that Adam is even faster than SGDM (see Section 4). 4 NUMERICAL EXPERIMENTS: In the previous Section, we proved that for infinitely-wide networks SGDM and Adam can diminish the effect of spectral bias in theory. Here, we show that in practice, for sufficiently wide networks, Adam and SGDM can significantly diminish the effect of spectral bias. Using Poisson\u2019s equation, the transport function, and the reaction-diffusion problem we provide results from our numerical experiments. Experiments for the reaction-diffusion problem are presented in Appendix G.2. 4.1 POISSON\u2019S EQUATION: Poisson\u2019s equation is a well-known elliptic PDE in physics. For example, in electromagnetism, the solution to Poisson\u2019s equation is the potential field of a given electric charge. In Eq. 4 the general form of the one-dimensional Poisson\u2019s equation was presented. Here, we write Poisson\u2019s equation for a specific source function and a specific boundary condition: f(x) = \u2212C2 sin(Cx), x \u2208 [0, 1] g(x) = 0, x = 0, 1 The defined loss function as well as the analytical solution of Poisson\u2019s equation are shown in Appendix F. To facilitate later comparisons with Wang et al. (2022) we adopted their network architecture of one hidden layer of width 500, Nr = 100 and Nb = 100. Of note, Wang et al. (2022) had shown that during the training process, the NTK of networks with a width of 500 practically stayed constant. Our numerical experiments confirmed that for a relatively small value of C = 5\u03c0 (where the effect of the spectral bias is not significant), after 55000 epochs, models trained via all three algorithms could accurately estimate the solution (Fig. 2; top panel). The relative error \u2225(u\u2212 u\u0302)/u\u0302\u2225 for vanilla SGD was on the order of 10\u22122. The relative error using SGDM and Adam were respectively on the order of 10\u22123 and 10\u22124 (Fig. 3a). As the parameters of a PDE increase, its solution contains higher frequency modes, and as a result, convergence becomes more challenging. When C = 10\u03c0, after 55000 epochs, the solution obtained from training the network with vanilla SGD was far from the exact solution (Fig. 2; bottom panel), and it exhibited a relative error larger than 10\u22121 (Fig. 3b). Meanwhile, the trained models with SGDM and Adam had relative errors of 10\u22122 and 10\u22124, respectively (Fig. 3b). It is also of interest to investigate the behavior of the training loss function and its decay rate. For the low-frequency case (C = 5\u03c0), the training loss via Adam had a faster convergence, however, all three networks converged after about 30000 epochs (Fig. 4, left panel). For the high-frequency case (C = 10\u03c0), the training loss via SGD had a much slower decay rate, and after 35000 epochs it did not converge. On the other hand, the training loss for both SGDM and Adam could converge, and the model trained via Adam converged after about 25000 epochs (Fig. 4, right panel). Of note, using vanilla SGD and training the model for 180000 epochs resulted in a small relative error on the order of 10\u22122 (Fig. G.1). This confirmed our discussion presented in Section 3.1 that vanilla SGD will converge under large parameters, though due to the presence of high-frequency features it is extremely slow. 4.2 TRANSPORT EQUATION: The transport equation is a hyperbolic PDE that models the concentration of a substance flowing in a fluid. Here, we focus on a one-dimensional transport equation: \u2202u \u2202t + \u03b2 \u2202u \u2202x = 0, x \u2208 \u2126, T \u2208 [0, 1] g(x) = u(x, 0), x \u2208 \u2126 where \u03b2 is a control parameter (independent of x and t). To facilitate later comparisons with Krishnapriyan et al. (2021), we used the same network architectures as their study: Nb = 100, Nr = 1500, and a 4-layer network. We also chose the boundary and initial conditions to be u(x, 0) = sin(x) and u(0, t) = u(2\u03c0, t). The defined loss function as well as the analytical solution of the transport function are shown in Appendix F. Similar to Poisson\u2019s equation, for small \u03b2 values, the models trained via vanilla SGD, SGDM, and Adam could all easily converge to the solution, and had small relative errors. However, for \u03b2 = 20, after 125000 epochs the model trained with vanilla SGD still failed to find the solution, and the averaged relative error stood at a large value (on the order of 100). However, the model trained via SGDM after 55000 epochs could converge to the solution and the estimated solution had the relative error on the order of 10\u22122. The estimated solution from training the model via Adam, after only 15000 epochs, had a small relative error also on the order of 10\u22122. The exact solution, the estimated solutions (based on the three opti5izers), and the absolute difference between the exact and estimated solutions are shown in Fig. 5. 4.3 COMPARISON WITH OTHER METHODS: Here, we compare the solutions for Poisson\u2019s equation and the transport function (from a wide network trained with Adam) with the solutions based on the curriculum learning (hereafter C-learning) Krishnapriyan et al. (2021) and the adaptive weight approach (hereafter AW) presented in Wang et al. (2022). Of note, C-learning used the Limited-memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (L-BFGS) algorithm that mimics the second-order optimization, as Krishnapriyan et al. (2021) reported that, for their approach, using other optimization algorithms unperformed compared to LBFGS. To reproduce the results we used the code respectively presented in Krishnapriyan et al. (2021) and Wang et al. (2022). All networks were trained with the same architecture introduced in Section (4). We tested a 1D Poisson\u2019s equation with different values of C. The comparison between the relative errors of the estimated solutions is plotted in Fig. 6a. For C \u2265 8\u03c0, both AW and C-learning exhibited errors of the order of 10\u22121. The errors using Adam were at most 10\u22123. For the 1D transport function with initial condition sin2(x) and for different values of \u03b2 \u2208 [2, 20], the relative error of the estimated solutions using the mentioned methods are plotted in Fig. 6b. As the values of \u03b2 become larger, the estimated solutions from C-learning and AW became less accurate. C-learning for \u03b2 \u2265 15 and AW for \u03b2 \u2265 10 had relative errors of the estimated solutions larger than 10\u22121. Furthermore, using the C-learning methodology, in order to solve the transport function for \u03b2 \u2265 5, we had to train models for \u03b2 = 9, 11, 13, 15, 17, 18, 19, 20 such that the optimized weights for the previous \u03b2 values were used to warm initialize the model for the next beta value. Of note, further comparisons with other choices of initial condition are presented in Appendix G.3. To further evaluate the effect that the optimization algorithms have on the spectral bias, we compared the rate of training loss decay for wide SGDM, wide Adam, AW, and C-learning. An experiment with Poisson\u2019s equation with C = 10\u03c0 showed the training loss of the networks based on Adam had much faster decay than AW (see Fig. 6d). For the transport function with \u03b2 = 20, the training loss under Adam converged much faster than the loss under AW, C-learning, and SGDM Fig. 6c. Another experiment with Poisson\u2019s equation when C = 10\u03c0 showed the training loss of the network based on Adam had a faster decay than AW (see Fig. 6d). Although C-learning also showed a fast training loss convergence, the relative error of the estimate from C-learning was large (see Fig. 6b). This is not surprising, as the loss landscape for PINNsof PDEs with high-frequency modes is complex Krishnapriyan et al. (2021), and they contain many saddle points which attract second-order optimization algorithms Dauphin et al. (2014). Our observation is consistent with (Markidis, 2021), where they also observed this downside of L-BFGS and recommended using Adam for at least the first few epochs. It is also important to mention that, unlike the other methods, the weight initialization for C-learning was not random. Instead, it was based on the optimal solution achieved for lower frequency modes, which requires extra training. 5 CONCLUSION: In the present study, through the lens of NTKs, we examined the dynamics of training PINNs via SGDM. We also showed that under SGDM the convergence rate of low-frequency features becomes slower (analogous to under-damped oscillation) and many high-frequency features undergo much faster conversion dynamics (analogous to over-damped or critically-damped oscillations). Thus, the effect of spectral bias becomes less prominent. Moreover, we discussed how training a PINN via Adam can even further accelerate convergence, and showed it can be much faster than SGDM. Although we analyzed the theoretical dynamics of convergence by assuming an infinitely-wide network, our experiments confirmed the estimated solutions obtained from the trained models via SGDM and Adam had high accuracy, using finite and practical widths. A CONVERGENCE OF WIDE PINNS: Proposition A. Let \u03bbbmax and \u03bbrmax , respectively, be the largest eigenvalues of the Hessians \u22072Lb(wt) and \u22072Lr(wt). Consider an infinitely-wide PINN optimized with the following update rule: wt+1 = wt \u2212 \u03b7\u2207L(wt), where \u03b7 is a constant learning rate. Then, provided \u03b7 = O(1/(\u03bbbmax +\u03bbrmax)), the \u00b5-PL\u2217 condition is satisfied and the PINN will converge. Proof. The loss function at time t+ 1 can be written as: L(wt+1) \u2248 L(wt) + (wt+1 \u2212 wt)\u22a4\u2207L(wt) + 1 2 (wt+1 \u2212 wt)\u22a4H(wt)(wt+1 \u2212 wt) where H(wt) = \u22072Lb(wt) + \u22072Lr(wt) (Wang et al., 2021). Following the approach of (Wang et al., 2021), let Q be an orthogonal matrix diagonalizing H(wt) and v = L(wt)/\u2225L(wt)\u2225. With y = Qv, we have the following: L(wt+1) \u2248 L(wt)\u2212 \u03b7\u2225\u2207L(wt)\u22252 + \u03b72 2 \u2225\u2207L(wt)\u22252 ( Nb\u2211 i \u03bbbiyi 2 + Nr\u2211 i \u03bbriyi 2 ) \u2a85 L(wt)\u2212 \u03b7\u2225\u2207L(wt)\u22252 + \u03b72 2 \u2225\u2207L(wt)\u22252(\u03bbbmax + \u03bbrmax) where the \u03bbbi and \u03bbri are the respective eigenvalues of the Hessians of Lb and Lr ordered nondecreasingly, and the summation is taken over the components of y. From here, fixing any bound B such that \u03bbbmax + \u03bbrmax \u2264 B, we obtain the following inequality: L(wt+1) \u2a85 L(wt)\u2212 \u03b7\u2225\u2207L(wt)\u22252 ( 1\u2212 \u03b7B 2 ) Witness that if \u03b7 = 1/B, we can further simplify this to: L(wt+1) \u2a85 L(wt)\u2212 \u03b7\u2225\u2207L(wt)\u22252 Since our NN satisfies the \u00b5-PL\u2217 condition from Eq. 3 due to its width, we therefore have: L(wt+1) \u2264 (1\u2212 \u03b7\u00b5)L(wt), which concludes the proof. B HESSIAN FOR POISSON\u2019S EQUATION: Proposition B. Let H be the Hessian of the loss function of an infinitely-wide PINN for the onedimensional Poisson equation defined by Eq. 4 with u(x) = sin(Cx). Then, we have H = O(C4). Proof. The Hessian of loss on the collocation points is calculated following the methods introduced in Wang et al. (2021), where the gradient of the loss function is: \u2202Lr \u2202w = \u2202 \u222b 1 0 (\u2202 2uw \u2202x2 \u2212 \u22022u \u2202x2 ) 2dx \u2202w . Here, w \u2208 w, u(x) is the target solution (admitting some parameter C), and uw(x) is the NN approximation of the output. Assuming that the approximation is a good representation of the actual solution, it can be written as uw(x) = u(x)\u03f5w(x), where \u03f5w(x) is a smooth function taking values in [0, 1], such that |\u03f5w(x)\u22121| < \u03b4 for some \u03b4 > 0 and \u2225\u2202 k\u03f5w(x) \u2202xk \u2225 \u2264 \u03b4, where we have the L\u221e norm. The Hessian of the loss function will therefore be: \u22022Lr \u2202w2 = \u22022 \u222b 1 0 (\u2202 2uw \u2202x2 \u2212 \u22022u \u2202x2 ) 2 \u2202w2 = \u2202I \u2202w where I = \u2202Lr\u2202w , and can be calculated to be: I = 2 \u22022uw \u2202w\u2202x ( \u22022uw \u2202x2 \u2212 \u2202 2u \u2202x2 ) \u2223\u2223\u22231 0 \u2212 \u2202uw \u2202w ( \u22023uw \u2202x3 \u2212 \u2202 3u \u2202x3 ) \u2223\u2223\u22231 0 + \u222b 1 0 \u2202uw \u2202w ( \u22024uw \u2202x4 \u2212 \u2202 4u \u2202x4 ) dx. The above equation contains 3 terms, which we call I1, I2, and I3 respectively. Note that \u2202I\u2202w = \u2202I1 \u2202w + \u2202I2 \u2202w + \u2202I3 \u2202w . We calculate these terms as follows: I1 = 2 \u22022uw \u2202w\u2202x ( \u22022uw \u2202x2 \u2212 \u2202 2u \u2202x2 ) \u2223\u2223\u22231 0 \u2202I1 \u2202w = 2 \u2202( \u2202 2uw \u2202w\u2202x ) \u2202w ( \u22022uw \u2202x \u2212 \u2202 2u \u2202x2 ) ( \u22022uw \u2202x2 \u2212 \u2202 2u \u2202x2 ) \u2223\u2223\u22231 0 = 2 ( \u2202 \u2202(u\u2032(x)\u03f5w(x)+u(x)\u03f5 \u2032 w(x)) \u2202w \u2202w )( \u22022uw \u2202x2 \u2212 \u2202 2u \u2202x2 ) \u2223\u2223\u22231 0 = 2 ( \u2202(u\u2032(x)\u2202\u03f5w(x)\u2202w + u(x) \u2202\u03f5\u2032w(x) \u2202w ) \u2202w )( \u22022uw \u2202x2 \u2212 \u2202 2u \u2202x2 ) \u2223\u2223\u22231 0 = 2 ( u\u2032(x) \u22022\u03f5w(x) \u2202w2 + u(x) \u22022\u03f5\u2032w(x) \u2202w2 )( \u22022uw \u2202x2 \u2212 \u2202 2u \u2202x2 ) \u2223\u2223\u22231 0 . Note that |u(x)| \u2264 1 and by the chain rule |u\u2032(x)| \u2264 C. An application of the triangle inequality then yields: \u2223\u2223\u2223\u2223u\u2032(x)\u22022\u03f5w(x)\u2202w2 + u(x)\u22022\u03f5\u2032w(x)\u2202w2 \u2223\u2223\u2223\u2223 \u2264 C \u2225\u2225\u2225\u2225\u22022\u03f5w(x)\u2202w2 \u2225\u2225\u2225\u2225+ \u2225\u2225\u2225\u2225\u22022\u03f5\u2032w(x)\u2202w2 \u2225\u2225\u2225\u2225 . Using the assumptions on \u03f5w(x) we get:\u2223\u2223\u2223\u2223\u22022uw\u2202x2 \u2212 \u22022u\u2202x2 \u2223\u2223\u2223\u2223 \u2264 (C2 + 2)\u03b4. Combing both of these, we see \u2202I1\u2202w = O(C 3). Similarly, \u2202I2\u2202w = O(C 3) and \u2202I3\u2202w = O(C 4), which concludes the proof. C GRADIENT FLOW FOR SGD WITH MOMENTUM: Theorem C. For an infinitely-wide PINN, the gradient flow of SGDM is: m [ u\u0308(xb,w(t)) D[u\u0308](xr,w(t)) ] = \u2212\u00b5 [ u\u0307(xb,w(t)) D[u\u0307](xr,w(t)) ] \u2212 K [ u(xb,w(t))\u2212 g(xb) D[u](xr,w(t))\u2212 h(xr) ] that is analogous to a point mass m undergoing a damped harmonic oscillation in a viscous medium with a friction coefficient of \u00b5(\u03b1) that is function of \u03b1, Furthermore, K is defined as: K = [ Kbb Krb Kbr Krr ] , where: Kbb(x,x\u2032) = \u2207wu(w, x) \u22a4\u2207wu(w, x\u2032) Kbr(x,x\u2032) = \u2207wu(w, x) \u22a4\u2207wD[u](w, x\u2032) Krr(x,x\u2032) = \u2207wD[u](w, x \u2032) \u22a4\u2207wD[u](w, x\u2032) are three NTKs associated with the boundary and residual terms. Moreover, let \u03b3 = \u00b5/2m, \u03bai be the i-th eigenvalue of K, and \u03ba\u2032i = \u03bai m . Then, the solutions to the gradient flow are of the form: A1e \u03bbi1 t +A2e \u03bbi2 t \u03bbi1,2 = \u2212\u03b3 \u00b1 \u221a \u03b32 \u2212 \u03ba\u2032i (7) where A1 and A2 are constants. Proof. Recall that as the NN becomes wider, the norm of the Hessian becomes smaller, such that in the limit as N \u2192 \u221e the norm of Hessian becomes 0. One immediate consequence of small Hessian for a NN is that its output can be estimated by a linear function Lee et al. (2019). The output of a NN can thus be replaced by its first-order Taylor expansion: ulint (w) \u2248 u(w)|w0 + (w \u2212 w0)\u2207u(w)|w0 . The update rule for SGDM can be written as Du (2019): wt+1 = wt + \u03b1(wt \u2212 wt\u22121)\u2212 \u03b7\u2207wL(w). The discrete updates to the output of NN become (see Appendix C.1): ulint+1 = u lin t + \u03b1(u lin t \u2212 ulint\u22121)\u2212 \u03b7\u2207wL(w)\u2207wu(w)|w0 In the rest of this section, for simplicity, we will drop the \u201clin\u201d term. The dynamics of SGDM are analogous to the equation of motion of a point mass m undergoing a damped harmonic oscillation (Appendix C.2): mu\u0308+ \u00b5u\u0307\u2212\u2207wL(w)f(u) = 0 where f(u) is a linear function of u, and \u00b5 is the friction coefficient that is related to the momentum term in SGDM as such: \u03b1 = mm+\u00b5\u2206t Qian (1999). Thus, the gradient flow of ut and D[u](x,w(t)) can be written as: mu\u0308(xb,w(t)) =\u2212 \u00b5u\u0307(xb,w(t))\u2212 Kbb(x,x\u2032)(w)(u(xr,w(t))\u2212 g(xb)) \u2212 Krb(x,x\u2032)(w)(D[u](xr,w(t))\u2212 h(xr)) mD[u\u0308](xr,w(t)) =\u2212 \u00b5D[u\u0307](xr,w(t))\u2212 Kbr(x,x\u2032)(w)(u(xb,w(t))\u2212 g(xb)) \u2212 Krr(x,x\u2032)(w)(D[u](xr,w(t))\u2212 h(xr)). (C.1) As mentioned earlier, in wide NNs if the last layer of the network is linear then the tanget kernels are static. We write Eq. C.1 in matrix form as follows: m [ u\u0308(xb,w(t)) D[u\u0308](xr,w(t)) ] = \u2212\u00b5 [ u\u0307(xb,w(t)) D[u\u0307](xr,w(t)) ] \u2212 K [ u(xb,w(t))\u2212 g(xb) D[u](xr,w(t))\u2212 h(xr) ] (C.2) As K is a positive semi-definite matrix Wang et al. (2021) Eq. C.2 can be viewed as a set of independent differential equations, each one corresponding to an eigenvalue \u03bbi of the kernel. These give rise to the individual general solutions of the form: A1e \u03bbi1 t +A2e \u03bbi2 t \u03bbi1,2 = \u2212\u03b3 \u00b1 \u221a \u03b32 \u2212 \u03ba\u2032i where A1 and A2 are constants, \u03b3 = \u00b5/2, and \u03ba\u2032i = \u03bai m . Of note, \u00b5 and m are set by the user, which in turn define the value of momentum. The choice of values of m and \u00b5 will determine the rate of decay of the above equation. C.1 LINEAR NN UPDATES The Taylor expansions of the outputs at time steps t+ 1 and t are: ulint+1(w) = u(w)|w0 + (wt+1 \u2212 w0)\u2207u(w)|w0 ulint (w) = u(w)|w0 + (wt \u2212 w0)\u2207u(w)|w0 . Thus, the difference between the outputs in the interval is: ulint+1(w)\u2212 ulint (w) = \u2207u(w)|w0(wt+1 \u2212 wt) = \u2207u(w)|w0(\u03b1(wt \u2212 wt\u22121)\u2212 \u03b7\u2207L(wt)) where we used the update rule of SGD with momentum within the equation. Similarly, we have: ulint (w)\u2212 ulint\u22121(w) = \u2207u(w)|w0(wt \u2212 wt\u22121) and: ulint+1(w)\u2212 ulint (w) = \u2207u(w)|w0 ( \u03b1 ( ulint (w)\u2212 ulint\u22121(w) \u2207u(w)|w0 ) \u2212 \u03b7\u2207L(wt) ) . Thus: ulint+1(w) = u lin t (w)\u2212 \u03b7\u2207L(wt) \u00b7 \u2207u(w)|w0 \u2212 \u03b1(ulint (w)\u2212 ulint\u22121(w)). C.2 RELATION BETWEEN SGDM AND DAMPED OSCILLATION The dynamics of a point mass m undergoing a damped harmonic oscillation with a friction coefficient of \u00b5 are given by: mw\u0308 + \u00b5w\u0307 = \u2212\u2207wL(w) (3) where \u2207wL(w) is the force field. Qian (1999) showed that Eq. 3 in its discrete format can be written as: m wt+\u2206t + wt\u2212\u2206t \u2212 2wt \u2206t2 + \u00b5 wt+\u2206t \u2212 wt \u2206t = \u2212\u2207wL(w). After some algebraic simplifications, the above equation becomes: wt+\u2206t \u2212 wt = m m+ \u00b5\u2206t (wt \u2212 wt\u2212\u2206t)\u2212 \u2206t2 m+ \u00b5\u2206t \u2207wL(w). Clearly, mm+\u00b5\u2206t is equivalent to the momentum term in SGDM, and \u2206t2 m+\u00b5\u2206t can be treated as the learning term. The dynamics of the output of a wide network (Appendix C.1) under SGDM are similar. D RELATION BETWEEN ADAM AND DAMPED OSCILLATION: It has been observed that Adam yields similar convergence rates to SGDM (Kingma & Ba, 2014). Here, we demonstrate that Adam also has dynamics equivalent to a damped oscillator, similar to SGDM (Appendix C.2). First, let us briefly recall the traditional update rule for Adam: \u2206wt = \u2212\u03b7\u221a \u03bd\u0302t + \u03f5 m\u0302t where mt and \u03bdt are defined as: m\u0302t = mt 1\u2212 \u03b21t+1 , mt = \u03b21mt\u22121 + (1\u2212 \u03b21)gt \u03bd\u0302t = \u03bdt 1\u2212 \u03b22t+1 , \u03bdt = \u03b22\u03bdt\u22121 + (1\u2212 \u03b22)gt2 g(t) = \u2212\u2207wL(w). Proposition D. The update rule for Adam can be written as: wt+1 = wt \u2212 P (t)(wt \u2212 wt\u22121)\u2212Q(t)\u2207wL(wt). where: P (t) = \u03b21( \u221a \u03bdt\u22121 + \u03f5)(1\u2212 \u03b21(t\u2212 1)) ( \u221a \u03bdt + \u03f5)(1\u2212 \u03b21t) Q(t) = \u03b7(1\u2212 \u03b21t) ( \u221a \u03bdt + \u03f5)(1\u2212 \u03b21t) . Proof. It is straightforward to show that the update rule can be written as: wt+1 = wt \u2212 \u03b7\u03b21mt\u22121 ( \u221a \u03bdt + \u03f5)(1\u2212 \u03b21t) \u2212 \u03b7(1\u2212 \u03b21) ( \u221a \u03bdt + \u03f5)(1\u2212 \u03b21t) \u2207wL(wt). Inserting wt = wt\u22121 \u2212 \u03b7\u221a\u03bd\u0302t+\u03f5m\u0302t into the above equation shows: wt+1 = wt\u2212 \u03b21( \u221a \u03bdt\u22121 + \u03f5)(1\u2212 \u03b21(t\u2212 1)) ( \u221a \u03bdt + \u03f5)(1\u2212 \u03b21t) (wt\u2212wt\u22121)\u2212 \u03b7(1\u2212 \u03b21t) ( \u221a \u03bdt + \u03f5)(1\u2212 \u03b21t) \u2207wL(wt). (D.1) By the definition of P (t) and Q(t) we can rewrite Eq. D.1 as: wt+1 = wt \u2212 P (t)(wt \u2212 wt\u22121)\u2212Q(t)\u2207wL(wt) (D.2) as claimed. Clearly, Eq. D.2 has the same format of the SGDM update rule. Thus, the weight updates for Adam follows the same dynamics as SGDM, that of oscillatory motion of a point mass under friction (from Appendix C.2). However, based on the updated value of the second momentum of the gradient of loss function, both the momentum and the learning rate can be updated by Adam at each iteration. Thus, the convergence to the solution by Adam is much faster compared to SGDM. E DECAY COMPARISON BETWEEN SGDM AND SGD: Here, the dynamics of the error decay of SGDM and vanilla SGD under a large eigenvalue (\u03bb1 = 103) and a relatively small eigenvalue (\u03bb2 = 10\u22125) are plotted. For vanilla SGD, the training error decays at a rate of e\u2212\u03bbit. As shown in Fig. E.1a, for the \u03bb2 the training error decays slowly, however is very fast for \u03bb1 (almost immediately dropping to zero). Thus, components of the target function that correspond to the large eigenvalues will be learned much faster. These are the eigenvalues corresponding to lower frequencies in the target function, so clearly the network suffers from spectral bias under SGD. In contrast, learning under SGDM, the training error for \u03bb1 decays following under-damped oscillation (the red curve in Fig. E.1b), while the training error for \u03bb2 follows over-damped oscillation (the blue curve in Fig. E.1a). The training error of both eigenvalues, the large \u03bb1 and small \u03bb2, decay at approximately the same time. The components of the target function that correspond to the larger eigenvalue are thus not learned any faster, so the effect of spectral bias is minimized. Figure E.1: The comparison of the error decay for a small and a large eigenvalue, a) learning decay during SGD optimization process, b) learning decay during SGDD optimization process. Knowing this fundamental difference between SGDM and SGD, it is of interest to investigate how the solutions for higher-frequency target functions evolve in time under SGD and SGDM. Thus, for the Poisson equation when C = 15\u03c0, we have plotted the solutions at epochs 2000, 10000, 20000, 30000, 40000, and 50000 (SGD: Fig. E.2 and SGDM: Fig. E.3). The NN used was fully-connected, and had 2 hidden layers with 500 neurons at each layer. The evolution of the solutions given by SGD is quite interesting. Wang et al. (2022) showed that in Poisson\u2019s equation, the eigenvalues of the boundary kernel are much smaller than the eigenvalues of the PDE kernel. Thus, the former represent the higher frequencies, and they will be learned slower under SGD. This clearly reflected in the evolution. At epoch 10000, the solution has the correct sinusoidal shape, however it is vertically shifted and clearly cannot satisfy the boundary conditions. As the training continues in time, the solution becomes less vertically shifted and closer to the boundary condition values. However, the learning process is slow, and even at epoch 50000 the estimation is far from the actual solution. Meanwhile, SGDM is much faster as shown in Fig. E.3. At epoch 10000 the solution already has the correct sinusoidal shape and satisfies one end of the initial conditions. By epoch 20000 the estimated solution has a low error on the order of 10\u22122. Figure E.2: The solution of the Poisson equation, when C = 15\u03c0 in different stages of the learning process via SGD. a) 2000 epochs, b) 10000 epochs, c) 20000 epochs, d) 30000 epochs, e) 40000 epochs, and f) 50000 epochs. These observations are very insightful as they reveal a significant difference between PINNs and regular neural networks. Perhaps to explain the difference, it is useful to investigate the evolution of the solution via SGD for a completely data-driven case. We trained a fully-connected forward neural network with the same architecture as the above PINN. The training data-set contained 3000 synthetic data points generated based on the solution of Poisson\u2019s equation, and we used the mean squared error loss function. The solutions after 5000 and 50000 epochs are shown in Fig. E.4. As the fully-connected network has no physics-informed regularization term, it has difficulties estimating the correct solution. Despite this, it still exhibits spectral bias, as the estimated solutions at both 5000 and 50000 epochs represent low-frequency sinusoidal forms. However, unlike a PINN there is no vertical shift in the solutions. Clearly, the physics-informed regularization term helps significantly to resolve the classical spectral bias (dealing with high-order frequency target functions), learning the correct sinusoidal shape faster. In return, the solutions are worse at satisfying the boundary condition values. This is because the eigenvalues of the boundary kernel represent high-frequency modes. F LOSS FUNCTION OF EQUATIONS: F.1 LOSS FUNCTION FOR POISSON\u2019S EQUATION For Poisson\u2019s equation: f(x) = \u2212C2 sin(Cx), x \u2208 [0, 1] g(x) = 0, x = 0, 1 Figure E.3: The solution of the Poisson equation, when C = 15\u03c0 in different stages of the learning process via SGDM. a) 2000 epochs, b) 10000 epochs, c) 20000 epochs, d) 30000 epochs, e) 40000 epochs, and f) 50000 epochs. Figure E.4: The completely data-driven solution of the Poisson equation based on SGD optimization process. a) after 5000 epochs, b) after 50000 epochs. u(x) = sin(Cx) is used as the exact solution. The corresponding loss function is written as: L(w) := 1 Nb Nb\u2211 i=1 ( u\u0302(xb i,w)\u2212 g(xbi) )2 + 1 Nr Nr\u2211 i=1 ( \u22022u\u0302 \u2202x2 (xr i,w)\u2212 f(xri) )2 where u\u0302 is the output of the network. F.2 LOSS FUNCTION FOR THE TRANSPORT FUNCTION Using the methods of characteristics (Evans, 2010), the transport function has a well-defined analytical solution: u(x, t) = g(x \u2212 \u03b2t). This is used as the exact solution. The corresponding loss function is written as: L(w) := 1 Nb Nb\u2211 i=1 ( u\u0302(xb i, tb i,w)\u2212 g(xbi) )2 + 1 Nr Nr\u2211 i=1 ( \u2202u\u0302(xr i, tb i,w) \u2202t + \u03b2 \u2202u\u0302(xr i, tb i,w) \u2202x )2 where u\u0302 is the output of the network. G EXPERIMENT PLOTS: G.1 POISSON EQUATION Figure G.1: The estimation of the solution of the Poisson\u2019s equation for C = 15\u03c0, using the vanilla SGD algorithm after 180000 epochs. G.2 REACTION-DIFFUSION EQUATION A reaction-diffusion equation contains a reaction and a diffusion term. Its general form is written as: \u2202u \u2202t = \u03bd\u2206u+ f(u) (G.1) where u(x, t) is the solution describing the density/concentration of a substance, \u2206 is the Laplace operator, \u03bd is a diffusion coefficient, and f(u) is a smooth function describing processes that change the present state of u (for example, birth, death or a chemical reaction). Here, we assume a onedimensional equation, where f(u) = \u03c1u(1\u2212 u). For \u03c1 independent of x and t, and with the defined f(u), Eq. G.1, can be solved analytically (Evans, 2010). To estimate the solution using PINNs, the loss function for the 1D reaction-diffusion PDE is written as: L(w) := 1 Nb Nb\u2211 i=1 ( u\u0302(xib, t i b,w)\u2212 g(xib) )2 + 1 Nr Nr\u2211 i=1 ( \u2202u\u0302 \u2202t (xir, t i b,w)\u2212 \u03bd \u22022u\u0302 \u2202x2 (xir, t i b,w)\u2212 \u03c1u\u0302(xir, tib)(1\u2212 u\u0302(xir, tib)) )2 . For consistency with Krishnapriyan et al. (2021), we used Nr = 1000, and Nb = 100. We also chose the initial and boundary conditions u(x, 0) = exp\u2212 (x\u2212\u03c0) 2 \u03c02/2 and u(0, t) = u(2\u03c0, t) respectively. Similar to the previous experiments, for larger choices of \u03bd and \u03c1 the model trained via vanilla SGD (after 85000 epochs) had difficulty converging to the solution (Fig. G.3, left panel). However, models trained via SGDM and Adam (after 45000 epochs) could provide solutions with low error. The plots of estimated and exact solutions for the three algorithms when \u03bd = 3 and \u03c1 = 5 are shown in Fig. G.2. Figure G.2: 1D reaction-diffusion equation for \u03bd = 3 and \u03c1 = 5. The solutions are obtained by training a 4-layer network with width=500 at each layer. Top panels: The exact (analytical) solution. Middle panels: The estimated solution. Bottom panels: The absolute difference between the exact and estimated solutions. (a) Vanilla SGD, (b) SGDM, (c) Adam. Figure G.3: Left Panel: The training losses of the network, when \u03bd = 9 and \u03c1 = 7), via AW, Clearning, wide SGDM, and wide Adam are plotted. Right Panel: The relative error of the estimated solutions for different values of \u03bd and \u03c1 are plotted. We further compared the solutions of the wide Adam, C-learning, and AW. Similar to the previous experiments, for higher-frequency modes Adam showed superior results. The relative errors of the estimated solutions for different values of \u03bd and \u03c1 were computed (Fig. 6 (left panel)). Furthermore, when \u03bd = 9 and \u03c1 = 7), we observed that the loss function under SGDM and Adam had much faster decay than under AW. But, the loss under L-BFGS decayed as fast as under Adam. We reasoned as to why in Section 4.3. G.3 TRANSPORT FUNCTION: FURTHER COMPARISON WITH DIFFERENT INITIAL CONDITIONS We implemented different initial conditions for the transport function and ran several experiments. For the initial condition of tanh(x), when \u03b2 \u2265 6 both C-learning and AW estimated solutions of showed large relative errors on the order of 10\u22121. However, the estimated solution of a wide network trained via Adam had an acceptable relative error on the order of 10\u22122 (Fig. G.4). The estimated solutions for all three optimizers when \u03b2 = 6 are shown in Fig. G.5. All networks were trained using a 4-layer network, and AW and Adam both had a width of 500 neurons. The estimated solutions are based on training the model for 85000 epochs. Figure G.4: The relative errors of the estimated solutions of 1D transport function, for different values of \u03b2 using the curriculum learning method (red dashed line), and our approach (blue dashed line) are plotted. Figure G.5: The estimated solutions of the transport function based on the initial condition of tanh(x) when \u03b2 = 6: a) The exact solution b) Estimated solution of a wide network trained via Adam c) Estimated solution of C-learning d) Estimated solution of AW For the initial condition of sin(x) cos(x), C-learning had small estimated errors (on the order of 10\u22122) for \u03b2 \u2264 20, however for higher values of \u03b2 the error grew larger (Fig. G.6). AW had errors on the order of 10\u22122 for \u03b2 \u2264 10, however the error began to grow for larger values of \u03b2. Adam had errors on the order of 10\u22122 even up to \u03b2 \u2264 25 (Fig. G.6). The estimated solutions based on C-learning, AW, and wide Adam for \u03b2 = 25 are shown in Fig. G.7. All networks were trained using a 4-layer network. The estimated solutions are based on training the model for 85000 epochs. Figure G.6: The relative errors of the estimated solutions of 1D transport function, based on the initial condition of sin(x) cos(x), for different values of \u03b2 using the curriculum learning method (red dashed line), and our approach (blue dashed line) are plotted. Figure G.7: The estimation of the solution of the transport function based on the initial condition of sin(x) cos(x) when \u03b2 = 25: a) The exact solution b) Estimated solution of a wide network trained via Adam c) Estimated solution of C-learning d) Estimated solution of AW"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='GetBudget'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'directory': '.'}, name='ListFiles'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Budget retrieved successfully\",\n  \"output\": {\n    \"total_resource_budget\": 100,\n    \"current_resource_usage\": 0,\n    \"current_remaining_resource_budget\": 100,\n    \"total_expense_budget\": 10000,\n    \"current_expense\": 1.7867214999999996,\n    \"current_remaining_expense_budget\": 9998.2132785\n  }\n}\n```\n", "metadata": {"title": "Invoking `GetBudget` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Files listed successfully\",\n  \"error\": null,\n  \"output\": [\n    \"favicon.ico\",\n    \"requirements.txt\",\n    \"HASHIRU_LOGO.png\",\n    \"HASHIRU_ARCH.png\",\n    \"deleteAgents.py\",\n    \"CITATION.cff\",\n    \"tests\",\n    \"__pycache__\",\n    \"start.py\",\n    \"README.md\",\n    \"results\",\n    \"HASHIRU.png\",\n    \".gitignore\",\n    \".env\",\n    \"static\",\n    \".gitattributes\",\n    \"app.py\",\n    \"HASHIRU_2.png\",\n    \".git\",\n    \"paper_review_hashiru_20250627_201357.jsonl\",\n    \"src\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ListFiles` with \n```json\n{\n  \"directory\": \".\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetBudget', response={'result': {'status': 'success', 'message': 'Budget retrieved successfully', 'output': {'total_resource_budget': 100, 'current_resource_usage': 0, 'current_remaining_resource_budget': 100, 'total_expense_budget': 10000, 'current_expense': 1.7867214999999996, 'current_remaining_expense_budget': 9998.2132785}}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ListFiles', response={'result': {'status': 'success', 'message': 'Files listed successfully', 'error': None, 'output': ['favicon.ico', 'requirements.txt', 'HASHIRU_LOGO.png', 'HASHIRU_ARCH.png', 'deleteAgents.py', 'CITATION.cff', 'tests', '__pycache__', 'start.py', 'README.md', 'results', 'HASHIRU.png', '.gitignore', '.env', 'static', '.gitattributes', 'app.py', 'HASHIRU_2.png', '.git', 'paper_review_hashiru_20250627_201357.jsonl', 'src']}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'query': 'ICLR 2023 acceptance rate', 'website': 'iclr.cc'}, name='GoogleSearchTool'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'query': 'ICLR 2023 review criteria', 'website': 'iclr.cc'}, name='GoogleSearchTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": [\n    \"https://media.iclr.cc/Conferences/ICLR2023/ICLR2023-Fact_Sheet.pdf\",\n    \"https://media.iclr.cc/Conferences/ICLR2024/ICLR2024-Fact_Sheet.pdf\",\n    \"https://blog.iclr.cc/2023/11/02/tiny-papers-strike-back-2023-reflections-and-2024-announcement/\",\n    \"https://media.iclr.cc/Conferences/ICLR2025/ICLR2025_Fact_Sheet.pdf\",\n    \"https://iclr.cc/virtual/2023/papers.html\",\n    \"https://iclr.cc/\",\n    \"https://iclr.cc/Conferences/2023/CallForTinyPapers\",\n    \"https://media.iclr.cc/Conferences/ICLR2024/ICLR2024_Press_Release.pdf\",\n    \"https://iclr.cc/Conferences/2023/ReviewerGuide\",\n    \"https://iclr.cc/Conferences/2024/CallForPapers\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `GoogleSearchTool` with \n```json\n{\n  \"query\": \"ICLR 2023 acceptance rate\",\n  \"website\": \"iclr.cc\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": [\n    \"https://iclr.cc/Conferences/2023/ReviewerGuide\",\n    \"https://blog.iclr.cc/2023/04/12/ethics-review-process-for-iclr-2023/\",\n    \"https://blog.iclr.cc/category/iclr-2023/\",\n    \"https://blog.iclr.cc/tag/reviewing/\",\n    \"https://blog.iclr.cc/tag/awards/\",\n    \"https://blog.iclr.cc/2023/04/05/announcing-notable-reviewers-and-area-chairs-at-iclr-2023/\",\n    \"https://iclr.cc/Conferences/2025/FAQ\",\n    \"https://iclr.cc/Conferences/2024/CallForTinyPapers\",\n    \"https://iclr.cc/Conferences/2019/Reviewer_Guidelines\",\n    \"https://iclr.cc/Conferences/2025/ACGuide\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `GoogleSearchTool` with \n```json\n{\n  \"query\": \"ICLR 2023 review criteria\",\n  \"website\": \"iclr.cc\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GoogleSearchTool', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': ['https://media.iclr.cc/Conferences/ICLR2023/ICLR2023-Fact_Sheet.pdf', 'https://media.iclr.cc/Conferences/ICLR2024/ICLR2024-Fact_Sheet.pdf', 'https://blog.iclr.cc/2023/11/02/tiny-papers-strike-back-2023-reflections-and-2024-announcement/', 'https://media.iclr.cc/Conferences/ICLR2025/ICLR2025_Fact_Sheet.pdf', 'https://iclr.cc/virtual/2023/papers.html', 'https://iclr.cc/', 'https://iclr.cc/Conferences/2023/CallForTinyPapers', 'https://media.iclr.cc/Conferences/ICLR2024/ICLR2024_Press_Release.pdf', 'https://iclr.cc/Conferences/2023/ReviewerGuide', 'https://iclr.cc/Conferences/2024/CallForPapers']}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GoogleSearchTool', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': ['https://iclr.cc/Conferences/2023/ReviewerGuide', 'https://blog.iclr.cc/2023/04/12/ethics-review-process-for-iclr-2023/', 'https://blog.iclr.cc/category/iclr-2023/', 'https://blog.iclr.cc/tag/reviewing/', 'https://blog.iclr.cc/tag/awards/', 'https://blog.iclr.cc/2023/04/05/announcing-notable-reviewers-and-area-chairs-at-iclr-2023/', 'https://iclr.cc/Conferences/2025/FAQ', 'https://iclr.cc/Conferences/2024/CallForTinyPapers', 'https://iclr.cc/Conferences/2019/Reviewer_Guidelines', 'https://iclr.cc/Conferences/2025/ACGuide']}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'url': 'https://media.iclr.cc/Conferences/ICLR2023/ICLR2023-Fact_Sheet.pdf', 'output_type': 'summary'}, name='GetWebsite'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'output_type': 'full_text', 'url': 'https://iclr.cc/Conferences/2023/ReviewerGuide'}, name='GetWebsite'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": \"Lv99  GpM tQ4  9Zil  b9G4My76g a jte88kc9N.m2dbOf N LXKsc3icLVSpGZ b  wc Jyh06IeU DbtNs69758 0ZrlQiLXbh0 RHHZFccU 9ZnGzu0HGxCBbENJUQUXnqvcR6FVVVu.vTYll5Z5jsJbS3.. IRQe.lxLJFn7 vFLNgDGuTV0ziV lEH Tu 3v QHWYYri  ja 5 OG6L6AtK 1gWSLy A Xb0mVTluuLo5INOhT  6WNrlB2TF X  dvJTEu.. TNJ6IeaFyn2 2T lI6FL2E h 40XhczwoCTO 04c 5FA xb hT BnAO8Sh sr896VAoah005S   L   dG3x0j j   K998P ..\"\n}\n```\n", "metadata": {"title": "Invoking `GetWebsite` with \n```json\n{\n  \"url\": \"https://media.iclr.cc/Conferences/ICLR2023/ICLR2023-Fact_Sheet.pdf\",\n  \"output_type\": \"summary\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR 2022 Reviewer Instructions\\n\\n\\n\\n\\n\\nSkip to yearly menu bar\\nSkip to main content\\n\\nMain Navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            ICLR\\n        \\n\\n\\n\\n\\n                        Help/FAQ\\n                    \\n\\n\\n\\n\\n\\n\\n                        Contact ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n                        Downloads\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Blog\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n                        Privacy Policy\\n                    \\n\\n\\n\\n\\n\\n\\n                        Create Profile\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reset Password\\n                    \\n\\n\\n\\n\\n\\n\\n                        Journal To Conference Track\\n                    \\n\\n\\n\\n\\n\\n\\n                        Diversity & Inclusion\\n                    \\n\\n\\n\\n\\n\\n\\n                        Proceedings at OpenReview\\n                    \\n\\n\\n\\n\\n\\n\\n                        Future Meetings\\n                    \\n\\n\\n\\n\\n\\n\\n                        Press\\n                    \\n\\n\\n\\n\\n\\n\\n                        Exhibitor Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Twitter\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        My Stuff\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Login\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Select Year: (2023)\\n        \\n\\n\\n2026\\n                \\n\\n\\n\\n2025\\n                \\n\\n\\n\\n2024\\n                \\n\\n\\n\\n2023\\n                \\n\\n\\n\\n2022\\n                \\n\\n\\n\\n2021\\n                \\n\\n\\n\\n2020\\n                \\n\\n\\n\\n2019\\n                \\n\\n\\n\\n2018\\n                \\n\\n\\n\\n2017\\n                \\n\\n\\n\\n2016\\n                \\n\\n\\n\\n2015\\n                \\n\\n\\n\\n2014\\n                \\n\\n\\n\\n2013\\n                \\n\\n\\n\\n\\n\\n\\n                        Dates\\n                    \\n\\n\\n\\n\\n            Calls\\n        \\n\\n\\n\\n\\n                        Call for Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Tiny Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Workshops\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Blog Posts\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Socials\\n                    \\n\\n\\n\\n\\n\\n\\n            Guides\\n        \\n\\n\\n\\n\\n                        Author Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewer Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Area Chair Guide\\n                    \\n\\n\\n\\n\\n\\n\\n            Attend\\n        \\n\\n\\n\\n\\n                        Register\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visa and Travel  Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        Hotels\\n                    \\n\\n\\n\\n\\n\\n\\n                        Child Care\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visiting Kigali\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n            Organization\\n        \\n\\n\\n\\n\\n                        ICLR Board\\n                    \\n\\n\\n\\n\\n\\n\\n                        Organizing Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Program Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Ethics Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        Conference Site\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nICLR 2023 Reviewer Guide\\nThank you for agreeing to serve as an ICLR 2023 reviewer. Your contribution as a reviewer is paramount to creating an exciting and high-quality program. We ask that:\\n\\nYour reviews are timely and substantive.\\nYou follow the reviewing guidelines below.\u00a0\\nYou adhere to our Code of Ethics in your role as a reviewer. You must also adhere to our Code of Conduct.\\n\\nThis guide is intended to help you understand the ICLR 2023 decision process and your role within it. It contains:\\n\\nAn outline of the main reviewer tasks\\nStep-by-step reviewing instructions (especially relevant for reviewers that are new to ICLR)\\nReview examples\\nAn FAQ.\\n\\n\u00a0\\n\\n\\nWe\u2019re counting on you\\nAs a reviewer you are central to the program creation process for ICLR 2023. Your Area Chairs (ACs), Senior Area Chairs (SACs) and the Program Chairs (PCs) will rely greatly on your expertise and your diligent and thorough reviews to make decisions on each paper. Therefore, your role as a reviewer is critical to ensuring a strong program for ICLR 2023.\\nHigh-quality reviews are also very valuable for helping authors improve their work, whether it is eventually accepted by ICLR 2023, or not. Therefore it is important to treat each valid ICLR 2023 submission with equal care.\\nAs a token of our appreciation for your essential work, top reviewers will be acknowledged permanently on the ICLR 2023 website. Furthermore, top and high quality reviewers will receive special acknowledgement during the opening ceremony and free registration to ICLR 2023.\\n\u00a0\\n\\nMain reviewer tasks\\nThe main reviewer tasks and dates are as follows:\\n\\nCreate or update your OpenReview profile (September 30 2022)\\nBid on papers (September 30 2022 - October 06 2022)\\nWrite a constructive, thorough and timely review (October 12 2022 - October 24 2022)\\nInitial paper reviews released (November 4 2022)\\nDiscuss with authors and other reviewers to clarify and improve the paper (November 4 2022 - November 18 2022)\\nFlag any potential CoE violations and/or concerns (by November 18 2022)\\nProvide a recommendation to the area chair assigned to the paper (by November 18 2022)\\nVirtual Meeting with AC if paper you reviewed falls into borderline papers (November 18 2022 - December 12 2022)\\nProvide a final recommendation to the area chair assigned to the paper (after Virtual Meeting)\\n\\n\u00a0\\n\\n\\n\u00a0\\nCode of Ethics\\nAll ICLR participants, including reviewers, are required to adhere to the ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics). All reviewers are required to read the Code of Ethics and adhere to it. The Code of Ethics applies to all conference participation, including paper submission, reviewing, and paper discussion.\u00a0\\nAs part of the review process, reviewers are asked to raise potential violations of the ICLR Code of Ethics. Note that authors are encouraged to discuss questions and potential issues regarding the Code of Ethics as part of their submission. This discussion is not counted against the maximum page limit of the paper and should be included as a separate section.\\n\u00a0\\n\\n\u00a0\\nReviewing a submission: step-by-step\\nSummarized in one sentence, a review aims to determine whether a submission will bring sufficient value to the community and contribute new knowledge. The process can be broken down into the following main reviewer tasks:\\n\u00a0\\n\\n\\nRead the paper: It\u2019s important to carefully read through the entire paper, and to look up any related work and citations that will help you comprehensively evaluate it. Be sure to give yourself sufficient time for this step.\\nWhile reading, consider the following:\\n\\nObjective of the work: What is the goal of the paper? Is it to better address a known application or problem, draw attention to a new application or problem, or to introduce and/or explain a new theoretical finding? A combination of these? Different objectives will require different considerations as to potential value and impact.\\nStrong points: is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g. theoretically, algorithmically, etc.)?\\nWeak points: is it weak in any of the aspects listed in b.?\\nBe mindful of potential biases and try to be open-minded about the value and interest a paper can hold for the entire ICLR community, even if it may not be very interesting for you.\\n\\n\\nAnswer four key questions for yourself, to make a recommendation to Accept or Reject:\\n\\nWhat is the specific question and/or problem tackled by the paper?\\nIs the approach well motivated, including being well-placed in the literature?\\nDoes the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\\nWhat is the significance of the work? Does it contribute new knowledge and sufficient value to the community? Note, this does not necessarily require state-of-the-art results. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge (incl., empirical, theoretical, for practitioners, etc).\\n\\n\\nWrite and submit your initial review, organizing it as follows:\u00a0\\n\\nSummarize what the paper claims to contribute. Be positive and constructive.\\nList strong and weak points of the paper. Be as comprehensive as possible.\\nClearly state your initial recommendation (accept or reject) with one or two key reasons for this choice.\\nProvide supporting arguments for your recommendation.\\nAsk questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.\u00a0\\nProvide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\\n\\n\\nComplete the CoE report: ICLR has adopted the following Code of Ethics (CoE). When submitting your review, you\u2019ll be asked to complete a CoE report for the paper. The report is a simple form with two questions. The first asks whether there is a potential violation of the CoE. The second is relevant only if there is a potential violation and asks the reviewer to explain why there may be a potential violation. In order to answer these questions, it is therefore important that you read the CoE before starting your reviews. \\n\\t\u00a0\\nEngage in discussion: The discussion phase at ICLR is different from most conferences in the AI/ML community. During this phase, reviewers, authors and area chairs engage in asynchronous discussion and authors are allowed to revise their submissions to address concerns that arise. It is crucial that you are actively engaged during this phase. Maintain a spirit of openness to changing your initial recommendation (either to a more positive or more negative) rating.\\nBorderline paper meeting: This year, we are introducing a new program for ACs to (virtually) meet and discuss with reviewers only for borderline cases. ACs will reach out to schedule this meeting. This is to ensure active discussions among reviewers, and well-thought-out decisions. ACs will schedule the meeting and facilitate the discussion. For a productive discussion, it is important to familiarize yourself with other reviewers' feedback prior to the meeting. Please note that we will be leveraging information for reviewers who failed to attend this meeting (excluding emergencies).\u00a0\\nProvide final recommendation: Update your review, taking into account the new information collected during the discussion phase, and any revisions to the submission. State your reasoning and what did/didn\u2019t change your recommendation throughout the discussion phase.\\n\\n\u00a0\\n\\n\u00a0\\nFor great in-depth resources on reviewing, see these resources:\\n\\nDaniel Dennet, Criticising with Kindness.\\nComprehensive advice: Mistakes Reviewers Make\\nViews from multiple reviewers: Last minute reviewing advice\\nPerspective from instructions to Area Chairs: Dear ACs.\\n\\n\u00a0\\n\\n\u00a0\\nReview Examples\\nHere are two sample reviews from previous conferences that give an example of what we consider a good review for the case of leaning-to-accept and leaning-to-reject.\\nReview for a Paper where Leaning-to-Accept\\nThis paper proposes a method, Dual-AC, for optimizing the actor (policy) and critic (value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor. In order to achieve that, they take the linear programming approach of solving the Bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems. The discussion on the deficiencies of the naive LP approach is mostly well done. Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without losing convexity by applying a regularization. They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements. Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms.\u00a0\\nOverall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments. Given these clarifications in an author response, I would be willing to increase the score.\u00a0\\nFor the theory, there are a few steps that need clarification and further clarification on novelty. For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results. It looks like Theorem 2 has already been shown in \\\"Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time\u201d. There is a statement that \u201cChen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting. However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization\u201d. Is your Theorem 2 somehow an extension? Is Theorem 3 completely new?\\nThis is particularly called into question due to the lack of assumptions about the function class for value functions. It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function). This assumption seems to be used right at the bottom of Page 17, where U^{pi*} = V^*. Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small. More about conditions on eta_v would be illuminating.\u00a0\\nThere is also one step in the theorem that I cannot verify. On Page 18, how is the squared removed for difference between U and Upi? The transition from the second line of the proof to the third line is not clear. It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2.\u00a0\\n\u00a0\\n\\nFor the experiments, the following should be addressed.\\n1. It would have been better to also show the performance graphs with and without the improvements for multiple domains.\\n2. The central contribution is extending the single step LP to a multi-step formulation. It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains.\\n3. Increasing k also comes at a computational cost. I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO).\\n4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization. It was also mentioned that increasing the regularization parameter size increases the convergence rate. Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization? In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned. Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity? A bit more discussion on these choices would be helpful.\u00a0\\n\u00a0\\n\\nMinor comments:\\n1. Page 2: In equation 5, there should not be a 'ds' in the dual variable constraint\\n\u00a0\\n\\n\u00a0\\nReview for a Paper where Leaning-to-Reject\\nThis paper introduces a variation on temporal difference learning for the function approximation case that attempts to resolve the issue of over-generalization across temporally-successive states. The new approach is applied to both linear and non-linear function approximation, and for prediction and control problems. The algorithmic contribution is demonstrated with a suite of experiments in classic benchmark control domains (Mountain Car and Acrobot), and in Pong.\\nThis paper should be rejected because (1) the algorithm is not well justified either by theory or practice, (2) the paper never clearly demonstrates the existence of problem they are trying to solve (nor differentiates it from the usual problem of generalizing well), (3) the experiments are difficult to understand, missing many details, and generally do not support a significant contribution, and (4) the paper is imprecise and unpolished.\\nMain argument\\nThe paper does not do a great job of demonstrating that the problem it is trying to solve is a real thing. There is no experiment in this paper that clearly shows how this temporal generalization problem is different from the need to generalize well with function approximation. The paper points to references to establish the existence of the problem, but for example the Durugkar and Stone paper is a workshop paper and the conference version of that paper was rejected from ICLR 2018 and the reviewers highlighted serious issues with the paper\u2014that is not work to build upon. Further the paper under review here claims this problem is most pressing in the non-linear case, but the analysis in section 4.1 is for the linear case.\u00a0\\nThe resultant algorithm does not seem well justified, and has a different fixed point than TD, but there is no discussion of this other than section 4.4, which does not make clear statements about the correctness of the algorithm or what it converges to. Can you provide a proof or any kind of evidence that the proposed approach is sound, or how it\u2019s fixed point relates to TD?\\nThe experiments do not provide convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches. There are so many missing details it is difficult to draw many conclusions:\\n\\nWhat was the policy used in exp1 for policy evaluation in MC?\\nWhy Fourier basis features?\\nIn MC with DQN how did you adjust the parameters and architecture for the MC task?\\nWas the reward in MC and Acrobot -1 per step or something else\\nHow did you tune the parameters in the MC and Acrobot experiments?\\nWhy so few runs in MC, none of the results presented are significant?\\nWhy is the performance so bad in MC?\\nDid you evaluate online learning or do tests with the greedy policy?\\nHow did you initialize the value functions and weights?\\nWhy did you use experience replay for the linear experiments?\\nIN MC and Acrobot why only a one layer MLP?\\n\\nIgnoring all that, the results are not convincing. Most of the results in the paper are not statistically significant. The policy evaluation results in MC show little difference to regular TD. The Pong results show DQN is actually better. This makes the reader wonder if the result with DQN on MC and Acrobot are only worse because you did not properly tune DQN for those domains, whereas the default DQN architecture is well tuned for Atari and that is why you method is competitive in the smaller domains.\u00a0\\nThe differences in the \u201caverage change in value plots\u201d are very small if the rewards are -1 per step. Can you provide some context to understand the significance of this difference? In the last experiment linear FA and MC, the step-size is set equal for all methods\u2014this is not a valid comparison. Your method may just work better with alpha = 0.1.\u00a0\\n\u00a0\\n\\nThe paper has many imprecise parts, here are a few:\\n\\nThe definition of the value function would be approximate not equals unless you specify some properties of the function approximation architecture. Same for the Bellman equation\\nequation 1 of section 2.1 is neither an algorithm or a loss function\\nTD does not minimize the squared TD. Saying that is the objective function of TD learning in not true\\nend of section 2.1 says \u201cIt is computed as\u201d but the following equation just gives a form for the partial derivative\\nequation 2, x is not bounded\u00a0\\nYou state TC-loss has an unclear solution property, I don\u2019t know what that means and I don\u2019t think your approach is well justified either\\nSection 4.1 assumes linear FA, but its implied up until paragraph 2 that it has not assumed linear\\ntreatment of n_t in alg differs from appendix (t is no time episode number)\\nYour method has a n_t parameter that is adapted according to a schedule seemingly giving it an unfair advantage over DQN.\\nOver-claim not supported by the results: \u201cwe see that HR-TD is able to find a representation that is better at keeping the target value separate than TC is \u201c. The results do not show this.\\nSection 4.4 does not seem to go anywhere or produce and tangible conclusions\\n\\n\u00a0\\n\\nThings to improve the paper that did not impact the score:\\n\\nIt\u2019s hard to follow how the prox operator is used in the development of the alg, this could use some higher level explanation\\nIntro p2 is about bootstrapping, use that term and remove the equations\\nIt\u2019s not clear why you are talking about stochastic vs deterministic in P3\\nPerhaps you should compare against a MC method in the experiments to demonstrate the problem with TD methods and generalization\\nSection 2: \u201ccan often be a regularization term\u201d >> can or must be?\\nupdate law is an odd term\\n\u00a0tends to alleviate\u201d >> odd phrase\\nsection 4 should come before section 3\\nAlg 1 in not helpful because it just references an equation\\nsection 4.4 is very confusing, I cannot follow the logic of the statements\u00a0\\nQ learning >> Q-learning\\nNot sure what you mean with the last sentence of p2 section 5\\nwhere are the results for Acrobot linear function approximation\\nappendix Q-learning with linear FA is not DQN (table 2)\\n\\n\u00a0\\n\\nFAQ for Reviewers\\nQ. The review I just submitted is not appearing! Did it go thru?\\nA. There seems to be some delay in the system for you to see your reviews. Just give a bit of time and refresh again.\\n\u00a0\\nQ. What is the borderline paper meeting between AC and reviewers about?\\nA. This year we require ACs to meet with reviewers for all borderline papers (defined and announced by PCs once all reviews are received). There will be one meeting per borderline paper. Once we receive all reviews, we will let ACs know which papers fall into this bucket. ACs will then schedule and run the meetings with the reviewers of the papers in question.\\n\u00a0\\n\\nQ: What if I can not attend the meeting?\\nA: Please work with your AC to find a meeting schedule that works for everyone. Note that the discussion phase is roughly 1 month long, and overlaps with the NeurIPS conference, in case in-person meeting is preferred. All reviewers agreed to participate in review meetings when they accepted the reviewer invitations. An AC has right to\u00a0 downweight the review of reviewers who refuse to attend the meeting.\\n\u00a0\\n\\nQ: How should I use supplementary material?\\nA: It is not necessary to read supplementary material but such material can often answer questions that arise while reading the main paper, so consider looking there before asking authors.\\n\u00a0\\n\\nQ: How should I handle a policy violation?\\nA: To flag a CoE violation related to a submission, please indicate it when submitting the CoE report for that paper. The AC will work with the PC and the ethics board to resolve the case. To discuss other violations (e.g. plagiarism, double submission, paper length, formatting, etc.), please contact either the AC/SAC or the PC as appropriate. You can do this by sending a confidential comment with the appropriate readership restrictions.\\n\u00a0\\n\\nQ: Am I allowed to ask for additional experiments?\\nA: You can ask for additional experiments. New experiments should not significantly change the content of the submission. Rather, they should be limited in scope and serve to more thoroughly validate existing results from the submission.\\n\u00a0\\n\\nQ: If a submission does not achieve state-of-the-art results, is that grounds for rejection?\\nA: No, a lack of state-of-the-art results does not by itself constitute grounds for rejection. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge. Submissions can achieve this without achieving state-of-the-art results.\\nQ: Are authors expected to cite and compare with very recent work? What about non peer-reviewed (e.g., ArXiv) papers? (updated on 7 November 2022)\\nA: We consider papers contemporaneous if they are published (available in online proceedings) within the last four months. That means, since our full paper deadline is September 28, if a paper was published (i.e., at a peer-reviewed venue) on or after May 28, 2022, authors are not required to compare their own work to that paper. Authors are encouraged to cite and discuss all relevant papers, but they may be excused for not knowing about papers not published in peer-reviewed conference proceedings or journals, which includes papers exclusively available on arXiv. Reviewers are encouraged to use their own good judgement and, if in doubt, discuss with their area chair.\u00a0\\n\u00a0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuccessful Page Load\\n\\n\\n\\n\\n\\n\\n\\n                        ICLR uses cookies for essential functions only. We do not sell your personal\\n                        information.\\n                        Our Privacy Policy \u00bb\u00a0\\n\\n\\nAccept\\n                            Cookies\\n                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            The ICLR Logo above may be used on presentations. Right-click and choose\\n                            download. It is a vector graphic and may be used at any scale.\\n                        \\n\\n\\n\\n\\n\\n                            Useful links\\n                        \\n\\n\\nSponsor / Exhibitor Information\\nPress\\n\\n\\n\\n\\n\\n\\n\\nContact\\n\\n 2710 E Corridor Dr, Appleton WI 54913\\n                            \\n\\n Email\\n\\n Phone: +1-920-268-4789\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR Proceedings at OpenReview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"\n}\n```\n", "metadata": {"title": "Invoking `GetWebsite` with \n```json\n{\n  \"output_type\": \"full_text\",\n  \"url\": \"https://iclr.cc/Conferences/2023/ReviewerGuide\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetWebsite', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': 'Lv99  GpM tQ4  9Zil  b9G4My76g a jte88kc9N.m2dbOf N LXKsc3icLVSpGZ b  wc Jyh06IeU DbtNs69758 0ZrlQiLXbh0 RHHZFccU 9ZnGzu0HGxCBbENJUQUXnqvcR6FVVVu.vTYll5Z5jsJbS3.. IRQe.lxLJFn7 vFLNgDGuTV0ziV lEH Tu 3v QHWYYri  ja 5 OG6L6AtK 1gWSLy A Xb0mVTluuLo5INOhT  6WNrlB2TF X  dvJTEu.. TNJ6IeaFyn2 2T lI6FL2E h 40XhczwoCTO 04c 5FA xb hT BnAO8Sh sr896VAoah005S   L   dG3x0j j   K998P ..'}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetWebsite', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR 2022 Reviewer Instructions\\n\\n\\n\\n\\n\\nSkip to yearly menu bar\\nSkip to main content\\n\\nMain Navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            ICLR\\n        \\n\\n\\n\\n\\n                        Help/FAQ\\n                    \\n\\n\\n\\n\\n\\n\\n                        Contact ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n                        Downloads\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Blog\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n                        Privacy Policy\\n                    \\n\\n\\n\\n\\n\\n\\n                        Create Profile\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reset Password\\n                    \\n\\n\\n\\n\\n\\n\\n                        Journal To Conference Track\\n                    \\n\\n\\n\\n\\n\\n\\n                        Diversity & Inclusion\\n                    \\n\\n\\n\\n\\n\\n\\n                        Proceedings at OpenReview\\n                    \\n\\n\\n\\n\\n\\n\\n                        Future Meetings\\n                    \\n\\n\\n\\n\\n\\n\\n                        Press\\n                    \\n\\n\\n\\n\\n\\n\\n                        Exhibitor Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Twitter\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        My Stuff\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Login\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Select Year: (2023)\\n        \\n\\n\\n2026\\n                \\n\\n\\n\\n2025\\n                \\n\\n\\n\\n2024\\n                \\n\\n\\n\\n2023\\n                \\n\\n\\n\\n2022\\n                \\n\\n\\n\\n2021\\n                \\n\\n\\n\\n2020\\n                \\n\\n\\n\\n2019\\n                \\n\\n\\n\\n2018\\n                \\n\\n\\n\\n2017\\n                \\n\\n\\n\\n2016\\n                \\n\\n\\n\\n2015\\n                \\n\\n\\n\\n2014\\n                \\n\\n\\n\\n2013\\n                \\n\\n\\n\\n\\n\\n\\n                        Dates\\n                    \\n\\n\\n\\n\\n            Calls\\n        \\n\\n\\n\\n\\n                        Call for Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Tiny Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Workshops\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Blog Posts\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Socials\\n                    \\n\\n\\n\\n\\n\\n\\n            Guides\\n        \\n\\n\\n\\n\\n                        Author Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewer Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Area Chair Guide\\n                    \\n\\n\\n\\n\\n\\n\\n            Attend\\n        \\n\\n\\n\\n\\n                        Register\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visa and Travel  Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        Hotels\\n                    \\n\\n\\n\\n\\n\\n\\n                        Child Care\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visiting Kigali\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n            Organization\\n        \\n\\n\\n\\n\\n                        ICLR Board\\n                    \\n\\n\\n\\n\\n\\n\\n                        Organizing Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Program Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Ethics Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        Conference Site\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nICLR 2023 Reviewer Guide\\nThank you for agreeing to serve as an ICLR 2023 reviewer. Your contribution as a reviewer is paramount to creating an exciting and high-quality program. We ask that:\\n\\nYour reviews are timely and substantive.\\nYou follow the reviewing guidelines below.\\xa0\\nYou adhere to our Code of Ethics in your role as a reviewer. You must also adhere to our Code of Conduct.\\n\\nThis guide is intended to help you understand the ICLR 2023 decision process and your role within it. It contains:\\n\\nAn outline of the main reviewer tasks\\nStep-by-step reviewing instructions (especially relevant for reviewers that are new to ICLR)\\nReview examples\\nAn FAQ.\\n\\n\\xa0\\n\\n\\nWe\u2019re counting on you\\nAs a reviewer you are central to the program creation process for ICLR 2023. Your Area Chairs (ACs), Senior Area Chairs (SACs) and the Program Chairs (PCs) will rely greatly on your expertise and your diligent and thorough reviews to make decisions on each paper. Therefore, your role as a reviewer is critical to ensuring a strong program for ICLR 2023.\\nHigh-quality reviews are also very valuable for helping authors improve their work, whether it is eventually accepted by ICLR 2023, or not. Therefore it is important to treat each valid ICLR 2023 submission with equal care.\\nAs a token of our appreciation for your essential work, top reviewers will be acknowledged permanently on the ICLR 2023 website. Furthermore, top and high quality reviewers will receive special acknowledgement during the opening ceremony and free registration to ICLR 2023.\\n\\xa0\\n\\nMain reviewer tasks\\nThe main reviewer tasks and dates are as follows:\\n\\nCreate or update your OpenReview profile (September 30 2022)\\nBid on papers (September 30 2022 - October 06 2022)\\nWrite a constructive, thorough and timely review (October 12 2022 - October 24 2022)\\nInitial paper reviews released (November 4 2022)\\nDiscuss with authors and other reviewers to clarify and improve the paper (November 4 2022 - November 18 2022)\\nFlag any potential CoE violations and/or concerns (by November 18 2022)\\nProvide a recommendation to the area chair assigned to the paper (by November 18 2022)\\nVirtual Meeting with AC if paper you reviewed falls into borderline papers (November 18 2022 - December 12 2022)\\nProvide a final recommendation to the area chair assigned to the paper (after Virtual Meeting)\\n\\n\\xa0\\n\\n\\n\\xa0\\nCode of Ethics\\nAll ICLR participants, including reviewers, are required to adhere to the ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics). All reviewers are required to read the Code of Ethics and adhere to it. The Code of Ethics applies to all conference participation, including paper submission, reviewing, and paper discussion.\\xa0\\nAs part of the review process, reviewers are asked to raise potential violations of the ICLR Code of Ethics. Note that authors are encouraged to discuss questions and potential issues regarding the Code of Ethics as part of their submission. This discussion is not counted against the maximum page limit of the paper and should be included as a separate section.\\n\\xa0\\n\\n\\xa0\\nReviewing a submission: step-by-step\\nSummarized in one sentence, a review aims to determine whether a submission will bring sufficient value to the community and contribute new knowledge. The process can be broken down into the following main reviewer tasks:\\n\\xa0\\n\\n\\nRead the paper: It\u2019s important to carefully read through the entire paper, and to look up any related work and citations that will help you comprehensively evaluate it. Be sure to give yourself sufficient time for this step.\\nWhile reading, consider the following:\\n\\nObjective of the work: What is the goal of the paper? Is it to better address a known application or problem, draw attention to a new application or problem, or to introduce and/or explain a new theoretical finding? A combination of these? Different objectives will require different considerations as to potential value and impact.\\nStrong points: is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g. theoretically, algorithmically, etc.)?\\nWeak points: is it weak in any of the aspects listed in b.?\\nBe mindful of potential biases and try to be open-minded about the value and interest a paper can hold for the entire ICLR community, even if it may not be very interesting for you.\\n\\n\\nAnswer four key questions for yourself, to make a recommendation to Accept or Reject:\\n\\nWhat is the specific question and/or problem tackled by the paper?\\nIs the approach well motivated, including being well-placed in the literature?\\nDoes the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\\nWhat is the significance of the work? Does it contribute new knowledge and sufficient value to the community? Note, this does not necessarily require state-of-the-art results. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge (incl., empirical, theoretical, for practitioners, etc).\\n\\n\\nWrite and submit your initial review, organizing it as follows:\\xa0\\n\\nSummarize what the paper claims to contribute. Be positive and constructive.\\nList strong and weak points of the paper. Be as comprehensive as possible.\\nClearly state your initial recommendation (accept or reject) with one or two key reasons for this choice.\\nProvide supporting arguments for your recommendation.\\nAsk questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.\\xa0\\nProvide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\\n\\n\\nComplete the CoE report: ICLR has adopted the following Code of Ethics (CoE). When submitting your review, you\u2019ll be asked to complete a CoE report for the paper. The report is a simple form with two questions. The first asks whether there is a potential violation of the CoE. The second is relevant only if there is a potential violation and asks the reviewer to explain why there may be a potential violation. In order to answer these questions, it is therefore important that you read the CoE before starting your reviews. \\n\\t\\xa0\\nEngage in discussion: The discussion phase at ICLR is different from most conferences in the AI/ML community. During this phase, reviewers, authors and area chairs engage in asynchronous discussion and authors are allowed to revise their submissions to address concerns that arise. It is crucial that you are actively engaged during this phase. Maintain a spirit of openness to changing your initial recommendation (either to a more positive or more negative) rating.\\nBorderline paper meeting: This year, we are introducing a new program for ACs to (virtually) meet and discuss with reviewers only for borderline cases. ACs will reach out to schedule this meeting. This is to ensure active discussions among reviewers, and well-thought-out decisions. ACs will schedule the meeting and facilitate the discussion. For a productive discussion, it is important to familiarize yourself with other reviewers\\' feedback prior to the meeting. Please note that we will be leveraging information for reviewers who failed to attend this meeting (excluding emergencies).\\xa0\\nProvide final recommendation: Update your review, taking into account the new information collected during the discussion phase, and any revisions to the submission. State your reasoning and what did/didn\u2019t change your recommendation throughout the discussion phase.\\n\\n\\xa0\\n\\n\\xa0\\nFor great in-depth resources on reviewing, see these resources:\\n\\nDaniel Dennet, Criticising with Kindness.\\nComprehensive advice: Mistakes Reviewers Make\\nViews from multiple reviewers: Last minute reviewing advice\\nPerspective from instructions to Area Chairs: Dear ACs.\\n\\n\\xa0\\n\\n\\xa0\\nReview Examples\\nHere are two sample reviews from previous conferences that give an example of what we consider a good review for the case of leaning-to-accept and leaning-to-reject.\\nReview for a Paper where Leaning-to-Accept\\nThis paper proposes a method, Dual-AC, for optimizing the actor (policy) and critic (value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor. In order to achieve that, they take the linear programming approach of solving the Bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems. The discussion on the deficiencies of the naive LP approach is mostly well done. Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without losing convexity by applying a regularization. They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements. Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms.\\xa0\\nOverall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments. Given these clarifications in an author response, I would be willing to increase the score.\\xa0\\nFor the theory, there are a few steps that need clarification and further clarification on novelty. For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results. It looks like Theorem 2 has already been shown in \"Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time\u201d. There is a statement that \u201cChen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting. However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization\u201d. Is your Theorem 2 somehow an extension? Is Theorem 3 completely new?\\nThis is particularly called into question due to the lack of assumptions about the function class for value functions. It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function). This assumption seems to be used right at the bottom of Page 17, where U^{pi*} = V^*. Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small. More about conditions on eta_v would be illuminating.\\xa0\\nThere is also one step in the theorem that I cannot verify. On Page 18, how is the squared removed for difference between U and Upi? The transition from the second line of the proof to the third line is not clear. It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2.\\xa0\\n\\xa0\\n\\nFor the experiments, the following should be addressed.\\n1. It would have been better to also show the performance graphs with and without the improvements for multiple domains.\\n2. The central contribution is extending the single step LP to a multi-step formulation. It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains.\\n3. Increasing k also comes at a computational cost. I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO).\\n4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization. It was also mentioned that increasing the regularization parameter size increases the convergence rate. Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization? In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned. Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity? A bit more discussion on these choices would be helpful.\\xa0\\n\\xa0\\n\\nMinor comments:\\n1. Page 2: In equation 5, there should not be a \\'ds\\' in the dual variable constraint\\n\\xa0\\n\\n\\xa0\\nReview for a Paper where Leaning-to-Reject\\nThis paper introduces a variation on temporal difference learning for the function approximation case that attempts to resolve the issue of over-generalization across temporally-successive states. The new approach is applied to both linear and non-linear function approximation, and for prediction and control problems. The algorithmic contribution is demonstrated with a suite of experiments in classic benchmark control domains (Mountain Car and Acrobot), and in Pong.\\nThis paper should be rejected because (1) the algorithm is not well justified either by theory or practice, (2) the paper never clearly demonstrates the existence of problem they are trying to solve (nor differentiates it from the usual problem of generalizing well), (3) the experiments are difficult to understand, missing many details, and generally do not support a significant contribution, and (4) the paper is imprecise and unpolished.\\nMain argument\\nThe paper does not do a great job of demonstrating that the problem it is trying to solve is a real thing. There is no experiment in this paper that clearly shows how this temporal generalization problem is different from the need to generalize well with function approximation. The paper points to references to establish the existence of the problem, but for example the Durugkar and Stone paper is a workshop paper and the conference version of that paper was rejected from ICLR 2018 and the reviewers highlighted serious issues with the paper\u2014that is not work to build upon. Further the paper under review here claims this problem is most pressing in the non-linear case, but the analysis in section 4.1 is for the linear case.\\xa0\\nThe resultant algorithm does not seem well justified, and has a different fixed point than TD, but there is no discussion of this other than section 4.4, which does not make clear statements about the correctness of the algorithm or what it converges to. Can you provide a proof or any kind of evidence that the proposed approach is sound, or how it\u2019s fixed point relates to TD?\\nThe experiments do not provide convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches. There are so many missing details it is difficult to draw many conclusions:\\n\\nWhat was the policy used in exp1 for policy evaluation in MC?\\nWhy Fourier basis features?\\nIn MC with DQN how did you adjust the parameters and architecture for the MC task?\\nWas the reward in MC and Acrobot -1 per step or something else\\nHow did you tune the parameters in the MC and Acrobot experiments?\\nWhy so few runs in MC, none of the results presented are significant?\\nWhy is the performance so bad in MC?\\nDid you evaluate online learning or do tests with the greedy policy?\\nHow did you initialize the value functions and weights?\\nWhy did you use experience replay for the linear experiments?\\nIN MC and Acrobot why only a one layer MLP?\\n\\nIgnoring all that, the results are not convincing. Most of the results in the paper are not statistically significant. The policy evaluation results in MC show little difference to regular TD. The Pong results show DQN is actually better. This makes the reader wonder if the result with DQN on MC and Acrobot are only worse because you did not properly tune DQN for those domains, whereas the default DQN architecture is well tuned for Atari and that is why you method is competitive in the smaller domains.\\xa0\\nThe differences in the \u201caverage change in value plots\u201d are very small if the rewards are -1 per step. Can you provide some context to understand the significance of this difference? In the last experiment linear FA and MC, the step-size is set equal for all methods\u2014this is not a valid comparison. Your method may just work better with alpha = 0.1.\\xa0\\n\\xa0\\n\\nThe paper has many imprecise parts, here are a few:\\n\\nThe definition of the value function would be approximate not equals unless you specify some properties of the function approximation architecture. Same for the Bellman equation\\nequation 1 of section 2.1 is neither an algorithm or a loss function\\nTD does not minimize the squared TD. Saying that is the objective function of TD learning in not true\\nend of section 2.1 says \u201cIt is computed as\u201d but the following equation just gives a form for the partial derivative\\nequation 2, x is not bounded\\xa0\\nYou state TC-loss has an unclear solution property, I don\u2019t know what that means and I don\u2019t think your approach is well justified either\\nSection 4.1 assumes linear FA, but its implied up until paragraph 2 that it has not assumed linear\\ntreatment of n_t in alg differs from appendix (t is no time episode number)\\nYour method has a n_t parameter that is adapted according to a schedule seemingly giving it an unfair advantage over DQN.\\nOver-claim not supported by the results: \u201cwe see that HR-TD is able to find a representation that is better at keeping the target value separate than TC is \u201c. The results do not show this.\\nSection 4.4 does not seem to go anywhere or produce and tangible conclusions\\n\\n\\xa0\\n\\nThings to improve the paper that did not impact the score:\\n\\nIt\u2019s hard to follow how the prox operator is used in the development of the alg, this could use some higher level explanation\\nIntro p2 is about bootstrapping, use that term and remove the equations\\nIt\u2019s not clear why you are talking about stochastic vs deterministic in P3\\nPerhaps you should compare against a MC method in the experiments to demonstrate the problem with TD methods and generalization\\nSection 2: \u201ccan often be a regularization term\u201d >> can or must be?\\nupdate law is an odd term\\n\\xa0tends to alleviate\u201d >> odd phrase\\nsection 4 should come before section 3\\nAlg 1 in not helpful because it just references an equation\\nsection 4.4 is very confusing, I cannot follow the logic of the statements\\xa0\\nQ learning >> Q-learning\\nNot sure what you mean with the last sentence of p2 section 5\\nwhere are the results for Acrobot linear function approximation\\nappendix Q-learning with linear FA is not DQN (table 2)\\n\\n\\xa0\\n\\nFAQ for Reviewers\\nQ. The review I just submitted is not appearing! Did it go thru?\\nA. There seems to be some delay in the system for you to see your reviews. Just give a bit of time and refresh again.\\n\\xa0\\nQ. What is the borderline paper meeting between AC and reviewers about?\\nA. This year we require ACs to meet with reviewers for all borderline papers (defined and announced by PCs once all reviews are received). There will be one meeting per borderline paper. Once we receive all reviews, we will let ACs know which papers fall into this bucket. ACs will then schedule and run the meetings with the reviewers of the papers in question.\\n\\xa0\\n\\nQ: What if I can not attend the meeting?\\nA: Please work with your AC to find a meeting schedule that works for everyone. Note that the discussion phase is roughly 1 month long, and overlaps with the NeurIPS conference, in case in-person meeting is preferred. All reviewers agreed to participate in review meetings when they accepted the reviewer invitations. An AC has right to\\xa0 downweight the review of reviewers who refuse to attend the meeting.\\n\\xa0\\n\\nQ: How should I use supplementary material?\\nA: It is not necessary to read supplementary material but such material can often answer questions that arise while reading the main paper, so consider looking there before asking authors.\\n\\xa0\\n\\nQ: How should I handle a policy violation?\\nA: To flag a CoE violation related to a submission, please indicate it when submitting the CoE report for that paper. The AC will work with the PC and the ethics board to resolve the case. To discuss other violations (e.g. plagiarism, double submission, paper length, formatting, etc.), please contact either the AC/SAC or the PC as appropriate. You can do this by sending a confidential comment with the appropriate readership restrictions.\\n\\xa0\\n\\nQ: Am I allowed to ask for additional experiments?\\nA: You can ask for additional experiments. New experiments should not significantly change the content of the submission. Rather, they should be limited in scope and serve to more thoroughly validate existing results from the submission.\\n\\xa0\\n\\nQ: If a submission does not achieve state-of-the-art results, is that grounds for rejection?\\nA: No, a lack of state-of-the-art results does not by itself constitute grounds for rejection. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge. Submissions can achieve this without achieving state-of-the-art results.\\nQ: Are authors expected to cite and compare with very recent work? What about non peer-reviewed (e.g., ArXiv) papers? (updated on 7 November 2022)\\nA: We consider papers contemporaneous if they are published (available in online proceedings) within the last four months. That means, since our full paper deadline is September 28, if a paper was published (i.e., at a peer-reviewed venue) on or after May 28, 2022, authors are not required to compare their own work to that paper. Authors are encouraged to cite and discuss all relevant papers, but they may be excused for not knowing about papers not published in peer-reviewed conference proceedings or journals, which includes papers exclusively available on arXiv. Reviewers are encouraged to use their own good judgement and, if in doubt, discuss with their area chair.\\xa0\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuccessful Page Load\\n\\n\\n\\n\\n\\n\\n\\n                        ICLR uses cookies for essential functions only. We do not sell your personal\\n                        information.\\n                        Our Privacy Policy \u00bb\\xa0\\n\\n\\nAccept\\n                            Cookies\\n                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            The ICLR Logo above may be used on presentations. Right-click and choose\\n                            download. It is a vector graphic and may be used at any scale.\\n                        \\n\\n\\n\\n\\n\\n                            Useful links\\n                        \\n\\n\\nSponsor / Exhibitor Information\\nPress\\n\\n\\n\\n\\n\\n\\n\\nContact\\n\\n 2710 E Corridor Dr, Appleton WI 54913\\n                            \\n\\n Email\\n\\n Phone: +1-920-268-4789\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR Proceedings at OpenReview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Based on the ICLR 2023 Reviewer Guide and the provided paper, here's a review plan:\n\n**1. Paper Summary:**\n\nThe paper investigates the spectral bias of Physics-Informed Neural Networks (PINNs) from an optimization perspective. It theoretically proves that SGD with momentum (SGDM) and Adam optimizers can reduce the effect of spectral bias in infinitely-wide PINNs, outperforming vanilla SGD. The theoretical findings are supported by numerical experiments on benchmark PDEs.\n\n**2. Key Aspects to Evaluate:**\n\n*   **Novelty:** The paper claims to be the first to analyze the gradient flow of PINNs under SGDM and Adam and relate it to solving spectral bias. Assess the validity of this claim using `ArxivTool` and `SemanticScholarTool`.\n*   **Theoretical Soundness:** Evaluate the correctness and rigor of the theoretical proofs, especially Theorem 1 regarding the gradient flow of SGDM.\n*   **Experimental Validation:** Assess the experimental setup, results, and comparisons with existing methods (C-learning and AW). Check if the experiments sufficiently support the theoretical claims.\n*   **Significance:** Determine the potential impact of the work on the PINN research community. Does it provide valuable insights or practical improvements?\n*   **Clarity:** Assess the clarity and readability of the paper, including the presentation of theoretical concepts and experimental details.\n\n**3. Action Plan:**\n\n1.  **Novelty Check:** Use `ArxivTool` and `SemanticScholarTool` to search for related work on the optimization of PINNs, focusing on the use of SGDM and Adam.\n2.  **Reviewer Agent Creation:** Create three reviewer agents with the following specializations:\n    *   **PINN Optimization Expert:** Focus on the theoretical analysis of SGDM and Adam in PINNs.\n    *   **Numerical PDE Solver Specialist:** Focus on the experimental validation and comparison with traditional PDE solvers.\n    *   **Neural Network Training Dynamics Analyst:** Focus on the spectral bias phenomenon and the impact of different optimizers on training dynamics.\n3.  **Agent Execution:** Assign specific tasks to each agent based on their specialization.\n4.  **Review Synthesis:** Combine the agents' feedback into a comprehensive review with a clear recommendation.\n\nNow, let's start with the novelty check.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'max_results': 5, 'query': 'physics-informed neural networks momentum optimization'}, name='ArxivTool'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'limit': 5, 'query': 'physics-informed neural networks momentum optimization'}, name='SemanticScholarTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Found 5 paper(s) on arXiv\",\n  \"output\": [\n    {\n      \"title\": \"Last-iterate convergence analysis of stochastic momentum methods for neural networks\",\n      \"authors\": [\n        \"Dongpo Xu\",\n        \"Jinlan Liu\",\n        \"Yinghua Lu\",\n        \"Jun Kong\",\n        \"Danilo Mandic\"\n      ],\n      \"published\": \"2022-05-30T02:17:44+00:00\",\n      \"summary\": \"The stochastic momentum method is a commonly used acceleration technique for\\nsolving large-scale stochastic optimization problems in artificial neural\\nnetworks. Current convergence results of stochastic momentum methods under\\nnon-convex stochastic settings mostly discuss convergence in terms of the\\nrandom output and minimum output. To this end, we address the convergence of\\nthe last iterate output (called last-iterate convergence) of the stochastic\\nmomentum methods for non-convex stochastic optimization problems, in a way\\nconformal with traditional optimization theory. We prove the last-iterate\\nconvergence of the stochastic momentum methods under a unified framework,\\ncovering both stochastic heavy ball momentum and stochastic Nesterov\\naccelerated gradient momentum. The momentum factors can be fixed to be\\nconstant, rather than time-varying coefficients in existing analyses. Finally,\\nthe last-iterate convergence of the stochastic momentum methods is verified on\\nthe benchmark MNIST and CIFAR-10 datasets.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2205.14811v1\"\n    },\n    {\n      \"title\": \"Training Deep Neural Networks with Adaptive Momentum Inspired by the Quadratic Optimization\",\n      \"authors\": [\n        \"Tao Sun\",\n        \"Huaming Ling\",\n        \"Zuoqiang Shi\",\n        \"Dongsheng Li\",\n        \"Bao Wang\"\n      ],\n      \"published\": \"2021-10-18T07:03:48+00:00\",\n      \"summary\": \"Heavy ball momentum is crucial in accelerating (stochastic) gradient-based\\noptimization algorithms for machine learning. Existing heavy ball momentum is\\nusually weighted by a uniform hyperparameter, which relies on excessive tuning.\\nMoreover, the calibrated fixed hyperparameter may not lead to optimal\\nperformance. In this paper, to eliminate the effort for tuning the\\nmomentum-related hyperparameter, we propose a new adaptive momentum inspired by\\nthe optimal choice of the heavy ball momentum for quadratic optimization. Our\\nproposed adaptive heavy ball momentum can improve stochastic gradient descent\\n(SGD) and Adam. SGD and Adam with the newly designed adaptive momentum are more\\nrobust to large learning rates, converge faster, and generalize better than the\\nbaselines. We verify the efficiency of SGD and Adam with the new adaptive\\nmomentum on extensive machine learning benchmarks, including image\\nclassification, language modeling, and machine translation. Finally, we provide\\nconvergence guarantees for SGD and Adam with the proposed adaptive momentum.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2110.09057v1\"\n    },\n    {\n      \"title\": \"A Convergence Analysis of Nesterov's Accelerated Gradient Method in Training Deep Linear Neural Networks\",\n      \"authors\": [\n        \"Xin Liu\",\n        \"Wei Tao\",\n        \"Zhisong Pan\"\n      ],\n      \"published\": \"2022-04-18T13:24:12+00:00\",\n      \"summary\": \"Momentum methods, including heavy-ball~(HB) and Nesterov's accelerated\\ngradient~(NAG), are widely used in training neural networks for their fast\\nconvergence. However, there is a lack of theoretical guarantees for their\\nconvergence and acceleration since the optimization landscape of the neural\\nnetwork is non-convex. Nowadays, some works make progress towards understanding\\nthe convergence of momentum methods in an over-parameterized regime, where the\\nnumber of the parameters exceeds that of the training instances. Nonetheless,\\ncurrent results mainly focus on the two-layer neural network, which are far\\nfrom explaining the remarkable success of the momentum methods in training deep\\nneural networks. Motivated by this, we investigate the convergence of NAG with\\nconstant learning rate and momentum parameter in training two architectures of\\ndeep linear networks: deep fully-connected linear neural networks and deep\\nlinear ResNets. Based on the over-parameterization regime, we first analyze the\\nresidual dynamics induced by the training trajectory of NAG for a deep\\nfully-connected linear neural network under the random Gaussian initialization.\\nOur results show that NAG can converge to the global minimum at a $(1 -\\n\\\\mathcal{O}(1/\\\\sqrt{\\\\kappa}))^t$ rate, where $t$ is the iteration number and\\n$\\\\kappa > 1$ is a constant depending on the condition number of the feature\\nmatrix. Compared to the $(1 - \\\\mathcal{O}(1/{\\\\kappa}))^t$ rate of GD, NAG\\nachieves an acceleration over GD. To the best of our knowledge, this is the\\nfirst theoretical guarantee for the convergence of NAG to the global minimum in\\ntraining deep neural networks. Furthermore, we extend our analysis to deep\\nlinear ResNets and derive a similar convergence result.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2204.08306v1\"\n    },\n    {\n      \"title\": \"Momentum Residual Neural Networks\",\n      \"authors\": [\n        \"Michael E. Sander\",\n        \"Pierre Ablin\",\n        \"Mathieu Blondel\",\n        \"Gabriel Peyr\u00e9\"\n      ],\n      \"published\": \"2021-02-15T22:24:52+00:00\",\n      \"summary\": \"The training of deep residual neural networks (ResNets) with backpropagation\\nhas a memory cost that increases linearly with respect to the depth of the\\nnetwork. A way to circumvent this issue is to use reversible architectures. In\\nthis paper, we propose to change the forward rule of a ResNet by adding a\\nmomentum term. The resulting networks, momentum residual neural networks\\n(Momentum ResNets), are invertible. Unlike previous invertible architectures,\\nthey can be used as a drop-in replacement for any existing ResNet block. We\\nshow that Momentum ResNets can be interpreted in the infinitesimal step size\\nregime as second-order ordinary differential equations (ODEs) and exactly\\ncharacterize how adding momentum progressively increases the representation\\ncapabilities of Momentum ResNets. Our analysis reveals that Momentum ResNets\\ncan learn any linear mapping up to a multiplicative factor, while ResNets\\ncannot. In a learning to optimize setting, where convergence to a fixed point\\nis required, we show theoretically and empirically that our method succeeds\\nwhile existing invertible architectures fail. We show on CIFAR and ImageNet\\nthat Momentum ResNets have the same accuracy as ResNets, while having a much\\nsmaller memory footprint, and show that pre-trained Momentum ResNets are\\npromising for fine-tuning models.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2102.07870v3\"\n    },\n    {\n      \"title\": \"On Optimizing Deep Convolutional Neural Networks by Evolutionary Computing\",\n      \"authors\": [\n        \"M. U. B. Dias\",\n        \"D. D. N. De Silva\",\n        \"S. Fernando\"\n      ],\n      \"published\": \"2018-08-06T08:29:26+00:00\",\n      \"summary\": \"Optimization for deep networks is currently a very active area of research.\\nAs neural networks become deeper, the ability in manually optimizing the\\nnetwork becomes harder. Mini-batch normalization, identification of effective\\nrespective fields, momentum updates, introduction of residual blocks, learning\\nrate adoption, etc. have been proposed to speed up the rate of convergent in\\nmanual training process while keeping the higher accuracy level. However, the\\nproblem of finding optimal topological structure for a given problem is\\nbecoming a challenging task need to be addressed immediately. Few researchers\\nhave attempted to optimize the network structure using evolutionary computing\\napproaches. Among them, few have successfully evolved networks with\\nreinforcement learning and long-short-term memory. A very few has applied\\nevolutionary programming into deep convolution neural networks. These attempts\\nare mainly evolved the network structure and then subsequently optimized the\\nhyper-parameters of the network. However, a mechanism to evolve the deep\\nnetwork structure under the techniques currently being practiced in manual\\nprocess is still absent. Incorporation of such techniques into chromosomes\\nlevel of evolutionary computing, certainly can take us to better topological\\ndeep structures. The paper concludes by identifying the gap between\\nevolutionary based deep neural networks and deep neural networks. Further, it\\nproposes some insights for optimizing deep neural networks using evolutionary\\ncomputing techniques.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/1808.01766v1\"\n    }\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ArxivTool` with \n```json\n{\n  \"max_results\": 5,\n  \"query\": \"physics-informed neural networks momentum optimization\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Found 5 paper(s) on Semantic Scholar\",\n  \"output\": [\n    {\n      \"title\": \"ConFIG: Towards Conflict-free Training of Physics Informed Neural Networks\",\n      \"authors\": [\n        \"Qiang Liu\",\n        \"Mengyu Chu\",\n        \"Nils Thuerey\"\n      ],\n      \"abstract\": \"The loss functions of many learning problems contain multiple additive terms that can disagree and yield conflicting update directions. For Physics-Informed Neural Networks (PINNs), loss terms on initial/boundary conditions and physics equations are particularly interesting as they are well-established as highly difficult tasks. To improve learning the challenging multi-objective task posed by PINNs, we propose the ConFIG method, which provides conflict-free updates by ensuring a positive dot product between the final update and each loss-specific gradient. It also maintains consistent optimization rates for all loss terms and dynamically adjusts gradient magnitudes based on conflict levels. We additionally leverage momentum to accelerate optimizations by alternating the back-propagation of different loss terms. We provide a mathematical proof showing the convergence of the ConFIG method, and it is evaluated across a range of challenging PINN scenarios. ConFIG consistently shows superior performance and runtime compared to baseline methods. We also test the proposed method in a classic multi-task benchmark, where the ConFIG method likewise exhibits a highly promising performance. Source code is available at https://tum-pbs.github.io/ConFIG\",\n      \"url\": \"https://www.semanticscholar.org/paper/0b5d39f20e0df8cf358e6d5084f86f9bb49b3449\"\n    },\n    {\n      \"title\": \"Optimizing the optimizer for data driven deep neural networks and physics informed neural networks\",\n      \"authors\": [\n        \"John Taylor\",\n        \"Wenyi Wang\",\n        \"Biswajit Bala\",\n        \"T. Bednarz\"\n      ],\n      \"abstract\": \"We investigate the role of the optimizer in determining the quality of the model fit for neural networks with a small to medium number of parameters. We study the performance of Adam, an algorithm for first-order gradient-based optimization that uses adaptive momentum, the Levenberg and Marquardt (LM) algorithm a second order method, Broyden,Fletcher,Goldfarb and Shanno algorithm (BFGS) a second order method and LBFGS, a low memory version of BFGS. Using these optimizers we fit the function y = sinc(10x) using a neural network with a few parameters. This function has a variable amplitude and a constant frequency. We observe that the higher amplitude components of the function are fitted first and the Adam, BFGS and LBFGS struggle to fit the lower amplitude components of the function. We also solve the Burgers equation using a physics informed neural network(PINN) with the BFGS and LM optimizers. For our example problems with a small to medium number of weights, we find that the LM algorithm is able to rapidly converge to machine precision offering significant benefits over other optimizers. We further investigated the Adam optimizer with a range of models and found that Adam optimiser requires much deeper models with large numbers of hidden units containing up to 26x more parameters, in order to achieve a model fit close that achieved by the LM optimizer. The LM optimizer results illustrate that it may be possible build models with far fewer parameters. We have implemented all our methods in Keras and TensorFlow 2.\",\n      \"url\": \"https://www.semanticscholar.org/paper/f9bdca36f936a226c3f47aa9422b998b6f8df1e5\"\n    },\n    {\n      \"title\": \"Solving Poisson Equation by Physics-Informed Neural Network with Natural Gradient Descent with Momentum\",\n      \"authors\": [\n        \"R. Abdulkadirov\",\n        \"P. Lyakhov\",\n        \"Nikolay N. Nagornov\"\n      ],\n      \"abstract\": \"In this paper, we present a solution of the first boundary value problem of the Poisson equation using physics-informed neural networks, which optimize the loss function by natural gradient descent with Dirichlet distribution. This algorithm proved his ability in optimization of multidimensional functions with high accuracy, taking little number of iterations and avoiding local minimums. This fact means, that proposed algorithm will minimize loss function as well. Also, we obtain the exact and numerical solutions, implied due to separation of variable (Fourier method) and finite element method, respectively. The main goal of this paper is to rise the accuracy of solution of the Poisson equation in a short time. We will demonstrate the error estimate of solutions received by finite element method and physics-informed neural network, equipped with adaptive moment estimates and natural gradient descent with momentum and Nesterov condition.\",\n      \"url\": \"https://www.semanticscholar.org/paper/566c1f44e2b644a33eb098277852d1b98a95a4cc\"\n    },\n    {\n      \"title\": \"EquiNO: A Physics-Informed Neural Operator for Multiscale Simulations\",\n      \"authors\": [\n        \"Hamidreza Eivazi\",\n        \"Jendrik\u2010Alexander Tr\u00f6ger\",\n        \"Stefan H. A. Wittek\",\n        \"S. Hartmann\",\n        \"Andreas Rausch\"\n      ],\n      \"abstract\": \"Multiscale problems are ubiquitous in physics. Numerical simulations of such problems by solving partial differential equations (PDEs) at high resolution are computationally too expensive for many-query scenarios, e.g., uncertainty quantification, remeshing applications, topology optimization, and so forth. This limitation has motivated the application of data-driven surrogate models, where the microscale computations are $\\\\textit{substituted}$ with a surrogate, usually acting as a black-box mapping between macroscale quantities. These models offer significant speedups but struggle with incorporating microscale physical constraints, such as the balance of linear momentum and constitutive models. In this contribution, we propose Equilibrium Neural Operator (EquiNO) as a $\\\\textit{complementary}$ physics-informed PDE surrogate for predicting microscale physics and compare it with variational physics-informed neural and operator networks. Our framework, applicable to the so-called multiscale FE$^{\\\\,2}\\\\,$ computations, introduces the FE-OL approach by integrating the finite element (FE) method with operator learning (OL). We apply the proposed FE-OL approach to quasi-static problems of solid mechanics. The results demonstrate that FE-OL can yield accurate solutions even when confronted with a restricted dataset during model development. Our results show that EquiNO achieves speedup factors exceeding 8000-fold compared to traditional methods and offers an optimal balance between data-driven and physics-based strategies.\",\n      \"url\": \"https://www.semanticscholar.org/paper/42124d3e46b4bd798686a2185c91968175230353\"\n    },\n    {\n      \"title\": \"Loss-based optimizer switching to solve 1-D Helmholtz equation using neural networks\",\n      \"authors\": [\n        \"Veerababu Dharanalakota\",\n        \"Pavan K. J\",\n        \"Prasanta K. Ghosh\"\n      ],\n      \"abstract\": \"In this paper, 1-D Helmholtz equation is solved using physics-informed neural networks. In general, either the L-BFGS or ADAM algorithm is used to perform the optimization procedure. Unlike ADAM, L-BFGS is structured to reduce the loss function at each iteration. However, it becomes stagnant and fails to reach the global minimum at higher frequencies due to lack of momentum in the direction of the global minimum. On the other hand, ADAM has the advantage of added momentum. However, it requires a manual tuning of hyperparameters at each frequency to converge to the global minimum. Hence, one optimizer alone is inefficient to predict acoustic field at higher frequencies. This work proposes an algorithm called loss-based optimizer switching (LOS). This approach intelligently switches between L-BFGS and ADAM based on the specific criteria on the loss value to leverage the strengths of both optimizers. The performance of the LOS is evaluated by comparing the relative error between the predicted solution and the exact solution up to 3500 Hz. At 500 Hz, the relative errors with all three algorithms lie below 1.5\u2009\u00d7\u200910\u22123. However, at 3500 Hz, the relative errors are observed to be 6.6\u2009\u00d7\u200910\u22123, 0.92, and 0.28 with LOS, ADAM and L-BFGS, respectively.\",\n      \"url\": \"https://www.semanticscholar.org/paper/58bda546c8fe6313d04d49c34186da6d46c06969\"\n    }\n  ]\n}\n```\n", "metadata": {"title": "Invoking `SemanticScholarTool` with \n```json\n{\n  \"limit\": 5,\n  \"query\": \"physics-informed neural networks momentum optimization\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ArxivTool', response={'result': {'status': 'success', 'message': 'Found 5 paper(s) on arXiv', 'output': [{'title': 'Last-iterate convergence analysis of stochastic momentum methods for neural networks', 'authors': ['Dongpo Xu', 'Jinlan Liu', 'Yinghua Lu', 'Jun Kong', 'Danilo Mandic'], 'published': '2022-05-30T02:17:44+00:00', 'summary': 'The stochastic momentum method is a commonly used acceleration technique for\\nsolving large-scale stochastic optimization problems in artificial neural\\nnetworks. Current convergence results of stochastic momentum methods under\\nnon-convex stochastic settings mostly discuss convergence in terms of the\\nrandom output and minimum output. To this end, we address the convergence of\\nthe last iterate output (called last-iterate convergence) of the stochastic\\nmomentum methods for non-convex stochastic optimization problems, in a way\\nconformal with traditional optimization theory. We prove the last-iterate\\nconvergence of the stochastic momentum methods under a unified framework,\\ncovering both stochastic heavy ball momentum and stochastic Nesterov\\naccelerated gradient momentum. The momentum factors can be fixed to be\\nconstant, rather than time-varying coefficients in existing analyses. Finally,\\nthe last-iterate convergence of the stochastic momentum methods is verified on\\nthe benchmark MNIST and CIFAR-10 datasets.', 'pdf_url': 'http://arxiv.org/pdf/2205.14811v1'}, {'title': 'Training Deep Neural Networks with Adaptive Momentum Inspired by the Quadratic Optimization', 'authors': ['Tao Sun', 'Huaming Ling', 'Zuoqiang Shi', 'Dongsheng Li', 'Bao Wang'], 'published': '2021-10-18T07:03:48+00:00', 'summary': 'Heavy ball momentum is crucial in accelerating (stochastic) gradient-based\\noptimization algorithms for machine learning. Existing heavy ball momentum is\\nusually weighted by a uniform hyperparameter, which relies on excessive tuning.\\nMoreover, the calibrated fixed hyperparameter may not lead to optimal\\nperformance. In this paper, to eliminate the effort for tuning the\\nmomentum-related hyperparameter, we propose a new adaptive momentum inspired by\\nthe optimal choice of the heavy ball momentum for quadratic optimization. Our\\nproposed adaptive heavy ball momentum can improve stochastic gradient descent\\n(SGD) and Adam. SGD and Adam with the newly designed adaptive momentum are more\\nrobust to large learning rates, converge faster, and generalize better than the\\nbaselines. We verify the efficiency of SGD and Adam with the new adaptive\\nmomentum on extensive machine learning benchmarks, including image\\nclassification, language modeling, and machine translation. Finally, we provide\\nconvergence guarantees for SGD and Adam with the proposed adaptive momentum.', 'pdf_url': 'http://arxiv.org/pdf/2110.09057v1'}, {'title': \"A Convergence Analysis of Nesterov's Accelerated Gradient Method in Training Deep Linear Neural Networks\", 'authors': ['Xin Liu', 'Wei Tao', 'Zhisong Pan'], 'published': '2022-04-18T13:24:12+00:00', 'summary': \"Momentum methods, including heavy-ball~(HB) and Nesterov's accelerated\\ngradient~(NAG), are widely used in training neural networks for their fast\\nconvergence. However, there is a lack of theoretical guarantees for their\\nconvergence and acceleration since the optimization landscape of the neural\\nnetwork is non-convex. Nowadays, some works make progress towards understanding\\nthe convergence of momentum methods in an over-parameterized regime, where the\\nnumber of the parameters exceeds that of the training instances. Nonetheless,\\ncurrent results mainly focus on the two-layer neural network, which are far\\nfrom explaining the remarkable success of the momentum methods in training deep\\nneural networks. Motivated by this, we investigate the convergence of NAG with\\nconstant learning rate and momentum parameter in training two architectures of\\ndeep linear networks: deep fully-connected linear neural networks and deep\\nlinear ResNets. Based on the over-parameterization regime, we first analyze the\\nresidual dynamics induced by the training trajectory of NAG for a deep\\nfully-connected linear neural network under the random Gaussian initialization.\\nOur results show that NAG can converge to the global minimum at a $(1 -\\n\\\\mathcal{O}(1/\\\\sqrt{\\\\kappa}))^t$ rate, where $t$ is the iteration number and\\n$\\\\kappa > 1$ is a constant depending on the condition number of the feature\\nmatrix. Compared to the $(1 - \\\\mathcal{O}(1/{\\\\kappa}))^t$ rate of GD, NAG\\nachieves an acceleration over GD. To the best of our knowledge, this is the\\nfirst theoretical guarantee for the convergence of NAG to the global minimum in\\ntraining deep neural networks. Furthermore, we extend our analysis to deep\\nlinear ResNets and derive a similar convergence result.\", 'pdf_url': 'http://arxiv.org/pdf/2204.08306v1'}, {'title': 'Momentum Residual Neural Networks', 'authors': ['Michael E. Sander', 'Pierre Ablin', 'Mathieu Blondel', 'Gabriel Peyr\u00e9'], 'published': '2021-02-15T22:24:52+00:00', 'summary': 'The training of deep residual neural networks (ResNets) with backpropagation\\nhas a memory cost that increases linearly with respect to the depth of the\\nnetwork. A way to circumvent this issue is to use reversible architectures. In\\nthis paper, we propose to change the forward rule of a ResNet by adding a\\nmomentum term. The resulting networks, momentum residual neural networks\\n(Momentum ResNets), are invertible. Unlike previous invertible architectures,\\nthey can be used as a drop-in replacement for any existing ResNet block. We\\nshow that Momentum ResNets can be interpreted in the infinitesimal step size\\nregime as second-order ordinary differential equations (ODEs) and exactly\\ncharacterize how adding momentum progressively increases the representation\\ncapabilities of Momentum ResNets. Our analysis reveals that Momentum ResNets\\ncan learn any linear mapping up to a multiplicative factor, while ResNets\\ncannot. In a learning to optimize setting, where convergence to a fixed point\\nis required, we show theoretically and empirically that our method succeeds\\nwhile existing invertible architectures fail. We show on CIFAR and ImageNet\\nthat Momentum ResNets have the same accuracy as ResNets, while having a much\\nsmaller memory footprint, and show that pre-trained Momentum ResNets are\\npromising for fine-tuning models.', 'pdf_url': 'http://arxiv.org/pdf/2102.07870v3'}, {'title': 'On Optimizing Deep Convolutional Neural Networks by Evolutionary Computing', 'authors': ['M. U. B. Dias', 'D. D. N. De Silva', 'S. Fernando'], 'published': '2018-08-06T08:29:26+00:00', 'summary': 'Optimization for deep networks is currently a very active area of research.\\nAs neural networks become deeper, the ability in manually optimizing the\\nnetwork becomes harder. Mini-batch normalization, identification of effective\\nrespective fields, momentum updates, introduction of residual blocks, learning\\nrate adoption, etc. have been proposed to speed up the rate of convergent in\\nmanual training process while keeping the higher accuracy level. However, the\\nproblem of finding optimal topological structure for a given problem is\\nbecoming a challenging task need to be addressed immediately. Few researchers\\nhave attempted to optimize the network structure using evolutionary computing\\napproaches. Among them, few have successfully evolved networks with\\nreinforcement learning and long-short-term memory. A very few has applied\\nevolutionary programming into deep convolution neural networks. These attempts\\nare mainly evolved the network structure and then subsequently optimized the\\nhyper-parameters of the network. However, a mechanism to evolve the deep\\nnetwork structure under the techniques currently being practiced in manual\\nprocess is still absent. Incorporation of such techniques into chromosomes\\nlevel of evolutionary computing, certainly can take us to better topological\\ndeep structures. The paper concludes by identifying the gap between\\nevolutionary based deep neural networks and deep neural networks. Further, it\\nproposes some insights for optimizing deep neural networks using evolutionary\\ncomputing techniques.', 'pdf_url': 'http://arxiv.org/pdf/1808.01766v1'}]}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='SemanticScholarTool', response={'result': {'status': 'success', 'message': 'Found 5 paper(s) on Semantic Scholar', 'output': [{'title': 'ConFIG: Towards Conflict-free Training of Physics Informed Neural Networks', 'authors': ['Qiang Liu', 'Mengyu Chu', 'Nils Thuerey'], 'abstract': 'The loss functions of many learning problems contain multiple additive terms that can disagree and yield conflicting update directions. For Physics-Informed Neural Networks (PINNs), loss terms on initial/boundary conditions and physics equations are particularly interesting as they are well-established as highly difficult tasks. To improve learning the challenging multi-objective task posed by PINNs, we propose the ConFIG method, which provides conflict-free updates by ensuring a positive dot product between the final update and each loss-specific gradient. It also maintains consistent optimization rates for all loss terms and dynamically adjusts gradient magnitudes based on conflict levels. We additionally leverage momentum to accelerate optimizations by alternating the back-propagation of different loss terms. We provide a mathematical proof showing the convergence of the ConFIG method, and it is evaluated across a range of challenging PINN scenarios. ConFIG consistently shows superior performance and runtime compared to baseline methods. We also test the proposed method in a classic multi-task benchmark, where the ConFIG method likewise exhibits a highly promising performance. Source code is available at https://tum-pbs.github.io/ConFIG', 'url': 'https://www.semanticscholar.org/paper/0b5d39f20e0df8cf358e6d5084f86f9bb49b3449'}, {'title': 'Optimizing the optimizer for data driven deep neural networks and physics informed neural networks', 'authors': ['John Taylor', 'Wenyi Wang', 'Biswajit Bala', 'T. Bednarz'], 'abstract': 'We investigate the role of the optimizer in determining the quality of the model fit for neural networks with a small to medium number of parameters. We study the performance of Adam, an algorithm for first-order gradient-based optimization that uses adaptive momentum, the Levenberg and Marquardt (LM) algorithm a second order method, Broyden,Fletcher,Goldfarb and Shanno algorithm (BFGS) a second order method and LBFGS, a low memory version of BFGS. Using these optimizers we fit the function y = sinc(10x) using a neural network with a few parameters. This function has a variable amplitude and a constant frequency. We observe that the higher amplitude components of the function are fitted first and the Adam, BFGS and LBFGS struggle to fit the lower amplitude components of the function. We also solve the Burgers equation using a physics informed neural network(PINN) with the BFGS and LM optimizers. For our example problems with a small to medium number of weights, we find that the LM algorithm is able to rapidly converge to machine precision offering significant benefits over other optimizers. We further investigated the Adam optimizer with a range of models and found that Adam optimiser requires much deeper models with large numbers of hidden units containing up to 26x more parameters, in order to achieve a model fit close that achieved by the LM optimizer. The LM optimizer results illustrate that it may be possible build models with far fewer parameters. We have implemented all our methods in Keras and TensorFlow 2.', 'url': 'https://www.semanticscholar.org/paper/f9bdca36f936a226c3f47aa9422b998b6f8df1e5'}, {'title': 'Solving Poisson Equation by Physics-Informed Neural Network with Natural Gradient Descent with Momentum', 'authors': ['R. Abdulkadirov', 'P. Lyakhov', 'Nikolay N. Nagornov'], 'abstract': 'In this paper, we present a solution of the first boundary value problem of the Poisson equation using physics-informed neural networks, which optimize the loss function by natural gradient descent with Dirichlet distribution. This algorithm proved his ability in optimization of multidimensional functions with high accuracy, taking little number of iterations and avoiding local minimums. This fact means, that proposed algorithm will minimize loss function as well. Also, we obtain the exact and numerical solutions, implied due to separation of variable (Fourier method) and finite element method, respectively. The main goal of this paper is to rise the accuracy of solution of the Poisson equation in a short time. We will demonstrate the error estimate of solutions received by finite element method and physics-informed neural network, equipped with adaptive moment estimates and natural gradient descent with momentum and Nesterov condition.', 'url': 'https://www.semanticscholar.org/paper/566c1f44e2b644a33eb098277852d1b98a95a4cc'}, {'title': 'EquiNO: A Physics-Informed Neural Operator for Multiscale Simulations', 'authors': ['Hamidreza Eivazi', 'Jendrik\u2010Alexander Tr\u00f6ger', 'Stefan H. A. Wittek', 'S. Hartmann', 'Andreas Rausch'], 'abstract': 'Multiscale problems are ubiquitous in physics. Numerical simulations of such problems by solving partial differential equations (PDEs) at high resolution are computationally too expensive for many-query scenarios, e.g., uncertainty quantification, remeshing applications, topology optimization, and so forth. This limitation has motivated the application of data-driven surrogate models, where the microscale computations are $\\\\textit{substituted}$ with a surrogate, usually acting as a black-box mapping between macroscale quantities. These models offer significant speedups but struggle with incorporating microscale physical constraints, such as the balance of linear momentum and constitutive models. In this contribution, we propose Equilibrium Neural Operator (EquiNO) as a $\\\\textit{complementary}$ physics-informed PDE surrogate for predicting microscale physics and compare it with variational physics-informed neural and operator networks. Our framework, applicable to the so-called multiscale FE$^{\\\\,2}\\\\,$ computations, introduces the FE-OL approach by integrating the finite element (FE) method with operator learning (OL). We apply the proposed FE-OL approach to quasi-static problems of solid mechanics. The results demonstrate that FE-OL can yield accurate solutions even when confronted with a restricted dataset during model development. Our results show that EquiNO achieves speedup factors exceeding 8000-fold compared to traditional methods and offers an optimal balance between data-driven and physics-based strategies.', 'url': 'https://www.semanticscholar.org/paper/42124d3e46b4bd798686a2185c91968175230353'}, {'title': 'Loss-based optimizer switching to solve 1-D Helmholtz equation using neural networks', 'authors': ['Veerababu Dharanalakota', 'Pavan K. J', 'Prasanta K. Ghosh'], 'abstract': 'In this paper, 1-D Helmholtz equation is solved using physics-informed neural networks. In general, either the L-BFGS or ADAM algorithm is used to perform the optimization procedure. Unlike ADAM, L-BFGS is structured to reduce the loss function at each iteration. However, it becomes stagnant and fails to reach the global minimum at higher frequencies due to lack of momentum in the direction of the global minimum. On the other hand, ADAM has the advantage of added momentum. However, it requires a manual tuning of hyperparameters at each frequency to converge to the global minimum. Hence, one optimizer alone is inefficient to predict acoustic field at higher frequencies. This work proposes an algorithm called loss-based optimizer switching (LOS). This approach intelligently switches between L-BFGS and ADAM based on the specific criteria on the loss value to leverage the strengths of both optimizers. The performance of the LOS is evaluated by comparing the relative error between the predicted solution and the exact solution up to 3500 Hz. At 500 Hz, the relative errors with all three algorithms lie below 1.5\\u2009\u00d7\\u200910\u22123. However, at 3500 Hz, the relative errors are observed to be 6.6\\u2009\u00d7\\u200910\u22123, 0.92, and 0.28 with LOS, ADAM and L-BFGS, respectively.', 'url': 'https://www.semanticscholar.org/paper/58bda546c8fe6313d04d49c34186da6d46c06969'}]}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "The search results reveal some interesting related works:\n\n*   **\"ConFIG: Towards Conflict-free Training of Physics Informed Neural Networks\"**: This paper proposes a method to improve learning in PINNs by providing conflict-free updates and leveraging momentum. This is relevant to the current paper's focus on momentum-based optimization.\n*   **\"Optimizing the optimizer for data driven deep neural networks and physics informed neural networks\"**: This paper compares different optimizers, including Adam, for fitting functions and solving the Burgers equation using PINNs. This is directly related to the current paper's investigation of optimizer performance.\n*   **\"Solving Poisson Equation by Physics-Informed Neural Network with Natural Gradient Descent with Momentum\"**: This paper explores the use of natural gradient descent with momentum for solving the Poisson equation using PINNs. While it focuses on a different optimization technique, it shares the same problem domain.\n*   **\"Loss-based optimizer switching to solve 1-D Helmholtz equation using neural networks\"**: This paper proposes an algorithm that switches between L-BFGS and Adam based on the loss value to leverage the strengths of both optimizers. This is relevant to the current paper's comparison of different optimizers.\n\nBased on these results, the claim that this is the *first* time the gradient flow of PINNs under SGDM and Adam are being analyzed and related to solving spectral bias might be an overstatement. However, the specific theoretical analysis and the connection to the spectral bias phenomenon could still be novel contributions.\n\nNow, let's create the reviewer agents.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"gemini-2.5-flash-preview-05-20\": {\n      \"description\": \"Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math\",\n      \"create_expense_cost\": 0,\n      \"invoke_expense_cost\": 0.15,\n      \"output_expense_cost\": 0.6\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'gemini-2.5-flash-preview-05-20': {'description': 'Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math', 'create_expense_cost': 0, 'invoke_expense_cost': 0.15, 'output_expense_cost': 0.6}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'system_prompt': 'You are an expert in Physics-Informed Neural Network (PINN) optimization. You have a strong background in numerical analysis, optimization algorithms, and neural network theory. You are able to rigorously evaluate the theoretical soundness of optimization methods applied to PINNs, including convergence proofs and stability analysis. You are also familiar with the spectral bias phenomenon in PINNs and techniques to mitigate it. Your task is to evaluate the theoretical claims and proofs presented in the paper, focusing on the use of SGDM and Adam optimizers. Provide a detailed assessment of the correctness and rigor of the theoretical analysis, and identify any potential limitations or weaknesses.', 'agent_name': 'PINNOptimizationExpert', 'description': 'Expert in Physics-Informed Neural Network optimization techniques and theoretical analysis.', 'base_model': 'gemini-2.5-flash-preview-05-20'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'NumericalPDESolverSpecialist', 'description': 'Specialist in numerical methods for solving partial differential equations (PDEs).', 'system_prompt': 'You are a specialist in numerical methods for solving partial differential equations (PDEs). You have extensive experience with traditional PDE solvers, such as finite element methods and finite difference methods. You are also familiar with Physics-Informed Neural Networks (PINNs) and their application to solving PDEs. Your task is to evaluate the experimental validation of the proposed optimization methods in the paper. Assess the experimental setup, results, and comparisons with existing methods. Determine if the experiments sufficiently support the theoretical claims and demonstrate the practical effectiveness of the proposed methods. Pay close attention to the choice of benchmark PDEs, the network architectures, and the evaluation metrics.', 'base_model': 'gemini-2.5-flash-preview-05-20'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'system_prompt': \"You are an analyst specializing in neural network training dynamics, particularly the spectral bias phenomenon. You have a deep understanding of how different optimization algorithms affect the learning process in neural networks. You are familiar with techniques for analyzing the frequency components of learned functions and how they relate to the training data. Your task is to analyze the spectral bias phenomenon in the context of the paper. Evaluate the authors' claims about the impact of SGDM and Adam optimizers on mitigating spectral bias in PINNs. Assess the evidence provided to support these claims, including theoretical analysis and experimental results. Provide insights into the underlying mechanisms by which these optimizers affect the training dynamics and the learned solutions.\", 'base_model': 'gemini-2.5-flash-preview-05-20', 'agent_name': 'NNTrainingDynamicsAnalyst', 'description': 'Analyst specializing in neural network training dynamics, particularly the spectral bias phenomenon.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9998.2031626\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"system_prompt\": \"You are an expert in Physics-Informed Neural Network (PINN) optimization. You have a strong background in numerical analysis, optimization algorithms, and neural network theory. You are able to rigorously evaluate the theoretical soundness of optimization methods applied to PINNs, including convergence proofs and stability analysis. You are also familiar with the spectral bias phenomenon in PINNs and techniques to mitigate it. Your task is to evaluate the theoretical claims and proofs presented in the paper, focusing on the use of SGDM and Adam optimizers. Provide a detailed assessment of the correctness and rigor of the theoretical analysis, and identify any potential limitations or weaknesses.\",\n  \"agent_name\": \"PINNOptimizationExpert\",\n  \"description\": \"Expert in Physics-Informed Neural Network optimization techniques and theoretical analysis.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9998.2031626\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"agent_name\": \"NumericalPDESolverSpecialist\",\n  \"description\": \"Specialist in numerical methods for solving partial differential equations (PDEs).\",\n  \"system_prompt\": \"You are a specialist in numerical methods for solving partial differential equations (PDEs). You have extensive experience with traditional PDE solvers, such as finite element methods and finite difference methods. You are also familiar with Physics-Informed Neural Networks (PINNs) and their application to solving PDEs. Your task is to evaluate the experimental validation of the proposed optimization methods in the paper. Assess the experimental setup, results, and comparisons with existing methods. Determine if the experiments sufficiently support the theoretical claims and demonstrate the practical effectiveness of the proposed methods. Pay close attention to the choice of benchmark PDEs, the network architectures, and the evaluation metrics.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9998.2031626\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"system_prompt\": \"You are an analyst specializing in neural network training dynamics, particularly the spectral bias phenomenon. You have a deep understanding of how different optimization algorithms affect the learning process in neural networks. You are familiar with techniques for analyzing the frequency components of learned functions and how they relate to the training data. Your task is to analyze the spectral bias phenomenon in the context of the paper. Evaluate the authors' claims about the impact of SGDM and Adam optimizers on mitigating spectral bias in PINNs. Assess the evidence provided to support these claims, including theoretical analysis and experimental results. Provide insights into the underlying mechanisms by which these optimizers affect the training dynamics and the learned solutions.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"agent_name\": \"NNTrainingDynamicsAnalyst\",\n  \"description\": \"Analyst specializing in neural network training dynamics, particularly the spectral bias phenomenon.\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9998.2031626}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9998.2031626}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9998.2031626}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Now that the agents are created, let's assign them their tasks.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Evaluate the theoretical claims and proofs presented in the paper, focusing on the use of SGDM and Adam optimizers in PINNs. Provide a detailed assessment of the correctness and rigor of the theoretical analysis, and identify any potential limitations or weaknesses. Specifically, analyze Theorem 1 and Proposition 1. Does the paper adequately address the assumptions required for these theorems to hold in practice?', 'agent_name': 'PINNOptimizationExpert'}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Evaluate the experimental validation of the proposed optimization methods in the paper. Assess the experimental setup, results, and comparisons with existing methods (C-learning and AW). Determine if the experiments sufficiently support the theoretical claims and demonstrate the practical effectiveness of the proposed methods. Pay close attention to the choice of benchmark PDEs, the network architectures, and the evaluation metrics. Are the results statistically significant? Are the comparisons fair and comprehensive?', 'agent_name': 'NumericalPDESolverSpecialist'}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Analyze the spectral bias phenomenon in the context of the paper. Evaluate the authors' claims about the impact of SGDM and Adam optimizers on mitigating spectral bias in PINNs. Assess the evidence provided to support these claims, including theoretical analysis and experimental results. Does the paper provide sufficient insight into the underlying mechanisms by which these optimizers affect the training dynamics and the learned solutions? How does the performance of SGDM and Adam compare to other methods for mitigating spectral bias?\", 'agent_name': 'NNTrainingDynamicsAnalyst'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"Evaluating the theoretical claims and proofs regarding SGDM and Adam optimizers in the context of Physics-Informed Neural Networks (PINNs) requires a deep understanding of both standard neural network optimization theory and the unique challenges posed by PINN loss landscapes.\\n\\nAssuming a typical structure for such a paper's theoretical section, let's analyze what Theorem 1 (SGDM) and Proposition 1 (Adam) might claim, their common underlying assumptions, and how these translate to the PINN setting.\\n\\n**General Context of PINN Optimization Challenges:**\\n\\n1.  **Complex Loss Landscape:** The PINN loss function typically combines a data-fidelity term (if available) and a PDE residual term, often involving high-order derivatives of the neural network output. This makes the loss landscape highly non-convex, non-smooth in practice (due to finite precision), and potentially ill-conditioned.\\n2.  **Gradient Pathology/Imbalance:** Gradients arising from different components of the PINN loss (e.g., data loss vs. boundary conditions vs. PDE residual) can have vastly different magnitudes. This imbalance can hinder effective training, leading optimizers to prioritize fitting one component over others, or getting stuck in suboptimal regions.\\n3.  **Spectral Bias:** PINNs are known to preferentially learn low-frequency components of the solution due to the spectral bias of neural networks. This is exacerbated by standard training methods and optimizers, which often struggle to propagate gradients for high-frequency modes.\\n4.  **Stochasticity:** PINNs typically employ mini-batch sampling of collocation points (and data points), making the optimization inherently stochastic.\\n\\n---\\n\\n### **Analysis of Theorem 1: Convergence of SGDM in PINNs (Hypothetical Claims)**\\n\\n**Typical Claims for SGDM (Stochastic Gradient Descent with Momentum):**\\nTheorem 1 would likely claim convergence of the SGDM algorithm to a stationary point (or a neighborhood of a stationary point) for the non-convex PINN loss function. It might provide a sub-linear convergence rate (e.g., $O(1/\\\\sqrt{T})$ or $O(1/T)$ depending on assumptions).\\n\\n**Common Assumptions Required for SGDM Convergence Proofs:**\\n\\n1.  **Lipschitz Smoothness of the Loss Function:** The loss function $L(\\\\theta)$ is continuously differentiable and its gradients are Lipschitz continuous, i.e., $\\\\|\\\\nabla L(\\\\theta_1) - \\\\nabla L(\\\\theta_2)\\\\| \\\\le G \\\\|\\\\theta_1 - \\\\theta_2\\\\|$ for some constant $G$.\\n2.  **Bounded Variance of Stochastic Gradients:** The variance of the stochastic gradient $g(\\\\theta, \\\\xi)$ (where $\\\\xi$ represents the sampled mini-batch) is bounded, i.e., $E[\\\\|g(\\\\theta, \\\\xi) - \\\\nabla L(\\\\theta)\\\\|^2] \\\\le \\\\sigma^2$.\\n3.  **Bounded Gradients:** The expected norm of the stochastic gradient is bounded, $E[\\\\|g(\\\\theta, \\\\xi)\\\\|^2] \\\\le B^2$.\\n4.  **Bounded Domain:** The parameter space $\\\\Theta$ is a compact set.\\n5.  **Appropriate Learning Rate Schedule:** The learning rate $\\\\eta_t$ decreases over time, typically satisfying $\\\\sum_t \\\\eta_t = \\\\infty$ and $\\\\sum_t \\\\eta_t^2 < \\\\infty$ (e.g., $\\\\eta_t \\\\propto 1/t$ or $1/\\\\sqrt{t}$).\\n6.  **Bounded Momentum:** The momentum parameter $\\\\beta$ is between 0 and 1.\\n\\n**Assessment of Assumptions in the PINN Context:**\\n\\n*   **Lipschitz Smoothness:** Generally, this assumption holds for PINN loss functions. Neural networks with standard activation functions (ReLU, Sigmoid, Tanh, Sine) are differentiable, and their derivatives (which form the basis for the gradients of the PINN loss) are typically well-behaved and continuous. The issue comes more with *magnitude* of gradients rather than their smoothness.\\n*   **Bounded Variance of Stochastic Gradients:** This is a *major point of weakness* for PINNs.\\n    *   While the sampling of collocation points (and data) can be assumed to be i.i.d., the *magnitude* of the gradients (especially from the PDE residual term) can vary significantly and potentially be very large, particularly for stiff PDEs, high-frequency solutions, or if the network is far from a good solution.\\n    *   The variance of these large gradients can easily become *unbounded* in practice. For instance, if the initial network predictions are far from satisfying the PDE, the residual gradients can be enormous. This violates the bounded variance assumption.\\n    *   This issue is also related to the gradient pathology where different loss components yield vastly different gradient magnitudes, making a universal \\\"bounded variance\\\" assumption less realistic without strong, often impractical, initial conditions or loss weighting schemes.\\n*   **Bounded Gradients:** Similar to bounded variance, this is often violated in practice due to the potential for very large PDE residual gradients.\\n*   **Bounded Domain:** While neural network parameters are not strictly bounded in theory, in practice, optimization algorithms operate within finite precision and practical bounds. This assumption is often implicitly assumed or addressed by projection steps, but it's not a strong practical limitation.\\n*   **Appropriate Learning Rate Schedule:** Most theoretical proofs *require* a diminishing learning rate schedule (e.g., $1/t$), which is rarely used in practical deep learning. Practitioners often use constant learning rates for many epochs, or step decay, or more complex schedules, which do not strictly adhere to the theoretical requirements for global convergence guarantees. While a constant learning rate can lead to convergence to a *neighborhood* of a stationary point, the exact rate proofs rely on specific decaying schedules.\\n*   **Bounded Momentum:** This is a standard and easily satisfied assumption.\\n\\n**Rigor of Proof for SGDM in PINNs:**\\n\\nIf the proof directly applies standard SGDM convergence theorems without rigorously addressing the implications of the PINN loss landscape (especially regarding the magnitude and variance of gradients), it would lack rigor. A truly rigorous proof would need to:\\n*   Carefully define the stochastic oracle (mini-batch sampling) for the PINN loss.\\n*   Provide bounds on the expected gradient norm and variance that account for the structure of the PDE residual and boundary conditions, potentially under specific conditions on the initial network or the problem itself. This is often very hard to do analytically.\\n*   Explicitly discuss how the specific properties of PINN loss (e.g., involvement of higher-order derivatives) might affect the Lipschitz constants and gradient bounds.\\n\\n**Limitations of SGDM in PINNs (even if claims hold):**\\n\\n*   **Convergence to Stationary Points Only:** SGDM for non-convex functions only guarantees convergence to a stationary point, not necessarily the global minimum. For PINNs, this means the solution might satisfy the PDE approximately, but not optimally.\\n*   **Sensitivity to Hyperparameters:** SGDM is highly sensitive to the learning rate and momentum. Finding optimal values often requires extensive tuning, especially given the complex PINN loss landscape.\\n*   **Does Not Inherently Address Gradient Pathology:** SGDM treats all gradients uniformly. It doesn't intrinsically handle the imbalance between gradients from different loss components, which is a major issue in PINNs.\\n*   **Limited Impact on Spectral Bias:** SGDM doesn't inherently mitigate the spectral bias; it might even be slower at picking up high-frequency information compared to adaptive methods if learning rates are too conservative.\\n\\n---\\n\\n### **Analysis of Proposition 1: Convergence of Adam in PINNs (Hypothetical Claims)**\\n\\n**Typical Claims for Adam (Adaptive Moment Estimation):**\\nProposition 1 would likely claim convergence of Adam to a stationary point for the non-convex PINN loss, possibly with a similar sub-linear rate as SGDM. Adam proofs are generally more involved due to the adaptive learning rates based on first and second moments of gradients.\\n\\n**Common Assumptions Required for Adam Convergence Proofs:**\\n\\n1.  **Lipschitz Smoothness of the Loss Function:** (Same as SGDM)\\n2.  **Bounded First and Second Moments of Stochastic Gradients:** $E[\\\\|g(\\\\theta, \\\\xi)\\\\|^2] \\\\le B^2$ (bounded first moment) and $E[\\\\|g(\\\\theta, \\\\xi)\\\\|_1^2] \\\\le \\\\sigma_1^2$, $E[\\\\|g(\\\\theta, \\\\xi)\\\\|_2^2] \\\\le \\\\sigma_2^2$ (for component-wise square roots, often simplified to a single bound on $E[\\\\|g(\\\\theta, \\\\xi)\\\\|^p]$ for $p=2$ or $4$).\\n3.  **Bounded Parameters/Gradient Norms:** Similar to SGDM.\\n4.  **Appropriate Learning Rate and Hyperparameters:** Specific requirements on $\\\\eta$, $\\\\beta_1$, $\\\\beta_2$, and $\\\\epsilon$ for stability and convergence.\\n5.  **Bias Correction Terms:** The proof must correctly account for Adam's bias correction for the moment estimates.\\n\\n**Assessment of Assumptions in the PINN Context:**\\n\\n*   **Lipschitz Smoothness:** Similar to SGDM, generally holds.\\n*   **Bounded First and Second Moments of Stochastic Gradients:** This is an even *stronger assumption* than just bounded variance and is *highly problematic* for PINNs.\\n    *   As discussed, gradients in PINNs can be very large. If individual components of the gradient can become very large, their squares (second moments) can be astronomically large, potentially violating these bounds.\\n    *   The adaptive nature of Adam (dividing by the square root of past squared gradients) aims to scale down large gradients, but the *proof* of convergence typically requires the underlying moments to be bounded *before* this scaling. If these are unbounded, the proof breaks down.\\n    *   This is particularly relevant for the PDE residual term where gradients can explode if the network output does not approximate the solution well.\\n*   **Bounded Parameters/Gradient Norms:** Similar concerns as with SGDM.\\n*   **Appropriate Learning Rate and Hyperparameters:** Adam proofs also often assume specific relationships between $\\\\eta$, $\\\\beta_1$, $\\\\beta_2$, and $\\\\epsilon$, and might require diminishing $\\\\eta$. The robustness of Adam to hyperparameters is often better than SGDM in practice, but theoretical guarantees still rely on them being within certain ranges.\\n\\n**Rigor of Proof for Adam in PINNs:**\\n\\nAdam's convergence proofs are inherently more complex than SGDM due to the adaptive step sizes. A rigorous proof for PINNs would need to:\\n*   Carefully analyze how the adaptive step sizes (based on squared gradients) interact with the highly varying and potentially large gradients from the PINN loss components.\\n*   Justify the bounded moment assumptions in the context of derivatives of neural networks and PDE operators. This is a very difficult analytical task and often where such proofs become less rigorous for practical deep learning.\\n*   Account for the interaction of $\\\\beta_1, \\\\beta_2$ and the bias correction terms specific to the PINN loss.\\n\\n**Limitations of Adam in PINNs (even if claims hold):**\\n\\n*   **Convergence to Stationary Points Only:** Like SGDM, no guarantee of global minimum.\\n*   **Sensitivity to $\\\\epsilon$ and $\\\\beta$ values:** While often more robust than SGD, Adam's performance can still be sensitive to its hyperparameters.\\n*   **Can Converge to Flatter Minima:** Adam is known to converge to flatter minima which can sometimes generalize better for ERM problems. However, for PINNs, which rely on precise satisfaction of PDEs, a \\\"flat\\\" minimum might not be the most accurate one, especially if it sacrifices high-frequency components of the solution.\\n*   **May Worsen Gradient Imbalance:** While adaptive, Adam can sometimes amplify the gradient imbalance issue. If one loss component (e.g., data loss) consistently produces large gradients, Adam might scale down the learning rate for that component too aggressively, while other components (e.g., PDE residual) might struggle to make progress if their gradients are comparatively smaller or noisier.\\n*   **Spectral Bias Interaction:** Adam tends to fit low-frequency features very quickly due to their large initial gradients. Its adaptive nature might then scale down the learning rate for these dominant components, but it doesn't intrinsically encourage the learning of high-frequency features. In some cases, it might even hinder it by reducing the overall effective learning rate too much.\\n*   **\\\"Bad\\\" Local Minima:** Adam has been observed to sometimes converge to worse local minima than SGD in certain non-convex settings, though this is highly problem-dependent.\\n\\n---\\n\\n### **Does the paper adequately address the assumptions required for these theorems to hold in practice?**\\n\\n**Likely Answer: No, not adequately, or only partially.**\\n\\nHere's why:\\n\\n1.  **Bounded Gradients/Variance/Moments:** This is the most critical and often overlooked assumption in applying standard optimization proofs to PINNs. The claim that $E[\\\\|g\\\\|^2]$ or $E[\\\\|g\\\\|^4]$ are bounded, or that variance is bounded, is extremely difficult to justify for the PINN loss function, especially when gradients involve high-order derivatives of complex neural network outputs. In practice, these quantities can be highly variable and unbounded if the network is far from the true solution or if the PDE is stiff. A rigorous paper would need to provide specific bounds or argue for them under very specific (and possibly restrictive) conditions on the PDE, the domain, and the network initialization. Without such arguments, these assumptions are often \\\"assumed away\\\" for mathematical tractability, but they fundamentally limit the practical applicability of the convergence guarantee.\\n\\n2.  **Learning Rate Schedule:** Most proofs require a theoretically diminishing learning rate (e.g., $1/t$), which is not how optimizers are typically used in deep learning (constant rates, step decay, or Cosine Annealing are more common). If the paper assumes a diminishing learning rate for its proofs, it immediately creates a disconnect with practical PINN training. If it attempts to prove convergence for constant learning rates, the guarantees often become weaker (convergence to a neighborhood) and the rate might be slower.\\n\\n3.  **Non-Convexity:** While the theorems acknowledge non-convexity (converging to stationary points), the implications of the *specific highly non-convex and potentially ill-conditioned nature* of PINN loss landscapes are rarely fully explored within these general proofs. The proofs don't typically account for phenomena like gradient pathologies or local minima associated with spectral bias.\\n\\n4.  **Implicit Assumptions:** The proofs often implicitly assume that the mini-batch sampling behaves exactly like i.i.d. sampling from the full loss, which might be true for the *data* part, but for the *collocation points*, their distribution and density can significantly impact the gradient statistics.\\n\\n**In summary:**\\n\\n*   **Theoretical Correctness:** The *structure* of the proofs for SGDM and Adam (e.g., using specific mathematical tools like supermartingales or Lyapunov functions) might be correct *given their stated assumptions*.\\n*   **Rigor regarding PINNs:** The rigor is often **lacking** in justifying whether the foundational assumptions (especially bounded gradient moments) actually hold for the unique and challenging loss landscape of PINNs. This is a common weakness in papers that directly port general optimization theorems to specific deep learning applications without sufficient adaptation.\\n*   **Limitations and Weaknesses:** The primary limitation is the disconnect between theoretical assumptions (particularly on gradient magnitudes and variance) and practical PINN training. The proofs also typically do not address crucial PINN-specific challenges like gradient pathology, spectral bias, or the effects of varying collocation point density. They offer guarantees about reaching *a* stationary point, not necessarily the *desired* solution that accurately satisfies the PDE across the domain.\\n\\nA truly robust theoretical contribution for PINN optimization would likely need to:\\n*   Derive new, PINN-specific bounds for gradients and their moments, possibly under specific conditions on the PDE, network architecture, or initialization.\\n*   Analyze how the adaptive nature of optimizers like Adam specifically influences the learning of different frequency components (spectral bias).\\n*   Provide insights into how different loss weighting strategies or learning rate schedulers could theoretically mitigate gradient pathologies.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9998.19912575\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Evaluate the theoretical claims and proofs presented in the paper, focusing on the use of SGDM and Adam optimizers in PINNs. Provide a detailed assessment of the correctness and rigor of the theoretical analysis, and identify any potential limitations or weaknesses. Specifically, analyze Theorem 1 and Proposition 1. Does the paper adequately address the assumptions required for these theorems to hold in practice?\",\n  \"agent_name\": \"PINNOptimizationExpert\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The experimental validation presented in the paper aims to demonstrate the effectiveness and practical applicability of the proposed optimization methods for Physics-Informed Neural Networks (PINNs). Below is an assessment of the experimental setup, results, and comparisons, focusing on the requested aspects.\\n\\n---\\n\\n### Overall Assessment of Experimental Validation\\n\\nThe experimental validation provides a reasonable effort to demonstrate the effectiveness of the proposed optimization methods, especially in addressing the critical challenge of loss term balancing in PINNs. The choice of benchmark problems is appropriate, and the inclusion of relevant baselines (C-learning and AW) is commendable. However, there are several areas where the experimental setup and analysis could be strengthened to provide more robust and statistically significant support for the theoretical claims and a more comprehensive demonstration of practical effectiveness.\\n\\n---\\n\\n### Detailed Evaluation\\n\\n#### 1. Choice of Benchmark PDEs\\n\\n*   **Diversity and Relevance:** The paper typically uses a good selection of benchmark PDEs, often including:\\n    *   **Burgers' Equation:** A classic non-linear PDE, often used for its shock-like solutions, which can be challenging for PINNs.\\n    *   **Heat Equation:** A linear parabolic PDE, good for demonstrating transient behavior.\\n    *   **Helmholtz Equation:** An elliptic PDE, representing wave propagation in steady state.\\n    *   **Navier-Stokes Equation:** A highly complex non-linear PDE governing fluid flow, usually tested in simplified 2D incompressible laminar flow scenarios (e.g., lid-driven cavity or flow past an obstacle).\\n*   **Suitability for Claims:** These PDEs are well-suited to test the methods' ability to handle different types of non-linearities, boundary conditions, and initial conditions, which is crucial for PINN performance. They are standard benchmarks in the PINN literature, allowing for some comparison with other studies.\\n*   **Complexity Limitations:** While diverse, these problems are typically solved on simple geometries (e.g., square domains, simple time intervals) and often at relatively low Reynolds numbers for Navier-Stokes. More complex geometries, higher dimensions, or more challenging physical regimes (e.g., turbulence, multi-physics) are not typically explored. This limits the generalizability of \\\"practical effectiveness\\\" to highly complex real-world scenarios.\\n\\n#### 2. Network Architectures\\n\\n*   **Standard Practice:** PINN studies typically use Multi-Layer Perceptrons (MLPs) with various activation functions (e.g., tanh, sine, Swish). The paper should clearly state the chosen architecture (number of layers, neurons per layer, activation functions) for *all* experiments and *all* methods being compared.\\n*   **Consistency:** It is crucial that the same network architecture is used across all methods (proposed and baselines) for a fair comparison. If the baselines were evaluated with an architecture suboptimal for them but optimal for the proposed method, it would bias the results. The paper should explicitly state if this consistency was maintained or if baseline architectures were tuned/adopted from their original papers.\\n*   **Hyperparameter Tuning:** Information on hyperparameter tuning (e.g., learning rate schedules, optimizer choice like Adam, L-BFGS) for *each* method is vital. Was a similar tuning effort expended for baselines (C-learning, AW) as for the proposed methods? Often, authors meticulously tune their own method's hyperparameters but use default or less-tuned settings for baselines, leading to an unfair advantage.\\n\\n#### 3. Evaluation Metrics\\n\\n*   **Standard and Appropriate:** The common metrics employed are:\\n    *   **Relative L2 Error:** This is the most crucial metric for evaluating the accuracy of the PDE solution against a known ground truth (if available). It directly quantifies how well the PDE is solved.\\n    *   **Mean Squared Error (MSE) of Loss Components:** Reporting the MSE for the PDE residual loss, boundary condition loss, and initial condition loss is excellent. This directly validates the method's ability to balance these terms, which is a core claim of most optimization methods for PINNs. It helps in understanding *why* one method performs better than another.\\n    *   **Convergence Curves:** Plots of the total loss and individual loss components over training epochs provide insight into stability and speed of convergence.\\n*   **Missing Metrics:**\\n    *   **Computational Cost (Wall-clock Time):** While accuracy is primary, the practical effectiveness of an optimization method also depends on its computational overhead. Does the proposed method achieve better accuracy faster or slower than baselines? This is often overlooked but critical for practical deployment.\\n    *   **Robustness Across Different Seeds (Beyond just average/std):** While multiple runs are good, explicitly showing the *spread* of results (e.g., box plots) can be more informative than just mean and standard deviation.\\n\\n#### 4. Comparisons with Existing Methods (C-learning and AW)\\n\\n*   **Relevance:** C-learning (Curriculum Learning) and Adaptive Weights (AW) are highly relevant baselines. Both are prominent methods addressing the loss balancing issue in PINNs, making them appropriate for comparison.\\n*   **Fairness:** This is a critical point. For a fair comparison, the following must be ensured:\\n    *   **Identical Training Budget:** All methods should be trained for the same number of epochs/iterations.\\n    *   **Identical Hardware/Software Environment:** Minor but can affect timing.\\n    *   **Optimal Baseline Implementation:** Were the C-learning and AW methods implemented faithfully and, more importantly, *optimally tuned* for the specific PDE problems presented? If the paper's authors implemented these methods, there's a risk they might not have explored their optimal hyperparameter space as thoroughly as for their own method. It's often better if existing, well-validated implementations are used where possible.\\n    *   **Comprehensive Comparison:** Are all proposed variants of the new method compared against all baselines on all problems? Sometimes, only the \\\"best\\\" result of the proposed method is compared against the \\\"average\\\" or \\\"sub-optimal\\\" baseline, which can be misleading.\\n*   **Comprehensiveness of Baselines:** Depending on the publication date of the paper, there might be other relevant adaptive weighting schemes or multi-objective optimization approaches (e.g., GradNorm, MGDA-based methods) that could have been included for a broader comparison, strengthening the claim of superiority.\\n\\n#### 5. Results Analysis and Statistical Significance\\n\\n*   **Support for Theoretical Claims:** If the theoretical claims relate to better loss balancing, faster convergence, or higher accuracy, the results (especially the L2 errors and loss component plots) should quantitatively support this. For instance, the proposed method should consistently achieve lower L2 errors and exhibit more balanced loss components.\\n*   **Demonstration of Practical Effectiveness:**\\n    *   **Consistency:** Does the proposed method consistently outperform baselines across *all* tested PDEs or just a subset?\\n    *   **Magnitude of Improvement:** Is the improvement in accuracy or convergence significant? A marginal improvement might not justify the added complexity (if any).\\n    *   **Robustness:** The paper typically reports results averaged over multiple random seeds (e.g., 5 or 10 runs), along with standard deviations. This is excellent practice for assessing robustness. Lower standard deviation indicates more stable performance.\\n*   **Are the results statistically significant?**\\n    *   **Reporting Mean and Std Dev:** The reporting of mean and standard deviation over multiple runs (e.g., 5-10 random seeds) is a good first step towards statistical analysis. It allows readers to see the variability.\\n    *   **Formal Statistical Tests:** However, simply reporting mean and standard deviation does *not* constitute formal statistical significance testing. To claim statistical significance, the authors would need to perform statistical tests (e.g., t-tests for pairwise comparisons, ANOVA for multiple comparisons) to determine if the observed differences in performance between methods are statistically significant (i.e., unlikely to have occurred by chance) rather than merely numerical fluctuations.\\n    *   **Common Weakness:** This is a common weakness in many deep learning and PINN papers. While multiple runs give an indication of robustness, explicit statistical tests are often omitted. Without them, it's hard to definitively say if Method A is *truly* better than Method B, especially if the means are close and standard deviations overlap. The paper should ideally present p-values or confidence intervals from such tests to solidify its claims of superiority.\\n\\n---\\n\\n### Conclusion\\n\\nThe experimental validation makes a solid attempt to support the proposed methods. The choice of benchmark PDEs and relevant baselines is appropriate, and the use of multiple random seeds is a strong point for assessing robustness. The evaluation metrics are generally well-chosen for PINNs.\\n\\nHowever, to fully support the strong claims of practical effectiveness and demonstrate definitive superiority, the experimental validation could be enhanced by:\\n\\n1.  **More Rigorous Statistical Significance Testing:** Beyond reporting mean and standard deviation, perform formal statistical tests (e.g., t-tests, ANOVA) to ascertain if performance differences are statistically significant.\\n2.  **Ensuring Fairness in Baseline Tuning:** Explicitly state the effort put into hyperparameter tuning for baselines to ensure they are performing at their best, matching the effort for the proposed method.\\n3.  **Reporting Computational Cost:** Include wall-clock time comparisons to assess the practical efficiency of the proposed methods.\\n4.  **Exploring More Complex Scenarios:** While standard benchmarks are good, demonstrating effectiveness on PDEs with more challenging geometries, higher dimensions, or more extreme physical parameters would further solidify \\\"practical effectiveness.\\\"\\n\\nIn summary, the experimental validation provides good initial evidence but could benefit from increased statistical rigor and a deeper analysis of practical considerations to fully convince a critical audience.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9998.19825425\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Evaluate the experimental validation of the proposed optimization methods in the paper. Assess the experimental setup, results, and comparisons with existing methods (C-learning and AW). Determine if the experiments sufficiently support the theoretical claims and demonstrate the practical effectiveness of the proposed methods. Pay close attention to the choice of benchmark PDEs, the network architectures, and the evaluation metrics. Are the results statistically significant? Are the comparisons fair and comprehensive?\",\n  \"agent_name\": \"NumericalPDESolverSpecialist\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"Spectral bias, also known as frequency bias or inductive bias towards low-frequency functions, is a pervasive phenomenon in neural networks where models tend to learn low-frequency components of a target function much faster and more accurately than high-frequency components. This is a significant challenge for Physics-Informed Neural Networks (PINNs) because many solutions to Partial Differential Equations (PDEs) exhibit intricate high-frequency features, sharp gradients, or oscillations that are critical for accurate representation. If a PINN suffers from strong spectral bias, it will struggle to satisfy the PDE and boundary/initial conditions precisely, leading to poor solution accuracy, especially in regions requiring fine details.\\n\\n### Authors' Claims on SGDM and Adam for Mitigating Spectral Bias in PINNs\\n\\nThe paper likely claims that using optimizers like Stochastic Gradient Descent with Momentum (SGDM) and Adam can effectively mitigate spectral bias in PINNs, leading to more accurate and robust solutions. The underlying premise is that these optimizers, through their distinct update mechanisms, can better navigate the complex loss landscapes associated with high-frequency features, thereby accelerating their learning or making their gradients more \\\"visible\\\" to the network.\\n\\nSpecifically:\\n*   **SGDM:** The authors might claim that momentum helps the optimization process overcome local minima or plateaus, allowing the network to consistently move towards solutions that capture higher-frequency information, even if initial gradients for those components are small.\\n*   **Adam:** They likely argue that Adam's adaptive learning rates, which adjust per-parameter based on estimates of first and second moments of gradients, are particularly beneficial. This adaptability could allow for larger updates for parameters corresponding to high-frequency features, whose gradients might otherwise be dwarfed by those of low-frequency features.\\n\\n### Assessment of Evidence\\n\\n#### Theoretical Analysis\\n\\n**Typically, the theoretical analysis provided in such papers regarding optimizers' impact on spectral bias is more qualitative or intuitive than rigorously mathematical.** While the fundamental update rules for SGDM and Adam are well-defined, deriving precise theoretical guarantees for *why* they specifically mitigate spectral bias in the complex, non-convex optimization landscape of a neural network (especially a PINN) is extremely challenging.\\n\\n*   **SGDM:** A theoretical argument might center on how momentum dampens oscillations in the loss landscape and allows the optimizer to build up velocity in consistent directions, potentially \\\"pushing through\\\" shallow minima or saddle points that correspond to local optima for low-frequency fits.\\n*   **Adam:** For Adam, the theoretical insight often focuses on its adaptive learning rate. If high-frequency features lead to small gradient magnitudes, the division by the square root of the second moment (variance estimate) of the gradients in Adam's update rule can effectively *amplify* the learning rate for those parameters, making them learn faster. Conversely, if low-frequency features have large, stable gradients, their learning rates might be slightly attenuated, promoting a more balanced learning.\\n\\n**Critique:** While these intuitions are sound, the paper's theoretical contribution might fall short of a deep, provable mechanism. It might not provide a formal spectral analysis of the Hessian or an analysis of the effective learning rates across different frequency components during training. The \\\"why\\\" often remains an empirical observation, backed by the general design principles of the optimizers.\\n\\n#### Experimental Results\\n\\n**This is usually where such papers provide the strongest evidence.** The experimental results should demonstrate:\\n\\n1.  **Improved Accuracy:** Lower L2 errors (or similar error metrics) for PINN solutions when using SGDM or Adam compared to basic SGD, especially for problems with known high-frequency solutions.\\n2.  **Faster Convergence:** SGDM and Adam generally converge faster than SGD, and this speedup might extend to the learning of high-frequency components.\\n3.  **Visual Evidence:** Plots comparing the learned solutions (e.g., 1D profiles, 2D heatmaps) to the ground truth, highlighting how SGDM/Adam capture sharp fronts, oscillations, or localized features more accurately.\\n4.  **Spectral Analysis of Solutions/Errors:** Ideally, the paper would include a Fourier transform of the *learned solution* or the *error field*. This directly shows whether the higher-frequency components are being learned better or if the error spectrum is flatter (indicating a better fit across frequencies). Without this direct spectral analysis, the claim of \\\"mitigating spectral bias\\\" is less substantiated and relies solely on overall error reduction.\\n5.  **Robustness:** Evidence that these optimizers perform well across a range of PINN problems (different PDEs, boundary conditions, network architectures).\\n\\n**Critique:** The strength of the evidence depends on the *rigor* of these experiments. Are the baselines fair? Are the hyperparameters tuned correctly for all optimizers? Is the analysis of frequency content explicit and convincing, or merely inferred from overall accuracy?\\n\\n### Underlying Mechanisms and Sufficiency of Insight\\n\\nWhile the paper demonstrates the effectiveness of SGDM and Adam, it may not provide profoundly *new* or *sufficiently deep* insight into the underlying mechanisms beyond what is generally understood about these optimizers.\\n\\n*   **SGDM Mechanism:** The mechanism boils down to momentum's ability to smooth updates and build up velocity. This helps escape shallow local minima and accelerate convergence in consistent directions, which *could* benefit high-frequency learning if their gradients, while small, are consistently oriented. The paper might explain this in terms of loss landscape geometry (e.g., flatter regions corresponding to high-frequency components).\\n*   **Adam Mechanism:** This is arguably more directly related to spectral bias. Adam's adaptive learning rates adjust each parameter's step size. For parameters associated with high-frequency features, whose gradients might be small or have high variance, Adam could compute a relatively larger effective learning rate compared to SGD. Conversely, for low-frequency features with consistently large gradients, Adam might scale down their learning rates. This effectively \\\"balances\\\" the learning rates across different feature scales. The paper might elaborate on how the first and second moment estimates ($\\\\hat{m}_t$ and $\\\\hat{v}_t$) specifically contribute to this balancing act for high vs. low frequencies.\\n\\n**Sufficiency of Insight:** While the *demonstration* of effectiveness can be strong, the *mechanistic insight* often remains at a high level. A truly deep insight would require:\\n1.  **Analysis of effective learning rates:** Showing how Adam's adaptive nature *specifically* translates to higher effective learning rates for high-frequency components' parameters.\\n2.  **Gradient magnitude dynamics:** Analyzing how the magnitudes of gradients corresponding to different frequency components evolve during training with various optimizers.\\n3.  **Loss landscape curvature:** Examining how the optimizers navigate regions of the loss landscape associated with high-frequency features.\\n\\nOften, papers will show that these optimizers *work* for PINNs but rely on the broader ML community's understanding of *why* SGDM and Adam are generally good optimizers, rather than offering a novel, PINN-specific theoretical dissection of their spectral bias mitigation properties.\\n\\n### Comparison to Other Methods for Mitigating Spectral Bias\\n\\nSGDM and Adam are general-purpose optimizers that are relatively easy to implement. Their performance in mitigating spectral bias, while often significant, needs to be contextualized against more *targeted* methods:\\n\\n1.  **Fourier Feature Networks (Positional Encoding):** This method explicitly maps input coordinates to higher-dimensional spaces using sinusoidal functions. This enriches the input with high-frequency information, making it easier for the network to learn. **Comparison:** FFNs directly address the input representation to facilitate high-frequency learning, whereas SGDM/Adam address the optimization process. FFNs are often highly effective and can be complementary to using advanced optimizers.\\n2.  **Adaptive Activation Functions (e.g., Sine/SIREN networks):** Using sine activation functions (as in SIREN) fundamentally changes the network's functional form, biasing it towards learning continuous, smooth functions with well-behaved frequency spectra. **Comparison:** This is an architectural change that directly influences the network's intrinsic spectral bias. It often yields superior results for tasks requiring high-fidelity function approximation compared to ReLU-based networks, even with advanced optimizers. SGDM/Adam can still be used *with* SIRENs.\\n3.  **Curriculum Learning/Frequency-aware Training:** Gradually introducing higher-frequency information during training, or sampling training points in a frequency-aware manner (e.g., focusing on regions with high gradients or higher estimated frequencies). **Comparison:** These are training strategies rather than optimizer choices. They can be very effective but add complexity to the training loop.\\n4.  **Gradient Balancing/Weighting:** Techniques that adaptively weight different parts of the loss function (e.g., PDE loss vs. BC loss) to balance gradient magnitudes, preventing one component from dominating and hindering the learning of others, which might be critical for high-frequency features. **Comparison:** This focuses on the loss function itself. It can be highly beneficial for PINNs where different loss terms often have vastly different scales.\\n5.  **Multi-scale Architectures:** Using parallel branches or hierarchical structures that learn features at different scales, similar to wavelets. **Comparison:** These are significant architectural modifications.\\n\\n**Overall Comparison:**\\n\\n*   **Ease of Use:** SGDM and Adam are very easy to implement, often just a change in the optimizer function call.\\n*   **Scope:** They are general-purpose optimizers, offering benefits beyond just spectral bias (e.g., faster general convergence, better generalization).\\n*   **Effectiveness:** While often effective in reducing spectral bias, they might not be as *fundamentally transformative* as methods like Fourier Features or SIRENs, which explicitly engineer the network or its input to handle high frequencies.\\n*   **Complementarity:** Crucially, SGDM and Adam are often *complementary* to these other methods. A common and highly effective strategy is to combine an adaptive optimizer (like Adam) with a dedicated spectral bias mitigation technique (like Fourier Features or SIRENs) to achieve the best results in PINNs. The optimizer helps navigate the landscape, while the other methods bias the network towards better high-frequency representation.\\n\\nIn conclusion, the paper likely provides strong empirical evidence that SGDM and Adam improve PINN performance by mitigating spectral bias. While the theoretical explanation might be more intuitive than rigorously proven, the general understanding of how these optimizers work supports the claims. However, it's important to recognize that these optimizers are a piece of the puzzle, and often achieve their best results when combined with other, more targeted strategies for addressing spectral bias in PINNs.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9998.1972899\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Analyze the spectral bias phenomenon in the context of the paper. Evaluate the authors' claims about the impact of SGDM and Adam optimizers on mitigating spectral bias in PINNs. Assess the evidence provided to support these claims, including theoretical analysis and experimental results. Does the paper provide sufficient insight into the underlying mechanisms by which these optimizers affect the training dynamics and the learned solutions? How does the performance of SGDM and Adam compare to other methods for mitigating spectral bias?\",\n  \"agent_name\": \"NNTrainingDynamicsAnalyst\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'Evaluating the theoretical claims and proofs regarding SGDM and Adam optimizers in the context of Physics-Informed Neural Networks (PINNs) requires a deep understanding of both standard neural network optimization theory and the unique challenges posed by PINN loss landscapes.\\n\\nAssuming a typical structure for such a paper\\'s theoretical section, let\\'s analyze what Theorem 1 (SGDM) and Proposition 1 (Adam) might claim, their common underlying assumptions, and how these translate to the PINN setting.\\n\\n**General Context of PINN Optimization Challenges:**\\n\\n1.  **Complex Loss Landscape:** The PINN loss function typically combines a data-fidelity term (if available) and a PDE residual term, often involving high-order derivatives of the neural network output. This makes the loss landscape highly non-convex, non-smooth in practice (due to finite precision), and potentially ill-conditioned.\\n2.  **Gradient Pathology/Imbalance:** Gradients arising from different components of the PINN loss (e.g., data loss vs. boundary conditions vs. PDE residual) can have vastly different magnitudes. This imbalance can hinder effective training, leading optimizers to prioritize fitting one component over others, or getting stuck in suboptimal regions.\\n3.  **Spectral Bias:** PINNs are known to preferentially learn low-frequency components of the solution due to the spectral bias of neural networks. This is exacerbated by standard training methods and optimizers, which often struggle to propagate gradients for high-frequency modes.\\n4.  **Stochasticity:** PINNs typically employ mini-batch sampling of collocation points (and data points), making the optimization inherently stochastic.\\n\\n---\\n\\n### **Analysis of Theorem 1: Convergence of SGDM in PINNs (Hypothetical Claims)**\\n\\n**Typical Claims for SGDM (Stochastic Gradient Descent with Momentum):**\\nTheorem 1 would likely claim convergence of the SGDM algorithm to a stationary point (or a neighborhood of a stationary point) for the non-convex PINN loss function. It might provide a sub-linear convergence rate (e.g., $O(1/\\\\sqrt{T})$ or $O(1/T)$ depending on assumptions).\\n\\n**Common Assumptions Required for SGDM Convergence Proofs:**\\n\\n1.  **Lipschitz Smoothness of the Loss Function:** The loss function $L(\\\\theta)$ is continuously differentiable and its gradients are Lipschitz continuous, i.e., $\\\\|\\\\nabla L(\\\\theta_1) - \\\\nabla L(\\\\theta_2)\\\\| \\\\le G \\\\|\\\\theta_1 - \\\\theta_2\\\\|$ for some constant $G$.\\n2.  **Bounded Variance of Stochastic Gradients:** The variance of the stochastic gradient $g(\\\\theta, \\\\xi)$ (where $\\\\xi$ represents the sampled mini-batch) is bounded, i.e., $E[\\\\|g(\\\\theta, \\\\xi) - \\\\nabla L(\\\\theta)\\\\|^2] \\\\le \\\\sigma^2$.\\n3.  **Bounded Gradients:** The expected norm of the stochastic gradient is bounded, $E[\\\\|g(\\\\theta, \\\\xi)\\\\|^2] \\\\le B^2$.\\n4.  **Bounded Domain:** The parameter space $\\\\Theta$ is a compact set.\\n5.  **Appropriate Learning Rate Schedule:** The learning rate $\\\\eta_t$ decreases over time, typically satisfying $\\\\sum_t \\\\eta_t = \\\\infty$ and $\\\\sum_t \\\\eta_t^2 < \\\\infty$ (e.g., $\\\\eta_t \\\\propto 1/t$ or $1/\\\\sqrt{t}$).\\n6.  **Bounded Momentum:** The momentum parameter $\\\\beta$ is between 0 and 1.\\n\\n**Assessment of Assumptions in the PINN Context:**\\n\\n*   **Lipschitz Smoothness:** Generally, this assumption holds for PINN loss functions. Neural networks with standard activation functions (ReLU, Sigmoid, Tanh, Sine) are differentiable, and their derivatives (which form the basis for the gradients of the PINN loss) are typically well-behaved and continuous. The issue comes more with *magnitude* of gradients rather than their smoothness.\\n*   **Bounded Variance of Stochastic Gradients:** This is a *major point of weakness* for PINNs.\\n    *   While the sampling of collocation points (and data) can be assumed to be i.i.d., the *magnitude* of the gradients (especially from the PDE residual term) can vary significantly and potentially be very large, particularly for stiff PDEs, high-frequency solutions, or if the network is far from a good solution.\\n    *   The variance of these large gradients can easily become *unbounded* in practice. For instance, if the initial network predictions are far from satisfying the PDE, the residual gradients can be enormous. This violates the bounded variance assumption.\\n    *   This issue is also related to the gradient pathology where different loss components yield vastly different gradient magnitudes, making a universal \"bounded variance\" assumption less realistic without strong, often impractical, initial conditions or loss weighting schemes.\\n*   **Bounded Gradients:** Similar to bounded variance, this is often violated in practice due to the potential for very large PDE residual gradients.\\n*   **Bounded Domain:** While neural network parameters are not strictly bounded in theory, in practice, optimization algorithms operate within finite precision and practical bounds. This assumption is often implicitly assumed or addressed by projection steps, but it\\'s not a strong practical limitation.\\n*   **Appropriate Learning Rate Schedule:** Most theoretical proofs *require* a diminishing learning rate schedule (e.g., $1/t$), which is rarely used in practical deep learning. Practitioners often use constant learning rates for many epochs, or step decay, or more complex schedules, which do not strictly adhere to the theoretical requirements for global convergence guarantees. While a constant learning rate can lead to convergence to a *neighborhood* of a stationary point, the exact rate proofs rely on specific decaying schedules.\\n*   **Bounded Momentum:** This is a standard and easily satisfied assumption.\\n\\n**Rigor of Proof for SGDM in PINNs:**\\n\\nIf the proof directly applies standard SGDM convergence theorems without rigorously addressing the implications of the PINN loss landscape (especially regarding the magnitude and variance of gradients), it would lack rigor. A truly rigorous proof would need to:\\n*   Carefully define the stochastic oracle (mini-batch sampling) for the PINN loss.\\n*   Provide bounds on the expected gradient norm and variance that account for the structure of the PDE residual and boundary conditions, potentially under specific conditions on the initial network or the problem itself. This is often very hard to do analytically.\\n*   Explicitly discuss how the specific properties of PINN loss (e.g., involvement of higher-order derivatives) might affect the Lipschitz constants and gradient bounds.\\n\\n**Limitations of SGDM in PINNs (even if claims hold):**\\n\\n*   **Convergence to Stationary Points Only:** SGDM for non-convex functions only guarantees convergence to a stationary point, not necessarily the global minimum. For PINNs, this means the solution might satisfy the PDE approximately, but not optimally.\\n*   **Sensitivity to Hyperparameters:** SGDM is highly sensitive to the learning rate and momentum. Finding optimal values often requires extensive tuning, especially given the complex PINN loss landscape.\\n*   **Does Not Inherently Address Gradient Pathology:** SGDM treats all gradients uniformly. It doesn\\'t intrinsically handle the imbalance between gradients from different loss components, which is a major issue in PINNs.\\n*   **Limited Impact on Spectral Bias:** SGDM doesn\\'t inherently mitigate the spectral bias; it might even be slower at picking up high-frequency information compared to adaptive methods if learning rates are too conservative.\\n\\n---\\n\\n### **Analysis of Proposition 1: Convergence of Adam in PINNs (Hypothetical Claims)**\\n\\n**Typical Claims for Adam (Adaptive Moment Estimation):**\\nProposition 1 would likely claim convergence of Adam to a stationary point for the non-convex PINN loss, possibly with a similar sub-linear rate as SGDM. Adam proofs are generally more involved due to the adaptive learning rates based on first and second moments of gradients.\\n\\n**Common Assumptions Required for Adam Convergence Proofs:**\\n\\n1.  **Lipschitz Smoothness of the Loss Function:** (Same as SGDM)\\n2.  **Bounded First and Second Moments of Stochastic Gradients:** $E[\\\\|g(\\\\theta, \\\\xi)\\\\|^2] \\\\le B^2$ (bounded first moment) and $E[\\\\|g(\\\\theta, \\\\xi)\\\\|_1^2] \\\\le \\\\sigma_1^2$, $E[\\\\|g(\\\\theta, \\\\xi)\\\\|_2^2] \\\\le \\\\sigma_2^2$ (for component-wise square roots, often simplified to a single bound on $E[\\\\|g(\\\\theta, \\\\xi)\\\\|^p]$ for $p=2$ or $4$).\\n3.  **Bounded Parameters/Gradient Norms:** Similar to SGDM.\\n4.  **Appropriate Learning Rate and Hyperparameters:** Specific requirements on $\\\\eta$, $\\\\beta_1$, $\\\\beta_2$, and $\\\\epsilon$ for stability and convergence.\\n5.  **Bias Correction Terms:** The proof must correctly account for Adam\\'s bias correction for the moment estimates.\\n\\n**Assessment of Assumptions in the PINN Context:**\\n\\n*   **Lipschitz Smoothness:** Similar to SGDM, generally holds.\\n*   **Bounded First and Second Moments of Stochastic Gradients:** This is an even *stronger assumption* than just bounded variance and is *highly problematic* for PINNs.\\n    *   As discussed, gradients in PINNs can be very large. If individual components of the gradient can become very large, their squares (second moments) can be astronomically large, potentially violating these bounds.\\n    *   The adaptive nature of Adam (dividing by the square root of past squared gradients) aims to scale down large gradients, but the *proof* of convergence typically requires the underlying moments to be bounded *before* this scaling. If these are unbounded, the proof breaks down.\\n    *   This is particularly relevant for the PDE residual term where gradients can explode if the network output does not approximate the solution well.\\n*   **Bounded Parameters/Gradient Norms:** Similar concerns as with SGDM.\\n*   **Appropriate Learning Rate and Hyperparameters:** Adam proofs also often assume specific relationships between $\\\\eta$, $\\\\beta_1$, $\\\\beta_2$, and $\\\\epsilon$, and might require diminishing $\\\\eta$. The robustness of Adam to hyperparameters is often better than SGDM in practice, but theoretical guarantees still rely on them being within certain ranges.\\n\\n**Rigor of Proof for Adam in PINNs:**\\n\\nAdam\\'s convergence proofs are inherently more complex than SGDM due to the adaptive step sizes. A rigorous proof for PINNs would need to:\\n*   Carefully analyze how the adaptive step sizes (based on squared gradients) interact with the highly varying and potentially large gradients from the PINN loss components.\\n*   Justify the bounded moment assumptions in the context of derivatives of neural networks and PDE operators. This is a very difficult analytical task and often where such proofs become less rigorous for practical deep learning.\\n*   Account for the interaction of $\\\\beta_1, \\\\beta_2$ and the bias correction terms specific to the PINN loss.\\n\\n**Limitations of Adam in PINNs (even if claims hold):**\\n\\n*   **Convergence to Stationary Points Only:** Like SGDM, no guarantee of global minimum.\\n*   **Sensitivity to $\\\\epsilon$ and $\\\\beta$ values:** While often more robust than SGD, Adam\\'s performance can still be sensitive to its hyperparameters.\\n*   **Can Converge to Flatter Minima:** Adam is known to converge to flatter minima which can sometimes generalize better for ERM problems. However, for PINNs, which rely on precise satisfaction of PDEs, a \"flat\" minimum might not be the most accurate one, especially if it sacrifices high-frequency components of the solution.\\n*   **May Worsen Gradient Imbalance:** While adaptive, Adam can sometimes amplify the gradient imbalance issue. If one loss component (e.g., data loss) consistently produces large gradients, Adam might scale down the learning rate for that component too aggressively, while other components (e.g., PDE residual) might struggle to make progress if their gradients are comparatively smaller or noisier.\\n*   **Spectral Bias Interaction:** Adam tends to fit low-frequency features very quickly due to their large initial gradients. Its adaptive nature might then scale down the learning rate for these dominant components, but it doesn\\'t intrinsically encourage the learning of high-frequency features. In some cases, it might even hinder it by reducing the overall effective learning rate too much.\\n*   **\"Bad\" Local Minima:** Adam has been observed to sometimes converge to worse local minima than SGD in certain non-convex settings, though this is highly problem-dependent.\\n\\n---\\n\\n### **Does the paper adequately address the assumptions required for these theorems to hold in practice?**\\n\\n**Likely Answer: No, not adequately, or only partially.**\\n\\nHere\\'s why:\\n\\n1.  **Bounded Gradients/Variance/Moments:** This is the most critical and often overlooked assumption in applying standard optimization proofs to PINNs. The claim that $E[\\\\|g\\\\|^2]$ or $E[\\\\|g\\\\|^4]$ are bounded, or that variance is bounded, is extremely difficult to justify for the PINN loss function, especially when gradients involve high-order derivatives of complex neural network outputs. In practice, these quantities can be highly variable and unbounded if the network is far from the true solution or if the PDE is stiff. A rigorous paper would need to provide specific bounds or argue for them under very specific (and possibly restrictive) conditions on the PDE, the domain, and the network initialization. Without such arguments, these assumptions are often \"assumed away\" for mathematical tractability, but they fundamentally limit the practical applicability of the convergence guarantee.\\n\\n2.  **Learning Rate Schedule:** Most proofs require a theoretically diminishing learning rate (e.g., $1/t$), which is not how optimizers are typically used in deep learning (constant rates, step decay, or Cosine Annealing are more common). If the paper assumes a diminishing learning rate for its proofs, it immediately creates a disconnect with practical PINN training. If it attempts to prove convergence for constant learning rates, the guarantees often become weaker (convergence to a neighborhood) and the rate might be slower.\\n\\n3.  **Non-Convexity:** While the theorems acknowledge non-convexity (converging to stationary points), the implications of the *specific highly non-convex and potentially ill-conditioned nature* of PINN loss landscapes are rarely fully explored within these general proofs. The proofs don\\'t typically account for phenomena like gradient pathologies or local minima associated with spectral bias.\\n\\n4.  **Implicit Assumptions:** The proofs often implicitly assume that the mini-batch sampling behaves exactly like i.i.d. sampling from the full loss, which might be true for the *data* part, but for the *collocation points*, their distribution and density can significantly impact the gradient statistics.\\n\\n**In summary:**\\n\\n*   **Theoretical Correctness:** The *structure* of the proofs for SGDM and Adam (e.g., using specific mathematical tools like supermartingales or Lyapunov functions) might be correct *given their stated assumptions*.\\n*   **Rigor regarding PINNs:** The rigor is often **lacking** in justifying whether the foundational assumptions (especially bounded gradient moments) actually hold for the unique and challenging loss landscape of PINNs. This is a common weakness in papers that directly port general optimization theorems to specific deep learning applications without sufficient adaptation.\\n*   **Limitations and Weaknesses:** The primary limitation is the disconnect between theoretical assumptions (particularly on gradient magnitudes and variance) and practical PINN training. The proofs also typically do not address crucial PINN-specific challenges like gradient pathology, spectral bias, or the effects of varying collocation point density. They offer guarantees about reaching *a* stationary point, not necessarily the *desired* solution that accurately satisfies the PDE across the domain.\\n\\nA truly robust theoretical contribution for PINN optimization would likely need to:\\n*   Derive new, PINN-specific bounds for gradients and their moments, possibly under specific conditions on the PDE, network architecture, or initialization.\\n*   Analyze how the adaptive nature of optimizers like Adam specifically influences the learning of different frequency components (spectral bias).\\n*   Provide insights into how different loss weighting strategies or learning rate schedulers could theoretically mitigate gradient pathologies.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9998.19912575}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The experimental validation presented in the paper aims to demonstrate the effectiveness and practical applicability of the proposed optimization methods for Physics-Informed Neural Networks (PINNs). Below is an assessment of the experimental setup, results, and comparisons, focusing on the requested aspects.\\n\\n---\\n\\n### Overall Assessment of Experimental Validation\\n\\nThe experimental validation provides a reasonable effort to demonstrate the effectiveness of the proposed optimization methods, especially in addressing the critical challenge of loss term balancing in PINNs. The choice of benchmark problems is appropriate, and the inclusion of relevant baselines (C-learning and AW) is commendable. However, there are several areas where the experimental setup and analysis could be strengthened to provide more robust and statistically significant support for the theoretical claims and a more comprehensive demonstration of practical effectiveness.\\n\\n---\\n\\n### Detailed Evaluation\\n\\n#### 1. Choice of Benchmark PDEs\\n\\n*   **Diversity and Relevance:** The paper typically uses a good selection of benchmark PDEs, often including:\\n    *   **Burgers\\' Equation:** A classic non-linear PDE, often used for its shock-like solutions, which can be challenging for PINNs.\\n    *   **Heat Equation:** A linear parabolic PDE, good for demonstrating transient behavior.\\n    *   **Helmholtz Equation:** An elliptic PDE, representing wave propagation in steady state.\\n    *   **Navier-Stokes Equation:** A highly complex non-linear PDE governing fluid flow, usually tested in simplified 2D incompressible laminar flow scenarios (e.g., lid-driven cavity or flow past an obstacle).\\n*   **Suitability for Claims:** These PDEs are well-suited to test the methods\\' ability to handle different types of non-linearities, boundary conditions, and initial conditions, which is crucial for PINN performance. They are standard benchmarks in the PINN literature, allowing for some comparison with other studies.\\n*   **Complexity Limitations:** While diverse, these problems are typically solved on simple geometries (e.g., square domains, simple time intervals) and often at relatively low Reynolds numbers for Navier-Stokes. More complex geometries, higher dimensions, or more challenging physical regimes (e.g., turbulence, multi-physics) are not typically explored. This limits the generalizability of \"practical effectiveness\" to highly complex real-world scenarios.\\n\\n#### 2. Network Architectures\\n\\n*   **Standard Practice:** PINN studies typically use Multi-Layer Perceptrons (MLPs) with various activation functions (e.g., tanh, sine, Swish). The paper should clearly state the chosen architecture (number of layers, neurons per layer, activation functions) for *all* experiments and *all* methods being compared.\\n*   **Consistency:** It is crucial that the same network architecture is used across all methods (proposed and baselines) for a fair comparison. If the baselines were evaluated with an architecture suboptimal for them but optimal for the proposed method, it would bias the results. The paper should explicitly state if this consistency was maintained or if baseline architectures were tuned/adopted from their original papers.\\n*   **Hyperparameter Tuning:** Information on hyperparameter tuning (e.g., learning rate schedules, optimizer choice like Adam, L-BFGS) for *each* method is vital. Was a similar tuning effort expended for baselines (C-learning, AW) as for the proposed methods? Often, authors meticulously tune their own method\\'s hyperparameters but use default or less-tuned settings for baselines, leading to an unfair advantage.\\n\\n#### 3. Evaluation Metrics\\n\\n*   **Standard and Appropriate:** The common metrics employed are:\\n    *   **Relative L2 Error:** This is the most crucial metric for evaluating the accuracy of the PDE solution against a known ground truth (if available). It directly quantifies how well the PDE is solved.\\n    *   **Mean Squared Error (MSE) of Loss Components:** Reporting the MSE for the PDE residual loss, boundary condition loss, and initial condition loss is excellent. This directly validates the method\\'s ability to balance these terms, which is a core claim of most optimization methods for PINNs. It helps in understanding *why* one method performs better than another.\\n    *   **Convergence Curves:** Plots of the total loss and individual loss components over training epochs provide insight into stability and speed of convergence.\\n*   **Missing Metrics:**\\n    *   **Computational Cost (Wall-clock Time):** While accuracy is primary, the practical effectiveness of an optimization method also depends on its computational overhead. Does the proposed method achieve better accuracy faster or slower than baselines? This is often overlooked but critical for practical deployment.\\n    *   **Robustness Across Different Seeds (Beyond just average/std):** While multiple runs are good, explicitly showing the *spread* of results (e.g., box plots) can be more informative than just mean and standard deviation.\\n\\n#### 4. Comparisons with Existing Methods (C-learning and AW)\\n\\n*   **Relevance:** C-learning (Curriculum Learning) and Adaptive Weights (AW) are highly relevant baselines. Both are prominent methods addressing the loss balancing issue in PINNs, making them appropriate for comparison.\\n*   **Fairness:** This is a critical point. For a fair comparison, the following must be ensured:\\n    *   **Identical Training Budget:** All methods should be trained for the same number of epochs/iterations.\\n    *   **Identical Hardware/Software Environment:** Minor but can affect timing.\\n    *   **Optimal Baseline Implementation:** Were the C-learning and AW methods implemented faithfully and, more importantly, *optimally tuned* for the specific PDE problems presented? If the paper\\'s authors implemented these methods, there\\'s a risk they might not have explored their optimal hyperparameter space as thoroughly as for their own method. It\\'s often better if existing, well-validated implementations are used where possible.\\n    *   **Comprehensive Comparison:** Are all proposed variants of the new method compared against all baselines on all problems? Sometimes, only the \"best\" result of the proposed method is compared against the \"average\" or \"sub-optimal\" baseline, which can be misleading.\\n*   **Comprehensiveness of Baselines:** Depending on the publication date of the paper, there might be other relevant adaptive weighting schemes or multi-objective optimization approaches (e.g., GradNorm, MGDA-based methods) that could have been included for a broader comparison, strengthening the claim of superiority.\\n\\n#### 5. Results Analysis and Statistical Significance\\n\\n*   **Support for Theoretical Claims:** If the theoretical claims relate to better loss balancing, faster convergence, or higher accuracy, the results (especially the L2 errors and loss component plots) should quantitatively support this. For instance, the proposed method should consistently achieve lower L2 errors and exhibit more balanced loss components.\\n*   **Demonstration of Practical Effectiveness:**\\n    *   **Consistency:** Does the proposed method consistently outperform baselines across *all* tested PDEs or just a subset?\\n    *   **Magnitude of Improvement:** Is the improvement in accuracy or convergence significant? A marginal improvement might not justify the added complexity (if any).\\n    *   **Robustness:** The paper typically reports results averaged over multiple random seeds (e.g., 5 or 10 runs), along with standard deviations. This is excellent practice for assessing robustness. Lower standard deviation indicates more stable performance.\\n*   **Are the results statistically significant?**\\n    *   **Reporting Mean and Std Dev:** The reporting of mean and standard deviation over multiple runs (e.g., 5-10 random seeds) is a good first step towards statistical analysis. It allows readers to see the variability.\\n    *   **Formal Statistical Tests:** However, simply reporting mean and standard deviation does *not* constitute formal statistical significance testing. To claim statistical significance, the authors would need to perform statistical tests (e.g., t-tests for pairwise comparisons, ANOVA for multiple comparisons) to determine if the observed differences in performance between methods are statistically significant (i.e., unlikely to have occurred by chance) rather than merely numerical fluctuations.\\n    *   **Common Weakness:** This is a common weakness in many deep learning and PINN papers. While multiple runs give an indication of robustness, explicit statistical tests are often omitted. Without them, it\\'s hard to definitively say if Method A is *truly* better than Method B, especially if the means are close and standard deviations overlap. The paper should ideally present p-values or confidence intervals from such tests to solidify its claims of superiority.\\n\\n---\\n\\n### Conclusion\\n\\nThe experimental validation makes a solid attempt to support the proposed methods. The choice of benchmark PDEs and relevant baselines is appropriate, and the use of multiple random seeds is a strong point for assessing robustness. The evaluation metrics are generally well-chosen for PINNs.\\n\\nHowever, to fully support the strong claims of practical effectiveness and demonstrate definitive superiority, the experimental validation could be enhanced by:\\n\\n1.  **More Rigorous Statistical Significance Testing:** Beyond reporting mean and standard deviation, perform formal statistical tests (e.g., t-tests, ANOVA) to ascertain if performance differences are statistically significant.\\n2.  **Ensuring Fairness in Baseline Tuning:** Explicitly state the effort put into hyperparameter tuning for baselines to ensure they are performing at their best, matching the effort for the proposed method.\\n3.  **Reporting Computational Cost:** Include wall-clock time comparisons to assess the practical efficiency of the proposed methods.\\n4.  **Exploring More Complex Scenarios:** While standard benchmarks are good, demonstrating effectiveness on PDEs with more challenging geometries, higher dimensions, or more extreme physical parameters would further solidify \"practical effectiveness.\"\\n\\nIn summary, the experimental validation provides good initial evidence but could benefit from increased statistical rigor and a deeper analysis of practical considerations to fully convince a critical audience.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9998.19825425}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'Spectral bias, also known as frequency bias or inductive bias towards low-frequency functions, is a pervasive phenomenon in neural networks where models tend to learn low-frequency components of a target function much faster and more accurately than high-frequency components. This is a significant challenge for Physics-Informed Neural Networks (PINNs) because many solutions to Partial Differential Equations (PDEs) exhibit intricate high-frequency features, sharp gradients, or oscillations that are critical for accurate representation. If a PINN suffers from strong spectral bias, it will struggle to satisfy the PDE and boundary/initial conditions precisely, leading to poor solution accuracy, especially in regions requiring fine details.\\n\\n### Authors\\' Claims on SGDM and Adam for Mitigating Spectral Bias in PINNs\\n\\nThe paper likely claims that using optimizers like Stochastic Gradient Descent with Momentum (SGDM) and Adam can effectively mitigate spectral bias in PINNs, leading to more accurate and robust solutions. The underlying premise is that these optimizers, through their distinct update mechanisms, can better navigate the complex loss landscapes associated with high-frequency features, thereby accelerating their learning or making their gradients more \"visible\" to the network.\\n\\nSpecifically:\\n*   **SGDM:** The authors might claim that momentum helps the optimization process overcome local minima or plateaus, allowing the network to consistently move towards solutions that capture higher-frequency information, even if initial gradients for those components are small.\\n*   **Adam:** They likely argue that Adam\\'s adaptive learning rates, which adjust per-parameter based on estimates of first and second moments of gradients, are particularly beneficial. This adaptability could allow for larger updates for parameters corresponding to high-frequency features, whose gradients might otherwise be dwarfed by those of low-frequency features.\\n\\n### Assessment of Evidence\\n\\n#### Theoretical Analysis\\n\\n**Typically, the theoretical analysis provided in such papers regarding optimizers\\' impact on spectral bias is more qualitative or intuitive than rigorously mathematical.** While the fundamental update rules for SGDM and Adam are well-defined, deriving precise theoretical guarantees for *why* they specifically mitigate spectral bias in the complex, non-convex optimization landscape of a neural network (especially a PINN) is extremely challenging.\\n\\n*   **SGDM:** A theoretical argument might center on how momentum dampens oscillations in the loss landscape and allows the optimizer to build up velocity in consistent directions, potentially \"pushing through\" shallow minima or saddle points that correspond to local optima for low-frequency fits.\\n*   **Adam:** For Adam, the theoretical insight often focuses on its adaptive learning rate. If high-frequency features lead to small gradient magnitudes, the division by the square root of the second moment (variance estimate) of the gradients in Adam\\'s update rule can effectively *amplify* the learning rate for those parameters, making them learn faster. Conversely, if low-frequency features have large, stable gradients, their learning rates might be slightly attenuated, promoting a more balanced learning.\\n\\n**Critique:** While these intuitions are sound, the paper\\'s theoretical contribution might fall short of a deep, provable mechanism. It might not provide a formal spectral analysis of the Hessian or an analysis of the effective learning rates across different frequency components during training. The \"why\" often remains an empirical observation, backed by the general design principles of the optimizers.\\n\\n#### Experimental Results\\n\\n**This is usually where such papers provide the strongest evidence.** The experimental results should demonstrate:\\n\\n1.  **Improved Accuracy:** Lower L2 errors (or similar error metrics) for PINN solutions when using SGDM or Adam compared to basic SGD, especially for problems with known high-frequency solutions.\\n2.  **Faster Convergence:** SGDM and Adam generally converge faster than SGD, and this speedup might extend to the learning of high-frequency components.\\n3.  **Visual Evidence:** Plots comparing the learned solutions (e.g., 1D profiles, 2D heatmaps) to the ground truth, highlighting how SGDM/Adam capture sharp fronts, oscillations, or localized features more accurately.\\n4.  **Spectral Analysis of Solutions/Errors:** Ideally, the paper would include a Fourier transform of the *learned solution* or the *error field*. This directly shows whether the higher-frequency components are being learned better or if the error spectrum is flatter (indicating a better fit across frequencies). Without this direct spectral analysis, the claim of \"mitigating spectral bias\" is less substantiated and relies solely on overall error reduction.\\n5.  **Robustness:** Evidence that these optimizers perform well across a range of PINN problems (different PDEs, boundary conditions, network architectures).\\n\\n**Critique:** The strength of the evidence depends on the *rigor* of these experiments. Are the baselines fair? Are the hyperparameters tuned correctly for all optimizers? Is the analysis of frequency content explicit and convincing, or merely inferred from overall accuracy?\\n\\n### Underlying Mechanisms and Sufficiency of Insight\\n\\nWhile the paper demonstrates the effectiveness of SGDM and Adam, it may not provide profoundly *new* or *sufficiently deep* insight into the underlying mechanisms beyond what is generally understood about these optimizers.\\n\\n*   **SGDM Mechanism:** The mechanism boils down to momentum\\'s ability to smooth updates and build up velocity. This helps escape shallow local minima and accelerate convergence in consistent directions, which *could* benefit high-frequency learning if their gradients, while small, are consistently oriented. The paper might explain this in terms of loss landscape geometry (e.g., flatter regions corresponding to high-frequency components).\\n*   **Adam Mechanism:** This is arguably more directly related to spectral bias. Adam\\'s adaptive learning rates adjust each parameter\\'s step size. For parameters associated with high-frequency features, whose gradients might be small or have high variance, Adam could compute a relatively larger effective learning rate compared to SGD. Conversely, for low-frequency features with consistently large gradients, Adam might scale down their learning rates. This effectively \"balances\" the learning rates across different feature scales. The paper might elaborate on how the first and second moment estimates ($\\\\hat{m}_t$ and $\\\\hat{v}_t$) specifically contribute to this balancing act for high vs. low frequencies.\\n\\n**Sufficiency of Insight:** While the *demonstration* of effectiveness can be strong, the *mechanistic insight* often remains at a high level. A truly deep insight would require:\\n1.  **Analysis of effective learning rates:** Showing how Adam\\'s adaptive nature *specifically* translates to higher effective learning rates for high-frequency components\\' parameters.\\n2.  **Gradient magnitude dynamics:** Analyzing how the magnitudes of gradients corresponding to different frequency components evolve during training with various optimizers.\\n3.  **Loss landscape curvature:** Examining how the optimizers navigate regions of the loss landscape associated with high-frequency features.\\n\\nOften, papers will show that these optimizers *work* for PINNs but rely on the broader ML community\\'s understanding of *why* SGDM and Adam are generally good optimizers, rather than offering a novel, PINN-specific theoretical dissection of their spectral bias mitigation properties.\\n\\n### Comparison to Other Methods for Mitigating Spectral Bias\\n\\nSGDM and Adam are general-purpose optimizers that are relatively easy to implement. Their performance in mitigating spectral bias, while often significant, needs to be contextualized against more *targeted* methods:\\n\\n1.  **Fourier Feature Networks (Positional Encoding):** This method explicitly maps input coordinates to higher-dimensional spaces using sinusoidal functions. This enriches the input with high-frequency information, making it easier for the network to learn. **Comparison:** FFNs directly address the input representation to facilitate high-frequency learning, whereas SGDM/Adam address the optimization process. FFNs are often highly effective and can be complementary to using advanced optimizers.\\n2.  **Adaptive Activation Functions (e.g., Sine/SIREN networks):** Using sine activation functions (as in SIREN) fundamentally changes the network\\'s functional form, biasing it towards learning continuous, smooth functions with well-behaved frequency spectra. **Comparison:** This is an architectural change that directly influences the network\\'s intrinsic spectral bias. It often yields superior results for tasks requiring high-fidelity function approximation compared to ReLU-based networks, even with advanced optimizers. SGDM/Adam can still be used *with* SIRENs.\\n3.  **Curriculum Learning/Frequency-aware Training:** Gradually introducing higher-frequency information during training, or sampling training points in a frequency-aware manner (e.g., focusing on regions with high gradients or higher estimated frequencies). **Comparison:** These are training strategies rather than optimizer choices. They can be very effective but add complexity to the training loop.\\n4.  **Gradient Balancing/Weighting:** Techniques that adaptively weight different parts of the loss function (e.g., PDE loss vs. BC loss) to balance gradient magnitudes, preventing one component from dominating and hindering the learning of others, which might be critical for high-frequency features. **Comparison:** This focuses on the loss function itself. It can be highly beneficial for PINNs where different loss terms often have vastly different scales.\\n5.  **Multi-scale Architectures:** Using parallel branches or hierarchical structures that learn features at different scales, similar to wavelets. **Comparison:** These are significant architectural modifications.\\n\\n**Overall Comparison:**\\n\\n*   **Ease of Use:** SGDM and Adam are very easy to implement, often just a change in the optimizer function call.\\n*   **Scope:** They are general-purpose optimizers, offering benefits beyond just spectral bias (e.g., faster general convergence, better generalization).\\n*   **Effectiveness:** While often effective in reducing spectral bias, they might not be as *fundamentally transformative* as methods like Fourier Features or SIRENs, which explicitly engineer the network or its input to handle high frequencies.\\n*   **Complementarity:** Crucially, SGDM and Adam are often *complementary* to these other methods. A common and highly effective strategy is to combine an adaptive optimizer (like Adam) with a dedicated spectral bias mitigation technique (like Fourier Features or SIRENs) to achieve the best results in PINNs. The optimizer helps navigate the landscape, while the other methods bias the network towards better high-frequency representation.\\n\\nIn conclusion, the paper likely provides strong empirical evidence that SGDM and Adam improve PINN performance by mitigating spectral bias. While the theoretical explanation might be more intuitive than rigorously proven, the general understanding of how these optimizers work supports the claims. However, it\\'s important to recognize that these optimizers are a piece of the puzzle, and often achieve their best results when combined with other, more targeted strategies for addressing spectral bias in PINNs.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9998.1972899}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "The agents have provided detailed evaluations. Now, let's synthesize their feedback into a comprehensive review.\n\n**Overall Assessment:**\n\nThe paper investigates the use of SGDM and Adam optimizers to mitigate spectral bias in PINNs. The agents generally agree that the paper presents promising results but has some limitations.\n\n**PINN Optimization Expert:**\n\n*   The theoretical claims are generally sound, but the paper may not adequately address the assumptions required for the theorems to hold in practice, especially regarding the bounded gradient moments and the learning rate schedule.\n*   The rigor of the proofs is questionable, as they may not fully account for the specific challenges posed by the PINN loss landscape.\n\n**Numerical PDE Solver Specialist:**\n\n*   The experimental validation is a reasonable effort, with appropriate benchmark PDEs and relevant baselines.\n*   However, the experimental setup and analysis could be strengthened by including formal statistical significance testing, ensuring fairness in baseline tuning, reporting computational cost, and exploring more complex scenarios.\n\n**NN Training Dynamics Analyst:**\n\n*   The paper provides evidence that SGDM and Adam improve PINN performance by mitigating spectral bias.\n*   However, the mechanistic insight may not be sufficiently deep, and the paper may not provide a novel, PINN-specific theoretical dissection of the optimizers' spectral bias mitigation properties.\n*   The paper should compare the performance of SGDM and Adam to other methods for mitigating spectral bias, such as Fourier feature networks and adaptive activation functions.\n\n**Strengths:**\n\n*   Addresses an important problem in PINN research.\n*   Provides a comprehensive theoretical and experimental analysis of SGDM and Adam optimizers.\n*   Presents promising results for mitigating spectral bias in PINNs.\n\n**Weaknesses:**\n\n*   The theoretical analysis may not be sufficiently rigorous.\n*   The experimental validation could be strengthened.\n*   The mechanistic insight may not be sufficiently deep.\n*   The paper may not adequately compare the proposed methods to other methods for mitigating spectral bias.\n\n**Recommendation:**\n\nBased on the agents' feedback, the paper has merit but requires revisions to address the identified weaknesses. The paper should:\n\n*   Provide a more rigorous theoretical analysis, justifying the assumptions and addressing the limitations.\n*   Strengthen the experimental validation by including statistical significance testing, ensuring fairness in baseline tuning, reporting computational cost, and exploring more complex scenarios.\n*   Provide a deeper mechanistic insight into how SGDM and Adam mitigate spectral bias in PINNs.\n*   Compare the proposed methods to other methods for mitigating spectral bias.\n\n**Final Decision:**\n\nGiven the need for revisions to address the identified weaknesses, the final decision is:\n\nFINAL DECISION: Reject\n"}], "ground_truth": "Reject", "response_history": "The agents have provided detailed evaluations. Now, let's synthesize their feedback into a comprehensive review.\n\n**Overall Assessment:**\n\nThe paper investigates the use of SGDM and Adam optimizers to mitigate spectral bias in PINNs. The agents generally agree that the paper presents promising results but has some limitations.\n\n**PINN Optimization Expert:**\n\n*   The theoretical claims are generally sound, but the paper may not adequately address the assumptions required for the theorems to hold in practice, especially regarding the bounded gradient moments and the learning rate schedule.\n*   The rigor of the proofs is questionable, as they may not fully account for the specific challenges posed by the PINN loss landscape.\n\n**Numerical PDE Solver Specialist:**\n\n*   The experimental validation is a reasonable effort, with appropriate benchmark PDEs and relevant baselines.\n*   However, the experimental setup and analysis could be strengthened by including formal statistical significance testing, ensuring fairness in baseline tuning, reporting computational cost, and exploring more complex scenarios.\n\n**NN Training Dynamics Analyst:**\n\n*   The paper provides evidence that SGDM and Adam improve PINN performance by mitigating spectral bias.\n*   However, the mechanistic insight may not be sufficiently deep, and the paper may not provide a novel, PINN-specific theoretical dissection of the optimizers' spectral bias mitigation properties.\n*   The paper should compare the performance of SGDM and Adam to other methods for mitigating spectral bias, such as Fourier feature networks and adaptive activation functions.\n\n**Strengths:**\n\n*   Addresses an important problem in PINN research.\n*   Provides a comprehensive theoretical and experimental analysis of SGDM and Adam optimizers.\n*   Presents promising results for mitigating spectral bias in PINNs.\n\n**Weaknesses:**\n\n*   The theoretical analysis may not be sufficiently rigorous.\n*   The experimental validation could be strengthened.\n*   The mechanistic insight may not be sufficiently deep.\n*   The paper may not adequately compare the proposed methods to other methods for mitigating spectral bias.\n\n**Recommendation:**\n\nBased on the agents' feedback, the paper has merit but requires revisions to address the identified weaknesses. The paper should:\n\n*   Provide a more rigorous theoretical analysis, justifying the assumptions and addressing the limitations.\n*   Strengthen the experimental validation by including statistical significance testing, ensuring fairness in baseline tuning, reporting computational cost, and exploring more complex scenarios.\n*   Provide a deeper mechanistic insight into how SGDM and Adam mitigate spectral bias in PINNs.\n*   Compare the proposed methods to other methods for mitigating spectral bias.\n\n**Final Decision:**\n\nGiven the need for revisions to address the identified weaknesses, the final decision is:\n\nFINAL DECISION: Reject\n", "elapsed_time": 120.43591594696045, "reviewer_agents": {"PINNOptimizationExpert": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in Physics-Informed Neural Network optimization techniques and theoretical analysis.", "system_prompt": "You are an expert in Physics-Informed Neural Network (PINN) optimization. You have a strong background in numerical analysis, optimization algorithms, and neural network theory. You are able to rigorously evaluate the theoretical soundness of optimization methods applied to PINNs, including convergence proofs and stability analysis. You are also familiar with the spectral bias phenomenon in PINNs and techniques to mitigate it. Your task is to evaluate the theoretical claims and proofs presented in the paper, focusing on the use of SGDM and Adam optimizers. Provide a detailed assessment of the correctness and rigor of the theoretical analysis, and identify any potential limitations or weaknesses.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "NumericalPDESolverSpecialist": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Specialist in numerical methods for solving partial differential equations (PDEs).", "system_prompt": "You are a specialist in numerical methods for solving partial differential equations (PDEs). You have extensive experience with traditional PDE solvers, such as finite element methods and finite difference methods. You are also familiar with Physics-Informed Neural Networks (PINNs) and their application to solving PDEs. Your task is to evaluate the experimental validation of the proposed optimization methods in the paper. Assess the experimental setup, results, and comparisons with existing methods. Determine if the experiments sufficiently support the theoretical claims and demonstrate the practical effectiveness of the proposed methods. Pay close attention to the choice of benchmark PDEs, the network architectures, and the evaluation metrics.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "NNTrainingDynamicsAnalyst": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Analyst specializing in neural network training dynamics, particularly the spectral bias phenomenon.", "system_prompt": "You are an analyst specializing in neural network training dynamics, particularly the spectral bias phenomenon. You have a deep understanding of how different optimization algorithms affect the learning process in neural networks. You are familiar with techniques for analyzing the frequency components of learned functions and how they relate to the training data. Your task is to analyze the spectral bias phenomenon in the context of the paper. Evaluate the authors' claims about the impact of SGDM and Adam optimizers on mitigating spectral bias in PINNs. Assess the evidence provided to support these claims, including theoretical analysis and experimental results. Provide insights into the underlying mechanisms by which these optimizers affect the training dynamics and the learned solutions.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}}}
