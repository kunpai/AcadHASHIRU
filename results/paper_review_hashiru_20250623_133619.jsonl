{"paper_id": "Th98b8dH4yr", "prompt": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Tangential Wasserstein Projections\n\n1 INTRODUCTION: The concept of projections, that is, approximating a target quantity of interest by an optimally weighted combination of other quantities, is of fundamental relevance in statistics. Projections are generally defined between random variables in appropriately defined linear spaces (e.g. van der Vaart, 2000, chapter 11). In modern statistics and machine learning applications, the objects of interest are often probability measures themselves. Examples range from object- and functional data (e.g. Marron & Alonso, 2014) to causal inference with individual heterogeneity (e.g. Athey & Imbens, 2015). We introduce a notion of projection between sets of probability measures supported on Euclidean spaces. The proposed definition is applicable between sets of general probability measures with different supports and possesses good computational and statistical properties. It also provides a unique solution to the projection problem under mild conditions and can replicate the geometric properties of the target measure, such as its shape and support. To achieve this, we work in the 2- Wasserstein space, that is, the set of all probability measures with finite second moments equipped with the 2-Wasserstein distance. Importantly, we focus on the multivariate setting, i.e. we consider the Wasserstein space over some Euclidean space Rd, denoted byW2, where the dimension d can be high. The multivariate setting poses particular challenges from a mathematical, computational, and statistical perspective. In particular, W2 is a positively curved metric space for d > 1 (e.g. Ambrosio et al., 2008, Kloeckner, 2010). Moreover, the 2-Wasserstein distance between two probability measures is defined as the value function of the Monge-Kantorovich optimal transportation problem (Villani, 2003, chapter 2), which does not have a closed-form solution in multivariate settings. This is coupled with a wellknown statistical curse of dimensionality for general measures (Ajtai et al., 1984, Dudley, 1969, Fournier & Guillin, 2015, Talagrand, 1992; 1994, Weed & Bach, 2019). These challenges have impeded the development of a method of projections between potentially high-dimensional probability measures. A focus so far has been on the univariate and lowdimensional setting. In particular, Chen et al. (2021), Ghodrati & Panaretos (2022), and Pegoraro & Beraha (2021) introduced frameworks for distribution-on-distribution regressions in the univariate setting for object data. Bigot et al. (2014), Cazelles et al. (2017) developed principal component analyses on the space of univariate probability measures using geodesics on the Wasserstein space. The most closely related works to ours are Bonneel et al. (2016) and Werenski et al. (2022). The former develops a regression approach in barycentric coordinates with applications in computer graphics as well as color and shape transport problems. Their method requires solving a computationally costly bilevel optimization problem, which does not need to achieve a global solution. The latter works on a tangential structure like we do, but is based on \u201cKarcher means\u201d (Karcher, 2014, Zemel & Panaretos, 2019). This implies that their method works between absolutely continuous measures with densities that are bounded away from zero, with the target measure lying in the convex hull of the control measures. The notion of projection we propose in this article circumvents these challenges in the multivariate setting by lifting the projection problem to the regular tangent space to W2 at the target measure based on generalized geodesics. These tangent spaces exist when the target measure is regular, i.e., if it does not give mass to sets of lower Hausdorff dimension. For general measures, we construct a regular tangent space using barycentric projections (Ambrosio et al., 2008, appendix 12). In contrast to the existing approaches, our method works for general probability measures, allows for the target measure to be outside the generalized geodesic convex hull of the control measures, and can be implemented by a standard constrained linear regression. The method hence transforms the projection problem on the positively curved Wasserstein space into a linear optimization problem in the regular tangent space, which provides a unique solution to the projection problem under mild assumptions. This problem takes the form of a deformable template (Boissard et al., 2015, Yuille, 1991), which connects our approach to this literature. The method can be implemented in three steps: (i) obtain the general tangent cone structure at the target measure, (ii) construct a regular tangent space if it does not exist, and (iii) perform a linear regression to carry out the projection in the tangent space. The challenging part of the implementation is lifting the problem to the tangential structure: this requires computing the corresponding optimal transport plans between the target and each measure used in the projection. Many methods have been developed for this, see for instance Benamou & Brenier (2000), Jacobs & L\u00e9ger (2020), Makkuva et al. (2020), Peyr\u00e9 & Cuturi (2019), Ruthotto et al. (2020) and references therein. Other alternatives compute approximations of the optimal transport plans via regularized optimal transport problems (Peyr\u00e9 & Cuturi, 2019), such as entropy regularized optimal transport (Galichon & Salani\u00e9, 2010, Cuturi, 2013). The proposed projection approach is compatible with any such method. We provide results for the statistical consistency when estimating the measures via their empirical counterparts in practice. To demonstrate the efficiency and utility of the proposed method, we extend the classical synthetic control estimator (Abadie & Gardeazabal, 2003, Abadie et al., 2010) to settings with observed individual heterogeneity in multivariate outcomes. This lets us perform the synthetic control method on the joint distribution of several outcomes and also estimate one set of optimal weights over all preintervention time periods, which complements the recently introduced method in Gunsilius (2022) designed for univariate outcomes. We apply this synthetic controls estimator to estimate the causal effect of Medicaid expansion on the population in individuals states and find that the policy induces nontrivial, positive effects on Medicaid enrolment, earnings, and labor supply, while its effect on employment agrees with some previous estimates, but less in magnitude compared to the other effects we estimated. 2 METHODOLOGY: 2.1 THE 2-WASSERSTEIN SPACEW2(Rd) For probability measures PX , PY \u2208 P(Rd) with supports X ,Y \u2286 Rd, respectively, the 2- Wasserstein distance W2(PX , PY ) is defined as W2(PX , PY ) , ( min \u03b3\u2208\u0393(PX ,PY ) \u222b X \u00d7Y |x\u2212 y|2 d\u03b3(x, y) ) 1 2 . (2.1) Here, | \u00b7 | denotes the Euclidean norm on Rd and \u0393(PX , PY ) , { \u03b3 \u2208P(Rd \u00d7 Rd) : (\u03c01)#\u03b3 = PX , (\u03c02)#\u03b3 = PY } is the set of all couplings of PX and PY . The maps \u03c01 and \u03c02 are the projections onto the first and second coordinate, respectively, and T#P denotes the pushforward measure of P via T , i.e. for any measurable A \u2286 Y , T#P (A) \u2261 P (T\u22121(A)). An optimal coupling \u03b3 \u2208 \u0393(PX , PY ) solving the optimal transport problem equation 2.1 is an optimal transport plan. By Prokhorov\u2019s theorem, a solution always exists in our setting. When PX is regular, i.e. when it does not give mass to sets of lower Hausdorff dimension in its support, then the optimal transport plan \u03b3 solving equation 2.1 is unique and takes the form \u03b3 = (Id\u00d7\u2207\u03d5)#PX , where Id is the identity map on Rd and \u2207\u03d5(x) is the gradient of some convex function. This result is known as Brenier\u2019s theorem (Brenier, 1991, McCann, 1997, Villani, 2003, Theorem 2.12). By definition, all measures that possess a density with respect to Lebesgue measure are regular. In the following we will distinguish between regular and general measures. The 2-Wasserstein spaceW2 , (P2(Rd),W2) is the metric space defined on the set P2(Rd) of all probability measures with finite second moments supported on Rd, with the 2-Wasserstein distance as the metric. It is a geodesically complete space in the sense that between any two measures P, P \u2032 \u2208 W2, one can define a geodesic Pt : [0, 1]\u2192W2 via the interpolation Pt , (\u03c0t)#\u03b3, where \u03b3 is an optimal transport plan and \u03c0t : Rd \u00d7 Rd \u2192 Rd is defined through \u03c0t(x, y) , (1\u2212 t)x+ ty (Ambrosio et al., 2008, McCann, 1997). Using this, it can be shown thatW2 is a positively curved metric space d > 1 (Ambrosio et al., 2008, Theorem 7.3.2) and flat for d = 1 (Kloeckner, 2010), where curvature is defined in the sense of Aleksandrov (Aleksandrov, 1951). This difference in the curvature properties is the main reason for why the multivariate setting requires different approaches compared to the established results for measures on the real line. 2.2 PROJECTIONS INW2 , BARYCENTERS, AND EXISTING CHALLENGES In linear spaces, the idea of projecting an element on a set of other elements is a linear or convex combination of these elements in the set. The analogues of averages in vector spaces are Fr\u00e9chet means or barycenters in metric spaces. For Wasserstein spaces, this concept has been introduced in our setting in Agueh & Carlier (2011) and in a more abstract sense in Carlier & Ekeland (2010). For any collection of probability measures { Pj } 16j6J \u2286 W2, their weighted barycenter P\u0304 (\u03bb) for given weights \u03bb \u2261 (\u03bb1, . . . , \u03bbJ) \u2208 \u2206J is a solution of the following minimization problem: P\u0304 (\u03bb) \u2208 arg min P\u2208P2(Rd) J\u2211 j=1 \u03bbj 2 W 22 (P, Pj). (2.2) The weights \u03bb are defined to lie in the J-dimensional probability simplex \u2206J of nonnegative vectors in RJ that sum to unity. Prokhorov\u2019s theorem implies that a solution to equation 2.2 exists (Agueh & Carlier, 2011, Proposition 2.3). Throughout, we focus on the case where \u03bb \u2208 \u2206J , because this provides a natural notion of interpolation between the given measures Pj . We could also construct a linear projection that corresponds to a \u201cgeodesic extrapolation\u201d by relaxing the requirement that all weights \u03bbj need to be non-negative. In the classical setting of random variables mapping to some Euclidean space and not random measures, this relaxation is the natural analogue to a linear regression, see Abadie et al. (2015) for a discussion. We focus on the interpolation setting because it is in line with the notion of a projection onto the convex hull spanned by other elements. All of our results can be extended to the extrapolation setting in principle. One way to extend the notion of projection between a target probability measure P0 and a set of control measures {Pj}j=1,...,J inW2 would hence consist in finding the optimal weights \u03bb\u2217 \u2208 \u2206J for which an induced barycenter P\u0304 (\u03bb\u2217) solving equation 2.2 is as close as possible to P0. This would lead to the following bi-level optimization problem, assuming that the barycenter P\u0304 (\u03bb) is unique for given \u03bb: \u03bb\u2217 \u2208 arg min \u03bb\u2208\u2206J W2(P0, P\u0304 (\u03bb)), where P\u0304 (\u03bb) = arg min P\u2208P2(Rd) J\u2211 j=1 \u03bbj 2 W 22 (P, Pj). (2.3) A version of this approach is used in Bonneel et al. (2016) to define a notion of regression between probability measures in low dimensions. The challenges with this approach are mathematical and computational. Importantly, the optimal weights \u03bb\u2217 need not be unique. This is not an issue for the applications considered in Bonneel et al. (2016), like color transport; however, it is important in statistical settings when the weights convey information used in further procedures, like causal inference via synthetic controls, where the optimal weights are used to introduced a counterfactual outcome of a treated unit had it not been treated (Abadie & Gardeazabal, 2003, Abadie et al., 2010, Abadie, 2021). Moreover, the bi-level optimization structure makes solving the problem prohibitively costly in higher dimensions. Bonneel et al. (2016) introduce a gradient descent approach based on an entropy-regularized analogue of W2 (Cuturi, 2013, Peyr\u00e9 & Cuturi, 2019) that can be implemented in low-dimensional settings. 2.3 TANGENTIAL STRUCTURES ONW2 We circumvent these difficulties by exploiting a tangential structure that can be defined on W2 (Ambrosio et al., 2008, Otto, 2001). In particular, it allows us to entirely circumvent solving a bi-level optimization problem as the one in equation 2.3. The tangential structure relies on the fact that geodesics Pt inW2 are linear in the transport plans (\u03c0t)#\u03b3. This implies a geometric tangent cone structure at each measure P \u2208 W that can be defined as the closure in P2(Rd) of the set G(P ) , { \u03b3 \u2208P2(Rd \u00d7 Rd) : (\u03c01)#\u03b3 = P, (\u03c01, \u03c01 + \u03b5\u03c02)#\u03b3 is optimal for some \u03b5 > 0 } with respect to the local distance W 2P (\u03b312, \u03b313) , min {\u222b (Rd)3 |x2 \u2212 x3|2 d\u03b3123 : \u03b3123 \u2208 \u03931(\u03b312, \u03b313) } , (2.4) where \u03b312 and \u03b313 are couplings between P and some other measures P2 and P3, respectively, and \u03931(\u03b312, \u03b313) is the set of all 3-couplings \u03b3123 such that the projection of \u03b3123 onto the first two elements is \u03b312 and the projection onto the first and third element is \u03b313 (Ambrosio et al., 2008, Appendix 12). We can then define the exponential map at P with respect to some tangent element \u03b3 \u2208 G(P ) by expP (\u03b3) = (\u03c01 + \u03c02)#\u03b3. This tangent cone can be constructed at every P \u2208 W , irrespective of its support. In the case whereP is absolutely continuous with respect to Lebesgue measure, the definition simplifies. The corresponding optimal transport plan between P and any other measure measure Q \u2208 W2 is then supported on the graph of the gradient of a convex function \u2207\u03d5 by Brenier\u2019s theorem. This allows the introduction of a regular tangent cone at P , defined via (Ambrosio et al., 2008, section 8.4) T P W2(Rd) , { t(\u2207\u03d5j \u2212 Id) : ( Id\u00d7\u2207\u03d5j ) # P is optimal in \u0393(P, (\u2207\u03d5j)#P ), t > 0 }L2(P ) , (2.5) where A L2(P ) defines the closure of the set A with respect to the distance induced by the L2-norm on P . Interestingly, this tangent cone is actually a linear tangent space (Ambrosio et al., 2008, Theorem 8.5.1). In the following, we therefore call TP W2 the regular tangent space equation 2.5. 2.4 TANGENTIAL WASSERSTEIN PROJECTIONS FOR REGULAR TARGET MEASURES: We want to define a natural analogue to the barycenter projection problem equation 2.3 in the regular tangent space of the Wasserstein space. A starting point for this is to consider a characterization of the barycenter P\u0304 (\u03bb) for fixed weights of a set {Pj}j\u2208JJK in regular tangent spaces. Agueh & Carlier (2011, Equation (3.10)) show that if at least one of the measures is absolutely continuous with respect to Lebesgue measure, then P\u0304 (\u03bb) can be characterized via J\u2211 j=1 \u03bbj ( \u2207\u03d5j \u2212 Id ) = 0, (2.6) where {\u03d5j}j\u2208JJK are the optimal transport maps from the barycenter to the respective measure Pj , i.e. (\u03d5j)#P\u0304 (\u03bb) = Pj . Each term of the summand in equation 2.6 is an element in T P\u0304 (\u03bb)W2(Rd) by construction. More generally, the condition equation 2.6 is a sufficient condition for P\u0304 (\u03bb) to be a \u201cKarcher mean\u201d (Karcher, 2014) inW2 (Zemel & Panaretos, 2019). In fact, a \u201cKarcher mean\u201d of a set of measures {Pj}j\u2208JJK is defined as the gradient of the Fr\u00e9chet functional in W2 and is characterized through equation 2.6 holding P\u0304 (\u03bb)-almost everywhere. equation 2.6 is a stronger condition because it is assumed to hold at every point in the support of P\u0304 (\u03bb), not just almost every point. \u00c1lvarez-Esteban et al. (2016) use this characterization to introduce a fixed-point approach to compute Wasserstein barycenters, and Werenski et al. (2022) use this structure to introduce a replication approach for absolutely continuous measures whose densities are bounded away from zero and whose target measure lies inside the convex hull of the control measures. Related is the recent definition of weak barycenters in Cazelles et al. (2021), where the authors replace the optimal transport maps from the classical optimal transport problem by the weak optimal transport problem introduced in Gozlan et al. (2017). Heuristically, this characterization is that of a deformable template. A measure P is a deformable template if there exists a set of deformations { \u03c8j } j=1,...,J such that \u03c8j#P = Pj , in a way that their weighted average is \u201cas close to the identity\u201d as possible. In our setting \u03c8j \u2261 \u2207\u03d5j\u2212Id (Anderes et al., 2015, Boissard et al., 2015, Yuille, 1991). In our setting of interest, we are given a target measure P0 that we want to replicate given a set {Pj}j\u2208JJK. The key idea for this is to adapt the characterization equation 2.6 with respect to the target measure P0. That is, we aim to find the optimal weights \u03bb\u2217 \u2208 \u2206J that satisfy \u03bb\u2217 , arg min \u03bb\u2208\u2206J \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj ( \u2207\u03d5j \u2212 Id )\u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) , (2.7) where \u2207\u03d5j are the optimal transport maps between the target P0 and the control measures Pj , j \u2208 JJK. Thick dashed lines are tangent vectors t(\u2207\u03d5j \u2212 Id). The gray shaded region is their convex hull in TP0W2 and \u03c0 is the projection of Id onto this convex hull. P\u03c0 , expP0(\u03c0) is the projection of P0 onto the generalized geodesic convex hull CoP0 ( {P1, P2, P3} ) \u2286 W2 (blue). We now show that this approach is in fact a projection of P0 onto the generalized geodesic convex hull of the control measures Pj with respect to P0 as illustrated in Figure 1. To define this notion of convex hull, we extend the definition of generalized geodesics (Ambrosio et al., 2008, section 9.2), and in particular the definition of WP to the multimarginal setting, by defining, for given couplings \u03b30j \u2208 \u0393(P0, Pj), j \u2208 JJK W 2P0;\u03bb(\u03b301, \u03b302, . . . , \u03b30J) , min  \u222b (Rd)J+1 J\u2211 j=1 \u03bbj \u2223\u2223xj \u2212 x0\u2223\u22232 d\u03b3 : \u03b3 \u2208 \u03931(\u03b301, . . . , \u03b30J)  , (2.8) where \u03931(\u03b301, . . . , \u03b30J) \u2286 \u0393(P0, P1, . . . , PJ) is the set of all (J + 1)-couplings \u03b3 such that the projection of \u03b3 onto the first- and j-th element is \u03b30j . Note that this definition is similar to the multimarginal definition of the 2-Wasserstein barycenter (Agueh & Carlier, 2011, Gangbo & S\u0301wie\u0328ch, 1998), but \u201ccentered\u201d at P0. Based on this, we define the generalized geodesic convex hull of measures {Pj}j\u2208JJK with respect to the measure P0 as CoP0 ({ Pj }J j=1 ) , P (\u03bb) \u2208P2(Rd) : P (\u03bb) =  J\u2211 j=1 \u03bbj\u03c0j+1  # \u03b3, \u03b3 solves W 2P0;\u03bb(\u03b301, . . . , \u03b30J), \u03b30j is optimal in \u0393(P0, Pj) \u2200j \u2208 JJK, \u03bb \u2208 \u2206 J  . (2.9) Based on these definitions we can show that our approach is a projection of the target P0 onto CoP0 ( {Pj}Jj=1 ) . Proposition 2.1. Consider a regular target measure P0 and a set {Pj}j\u2208JJK of general control measures. Construct the measure P\u03c0 as P\u03c0 , expP0  J\u2211 j=1 \u03bb\u2217j (\u2207\u03d5j \u2212 Id)  , where the optimal weights \u03bb\u2217 \u2208 \u2206J are obtained by solving equation 2.7 and \u2207\u03d5j are the optimal maps transporting P0 to Pj , respectively. Then P\u03c0 is the unique metric projection of P0 onto CoP0 ({ Pj }J j=1 ) . 2.5 TANGENTIAL WASSERSTEIN PROJECTIONS FOR GENERAL TARGET MEASURES: Proposition 2.1 holds for a regular target P0. In many practical settings, however, the target outcome is not a regular measure, as in our application to Medicaid expansion in section 4. In such settings, the corresponding optimal transport problems equation 2.1 between the target and the respective control measures Pj is only achieved via optimal transport plans \u03b30j , not maps \u2207\u03d5j . In contrast to the regular setting, these transport maps also do not need to be unique. To apply our method in this more general setting would lead us to solving \u03bb\u2217 , arg min \u03bb\u2208\u2206J W 2P0;\u03bb(\u03b301, . . . , \u03b30J), (2.10) where \u03b30j are optimal transport plans between the target P0 and the respective control measure Pj , which, again would be a computationally prohibitive bi-level optimization problem. We therefore rely on barycentric projections to reduce the complex general setting to the regular tangent space and subsequently apply the projection equation 2.7. This is computationally inexpensive, as it amounts to computing \u03bb\u2217 , arg min \u03bb\u2208\u2206J \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj ( b\u03b30j \u2212 Id )\u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) , (2.11) where b\u03b30j (x1) , \u222b Rd x2 d\u03b30j,x1(x2) are the barycentric projections of optimal transport plans \u03b30j between P0 and Pj . Here, \u03b3x1 denotes the disintegration of the optimal transport plan \u03b3 with respect to P0. This approach is a natural extension of the regular setting to general probability measures for two reasons. First, if the optimal transport plans \u03b30j are actually induced by some optimal transport map \u2207\u03b3j , then b\u03b30j reduces to this optimal transport map; in this case the general tangent cone G(P0) reduces to the regular tangent cone TP0W2 (Ambrosio et al., 2008, Theorem 12.4.4). Second, by the definition of b\u03b3 and disintegrations in conjunction with Jensen\u2019s inequality it holds for all \u03bb \u2208 \u2206J that \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj ( b\u03b30j \u2212 Id )\u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) 6W 2P0;\u03bb(\u03b301, . . . , \u03b30J). (2.12) This implies that for general P0 we can also define a convex hull based on barycentric projections, which is of the form C\u0303oP0 ({ Pj }J j=1 ) , P (\u03bb) \u2208P2(Rd) : P (\u03bb) =  J\u2211 j=1 \u03bbjb\u03b30j  # P0, \u03bb \u2208 \u2206J  . (2.13) Furthermore, the contraction property equation 2.12 implies that CoP0 \u2286 C\u0303oP0 , with equality when all transport plans are achieved via maps \u2207\u03d5j . Using these definitions, the following defines our notion of projection for general P0 and shows that it projects onto C\u0303oP0 . Proposition 2.2. Consider a general target measure P0 and a set {Pj}j\u2208JJK of general control measures. Construct the measure P\u0303\u03c0 as P\u0303\u03c0 , expP0  J\u2211 j=1 \u03bb\u2217j b\u03b30j \u2212 Id  , where the optimal weights \u03bb\u2217 \u2208 \u2206J are obtained by solving equation 2.11 and \u03b30j are optimal plans transporting P0 to Pj , respectively. Then for given optimal plans \u03b30j , P\u0303\u03c0 is the unique metric projection of P0 onto C\u0303oP0 ({ Pj }J j=1 ) . In contrast to the regular case in Proposition 2.1, the optimal plans \u03b30j transporting P0 to Pj need not be unique. However, the projection for fixed \u03b30j is unique. 3 STATISTICAL PROPERTIES OF THE WEIGHTS AND TANGENTIAL PROJECTION: We now provide statistical consistency results for our method when the corresponding measures {Pj}j\u2208JJK are estimated from data. We consider the case where the measures Pj are replaced by their empirical counterparts PNj (A) , N \u22121 j Nj\u2211 n=1 \u03b4Xn(A) for every measurable set A in the Borel \u03c3-algebra on Rd, where \u03b4x(A) is the Dirac measure and( X1j , . . . , XNj ,j ) is an independent and identically distributed set of random variables whose distri- bution is Pj . We explicitly allow for different sample sizes \u22c3J j=0Nj = N for the different measures. To save on notation we write \u03d5\u0302Nj \u2261 \u03d5\u0302j , b\u03020j \u2261 b\u0302\u03b30j ,Nj and \u03b3\u03020j \u2261 \u03b3\u0302Nj ,N0 in the following. Proposition 3.1 (Consistency of the optimal weights). Let { PNj }J j=0 be the empirical measures corresponding to the data ( X1j , . . . , XNjj )J j=0 which are independent and identical draws from Pj , respectively, and are supported on some common latent probability space (\u2126,A , P ). Assume Pj has finite second moment. As Nj \u2192 \u221e for all j \u2208 JJK, the corresponding optimal weights \u03bb\u0302\u2217N = ( \u03bb\u0302\u2217N1 , . . . , \u03bb\u0302 \u2217 NJ ) \u2208 \u2206J obtained via \u03bb\u0302\u2217N , arg min \u03bb\u2208\u2206J \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj ( b\u03020j \u2212 Id )\u2225\u2225\u2225\u2225\u2225\u2225 2 L2(PN0 ) , (3.1) satisfy P (\u2223\u2223\u2223\u03bb\u0302\u2217N \u2212 \u03bb\u2217\u2223\u2223\u2223 > \u03b5)\u2192 0 for all \u03b5 > 0 , where \u03bb\u2217 solve equation 2.11. This consistency result directly implies consistency of the optimal weights in case the optimal transport problems between PN0 and each PNj are achieved by optimal transport maps \u2207\u03d5\u0302Nj . We also have a consistency result for the empirical counterparts P\u0303\u03c0,N of the optimal projection P\u0303\u03c0 . Corollary 3.1 (Consistency of the optimal projections). In the setting of Proposition 3.1, the estimated projections P\u0303\u03c0,N converge weakly in probability to the projection P\u0303\u03c0 as Nj \u2192 \u221e for all j \u2208 JJK. Proposition 3.1 and Corollary 3.1 hold in all generality and without any assumptions on the corresponding measures Pj , except that they possess finite second moments. To get stronger results, for instance parametric rates of convergences one needs to make strong regularity assumptions on the measures Pj . Without these, the rate of convergence of optimal transport maps in terms of expected square loss is as slow as n\u22122/d (H\u00fctter & Rigollet, 2021). Under such additional regularity conditions, the results for the asymptotic properties are standard, because the proposed method reduces to a classical semiparametric estimation problem, as the weights \u03bbj are finite-dimensional. 4 APPLICATION TO ESTIMATING CAUSAL EFFECTS VIA SYNTHETIC CONTROLS: In this section, we introduce a practical application of our method to causal inference by estimating the causal effect of Medicaid expansion in Montana in 2016 via our generalization of the classical synthetic controls estimator (Abadie & Gardeazabal, 2003, Abadie et al., 2010) to multivariate outcome distributions, complementing the univariate method in Gunsilius (2022). In particular, we can account for general heterogeneity in the multivariate outcomes by considering the entire distribution of several outcomes of interest jointly, something which has been out of reach of existing synthetic controls methods. We use the POT package (Flamary et al., 2021) to obtain the optimal transport plans, and CVXPY (Diamond & Boyd, 2016, Agrawal et al., 2018) to compute equation 2.7. Additional details of our applications as well as other simulations are contained in Appendix B. We study the effect of health insurance coverage following state-level Medicaid expansion in the United States. A provision within the Affordable Care Act (ACA) allows states to decide whether to expand Medicaid for low-income households. Some states decided to adopt such expansions early on, while others did not (and still have not done so). We investigate the economic and behavioral effects of Medicaid expansion. Specifically, we consider some first-order effects (i.e. the extensive margin of Medicaid enrollment post-expansion and disemployment effects) and second-order effects (i.e. income and labor supply effects) of expanded Medicaid access. The observational data we use is the American Community Survey (ACS; additional details in Appendix B). From it, we collect the following variables: 1. HINSCAID, which indicates whether the individual interviewed is covered by Medicaid, 2. EMPSTAT, which indicates employment status (either employed or unemployed), 3. UHRSWORK, which indicates number of hours worked, 4. INCWAGE, which indicates previous-twelve-month labor income level. We measure labor hours supplied and labor income in logs instead of levels. We consider Montana as the treated unit for this application. Montana adopted such expansion in 2016, which is the treated year. For control units, we use the twelve states for which such expansion has never occurred. As of 2022, the twelve states are: Alabama, Florida, Georgia, Kansas, Mississippi, North Carolina, South Carolina, South Dakota, Tennessee, Texas, Wisconsin, Wyoming. We use data from 2010 to 2016 to estimate optimal weights for the control states in order to create the \u201csynthetic Montana\u201d, i.e. Montana had it not adopted Medicaid expansion. Once we obtain the optimal weights \u03bb\u2217, we estimate the counterfactual outcomes of interest for the four years post-Medicaid expansion in Montana. This involves solving equation 2.2 with the optimal weights \u03bb\u2217 obtained in the preintervention periods. Details of sample selection and estimating \u201csynthetic Montana\u201d are described in Appendix B.3. As we show there, the combination of control states replicating Montana is close to actual Montana in the pre-intervention period, implying that the observed differences in postintervention periods can be attributed to the causal effect of the Medicaid expansion. Furthermore, therein, we also develop and show the results of a placebo permutation test in the sense of Abadie et al. (2010) to validate our findings. Consistent with findings in Courtemanche et al. (2017), Mazurenko et al. (2018), we find significant first- and second order effects of Medicaid expansion, which are summarized in the top row and the bottom row of Figure 2. \u201cSynthetic Montana\u201d has much lower proportion of individuals insured under Medicaid, suggesting that expanding Medicaid eligibility directly affects the extensive margin of Medicaid enrollment. The disemployment effect is less pronounced in comparison to the enrollment effect we estimated, but nonetheless positive and nontrivial, consistent with the findings in, e.g., Peng et al. (2020), but inconsistent with those in, e.g., Gooptu et al. (2016). We also find positive second-order effects, summarized in the bottom row of Figure 2. Additional details are in Appendix B. 5 CONCLUSION: We have developed a projection method between sets of probability measures supported on Rd based on the tangential structure of the 2-Wasserstein space. Our method seeks to best approximate some target distribution that is potentially multivariate, using some chosen set of control distributions. We provide an implementation which gives unique, interpretable weights in a setting of regular probability measures. For general probability measures, we construct our projection by first creating a regular tangent space through applying barycentric projection to optimal transport plans. Our application to evaluating the first- and second-order effects of Medicaid expansion in Montana via an extension of the synthetic controls estimator (Abadie & Gardeazabal, 2003, Abadie et al., 2010) demonstrates the method\u2019s efficiency and the necessity to have a method that is applicable for general proabbility measures. The approach still works without restricting optimal weights to be in the unit simplex, which would allow for extrapolation beyond the convex hull of the control units, providing a notion of tangential regression. It can also be extended to a continuum of measures, using established consistency results of barycenters (e.g. Le Gouic & Loubes, 2017). A PROOFS FROM THE MAIN TEXT: Proof of Proposition 2.1. Define the following closed and convex subset C \u2286 L2(P0) for fixed optimal transportation maps between P0 and Pj , denoted\u2207\u03d5j : C , f \u2208 L2(P0) : f = J\u2211 j=1 \u03bbj \u2207\u03d5j for some \u03bb \u2208 \u2206J  . Recall that the transport maps \u2207\u03d5j exist since P0 is regular. Using C, we can rewrite equation 2.7 as arg min \u03bb\u2208\u2206J \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj \u2207\u03d5j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) = arg min f\u2208C \u2016f \u2212 Id\u20162L2(P0) , which by definition is the metric projection of Id onto C. Since C is a non-empty closed and convex subset of the Hilbert space L2(P0), this metric projection exists and is unique (Aliprantis & Border, 1999, Theorem 6.53). Moreover, if Id \u2208 C, then \u03c0C = Id; otherwise, \u03c0C \u2208 \u2202C, where \u2202C is the boundary of C (Aliprantis & Border, 1999, Lemma 6.54). Since P0 is regular, the exponential map is continuous. In fact, for every j 6= k, W 22 (Pj , Pk) = W 2 2 ((\u2207\u03d5j)#P0, (\u2207\u03d5k)#P0) 6 \u222b Rd \u2223\u2223\u2207\u03d5j \u2212\u2207\u03d5k\u2223\u22232 dP0(x). In other words, the distance between Pj and Pk inW2(Rd) is smaller than that between corresponding elements\u2207\u03d5j ,\u2207\u03d5k in the tangent space. This implies continuity of the exponential map. Furthermore, in this regular setting, the exponential map sends convex sets in T P0W2 to generalized geodesically convex sets in W2. Mechanically, for any two (scaled) elements t(\u2207\u03d5j \u2212 Id) and s(\u2207\u03d5k \u2212 Id) in TP0W2, and any \u03c1 \u2208 [0, 1], expP0(\u03c1t(\u2207\u03d5j \u2212 Id) + (1\u2212 \u03c1)s(\u2207\u03d5k \u2212 Id)) = expP0((\u03c1t\u2207\u03d5j + (1\u2212 \u03c1)s\u2207\u03d5k)\u2212 (\u03c1t+ (1\u2212 \u03c1)s) Id) = expP0 \u03c1\u0303[[\u03c1t \u03c1\u0303 \u2207\u03d5j + (1\u2212 \u03c1)s \u03c1\u0303 \u2207\u03d5k ] \u2212 Id ] = ([ \u03c1t\u2207\u03d5j + (1\u2212 \u03c1)s\u2207\u03d5k ] + (1\u2212 \u03c1\u0303) Id ) # P0 = ([ \u03c1t(\u2207\u03d5j \u2212 Id) + (1\u2212 \u03c1)s(\u2207\u03d5k \u2212 Id) ] + Id ) # P0 where \u03c1\u0303 , \u03c1t + (1 \u2212 \u03c1)s. This is a generalized geodesic connecting Pj and Pk, via the optimal transport map between them and P0 (Ambrosio et al., 2008, section 9.2). The same argument holds when extending generalized geodesics to generalized barycenters by taking convex combination of more measures than a binary interpolation with respect to \u03c1. Mechanically, for any \u03bb \u2208 \u2206J and tj > 0 for all j \u2208 JJK, expP0  J\u2211 j=1 \u03bbjtj(\u2207\u03d5j \u2212 Id)  = expP0  J\u2211 j=1 \u03bbjtj \u2207\u03d5j \u2212 J\u2211 j=1 \u03bbjtj Id  = expP0 \u03c1\u0303J  J\u2211 j=1 \u03c1\u0303J\u2207\u03d5j \u2212 Id   =   J\u2211 j=1 \u03bbjtj\u2207\u03d5j + (1\u2212 \u03c1\u0303J) Id  # P0 =   J\u2211 j=1 \u03bbjtj(\u2207\u03d5j \u2212 Id) + Id  # P0 where \u03c1\u0303J , \u2211J j=1 \u03bbjtj . This proves the exponential map is generalized geodesically convex. From above it follows that P\u03c0 , expP0(\u03c0C) is either in the interior of C, which is the case if Id \u2208 C, or on its boundary: since \u03c0C \u2208 \u2202C, expP0(\u03c0C) \u2208 expP0(\u2202C). By continuity of the exponential map it follows that expP0(\u2202C) = \u2202 expP0(C). Combining all steps above show that P\u03c0 is a geodesic metric projection of P0 onto the geodesic convex hull of { Pj }J j=1 . Proof of Proposition 2.2. The result follows from the same argument as the proof of Proposition 2.1. Theorem 12.4.4 in Ambrosio et al. (2008) shows that TP0W2 is the image of the barycentric projection of measures in the general tangent cone: b\u03b3(x) is an optimal transport map if \u03b3 is an optimal transport plan. But the exponential map satisfies expP0(v) = (v + Id)# P0 for all v \u2208 TP0W2. This implies that P\u0303\u03c0 , expP0  J\u2211 j=1 \u03bb\u2217j b\u03b30j \u2212 Id  =  J\u2211 j=1 \u03bb\u2217j b\u03b30j  # P0 \u2208 C\u0303oP0 ({ Pj }J j=1 ) , since the convex combination of elements in the subgradients of convex functions lie in the subgradient of a convex function (provided the subgradient of each convex function is nonempty, which is the case here). Then the continuity and generalized convexity of the exponential map for elements in the regular tangent space TP0W2 implies the result. Proof of Proposition 3.1. We split the proof into two parts. In the first part we prove the convergence in probability of the family of objective functions equation 3.1 to their population counterparts equation 2.11 if the empirical measures PNj converge weakly in probability to the population measures Pj . In the second step we use the fact that \u03bb\u0302\u2217 is a classical semiparametric estimator (Andrews, 1994, Newey & McFadden, 1994) to derive the convergence of the weights. Step 1: Convergence of the objective functions To show the convergence of the of the objective functions for obtaining the weights \u03bb\u2217, we write\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbjb0j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) \u2212 \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj b\u03020j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(PN0 ) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u222b \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjb0j(x)\u2212 x \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0 \u2212 \u222b \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbj b\u03020j(x)\u2212 x \u2223\u2223\u2223\u2223\u2223\u2223 2 dPN0 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 . We hence want to show that lim\u2227 j Nj\u2192\u221e \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u222b \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjb0j(x)\u2212 x \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0(x)\u2212 \u222b \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbj b\u03020j(x)\u2212 x \u2223\u2223\u2223\u2223\u2223\u2223 2 dPN0(x) \u2223\u2223\u2223\u2223\u2223\u2223\u2223 = 0 , where \u2227 j Nj \u2261 min {N0, . . . , NJ}. We split the result into two parts. The first part shows that lim inf\u2227 j Nj\u2192\u221e \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbj b\u03020j(x0)\u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 dPN0(x0) > \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjb0j(x0)\u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0(x0). In the second part we use the L2(P0) convergence of the barycentric projections to prove that the limit exists and coincides with the limit inferior. For the first part, we have lim inf\u2227 j Nj\u2192\u221e \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbj b\u03020j(x0)\u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 dPN0(x0) = lim inf\u2227 j Nj\u2192\u221e \u222b (Rd)J+1 \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3\u0302N (x0, x1, . . . , xJ), where \u03b3\u0302N (x0, x1, . . . , xJ) is a measure that solves min  \u222b (Rd)J+1 J\u2211 j=1 \u03bbj \u2223\u2223xj \u2212 x0\u2223\u22232 d\u03b3 : \u03b3 \u2208 \u03931(\u03b3\u030201, . . . , \u03b3\u03020J)  , \u03b3\u03020j are the optimal couplings between PN0 and P\u0303Nj , ( b\u03020j ) # PN0 . Since all measures are defined on the complete and separable space Rd, and by assumption of finite second moments, i.e. max j\u2208JJK sup Nj \u222b \u2223\u2223xj \u2212 x0\u2223\u22232 d\u03b3\u03020j < +\u221e , it holds that each sequence \u03b3\u03020j is tight by Ulam\u2019s theorem (Dudley, 2018, Theorem 7.1.4). Using the fact that \u03bb \u2208 \u2206J and \u03b3\u0302N \u2208 \u03931 (\u03b3\u030201, . . . , \u03b3\u03020J), applying Jensen\u2019s inequality gives us max j\u2208JJK sup Nj \u222b (Rd) J+1 \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3\u0302N 6 max j\u2208JJK sup Nj J\u2211 j=1 \u03bbj \u222b Rd \u2223\u2223xj \u2212 x0\u2223\u22232 d\u03b3\u03020j < +\u221e , which implies that \u03b3\u0302N is tight. By Prokhorov\u2019s theorem, there exists a subsequence \u03b3\u0302Nk that weakly converges to a limit measure \u03b3. Therefore, by the continuity of the map (x0, xj) 7\u2192 \u2211 j \u03bbjxj \u2212 x0, it follows from classical convergence results (Ambrosio et al., 2008, Lemma 5.1.12(d)) that lim inf\u2227 j Nj\u2192\u221e \u222b (Rd)J+1 \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3\u0302N (x0, x1, . . . , xJ) = \u222b (Rd)J+1 \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3(x0, . . . , xJ). Furthermore, by the same argument via Jensen\u2019s inequality, i.e., \u222b (Rd)J+1 \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3(x0, . . . , xJ) 6 J\u2211 j=1 \u222b (Rd)2 \u2223\u2223\u03bbjxj \u2212 x0\u2223\u22232 d\u03b30j(x0, xj) < +\u221e , it follows that the limit \u03b3 \u2208 \u03931 (\u03b301, . . . , \u03b30J). Now note that by the definition of disintegration it follows that (Ambrosio et al., 2008, Lemma 5.3.2) \u03b3 \u2208 \u03931(\u03b301, . . . , \u03b30J) \u21d0\u21d2 \u03b3x0 \u2208 \u0393 ( \u03b31|x0 , . . . , \u03b3J|x0 ) , where \u03b3 = \u222b \u03b3x0 dP0(x0) and \u03b30j = \u222b \u03b3j|x0 dP0(x0) are the disintegrations of \u03b3 and \u03b30j with respect to P0, respectively. Therefore, we have\u222b (Rd)J+1 \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3(x0, . . . , xJ) = \u222b Rd \u222b (Rd) J \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3x0(x1, . . . , xJ) dP0(x0) > \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 \u222b (Rd) J  J\u2211 j=1 \u03bbjxj \u2212 x0  d\u03b3x0(x1, . . . , xJ) \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0(x0) = \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbj \u222b (Rd) J xj d\u03b3x0(x1, . . . , xJ)\u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0(x0) = \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbj \u222b Rd xj d\u03b3j|x0(xj)\u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0(x0) = \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjb0j(x0)\u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0(x0), where the third lines follows from Jensen\u2019s inequality and the fifth line from \u03b3x0 \u2208 \u0393 ( \u03b31|x0 , . . . , \u03b3J|x0 ) . This shows the first part. For the second part we use the fact that each barycentric projection b\u03020j(x1) is an optimal transport map between PN0 and P\u0303Nj if \u03b3\u03020j is an optimal transport plan between PN0 and PNj , which follows from Theorem 12.4.4 in Ambrosio et al. (2008). As before, we know that ( b\u03020j ) # PN0 is a tight sequence that converges to some P\u0303j . By definition and the fact that b\u03020j is the gradient of a convex function between PN0 and P\u0303Nj , b\u03020j is the unique optimal transport map between PN0 and P\u0303Nj for all Nj and all j. Since the measures Pj have finite second moments by assumption, we have lim sup N0\u2227Nj\u2192\u221e \u222b Rd |xj |2 dP\u0303Nj = lim sup N0\u2227Nj\u2192\u221e \u222b Rd \u2223\u2223\u2223\u0302b0j(x0)\u2223\u2223\u22232 dPN0 = lim sup N0\u2227Nj\u2192\u221e \u222b Rd \u2223\u2223\u2223\u2223\u222b Rd xj d\u03b3\u0302j|x0(xj) \u2223\u2223\u2223\u22232 dPN0 6 lim sup N0\u2227Nj\u2192\u221e \u222b (Rd) 2 \u2223\u2223xj\u2223\u22232 d\u03b3\u03020j(x0, xj) = \u222b (Rd) 2 \u2223\u2223xj\u2223\u22232 d\u03b30j(x0, xj) < +\u221e, where the last equality follows from the tightness of \u03b3\u03020j , as shown earlier. Therefore, by standard stability results for optimal transport maps, in particular Proposition 6 in Zemel & Panaretos (2019), it holds that b\u03020j converges uniformly on every compact subset K \u2286 Rd in the interior of the support of the limit measure P\u0303j , that is lim N0\u2227Nj\u2192\u221e sup x0\u2208K \u2223\u2223\u2223\u0302b0j(x0)\u2212 vj(x0)\u2223\u2223\u2223 = 0 , where vj is the optimal transport map between P0 and P\u0303j . We now show that vj = b0j P0-almost everywhere. From the local uniform convergence, we can then derive \u201cstrong L2-convergence\u201d (Ambrosio et al., 2008, Definition 5.4.3) of the potentials: lim sup N0\u2227Nj\u2192\u221e \u2223\u2223\u2223\u2223\u2223\u2225\u2225\u2225b\u03020j\u2225\u2225\u2225L2(PN0 ) \u2212 \u2225\u2225vj\u2225\u2225L2(P0) \u2223\u2223\u2223\u2223\u2223 6 lim sup N0\u2227Nj\u2192\u221e \u2223\u2223\u2223\u2223\u2223\u2225\u2225\u2225b\u03020j\u2225\u2225\u2225L2(PN0 ) \u2212 \u2225\u2225vj\u2225\u2225L2(PN0 ) \u2223\u2223\u2223\u2223\u2223+ lim supN0\u2192\u221e \u2223\u2223\u2223\u2223\u2225\u2225vj\u2225\u2225L2(PN0 ) \u2212 \u2225\u2225vj\u2225\u2225L2(P0) \u2223\u2223\u2223\u2223 6 lim sup N0\u2227Nj\u2192\u221e \u2225\u2225\u2225b\u03020j \u2212 vj\u2225\u2225\u2225 L2(PN0 ) + lim sup N0\u2192\u221e \u2223\u2223\u2223\u2223\u2225\u2225vj\u2225\u2225L2(PN0 ) \u2212 \u2225\u2225vj\u2225\u2225L2(P0) \u2223\u2223\u2223\u2223 Now the first term converges to zero by H\u00f6lder\u2019s inequality and the local uniform convergence of the optimal transport maps from above. The second term satisfies lim sup N0\u2192\u221e \u2223\u2223\u2223\u2223\u2225\u2225vj\u2225\u2225L2(PN0 ) \u2212 \u2225\u2225vj\u2225\u2225L2(P0) \u2223\u2223\u2223\u2223 = lim sup N0\u2192\u221e \u2223\u2223\u2223\u2223\u2223 (\u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dPN0)1/2 \u2212 (\u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dP0)1/2 \u2223\u2223\u2223\u2223\u2223 6 lim sup N0\u2192\u221e \u2223\u2223\u2223\u2223\u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dPN0 \u2212\u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dP0\u2223\u2223\u2223\u22231/2 . But since P0 has finite second moments, it holds that this term also converges to zero. Based on this we can show that \u03b3\u03020j \u2261 ( Id, b\u03020j ) converge weakly to \u03b30j \u2261 ( Id, vj ) . Indeed, if \u03b30j is a limit point of the sequence \u03b3\u03020j , it holds that\u222b (Rd) 2 \u2223\u2223xj\u2223\u22232 d\u03b30j(x0, xj) 6 lim inf N0\u2227Nj\u2192\u221e \u222b (Rd) 2 \u2223\u2223xj\u2223\u22232 d\u03b3\u03020j(x0, xj) 6 lim sup N0\u2227Nj\u2192\u221e \u222b (Rd) 2 \u2223\u2223xj\u2223\u22232 d\u03b3\u03020j(x0, xj) = lim sup N0\u2227Nj\u2192\u221e \u222b Rd \u2223\u2223\u2223\u0302b0j(x0)\u2223\u2223\u22232 dPN0(x0) = \u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dP0(x0). Disintegrating the left-hand side with respect to P0, and applying Jensen\u2019s inequality, gives\u222b (Rd) 2 \u2223\u2223xj\u2223\u22232 d\u03b30j(x0, xj) = \u222b Rd \u222b Rd \u2223\u2223xj\u2223\u22232 d\u03b3j|x0(xj) dP0(x0) > \u222b Rd \u2223\u2223\u2223\u2223\u222b Rd xj d\u03b3j|x0(xj) \u2223\u2223\u2223\u22232 dP0(x0) = \u222b Rd \u2223\u2223b0j(x0)\u2223\u22232 dP0(x0), that is, \u222b Rd \u2223\u2223b0j(x0)\u2223\u22232 dP0(x0) 6 \u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dP0(x0). But since vj is an optimal transport map between P0 and P\u0303j by definition, it holds that\u222b Rd \u2223\u2223b0j(x0)\u2223\u22232 dP0(x0) > \u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dP0(x0) , which implies that equality holds and we have that\u222b Rd [\u2223\u2223b0j(x0)\u2223\u22232 \u2212 \u2223\u2223vj(x0)\u2223\u22232] dP0(x0) = 0 , which implies that b0j = vj P0-almost everywhere. We have hence shown that ( Id, b\u03020j ) # PN0 converges weakly to ( Id, b0j ) # P0 for all j, where the barycentric projection b0j is the optimal transport map between P0 and P\u0303j (e.g. Villani, 2003, Theorem 2.12.(iii)). Moreover, we have shown \u201cstrong L2-convergence\u201d of the barycentric projections in terms of Definition 5.4.3 in Ambrosio et al. (2008). Since this holds for all j, it also holds for their convex combination for fixed weights \u03bb \u2208 \u2206J . Putting everything together, we then have that lim\u2227 j Nj\u2192\u221e \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj b\u03020j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(PN0 ) = \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbjb0j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) . Since all observable measures Pj are empirical measures, they converge weakly in probability (Varadarajan, 1958), which implies that lim\u2227 j Nj\u2192\u221e P  \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj b\u03020j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(PN0 ) \u2212 \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbjb0j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 > \u03b5  = 0 for all \u03b5 > 0. This shows convergence in probability of the objective function for fixed \u03bb. Step 2: Convergence of the optimal weights \u03bb\u0302\u2217N The convergence of the optimal weights now follows from standard consistency results in semiparametric estimation. In particular, the objective functions are all convex for any \u03bb \u2208 RJ , which implies that they converge uniformly on any compact set (Rockafellar, 1970, Theorem 10.8), so the objective function converges uniformly on \u2206J . Now a standard consistency result like Theorem 2.1 in Newey & McFadden (1994) then implies that lim\u2227 j Nj\u2192\u221e P (\u2223\u2223\u2223\u03bb\u0302\u2217N \u2212 \u03bb\u2217\u2223\u2223\u2223 > \u03b5) = 0 for all \u03b5 > 0 , which is what we wanted to show. Note that the result can also be shown if we allow the weights \u03bb to be negative, i.e., if we only require that \u2211J j=1 \u03bbj = 1. In this case, the fact that the objective functions are convex and coercive implies that an optimal \u03bb\u2217 will be achieved at the interior of the extended Euclidean space, from which consistency follows by Theorem 2.7 in Newey & McFadden (1994). Proof of Corollary 3.1. We want to show that (\u2211J j=1 \u03bb\u0302 \u2217 Nj b\u03020j ) # PN0 converges weakly in prob- ability to (\u2211J j=1 \u03bb \u2217 j b0j ) # P0, where \u03bb\u0302\u2217N , ( \u03bb\u0302\u2217N1 , . . . , \u03bb\u0302 \u2217 NJ ) are the optimal weights obtained in equation 3.1 and equation 2.11, respectively. The result follows by applying the extended continuous mapping theorem (van der Vaart & Wellner, 2013, Theorem 1.11.1) as follows. As shown in the proof of Proposition 3.1 we have \u201cstrong L2-convergence\u201d of the maps\u2211J j=1 \u03bb\u0302 \u2217 Nj b\u03020j \u2212 Id to \u2211J j=1 \u03bb \u2217 j b0j \u2212 Id. Therefore, by Theorem 5.4.4 (iii) in Ambrosio et al. (2008), it holds that lim \u2227jNj\u2192\u221e \u222b Rd f x0, J\u2211 j=1 \u03bb\u0302\u2217Nj b\u03020j(x0)\u2212 x0  dPN0(x0) = \u222b Rd f x0, J\u2211 j=1 \u03bb\u2217j b0j(x0)\u2212 x0  dP0(x0) for any continuous function such that |f(x0)| 6 C1 + C2 |x0 \u2212 x0|2 for all x0 in the support of P0, where C1, C2 < +\u221e are some constants and x0 in some element in the support of P0 (Ambrosio et al., 2008, equation (5.1.21)). In particular, this holds for any bounded and continuous function f , which implies that lim \u2227jNj\u2192\u221e \u222b Rd f  J\u2211 j=1 \u03bb\u0302\u2217Nj b\u03020j(x0) dPN0(x0) = \u222b Rd f  J\u2211 j=1 \u03bb\u2217j b0j(x0) dP0(x0) for any bounded and continuous function, which implies that (\u2211J j=1 \u03bb\u0302 \u2217 Nj b\u03020j ) # PN0 converges weakly to (\u2211J j=1 \u03bb \u2217 j b0j ) # P0 if PNj converge weakly to Pj , j \u2208 JJK. Now we apply the extended continuous mapping theorem (van der Vaart & Wellner, 2013, Theorem 1.11.1). Equip P2(Rd) with any metric d\u0303(\u00b7, \u00b7) that metrizes weak convergence. We define the maps g :\u00d7Jj=0 ( P2(Rd), d\u0303 ) j \u2192 ( P2(Rd), d\u0303 ) by g (P0, . . . , PJ) =  J\u2211 j=1 \u03bb\u2217j b0j  # P0 , and analogously for their empirical counterparts gN . Note that g and gN are non-random functions if the measures Pj and PNj are non-random themselves for all j \u2208 JJK. Moreover, by definition, g and gN are continuous maps because \u2211J j=1 \u03bb \u2217 j b0j are gradients of convex functions, which are continuous P0-almost everywhere; the same thing holds for their empirical counterparts. Now from what we have shown above and in Proposition 3.1, it holds that gN ( PN0 , . . . ,PNJ ) \u2192 g (P0, . . . , PJ) as PNj converge weakly to Pj . Since {PNj}Jj=1 here instead are the only random elements in \u00d7Jj=0 ( P2(Rd), d\u0303 ) j , the extended continuous mapping theorem implies that lim\u2227 j Nj\u2192\u221e P ( d\u0303 ( gN ( PN0 , . . . ,PNJ ) , g (P0, . . . , PJ) ) > \u03b5 ) = 0 for all \u03b5 > 0 , which is what we wanted to show. B DETAILS FROM SIMULATIONS AND APPLICATIONS: B.1 MIXTURES OF GAUSSIAN SIMULATION As a simple first simulation to show that the method is applicable in higher dimensions, we consider mixtures of Gaussian in dimension d = 20. We draw from the following Gaussians: Xj \u223c N ( \u00b5j ,\u03a3 ) , j = 0, 1, 2, 3 , where \u00b50 = [10, 10, . . . , 10], \u00b51 = [50, 50, . . . , 50], \u00b52 = [200, 200, . . . , 200], \u00b53 = [\u221250,\u221250, . . . ,\u221250] and \u03a3 = Id20 +0.8 Id\u221210, with Id \u2212 10 the 10 \u00d7 10 matrix with zeros on the main diagonal and ones on all off-diagonal terms. We then define the following mixtures: Y0 as target, and Y1, Y2, and Y3 as controls, where Y0 = 0.3X0 + 0.6X1 + 0.1X2 Y1 = 0.8X0 + 0.1X1 + 0.1X2 Y2 = 0.2X1 + 0.7X2 + 0.1X3 Y3 = 0.2X0 + 0.2X2 + 0.6X3 We sample N0 = N1 = N2 = N3 = 10000 points from each mixture. The estimated optimal weights are \u03bb\u2217 = [0.8204, 0.1796, 0]. The results are as expected, since Y1 is defined by the same set of X\u2019s as those for Y0. In the case of Y3, it is not defined with X1, therefore we expected the associated weight to be very small. Table 1 suggests the projection is close to the target distribution when only considering the mean, despite only having three control units. B.2 SIMULATION: REPLICATING IMAGES To demonstrate the property that our method provides sparse weights, we provide an application on replicating a target image of an object using images of the same object taken from different angles. We use the Lego Bricks dataset available from Kaggle, which contains approximately 12,700 images of 16 different Lego bricks. These images are in RGBA format. For our application, the target image is contained in Figure 3(A), and the control images are in Figure 4. The result, as shown in Figure 3, indicates the optimally-weighted projection replicates the target image well, even in a setting with only a few controls. We note two interesting findings. One, only the control images in the first row of Figure 4 received nontrivial weights; all others received essentially zero weights. This suggests that our method use most information from controls which look sufficiently like the target and demonstrates that our method provides sparse weights. Two, even with few controls, the optimally weighted projection approximates the target well in this application. B.3 DETAILS OF MEDICAID EXPANSION APPLICATION We use the ACS data with harmonized variables made available by IPUMS, a unified source of Census and survey data collected around the world. The data is at the household-person-year level. For our application, we select the household head and the spouse as our unit of analysis. The continuous outcomes are adjusted using the person-level sample weights available in the data. We adopt the following sample restriction criteria: we included individuals \u2022 of working age, i.e. between ages 18 and 65 \u2022 who has no missing outcomes (for those listed in the main text) \u2022 who has no top-coded responses \u2022 who are either household heads or their spouses The sample size breakdown by states are follows: We randomly select N = 1500 observations from each unit for estimating \u03bb\u2217. In the Python implementation, we face a challenge where if the entries of the target and control data are large enough, equation 2.7 becomes too large for CVXPY to compute an optimal solution. Therefore, we introduce a stabilizing constant to prevent this. This stabilizing constant is determined by the mean value and dimensions of the target distribution, and the number of controls. The weights we obtained are sparse and are displayed in Table 3. We check whether the obtained weights are fit for creating synthetic Montana by examining if they well-approximate actual Montana in the pre-treatment period. As seen in Figures 5 and 6, our projection is similar to the actual data. Once we obtain the optimal weights \u03bb\u2217, we estimate the counterfactual outcomes of interest for the four years after Medicaid expansion in Montana (namely, between 2017 and 2020). This involves solving equation 2.2 with \u03bb\u2217 obtained from the pre-intervention period. Implementation-wise, we computed the free-support barycenter, using the POT package; this does not fix the support of the barycenter a priori, and allows it to be different from those of the control distributions. We plot the densities and distributions of the counterfactual outcomes in Figure 2 of the main text. To perform inference on the estimated causal effect, we use a placebo permutation test in analogy to Abadie et al. (2010), Gunsilius (2022). The idea is to repeatedly apply the procedure described above to each control unit, pretending in turn each control unit is the treated unit. Post-intervention, if an actual treatment effect only appears in the treatment unit (Montana, in this application), then the estimated effect for the actual treatment unit should be among the largest. We plot the 2-Wasserstein distance between the treated, joint distribution of all outcomes and the pre-/post-intervention optimal projection (i.e. equation 2.2 with \u03bb\u2217). We present two sets of results in Figure 7: in panel (A), the optimal projection is computed using \u03bb\u2217 estimated using all years in the pre-intervention period; in panel (B), the \u03bb\u2217 used is constructed from taking a simple average of weights estimated in each year of the pre-intervention period. Our results suggest that the estimated causal effect is valid in the post-intervention period, as we consistently observe the largest difference coming from Montana, especially from 2017-2019. The effect is less pronounced in 2020, however. To accompany Figure 7, we also compute p-values, which we denote by and define as pt , r(d1t) J+1 , where d1t is the 2-Wasserstein distance from the optimal projection to actual distribution when the target unit is Montana, r(d1t) is the rank of d1t amongst djts at given time t, and J is the number of control units. Results are described in Table 4. A smaller pt value indicates larger treatment effect. We observed that r(d1t) = 1 for 2018 and 2019, implying a nontrivial effect of the Medicaid expansion in Montana during these years. The values are pt are comparably higher in 2017 and 2020, which we attribute to the fact that it was the first year of the policy implementation, and the COVID-19 pandemic, respectively.", "agent_review": [{"role": "user", "content": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Tangential Wasserstein Projections\n\n1 INTRODUCTION: The concept of projections, that is, approximating a target quantity of interest by an optimally weighted combination of other quantities, is of fundamental relevance in statistics. Projections are generally defined between random variables in appropriately defined linear spaces (e.g. van der Vaart, 2000, chapter 11). In modern statistics and machine learning applications, the objects of interest are often probability measures themselves. Examples range from object- and functional data (e.g. Marron & Alonso, 2014) to causal inference with individual heterogeneity (e.g. Athey & Imbens, 2015). We introduce a notion of projection between sets of probability measures supported on Euclidean spaces. The proposed definition is applicable between sets of general probability measures with different supports and possesses good computational and statistical properties. It also provides a unique solution to the projection problem under mild conditions and can replicate the geometric properties of the target measure, such as its shape and support. To achieve this, we work in the 2- Wasserstein space, that is, the set of all probability measures with finite second moments equipped with the 2-Wasserstein distance. Importantly, we focus on the multivariate setting, i.e. we consider the Wasserstein space over some Euclidean space Rd, denoted byW2, where the dimension d can be high. The multivariate setting poses particular challenges from a mathematical, computational, and statistical perspective. In particular, W2 is a positively curved metric space for d > 1 (e.g. Ambrosio et al., 2008, Kloeckner, 2010). Moreover, the 2-Wasserstein distance between two probability measures is defined as the value function of the Monge-Kantorovich optimal transportation problem (Villani, 2003, chapter 2), which does not have a closed-form solution in multivariate settings. This is coupled with a wellknown statistical curse of dimensionality for general measures (Ajtai et al., 1984, Dudley, 1969, Fournier & Guillin, 2015, Talagrand, 1992; 1994, Weed & Bach, 2019). These challenges have impeded the development of a method of projections between potentially high-dimensional probability measures. A focus so far has been on the univariate and lowdimensional setting. In particular, Chen et al. (2021), Ghodrati & Panaretos (2022), and Pegoraro & Beraha (2021) introduced frameworks for distribution-on-distribution regressions in the univariate setting for object data. Bigot et al. (2014), Cazelles et al. (2017) developed principal component analyses on the space of univariate probability measures using geodesics on the Wasserstein space. The most closely related works to ours are Bonneel et al. (2016) and Werenski et al. (2022). The former develops a regression approach in barycentric coordinates with applications in computer graphics as well as color and shape transport problems. Their method requires solving a computationally costly bilevel optimization problem, which does not need to achieve a global solution. The latter works on a tangential structure like we do, but is based on \u201cKarcher means\u201d (Karcher, 2014, Zemel & Panaretos, 2019). This implies that their method works between absolutely continuous measures with densities that are bounded away from zero, with the target measure lying in the convex hull of the control measures. The notion of projection we propose in this article circumvents these challenges in the multivariate setting by lifting the projection problem to the regular tangent space to W2 at the target measure based on generalized geodesics. These tangent spaces exist when the target measure is regular, i.e., if it does not give mass to sets of lower Hausdorff dimension. For general measures, we construct a regular tangent space using barycentric projections (Ambrosio et al., 2008, appendix 12). In contrast to the existing approaches, our method works for general probability measures, allows for the target measure to be outside the generalized geodesic convex hull of the control measures, and can be implemented by a standard constrained linear regression. The method hence transforms the projection problem on the positively curved Wasserstein space into a linear optimization problem in the regular tangent space, which provides a unique solution to the projection problem under mild assumptions. This problem takes the form of a deformable template (Boissard et al., 2015, Yuille, 1991), which connects our approach to this literature. The method can be implemented in three steps: (i) obtain the general tangent cone structure at the target measure, (ii) construct a regular tangent space if it does not exist, and (iii) perform a linear regression to carry out the projection in the tangent space. The challenging part of the implementation is lifting the problem to the tangential structure: this requires computing the corresponding optimal transport plans between the target and each measure used in the projection. Many methods have been developed for this, see for instance Benamou & Brenier (2000), Jacobs & L\u00e9ger (2020), Makkuva et al. (2020), Peyr\u00e9 & Cuturi (2019), Ruthotto et al. (2020) and references therein. Other alternatives compute approximations of the optimal transport plans via regularized optimal transport problems (Peyr\u00e9 & Cuturi, 2019), such as entropy regularized optimal transport (Galichon & Salani\u00e9, 2010, Cuturi, 2013). The proposed projection approach is compatible with any such method. We provide results for the statistical consistency when estimating the measures via their empirical counterparts in practice. To demonstrate the efficiency and utility of the proposed method, we extend the classical synthetic control estimator (Abadie & Gardeazabal, 2003, Abadie et al., 2010) to settings with observed individual heterogeneity in multivariate outcomes. This lets us perform the synthetic control method on the joint distribution of several outcomes and also estimate one set of optimal weights over all preintervention time periods, which complements the recently introduced method in Gunsilius (2022) designed for univariate outcomes. We apply this synthetic controls estimator to estimate the causal effect of Medicaid expansion on the population in individuals states and find that the policy induces nontrivial, positive effects on Medicaid enrolment, earnings, and labor supply, while its effect on employment agrees with some previous estimates, but less in magnitude compared to the other effects we estimated. 2 METHODOLOGY: 2.1 THE 2-WASSERSTEIN SPACEW2(Rd) For probability measures PX , PY \u2208 P(Rd) with supports X ,Y \u2286 Rd, respectively, the 2- Wasserstein distance W2(PX , PY ) is defined as W2(PX , PY ) , ( min \u03b3\u2208\u0393(PX ,PY ) \u222b X \u00d7Y |x\u2212 y|2 d\u03b3(x, y) ) 1 2 . (2.1) Here, | \u00b7 | denotes the Euclidean norm on Rd and \u0393(PX , PY ) , { \u03b3 \u2208P(Rd \u00d7 Rd) : (\u03c01)#\u03b3 = PX , (\u03c02)#\u03b3 = PY } is the set of all couplings of PX and PY . The maps \u03c01 and \u03c02 are the projections onto the first and second coordinate, respectively, and T#P denotes the pushforward measure of P via T , i.e. for any measurable A \u2286 Y , T#P (A) \u2261 P (T\u22121(A)). An optimal coupling \u03b3 \u2208 \u0393(PX , PY ) solving the optimal transport problem equation 2.1 is an optimal transport plan. By Prokhorov\u2019s theorem, a solution always exists in our setting. When PX is regular, i.e. when it does not give mass to sets of lower Hausdorff dimension in its support, then the optimal transport plan \u03b3 solving equation 2.1 is unique and takes the form \u03b3 = (Id\u00d7\u2207\u03d5)#PX , where Id is the identity map on Rd and \u2207\u03d5(x) is the gradient of some convex function. This result is known as Brenier\u2019s theorem (Brenier, 1991, McCann, 1997, Villani, 2003, Theorem 2.12). By definition, all measures that possess a density with respect to Lebesgue measure are regular. In the following we will distinguish between regular and general measures. The 2-Wasserstein spaceW2 , (P2(Rd),W2) is the metric space defined on the set P2(Rd) of all probability measures with finite second moments supported on Rd, with the 2-Wasserstein distance as the metric. It is a geodesically complete space in the sense that between any two measures P, P \u2032 \u2208 W2, one can define a geodesic Pt : [0, 1]\u2192W2 via the interpolation Pt , (\u03c0t)#\u03b3, where \u03b3 is an optimal transport plan and \u03c0t : Rd \u00d7 Rd \u2192 Rd is defined through \u03c0t(x, y) , (1\u2212 t)x+ ty (Ambrosio et al., 2008, McCann, 1997). Using this, it can be shown thatW2 is a positively curved metric space d > 1 (Ambrosio et al., 2008, Theorem 7.3.2) and flat for d = 1 (Kloeckner, 2010), where curvature is defined in the sense of Aleksandrov (Aleksandrov, 1951). This difference in the curvature properties is the main reason for why the multivariate setting requires different approaches compared to the established results for measures on the real line. 2.2 PROJECTIONS INW2 , BARYCENTERS, AND EXISTING CHALLENGES In linear spaces, the idea of projecting an element on a set of other elements is a linear or convex combination of these elements in the set. The analogues of averages in vector spaces are Fr\u00e9chet means or barycenters in metric spaces. For Wasserstein spaces, this concept has been introduced in our setting in Agueh & Carlier (2011) and in a more abstract sense in Carlier & Ekeland (2010). For any collection of probability measures { Pj } 16j6J \u2286 W2, their weighted barycenter P\u0304 (\u03bb) for given weights \u03bb \u2261 (\u03bb1, . . . , \u03bbJ) \u2208 \u2206J is a solution of the following minimization problem: P\u0304 (\u03bb) \u2208 arg min P\u2208P2(Rd) J\u2211 j=1 \u03bbj 2 W 22 (P, Pj). (2.2) The weights \u03bb are defined to lie in the J-dimensional probability simplex \u2206J of nonnegative vectors in RJ that sum to unity. Prokhorov\u2019s theorem implies that a solution to equation 2.2 exists (Agueh & Carlier, 2011, Proposition 2.3). Throughout, we focus on the case where \u03bb \u2208 \u2206J , because this provides a natural notion of interpolation between the given measures Pj . We could also construct a linear projection that corresponds to a \u201cgeodesic extrapolation\u201d by relaxing the requirement that all weights \u03bbj need to be non-negative. In the classical setting of random variables mapping to some Euclidean space and not random measures, this relaxation is the natural analogue to a linear regression, see Abadie et al. (2015) for a discussion. We focus on the interpolation setting because it is in line with the notion of a projection onto the convex hull spanned by other elements. All of our results can be extended to the extrapolation setting in principle. One way to extend the notion of projection between a target probability measure P0 and a set of control measures {Pj}j=1,...,J inW2 would hence consist in finding the optimal weights \u03bb\u2217 \u2208 \u2206J for which an induced barycenter P\u0304 (\u03bb\u2217) solving equation 2.2 is as close as possible to P0. This would lead to the following bi-level optimization problem, assuming that the barycenter P\u0304 (\u03bb) is unique for given \u03bb: \u03bb\u2217 \u2208 arg min \u03bb\u2208\u2206J W2(P0, P\u0304 (\u03bb)), where P\u0304 (\u03bb) = arg min P\u2208P2(Rd) J\u2211 j=1 \u03bbj 2 W 22 (P, Pj). (2.3) A version of this approach is used in Bonneel et al. (2016) to define a notion of regression between probability measures in low dimensions. The challenges with this approach are mathematical and computational. Importantly, the optimal weights \u03bb\u2217 need not be unique. This is not an issue for the applications considered in Bonneel et al. (2016), like color transport; however, it is important in statistical settings when the weights convey information used in further procedures, like causal inference via synthetic controls, where the optimal weights are used to introduced a counterfactual outcome of a treated unit had it not been treated (Abadie & Gardeazabal, 2003, Abadie et al., 2010, Abadie, 2021). Moreover, the bi-level optimization structure makes solving the problem prohibitively costly in higher dimensions. Bonneel et al. (2016) introduce a gradient descent approach based on an entropy-regularized analogue of W2 (Cuturi, 2013, Peyr\u00e9 & Cuturi, 2019) that can be implemented in low-dimensional settings. 2.3 TANGENTIAL STRUCTURES ONW2 We circumvent these difficulties by exploiting a tangential structure that can be defined on W2 (Ambrosio et al., 2008, Otto, 2001). In particular, it allows us to entirely circumvent solving a bi-level optimization problem as the one in equation 2.3. The tangential structure relies on the fact that geodesics Pt inW2 are linear in the transport plans (\u03c0t)#\u03b3. This implies a geometric tangent cone structure at each measure P \u2208 W that can be defined as the closure in P2(Rd) of the set G(P ) , { \u03b3 \u2208P2(Rd \u00d7 Rd) : (\u03c01)#\u03b3 = P, (\u03c01, \u03c01 + \u03b5\u03c02)#\u03b3 is optimal for some \u03b5 > 0 } with respect to the local distance W 2P (\u03b312, \u03b313) , min {\u222b (Rd)3 |x2 \u2212 x3|2 d\u03b3123 : \u03b3123 \u2208 \u03931(\u03b312, \u03b313) } , (2.4) where \u03b312 and \u03b313 are couplings between P and some other measures P2 and P3, respectively, and \u03931(\u03b312, \u03b313) is the set of all 3-couplings \u03b3123 such that the projection of \u03b3123 onto the first two elements is \u03b312 and the projection onto the first and third element is \u03b313 (Ambrosio et al., 2008, Appendix 12). We can then define the exponential map at P with respect to some tangent element \u03b3 \u2208 G(P ) by expP (\u03b3) = (\u03c01 + \u03c02)#\u03b3. This tangent cone can be constructed at every P \u2208 W , irrespective of its support. In the case whereP is absolutely continuous with respect to Lebesgue measure, the definition simplifies. The corresponding optimal transport plan between P and any other measure measure Q \u2208 W2 is then supported on the graph of the gradient of a convex function \u2207\u03d5 by Brenier\u2019s theorem. This allows the introduction of a regular tangent cone at P , defined via (Ambrosio et al., 2008, section 8.4) T P W2(Rd) , { t(\u2207\u03d5j \u2212 Id) : ( Id\u00d7\u2207\u03d5j ) # P is optimal in \u0393(P, (\u2207\u03d5j)#P ), t > 0 }L2(P ) , (2.5) where A L2(P ) defines the closure of the set A with respect to the distance induced by the L2-norm on P . Interestingly, this tangent cone is actually a linear tangent space (Ambrosio et al., 2008, Theorem 8.5.1). In the following, we therefore call TP W2 the regular tangent space equation 2.5. 2.4 TANGENTIAL WASSERSTEIN PROJECTIONS FOR REGULAR TARGET MEASURES: We want to define a natural analogue to the barycenter projection problem equation 2.3 in the regular tangent space of the Wasserstein space. A starting point for this is to consider a characterization of the barycenter P\u0304 (\u03bb) for fixed weights of a set {Pj}j\u2208JJK in regular tangent spaces. Agueh & Carlier (2011, Equation (3.10)) show that if at least one of the measures is absolutely continuous with respect to Lebesgue measure, then P\u0304 (\u03bb) can be characterized via J\u2211 j=1 \u03bbj ( \u2207\u03d5j \u2212 Id ) = 0, (2.6) where {\u03d5j}j\u2208JJK are the optimal transport maps from the barycenter to the respective measure Pj , i.e. (\u03d5j)#P\u0304 (\u03bb) = Pj . Each term of the summand in equation 2.6 is an element in T P\u0304 (\u03bb)W2(Rd) by construction. More generally, the condition equation 2.6 is a sufficient condition for P\u0304 (\u03bb) to be a \u201cKarcher mean\u201d (Karcher, 2014) inW2 (Zemel & Panaretos, 2019). In fact, a \u201cKarcher mean\u201d of a set of measures {Pj}j\u2208JJK is defined as the gradient of the Fr\u00e9chet functional in W2 and is characterized through equation 2.6 holding P\u0304 (\u03bb)-almost everywhere. equation 2.6 is a stronger condition because it is assumed to hold at every point in the support of P\u0304 (\u03bb), not just almost every point. \u00c1lvarez-Esteban et al. (2016) use this characterization to introduce a fixed-point approach to compute Wasserstein barycenters, and Werenski et al. (2022) use this structure to introduce a replication approach for absolutely continuous measures whose densities are bounded away from zero and whose target measure lies inside the convex hull of the control measures. Related is the recent definition of weak barycenters in Cazelles et al. (2021), where the authors replace the optimal transport maps from the classical optimal transport problem by the weak optimal transport problem introduced in Gozlan et al. (2017). Heuristically, this characterization is that of a deformable template. A measure P is a deformable template if there exists a set of deformations { \u03c8j } j=1,...,J such that \u03c8j#P = Pj , in a way that their weighted average is \u201cas close to the identity\u201d as possible. In our setting \u03c8j \u2261 \u2207\u03d5j\u2212Id (Anderes et al., 2015, Boissard et al., 2015, Yuille, 1991). In our setting of interest, we are given a target measure P0 that we want to replicate given a set {Pj}j\u2208JJK. The key idea for this is to adapt the characterization equation 2.6 with respect to the target measure P0. That is, we aim to find the optimal weights \u03bb\u2217 \u2208 \u2206J that satisfy \u03bb\u2217 , arg min \u03bb\u2208\u2206J \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj ( \u2207\u03d5j \u2212 Id )\u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) , (2.7) where \u2207\u03d5j are the optimal transport maps between the target P0 and the control measures Pj , j \u2208 JJK. Thick dashed lines are tangent vectors t(\u2207\u03d5j \u2212 Id). The gray shaded region is their convex hull in TP0W2 and \u03c0 is the projection of Id onto this convex hull. P\u03c0 , expP0(\u03c0) is the projection of P0 onto the generalized geodesic convex hull CoP0 ( {P1, P2, P3} ) \u2286 W2 (blue). We now show that this approach is in fact a projection of P0 onto the generalized geodesic convex hull of the control measures Pj with respect to P0 as illustrated in Figure 1. To define this notion of convex hull, we extend the definition of generalized geodesics (Ambrosio et al., 2008, section 9.2), and in particular the definition of WP to the multimarginal setting, by defining, for given couplings \u03b30j \u2208 \u0393(P0, Pj), j \u2208 JJK W 2P0;\u03bb(\u03b301, \u03b302, . . . , \u03b30J) , min  \u222b (Rd)J+1 J\u2211 j=1 \u03bbj \u2223\u2223xj \u2212 x0\u2223\u22232 d\u03b3 : \u03b3 \u2208 \u03931(\u03b301, . . . , \u03b30J)  , (2.8) where \u03931(\u03b301, . . . , \u03b30J) \u2286 \u0393(P0, P1, . . . , PJ) is the set of all (J + 1)-couplings \u03b3 such that the projection of \u03b3 onto the first- and j-th element is \u03b30j . Note that this definition is similar to the multimarginal definition of the 2-Wasserstein barycenter (Agueh & Carlier, 2011, Gangbo & S\u0301wie\u0328ch, 1998), but \u201ccentered\u201d at P0. Based on this, we define the generalized geodesic convex hull of measures {Pj}j\u2208JJK with respect to the measure P0 as CoP0 ({ Pj }J j=1 ) , P (\u03bb) \u2208P2(Rd) : P (\u03bb) =  J\u2211 j=1 \u03bbj\u03c0j+1  # \u03b3, \u03b3 solves W 2P0;\u03bb(\u03b301, . . . , \u03b30J), \u03b30j is optimal in \u0393(P0, Pj) \u2200j \u2208 JJK, \u03bb \u2208 \u2206 J  . (2.9) Based on these definitions we can show that our approach is a projection of the target P0 onto CoP0 ( {Pj}Jj=1 ) . Proposition 2.1. Consider a regular target measure P0 and a set {Pj}j\u2208JJK of general control measures. Construct the measure P\u03c0 as P\u03c0 , expP0  J\u2211 j=1 \u03bb\u2217j (\u2207\u03d5j \u2212 Id)  , where the optimal weights \u03bb\u2217 \u2208 \u2206J are obtained by solving equation 2.7 and \u2207\u03d5j are the optimal maps transporting P0 to Pj , respectively. Then P\u03c0 is the unique metric projection of P0 onto CoP0 ({ Pj }J j=1 ) . 2.5 TANGENTIAL WASSERSTEIN PROJECTIONS FOR GENERAL TARGET MEASURES: Proposition 2.1 holds for a regular target P0. In many practical settings, however, the target outcome is not a regular measure, as in our application to Medicaid expansion in section 4. In such settings, the corresponding optimal transport problems equation 2.1 between the target and the respective control measures Pj is only achieved via optimal transport plans \u03b30j , not maps \u2207\u03d5j . In contrast to the regular setting, these transport maps also do not need to be unique. To apply our method in this more general setting would lead us to solving \u03bb\u2217 , arg min \u03bb\u2208\u2206J W 2P0;\u03bb(\u03b301, . . . , \u03b30J), (2.10) where \u03b30j are optimal transport plans between the target P0 and the respective control measure Pj , which, again would be a computationally prohibitive bi-level optimization problem. We therefore rely on barycentric projections to reduce the complex general setting to the regular tangent space and subsequently apply the projection equation 2.7. This is computationally inexpensive, as it amounts to computing \u03bb\u2217 , arg min \u03bb\u2208\u2206J \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj ( b\u03b30j \u2212 Id )\u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) , (2.11) where b\u03b30j (x1) , \u222b Rd x2 d\u03b30j,x1(x2) are the barycentric projections of optimal transport plans \u03b30j between P0 and Pj . Here, \u03b3x1 denotes the disintegration of the optimal transport plan \u03b3 with respect to P0. This approach is a natural extension of the regular setting to general probability measures for two reasons. First, if the optimal transport plans \u03b30j are actually induced by some optimal transport map \u2207\u03b3j , then b\u03b30j reduces to this optimal transport map; in this case the general tangent cone G(P0) reduces to the regular tangent cone TP0W2 (Ambrosio et al., 2008, Theorem 12.4.4). Second, by the definition of b\u03b3 and disintegrations in conjunction with Jensen\u2019s inequality it holds for all \u03bb \u2208 \u2206J that \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj ( b\u03b30j \u2212 Id )\u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) 6W 2P0;\u03bb(\u03b301, . . . , \u03b30J). (2.12) This implies that for general P0 we can also define a convex hull based on barycentric projections, which is of the form C\u0303oP0 ({ Pj }J j=1 ) , P (\u03bb) \u2208P2(Rd) : P (\u03bb) =  J\u2211 j=1 \u03bbjb\u03b30j  # P0, \u03bb \u2208 \u2206J  . (2.13) Furthermore, the contraction property equation 2.12 implies that CoP0 \u2286 C\u0303oP0 , with equality when all transport plans are achieved via maps \u2207\u03d5j . Using these definitions, the following defines our notion of projection for general P0 and shows that it projects onto C\u0303oP0 . Proposition 2.2. Consider a general target measure P0 and a set {Pj}j\u2208JJK of general control measures. Construct the measure P\u0303\u03c0 as P\u0303\u03c0 , expP0  J\u2211 j=1 \u03bb\u2217j b\u03b30j \u2212 Id  , where the optimal weights \u03bb\u2217 \u2208 \u2206J are obtained by solving equation 2.11 and \u03b30j are optimal plans transporting P0 to Pj , respectively. Then for given optimal plans \u03b30j , P\u0303\u03c0 is the unique metric projection of P0 onto C\u0303oP0 ({ Pj }J j=1 ) . In contrast to the regular case in Proposition 2.1, the optimal plans \u03b30j transporting P0 to Pj need not be unique. However, the projection for fixed \u03b30j is unique. 3 STATISTICAL PROPERTIES OF THE WEIGHTS AND TANGENTIAL PROJECTION: We now provide statistical consistency results for our method when the corresponding measures {Pj}j\u2208JJK are estimated from data. We consider the case where the measures Pj are replaced by their empirical counterparts PNj (A) , N \u22121 j Nj\u2211 n=1 \u03b4Xn(A) for every measurable set A in the Borel \u03c3-algebra on Rd, where \u03b4x(A) is the Dirac measure and( X1j , . . . , XNj ,j ) is an independent and identically distributed set of random variables whose distri- bution is Pj . We explicitly allow for different sample sizes \u22c3J j=0Nj = N for the different measures. To save on notation we write \u03d5\u0302Nj \u2261 \u03d5\u0302j , b\u03020j \u2261 b\u0302\u03b30j ,Nj and \u03b3\u03020j \u2261 \u03b3\u0302Nj ,N0 in the following. Proposition 3.1 (Consistency of the optimal weights). Let { PNj }J j=0 be the empirical measures corresponding to the data ( X1j , . . . , XNjj )J j=0 which are independent and identical draws from Pj , respectively, and are supported on some common latent probability space (\u2126,A , P ). Assume Pj has finite second moment. As Nj \u2192 \u221e for all j \u2208 JJK, the corresponding optimal weights \u03bb\u0302\u2217N = ( \u03bb\u0302\u2217N1 , . . . , \u03bb\u0302 \u2217 NJ ) \u2208 \u2206J obtained via \u03bb\u0302\u2217N , arg min \u03bb\u2208\u2206J \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj ( b\u03020j \u2212 Id )\u2225\u2225\u2225\u2225\u2225\u2225 2 L2(PN0 ) , (3.1) satisfy P (\u2223\u2223\u2223\u03bb\u0302\u2217N \u2212 \u03bb\u2217\u2223\u2223\u2223 > \u03b5)\u2192 0 for all \u03b5 > 0 , where \u03bb\u2217 solve equation 2.11. This consistency result directly implies consistency of the optimal weights in case the optimal transport problems between PN0 and each PNj are achieved by optimal transport maps \u2207\u03d5\u0302Nj . We also have a consistency result for the empirical counterparts P\u0303\u03c0,N of the optimal projection P\u0303\u03c0 . Corollary 3.1 (Consistency of the optimal projections). In the setting of Proposition 3.1, the estimated projections P\u0303\u03c0,N converge weakly in probability to the projection P\u0303\u03c0 as Nj \u2192 \u221e for all j \u2208 JJK. Proposition 3.1 and Corollary 3.1 hold in all generality and without any assumptions on the corresponding measures Pj , except that they possess finite second moments. To get stronger results, for instance parametric rates of convergences one needs to make strong regularity assumptions on the measures Pj . Without these, the rate of convergence of optimal transport maps in terms of expected square loss is as slow as n\u22122/d (H\u00fctter & Rigollet, 2021). Under such additional regularity conditions, the results for the asymptotic properties are standard, because the proposed method reduces to a classical semiparametric estimation problem, as the weights \u03bbj are finite-dimensional. 4 APPLICATION TO ESTIMATING CAUSAL EFFECTS VIA SYNTHETIC CONTROLS: In this section, we introduce a practical application of our method to causal inference by estimating the causal effect of Medicaid expansion in Montana in 2016 via our generalization of the classical synthetic controls estimator (Abadie & Gardeazabal, 2003, Abadie et al., 2010) to multivariate outcome distributions, complementing the univariate method in Gunsilius (2022). In particular, we can account for general heterogeneity in the multivariate outcomes by considering the entire distribution of several outcomes of interest jointly, something which has been out of reach of existing synthetic controls methods. We use the POT package (Flamary et al., 2021) to obtain the optimal transport plans, and CVXPY (Diamond & Boyd, 2016, Agrawal et al., 2018) to compute equation 2.7. Additional details of our applications as well as other simulations are contained in Appendix B. We study the effect of health insurance coverage following state-level Medicaid expansion in the United States. A provision within the Affordable Care Act (ACA) allows states to decide whether to expand Medicaid for low-income households. Some states decided to adopt such expansions early on, while others did not (and still have not done so). We investigate the economic and behavioral effects of Medicaid expansion. Specifically, we consider some first-order effects (i.e. the extensive margin of Medicaid enrollment post-expansion and disemployment effects) and second-order effects (i.e. income and labor supply effects) of expanded Medicaid access. The observational data we use is the American Community Survey (ACS; additional details in Appendix B). From it, we collect the following variables: 1. HINSCAID, which indicates whether the individual interviewed is covered by Medicaid, 2. EMPSTAT, which indicates employment status (either employed or unemployed), 3. UHRSWORK, which indicates number of hours worked, 4. INCWAGE, which indicates previous-twelve-month labor income level. We measure labor hours supplied and labor income in logs instead of levels. We consider Montana as the treated unit for this application. Montana adopted such expansion in 2016, which is the treated year. For control units, we use the twelve states for which such expansion has never occurred. As of 2022, the twelve states are: Alabama, Florida, Georgia, Kansas, Mississippi, North Carolina, South Carolina, South Dakota, Tennessee, Texas, Wisconsin, Wyoming. We use data from 2010 to 2016 to estimate optimal weights for the control states in order to create the \u201csynthetic Montana\u201d, i.e. Montana had it not adopted Medicaid expansion. Once we obtain the optimal weights \u03bb\u2217, we estimate the counterfactual outcomes of interest for the four years post-Medicaid expansion in Montana. This involves solving equation 2.2 with the optimal weights \u03bb\u2217 obtained in the preintervention periods. Details of sample selection and estimating \u201csynthetic Montana\u201d are described in Appendix B.3. As we show there, the combination of control states replicating Montana is close to actual Montana in the pre-intervention period, implying that the observed differences in postintervention periods can be attributed to the causal effect of the Medicaid expansion. Furthermore, therein, we also develop and show the results of a placebo permutation test in the sense of Abadie et al. (2010) to validate our findings. Consistent with findings in Courtemanche et al. (2017), Mazurenko et al. (2018), we find significant first- and second order effects of Medicaid expansion, which are summarized in the top row and the bottom row of Figure 2. \u201cSynthetic Montana\u201d has much lower proportion of individuals insured under Medicaid, suggesting that expanding Medicaid eligibility directly affects the extensive margin of Medicaid enrollment. The disemployment effect is less pronounced in comparison to the enrollment effect we estimated, but nonetheless positive and nontrivial, consistent with the findings in, e.g., Peng et al. (2020), but inconsistent with those in, e.g., Gooptu et al. (2016). We also find positive second-order effects, summarized in the bottom row of Figure 2. Additional details are in Appendix B. 5 CONCLUSION: We have developed a projection method between sets of probability measures supported on Rd based on the tangential structure of the 2-Wasserstein space. Our method seeks to best approximate some target distribution that is potentially multivariate, using some chosen set of control distributions. We provide an implementation which gives unique, interpretable weights in a setting of regular probability measures. For general probability measures, we construct our projection by first creating a regular tangent space through applying barycentric projection to optimal transport plans. Our application to evaluating the first- and second-order effects of Medicaid expansion in Montana via an extension of the synthetic controls estimator (Abadie & Gardeazabal, 2003, Abadie et al., 2010) demonstrates the method\u2019s efficiency and the necessity to have a method that is applicable for general proabbility measures. The approach still works without restricting optimal weights to be in the unit simplex, which would allow for extrapolation beyond the convex hull of the control units, providing a notion of tangential regression. It can also be extended to a continuum of measures, using established consistency results of barycenters (e.g. Le Gouic & Loubes, 2017). A PROOFS FROM THE MAIN TEXT: Proof of Proposition 2.1. Define the following closed and convex subset C \u2286 L2(P0) for fixed optimal transportation maps between P0 and Pj , denoted\u2207\u03d5j : C , f \u2208 L2(P0) : f = J\u2211 j=1 \u03bbj \u2207\u03d5j for some \u03bb \u2208 \u2206J  . Recall that the transport maps \u2207\u03d5j exist since P0 is regular. Using C, we can rewrite equation 2.7 as arg min \u03bb\u2208\u2206J \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj \u2207\u03d5j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) = arg min f\u2208C \u2016f \u2212 Id\u20162L2(P0) , which by definition is the metric projection of Id onto C. Since C is a non-empty closed and convex subset of the Hilbert space L2(P0), this metric projection exists and is unique (Aliprantis & Border, 1999, Theorem 6.53). Moreover, if Id \u2208 C, then \u03c0C = Id; otherwise, \u03c0C \u2208 \u2202C, where \u2202C is the boundary of C (Aliprantis & Border, 1999, Lemma 6.54). Since P0 is regular, the exponential map is continuous. In fact, for every j 6= k, W 22 (Pj , Pk) = W 2 2 ((\u2207\u03d5j)#P0, (\u2207\u03d5k)#P0) 6 \u222b Rd \u2223\u2223\u2207\u03d5j \u2212\u2207\u03d5k\u2223\u22232 dP0(x). In other words, the distance between Pj and Pk inW2(Rd) is smaller than that between corresponding elements\u2207\u03d5j ,\u2207\u03d5k in the tangent space. This implies continuity of the exponential map. Furthermore, in this regular setting, the exponential map sends convex sets in T P0W2 to generalized geodesically convex sets in W2. Mechanically, for any two (scaled) elements t(\u2207\u03d5j \u2212 Id) and s(\u2207\u03d5k \u2212 Id) in TP0W2, and any \u03c1 \u2208 [0, 1], expP0(\u03c1t(\u2207\u03d5j \u2212 Id) + (1\u2212 \u03c1)s(\u2207\u03d5k \u2212 Id)) = expP0((\u03c1t\u2207\u03d5j + (1\u2212 \u03c1)s\u2207\u03d5k)\u2212 (\u03c1t+ (1\u2212 \u03c1)s) Id) = expP0 \u03c1\u0303[[\u03c1t \u03c1\u0303 \u2207\u03d5j + (1\u2212 \u03c1)s \u03c1\u0303 \u2207\u03d5k ] \u2212 Id ] = ([ \u03c1t\u2207\u03d5j + (1\u2212 \u03c1)s\u2207\u03d5k ] + (1\u2212 \u03c1\u0303) Id ) # P0 = ([ \u03c1t(\u2207\u03d5j \u2212 Id) + (1\u2212 \u03c1)s(\u2207\u03d5k \u2212 Id) ] + Id ) # P0 where \u03c1\u0303 , \u03c1t + (1 \u2212 \u03c1)s. This is a generalized geodesic connecting Pj and Pk, via the optimal transport map between them and P0 (Ambrosio et al., 2008, section 9.2). The same argument holds when extending generalized geodesics to generalized barycenters by taking convex combination of more measures than a binary interpolation with respect to \u03c1. Mechanically, for any \u03bb \u2208 \u2206J and tj > 0 for all j \u2208 JJK, expP0  J\u2211 j=1 \u03bbjtj(\u2207\u03d5j \u2212 Id)  = expP0  J\u2211 j=1 \u03bbjtj \u2207\u03d5j \u2212 J\u2211 j=1 \u03bbjtj Id  = expP0 \u03c1\u0303J  J\u2211 j=1 \u03c1\u0303J\u2207\u03d5j \u2212 Id   =   J\u2211 j=1 \u03bbjtj\u2207\u03d5j + (1\u2212 \u03c1\u0303J) Id  # P0 =   J\u2211 j=1 \u03bbjtj(\u2207\u03d5j \u2212 Id) + Id  # P0 where \u03c1\u0303J , \u2211J j=1 \u03bbjtj . This proves the exponential map is generalized geodesically convex. From above it follows that P\u03c0 , expP0(\u03c0C) is either in the interior of C, which is the case if Id \u2208 C, or on its boundary: since \u03c0C \u2208 \u2202C, expP0(\u03c0C) \u2208 expP0(\u2202C). By continuity of the exponential map it follows that expP0(\u2202C) = \u2202 expP0(C). Combining all steps above show that P\u03c0 is a geodesic metric projection of P0 onto the geodesic convex hull of { Pj }J j=1 . Proof of Proposition 2.2. The result follows from the same argument as the proof of Proposition 2.1. Theorem 12.4.4 in Ambrosio et al. (2008) shows that TP0W2 is the image of the barycentric projection of measures in the general tangent cone: b\u03b3(x) is an optimal transport map if \u03b3 is an optimal transport plan. But the exponential map satisfies expP0(v) = (v + Id)# P0 for all v \u2208 TP0W2. This implies that P\u0303\u03c0 , expP0  J\u2211 j=1 \u03bb\u2217j b\u03b30j \u2212 Id  =  J\u2211 j=1 \u03bb\u2217j b\u03b30j  # P0 \u2208 C\u0303oP0 ({ Pj }J j=1 ) , since the convex combination of elements in the subgradients of convex functions lie in the subgradient of a convex function (provided the subgradient of each convex function is nonempty, which is the case here). Then the continuity and generalized convexity of the exponential map for elements in the regular tangent space TP0W2 implies the result. Proof of Proposition 3.1. We split the proof into two parts. In the first part we prove the convergence in probability of the family of objective functions equation 3.1 to their population counterparts equation 2.11 if the empirical measures PNj converge weakly in probability to the population measures Pj . In the second step we use the fact that \u03bb\u0302\u2217 is a classical semiparametric estimator (Andrews, 1994, Newey & McFadden, 1994) to derive the convergence of the weights. Step 1: Convergence of the objective functions To show the convergence of the of the objective functions for obtaining the weights \u03bb\u2217, we write\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbjb0j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) \u2212 \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj b\u03020j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(PN0 ) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u222b \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjb0j(x)\u2212 x \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0 \u2212 \u222b \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbj b\u03020j(x)\u2212 x \u2223\u2223\u2223\u2223\u2223\u2223 2 dPN0 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 . We hence want to show that lim\u2227 j Nj\u2192\u221e \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u222b \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjb0j(x)\u2212 x \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0(x)\u2212 \u222b \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbj b\u03020j(x)\u2212 x \u2223\u2223\u2223\u2223\u2223\u2223 2 dPN0(x) \u2223\u2223\u2223\u2223\u2223\u2223\u2223 = 0 , where \u2227 j Nj \u2261 min {N0, . . . , NJ}. We split the result into two parts. The first part shows that lim inf\u2227 j Nj\u2192\u221e \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbj b\u03020j(x0)\u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 dPN0(x0) > \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjb0j(x0)\u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0(x0). In the second part we use the L2(P0) convergence of the barycentric projections to prove that the limit exists and coincides with the limit inferior. For the first part, we have lim inf\u2227 j Nj\u2192\u221e \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbj b\u03020j(x0)\u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 dPN0(x0) = lim inf\u2227 j Nj\u2192\u221e \u222b (Rd)J+1 \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3\u0302N (x0, x1, . . . , xJ), where \u03b3\u0302N (x0, x1, . . . , xJ) is a measure that solves min  \u222b (Rd)J+1 J\u2211 j=1 \u03bbj \u2223\u2223xj \u2212 x0\u2223\u22232 d\u03b3 : \u03b3 \u2208 \u03931(\u03b3\u030201, . . . , \u03b3\u03020J)  , \u03b3\u03020j are the optimal couplings between PN0 and P\u0303Nj , ( b\u03020j ) # PN0 . Since all measures are defined on the complete and separable space Rd, and by assumption of finite second moments, i.e. max j\u2208JJK sup Nj \u222b \u2223\u2223xj \u2212 x0\u2223\u22232 d\u03b3\u03020j < +\u221e , it holds that each sequence \u03b3\u03020j is tight by Ulam\u2019s theorem (Dudley, 2018, Theorem 7.1.4). Using the fact that \u03bb \u2208 \u2206J and \u03b3\u0302N \u2208 \u03931 (\u03b3\u030201, . . . , \u03b3\u03020J), applying Jensen\u2019s inequality gives us max j\u2208JJK sup Nj \u222b (Rd) J+1 \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3\u0302N 6 max j\u2208JJK sup Nj J\u2211 j=1 \u03bbj \u222b Rd \u2223\u2223xj \u2212 x0\u2223\u22232 d\u03b3\u03020j < +\u221e , which implies that \u03b3\u0302N is tight. By Prokhorov\u2019s theorem, there exists a subsequence \u03b3\u0302Nk that weakly converges to a limit measure \u03b3. Therefore, by the continuity of the map (x0, xj) 7\u2192 \u2211 j \u03bbjxj \u2212 x0, it follows from classical convergence results (Ambrosio et al., 2008, Lemma 5.1.12(d)) that lim inf\u2227 j Nj\u2192\u221e \u222b (Rd)J+1 \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3\u0302N (x0, x1, . . . , xJ) = \u222b (Rd)J+1 \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3(x0, . . . , xJ). Furthermore, by the same argument via Jensen\u2019s inequality, i.e., \u222b (Rd)J+1 \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3(x0, . . . , xJ) 6 J\u2211 j=1 \u222b (Rd)2 \u2223\u2223\u03bbjxj \u2212 x0\u2223\u22232 d\u03b30j(x0, xj) < +\u221e , it follows that the limit \u03b3 \u2208 \u03931 (\u03b301, . . . , \u03b30J). Now note that by the definition of disintegration it follows that (Ambrosio et al., 2008, Lemma 5.3.2) \u03b3 \u2208 \u03931(\u03b301, . . . , \u03b30J) \u21d0\u21d2 \u03b3x0 \u2208 \u0393 ( \u03b31|x0 , . . . , \u03b3J|x0 ) , where \u03b3 = \u222b \u03b3x0 dP0(x0) and \u03b30j = \u222b \u03b3j|x0 dP0(x0) are the disintegrations of \u03b3 and \u03b30j with respect to P0, respectively. Therefore, we have\u222b (Rd)J+1 \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3(x0, . . . , xJ) = \u222b Rd \u222b (Rd) J \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjxj \u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 d\u03b3x0(x1, . . . , xJ) dP0(x0) > \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 \u222b (Rd) J  J\u2211 j=1 \u03bbjxj \u2212 x0  d\u03b3x0(x1, . . . , xJ) \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0(x0) = \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbj \u222b (Rd) J xj d\u03b3x0(x1, . . . , xJ)\u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0(x0) = \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbj \u222b Rd xj d\u03b3j|x0(xj)\u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0(x0) = \u222b Rd \u2223\u2223\u2223\u2223\u2223\u2223 J\u2211 j=1 \u03bbjb0j(x0)\u2212 x0 \u2223\u2223\u2223\u2223\u2223\u2223 2 dP0(x0), where the third lines follows from Jensen\u2019s inequality and the fifth line from \u03b3x0 \u2208 \u0393 ( \u03b31|x0 , . . . , \u03b3J|x0 ) . This shows the first part. For the second part we use the fact that each barycentric projection b\u03020j(x1) is an optimal transport map between PN0 and P\u0303Nj if \u03b3\u03020j is an optimal transport plan between PN0 and PNj , which follows from Theorem 12.4.4 in Ambrosio et al. (2008). As before, we know that ( b\u03020j ) # PN0 is a tight sequence that converges to some P\u0303j . By definition and the fact that b\u03020j is the gradient of a convex function between PN0 and P\u0303Nj , b\u03020j is the unique optimal transport map between PN0 and P\u0303Nj for all Nj and all j. Since the measures Pj have finite second moments by assumption, we have lim sup N0\u2227Nj\u2192\u221e \u222b Rd |xj |2 dP\u0303Nj = lim sup N0\u2227Nj\u2192\u221e \u222b Rd \u2223\u2223\u2223\u0302b0j(x0)\u2223\u2223\u22232 dPN0 = lim sup N0\u2227Nj\u2192\u221e \u222b Rd \u2223\u2223\u2223\u2223\u222b Rd xj d\u03b3\u0302j|x0(xj) \u2223\u2223\u2223\u22232 dPN0 6 lim sup N0\u2227Nj\u2192\u221e \u222b (Rd) 2 \u2223\u2223xj\u2223\u22232 d\u03b3\u03020j(x0, xj) = \u222b (Rd) 2 \u2223\u2223xj\u2223\u22232 d\u03b30j(x0, xj) < +\u221e, where the last equality follows from the tightness of \u03b3\u03020j , as shown earlier. Therefore, by standard stability results for optimal transport maps, in particular Proposition 6 in Zemel & Panaretos (2019), it holds that b\u03020j converges uniformly on every compact subset K \u2286 Rd in the interior of the support of the limit measure P\u0303j , that is lim N0\u2227Nj\u2192\u221e sup x0\u2208K \u2223\u2223\u2223\u0302b0j(x0)\u2212 vj(x0)\u2223\u2223\u2223 = 0 , where vj is the optimal transport map between P0 and P\u0303j . We now show that vj = b0j P0-almost everywhere. From the local uniform convergence, we can then derive \u201cstrong L2-convergence\u201d (Ambrosio et al., 2008, Definition 5.4.3) of the potentials: lim sup N0\u2227Nj\u2192\u221e \u2223\u2223\u2223\u2223\u2223\u2225\u2225\u2225b\u03020j\u2225\u2225\u2225L2(PN0 ) \u2212 \u2225\u2225vj\u2225\u2225L2(P0) \u2223\u2223\u2223\u2223\u2223 6 lim sup N0\u2227Nj\u2192\u221e \u2223\u2223\u2223\u2223\u2223\u2225\u2225\u2225b\u03020j\u2225\u2225\u2225L2(PN0 ) \u2212 \u2225\u2225vj\u2225\u2225L2(PN0 ) \u2223\u2223\u2223\u2223\u2223+ lim supN0\u2192\u221e \u2223\u2223\u2223\u2223\u2225\u2225vj\u2225\u2225L2(PN0 ) \u2212 \u2225\u2225vj\u2225\u2225L2(P0) \u2223\u2223\u2223\u2223 6 lim sup N0\u2227Nj\u2192\u221e \u2225\u2225\u2225b\u03020j \u2212 vj\u2225\u2225\u2225 L2(PN0 ) + lim sup N0\u2192\u221e \u2223\u2223\u2223\u2223\u2225\u2225vj\u2225\u2225L2(PN0 ) \u2212 \u2225\u2225vj\u2225\u2225L2(P0) \u2223\u2223\u2223\u2223 Now the first term converges to zero by H\u00f6lder\u2019s inequality and the local uniform convergence of the optimal transport maps from above. The second term satisfies lim sup N0\u2192\u221e \u2223\u2223\u2223\u2223\u2225\u2225vj\u2225\u2225L2(PN0 ) \u2212 \u2225\u2225vj\u2225\u2225L2(P0) \u2223\u2223\u2223\u2223 = lim sup N0\u2192\u221e \u2223\u2223\u2223\u2223\u2223 (\u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dPN0)1/2 \u2212 (\u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dP0)1/2 \u2223\u2223\u2223\u2223\u2223 6 lim sup N0\u2192\u221e \u2223\u2223\u2223\u2223\u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dPN0 \u2212\u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dP0\u2223\u2223\u2223\u22231/2 . But since P0 has finite second moments, it holds that this term also converges to zero. Based on this we can show that \u03b3\u03020j \u2261 ( Id, b\u03020j ) converge weakly to \u03b30j \u2261 ( Id, vj ) . Indeed, if \u03b30j is a limit point of the sequence \u03b3\u03020j , it holds that\u222b (Rd) 2 \u2223\u2223xj\u2223\u22232 d\u03b30j(x0, xj) 6 lim inf N0\u2227Nj\u2192\u221e \u222b (Rd) 2 \u2223\u2223xj\u2223\u22232 d\u03b3\u03020j(x0, xj) 6 lim sup N0\u2227Nj\u2192\u221e \u222b (Rd) 2 \u2223\u2223xj\u2223\u22232 d\u03b3\u03020j(x0, xj) = lim sup N0\u2227Nj\u2192\u221e \u222b Rd \u2223\u2223\u2223\u0302b0j(x0)\u2223\u2223\u22232 dPN0(x0) = \u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dP0(x0). Disintegrating the left-hand side with respect to P0, and applying Jensen\u2019s inequality, gives\u222b (Rd) 2 \u2223\u2223xj\u2223\u22232 d\u03b30j(x0, xj) = \u222b Rd \u222b Rd \u2223\u2223xj\u2223\u22232 d\u03b3j|x0(xj) dP0(x0) > \u222b Rd \u2223\u2223\u2223\u2223\u222b Rd xj d\u03b3j|x0(xj) \u2223\u2223\u2223\u22232 dP0(x0) = \u222b Rd \u2223\u2223b0j(x0)\u2223\u22232 dP0(x0), that is, \u222b Rd \u2223\u2223b0j(x0)\u2223\u22232 dP0(x0) 6 \u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dP0(x0). But since vj is an optimal transport map between P0 and P\u0303j by definition, it holds that\u222b Rd \u2223\u2223b0j(x0)\u2223\u22232 dP0(x0) > \u222b Rd \u2223\u2223vj(x0)\u2223\u22232 dP0(x0) , which implies that equality holds and we have that\u222b Rd [\u2223\u2223b0j(x0)\u2223\u22232 \u2212 \u2223\u2223vj(x0)\u2223\u22232] dP0(x0) = 0 , which implies that b0j = vj P0-almost everywhere. We have hence shown that ( Id, b\u03020j ) # PN0 converges weakly to ( Id, b0j ) # P0 for all j, where the barycentric projection b0j is the optimal transport map between P0 and P\u0303j (e.g. Villani, 2003, Theorem 2.12.(iii)). Moreover, we have shown \u201cstrong L2-convergence\u201d of the barycentric projections in terms of Definition 5.4.3 in Ambrosio et al. (2008). Since this holds for all j, it also holds for their convex combination for fixed weights \u03bb \u2208 \u2206J . Putting everything together, we then have that lim\u2227 j Nj\u2192\u221e \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj b\u03020j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(PN0 ) = \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbjb0j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) . Since all observable measures Pj are empirical measures, they converge weakly in probability (Varadarajan, 1958), which implies that lim\u2227 j Nj\u2192\u221e P  \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbj b\u03020j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(PN0 ) \u2212 \u2225\u2225\u2225\u2225\u2225\u2225 J\u2211 j=1 \u03bbjb0j \u2212 Id \u2225\u2225\u2225\u2225\u2225\u2225 2 L2(P0) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 > \u03b5  = 0 for all \u03b5 > 0. This shows convergence in probability of the objective function for fixed \u03bb. Step 2: Convergence of the optimal weights \u03bb\u0302\u2217N The convergence of the optimal weights now follows from standard consistency results in semiparametric estimation. In particular, the objective functions are all convex for any \u03bb \u2208 RJ , which implies that they converge uniformly on any compact set (Rockafellar, 1970, Theorem 10.8), so the objective function converges uniformly on \u2206J . Now a standard consistency result like Theorem 2.1 in Newey & McFadden (1994) then implies that lim\u2227 j Nj\u2192\u221e P (\u2223\u2223\u2223\u03bb\u0302\u2217N \u2212 \u03bb\u2217\u2223\u2223\u2223 > \u03b5) = 0 for all \u03b5 > 0 , which is what we wanted to show. Note that the result can also be shown if we allow the weights \u03bb to be negative, i.e., if we only require that \u2211J j=1 \u03bbj = 1. In this case, the fact that the objective functions are convex and coercive implies that an optimal \u03bb\u2217 will be achieved at the interior of the extended Euclidean space, from which consistency follows by Theorem 2.7 in Newey & McFadden (1994). Proof of Corollary 3.1. We want to show that (\u2211J j=1 \u03bb\u0302 \u2217 Nj b\u03020j ) # PN0 converges weakly in prob- ability to (\u2211J j=1 \u03bb \u2217 j b0j ) # P0, where \u03bb\u0302\u2217N , ( \u03bb\u0302\u2217N1 , . . . , \u03bb\u0302 \u2217 NJ ) are the optimal weights obtained in equation 3.1 and equation 2.11, respectively. The result follows by applying the extended continuous mapping theorem (van der Vaart & Wellner, 2013, Theorem 1.11.1) as follows. As shown in the proof of Proposition 3.1 we have \u201cstrong L2-convergence\u201d of the maps\u2211J j=1 \u03bb\u0302 \u2217 Nj b\u03020j \u2212 Id to \u2211J j=1 \u03bb \u2217 j b0j \u2212 Id. Therefore, by Theorem 5.4.4 (iii) in Ambrosio et al. (2008), it holds that lim \u2227jNj\u2192\u221e \u222b Rd f x0, J\u2211 j=1 \u03bb\u0302\u2217Nj b\u03020j(x0)\u2212 x0  dPN0(x0) = \u222b Rd f x0, J\u2211 j=1 \u03bb\u2217j b0j(x0)\u2212 x0  dP0(x0) for any continuous function such that |f(x0)| 6 C1 + C2 |x0 \u2212 x0|2 for all x0 in the support of P0, where C1, C2 < +\u221e are some constants and x0 in some element in the support of P0 (Ambrosio et al., 2008, equation (5.1.21)). In particular, this holds for any bounded and continuous function f , which implies that lim \u2227jNj\u2192\u221e \u222b Rd f  J\u2211 j=1 \u03bb\u0302\u2217Nj b\u03020j(x0) dPN0(x0) = \u222b Rd f  J\u2211 j=1 \u03bb\u2217j b0j(x0) dP0(x0) for any bounded and continuous function, which implies that (\u2211J j=1 \u03bb\u0302 \u2217 Nj b\u03020j ) # PN0 converges weakly to (\u2211J j=1 \u03bb \u2217 j b0j ) # P0 if PNj converge weakly to Pj , j \u2208 JJK. Now we apply the extended continuous mapping theorem (van der Vaart & Wellner, 2013, Theorem 1.11.1). Equip P2(Rd) with any metric d\u0303(\u00b7, \u00b7) that metrizes weak convergence. We define the maps g :\u00d7Jj=0 ( P2(Rd), d\u0303 ) j \u2192 ( P2(Rd), d\u0303 ) by g (P0, . . . , PJ) =  J\u2211 j=1 \u03bb\u2217j b0j  # P0 , and analogously for their empirical counterparts gN . Note that g and gN are non-random functions if the measures Pj and PNj are non-random themselves for all j \u2208 JJK. Moreover, by definition, g and gN are continuous maps because \u2211J j=1 \u03bb \u2217 j b0j are gradients of convex functions, which are continuous P0-almost everywhere; the same thing holds for their empirical counterparts. Now from what we have shown above and in Proposition 3.1, it holds that gN ( PN0 , . . . ,PNJ ) \u2192 g (P0, . . . , PJ) as PNj converge weakly to Pj . Since {PNj}Jj=1 here instead are the only random elements in \u00d7Jj=0 ( P2(Rd), d\u0303 ) j , the extended continuous mapping theorem implies that lim\u2227 j Nj\u2192\u221e P ( d\u0303 ( gN ( PN0 , . . . ,PNJ ) , g (P0, . . . , PJ) ) > \u03b5 ) = 0 for all \u03b5 > 0 , which is what we wanted to show. B DETAILS FROM SIMULATIONS AND APPLICATIONS: B.1 MIXTURES OF GAUSSIAN SIMULATION As a simple first simulation to show that the method is applicable in higher dimensions, we consider mixtures of Gaussian in dimension d = 20. We draw from the following Gaussians: Xj \u223c N ( \u00b5j ,\u03a3 ) , j = 0, 1, 2, 3 , where \u00b50 = [10, 10, . . . , 10], \u00b51 = [50, 50, . . . , 50], \u00b52 = [200, 200, . . . , 200], \u00b53 = [\u221250,\u221250, . . . ,\u221250] and \u03a3 = Id20 +0.8 Id\u221210, with Id \u2212 10 the 10 \u00d7 10 matrix with zeros on the main diagonal and ones on all off-diagonal terms. We then define the following mixtures: Y0 as target, and Y1, Y2, and Y3 as controls, where Y0 = 0.3X0 + 0.6X1 + 0.1X2 Y1 = 0.8X0 + 0.1X1 + 0.1X2 Y2 = 0.2X1 + 0.7X2 + 0.1X3 Y3 = 0.2X0 + 0.2X2 + 0.6X3 We sample N0 = N1 = N2 = N3 = 10000 points from each mixture. The estimated optimal weights are \u03bb\u2217 = [0.8204, 0.1796, 0]. The results are as expected, since Y1 is defined by the same set of X\u2019s as those for Y0. In the case of Y3, it is not defined with X1, therefore we expected the associated weight to be very small. Table 1 suggests the projection is close to the target distribution when only considering the mean, despite only having three control units. B.2 SIMULATION: REPLICATING IMAGES To demonstrate the property that our method provides sparse weights, we provide an application on replicating a target image of an object using images of the same object taken from different angles. We use the Lego Bricks dataset available from Kaggle, which contains approximately 12,700 images of 16 different Lego bricks. These images are in RGBA format. For our application, the target image is contained in Figure 3(A), and the control images are in Figure 4. The result, as shown in Figure 3, indicates the optimally-weighted projection replicates the target image well, even in a setting with only a few controls. We note two interesting findings. One, only the control images in the first row of Figure 4 received nontrivial weights; all others received essentially zero weights. This suggests that our method use most information from controls which look sufficiently like the target and demonstrates that our method provides sparse weights. Two, even with few controls, the optimally weighted projection approximates the target well in this application. B.3 DETAILS OF MEDICAID EXPANSION APPLICATION We use the ACS data with harmonized variables made available by IPUMS, a unified source of Census and survey data collected around the world. The data is at the household-person-year level. For our application, we select the household head and the spouse as our unit of analysis. The continuous outcomes are adjusted using the person-level sample weights available in the data. We adopt the following sample restriction criteria: we included individuals \u2022 of working age, i.e. between ages 18 and 65 \u2022 who has no missing outcomes (for those listed in the main text) \u2022 who has no top-coded responses \u2022 who are either household heads or their spouses The sample size breakdown by states are follows: We randomly select N = 1500 observations from each unit for estimating \u03bb\u2217. In the Python implementation, we face a challenge where if the entries of the target and control data are large enough, equation 2.7 becomes too large for CVXPY to compute an optimal solution. Therefore, we introduce a stabilizing constant to prevent this. This stabilizing constant is determined by the mean value and dimensions of the target distribution, and the number of controls. The weights we obtained are sparse and are displayed in Table 3. We check whether the obtained weights are fit for creating synthetic Montana by examining if they well-approximate actual Montana in the pre-treatment period. As seen in Figures 5 and 6, our projection is similar to the actual data. Once we obtain the optimal weights \u03bb\u2217, we estimate the counterfactual outcomes of interest for the four years after Medicaid expansion in Montana (namely, between 2017 and 2020). This involves solving equation 2.2 with \u03bb\u2217 obtained from the pre-intervention period. Implementation-wise, we computed the free-support barycenter, using the POT package; this does not fix the support of the barycenter a priori, and allows it to be different from those of the control distributions. We plot the densities and distributions of the counterfactual outcomes in Figure 2 of the main text. To perform inference on the estimated causal effect, we use a placebo permutation test in analogy to Abadie et al. (2010), Gunsilius (2022). The idea is to repeatedly apply the procedure described above to each control unit, pretending in turn each control unit is the treated unit. Post-intervention, if an actual treatment effect only appears in the treatment unit (Montana, in this application), then the estimated effect for the actual treatment unit should be among the largest. We plot the 2-Wasserstein distance between the treated, joint distribution of all outcomes and the pre-/post-intervention optimal projection (i.e. equation 2.2 with \u03bb\u2217). We present two sets of results in Figure 7: in panel (A), the optimal projection is computed using \u03bb\u2217 estimated using all years in the pre-intervention period; in panel (B), the \u03bb\u2217 used is constructed from taking a simple average of weights estimated in each year of the pre-intervention period. Our results suggest that the estimated causal effect is valid in the post-intervention period, as we consistently observe the largest difference coming from Montana, especially from 2017-2019. The effect is less pronounced in 2020, however. To accompany Figure 7, we also compute p-values, which we denote by and define as pt , r(d1t) J+1 , where d1t is the 2-Wasserstein distance from the optimal projection to actual distribution when the target unit is Montana, r(d1t) is the rank of d1t amongst djts at given time t, and J is the number of control units. Results are described in Table 4. A smaller pt value indicates larger treatment effect. We observed that r(d1t) = 1 for 2018 and 2019, implying a nontrivial effect of the Medicaid expansion in Montana during these years. The values are pt are comparably higher in 2017 and 2020, which we attribute to the fact that it was the first year of the policy implementation, and the COVID-19 pandemic, respectively."}, {"role": "assistant", "content": "I have reviewed the ICLR 2023 Reviewer Guide. Key aspects include:\n\n*   **Timeliness and Substantiveness:** Reviews should be on time and provide meaningful insights.\n*   **Constructive Criticism:** Focus on helping authors improve their work.\n*   **Code of Ethics:** Adherence to the ICLR Code of Ethics is mandatory.\n*   **Key Questions:** Reviewers should address the problem tackled, motivation, claim support, and significance of the work.\n*   **Review Structure:** Summarize contribution, list strengths/weaknesses, state recommendation with reasons, ask clarifying questions, and provide additional feedback.\n*   **Discussion Phase:** Active engagement in discussions with authors and other reviewers is crucial.\n*   **Borderline Paper Meetings:** ACs will meet with reviewers for borderline cases.\n*   **No State-of-the-Art Requirement:** Submissions can be valuable without achieving state-of-the-art results.\n\nNow I will proceed to Step 2: Paper Context Research and Novelty Investigation. I will read the paper, clarify terminology, explore related work, and assess novelty.\n"}, {"role": "user", "content": "Please finish the review and give the FINAL DECISION line."}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'max_results': 5, 'query': 'Wasserstein projections'}, name='ArxivTool'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'max_results': 5, 'query': 'Tangential Wasserstein space'}, name='ArxivTool'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'query': 'Synthetic control estimator multivariate outcomes', 'max_results': 5}, name='ArxivTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Found 5 paper(s) on arXiv\",\n  \"output\": [\n    {\n      \"title\": \"Fixed Support Tree-Sliced Wasserstein Barycenter\",\n      \"authors\": [\n        \"Yuki Takezawa\",\n        \"Ryoma Sato\",\n        \"Zornitsa Kozareva\",\n        \"Sujith Ravi\",\n        \"Makoto Yamada\"\n      ],\n      \"published\": \"2021-09-08T04:50:33+00:00\",\n      \"summary\": \"The Wasserstein barycenter has been widely studied in various fields,\\nincluding natural language processing, and computer vision. However, it\\nrequires a high computational cost to solve the Wasserstein barycenter problem\\nbecause the computation of the Wasserstein distance requires a quadratic time\\nwith respect to the number of supports. By contrast, the Wasserstein distance\\non a tree, called the tree-Wasserstein distance, can be computed in linear time\\nand allows for the fast comparison of a large number of distributions. In this\\nstudy, we propose a barycenter under the tree-Wasserstein distance, called the\\nfixed support tree-Wasserstein barycenter (FS-TWB) and its extension, called\\nthe fixed support tree-sliced Wasserstein barycenter (FS-TSWB). More\\nspecifically, we first show that the FS-TWB and FS-TSWB problems are convex\\noptimization problems and can be solved by using the projected subgradient\\ndescent. Moreover, we propose a more efficient algorithm to compute the\\nsubgradient and objective function value by using the properties of\\ntree-Wasserstein barycenter problems. Through real-world experiments, we show\\nthat, by using the proposed algorithm, the FS-TWB and FS-TSWB can be solved two\\norders of magnitude faster than the original Wasserstein barycenter.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2109.03431v2\"\n    },\n    {\n      \"title\": \"Inference for Projection-Based Wasserstein Distances on Finite Spaces\",\n      \"authors\": [\n        \"Ryo Okano\",\n        \"Masaaki Imaizumi\"\n      ],\n      \"published\": \"2022-02-11T08:09:39+00:00\",\n      \"summary\": \"The Wasserstein distance is a distance between two probability distributions\\nand has recently gained increasing popularity in statistics and machine\\nlearning, owing to its attractive properties. One important approach to\\nextending this distance is using low-dimensional projections of distributions\\nto avoid a high computational cost and the curse of dimensionality in empirical\\nestimation, such as the sliced Wasserstein or max-sliced Wasserstein distances.\\nDespite their practical success in machine learning tasks, the availability of\\nstatistical inferences for projection-based Wasserstein distances is limited\\nowing to the lack of distributional limit results. In this paper, we consider\\ndistances defined by integrating or maximizing Wasserstein distances between\\nlow-dimensional projections of two probability distributions. Then we derive\\nlimit distributions regarding these distances when the two distributions are\\nsupported on finite points. We also propose a bootstrap procedure to estimate\\nquantiles of limit distributions from data. This facilitates asymptotically\\nexact interval estimation and hypothesis testing for these distances. Our\\ntheoretical results are based on the arguments of Sommerfeld and Munk (2018)\\nfor deriving distributional limits regarding the original Wasserstein distance\\non finite spaces and the theory of sensitivity analysis in nonlinear\\nprogramming. Finally, we conduct numerical experiments to illustrate the\\ntheoretical results and demonstrate the applicability of our inferential\\nmethods to real data analysis.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2202.05495v1\"\n    },\n    {\n      \"title\": \"Wasserstein Adversarial Examples on Univariant Time Series Data\",\n      \"authors\": [\n        \"Wenjie Wang\",\n        \"Li Xiong\",\n        \"Jian Lou\"\n      ],\n      \"published\": \"2023-03-22T07:50:15+00:00\",\n      \"summary\": \"Adversarial examples are crafted by adding indistinguishable perturbations to\\nnormal examples in order to fool a well-trained deep learning model to\\nmisclassify. In the context of computer vision, this notion of\\nindistinguishability is typically bounded by $L_{\\\\infty}$ or other norms.\\nHowever, these norms are not appropriate for measuring indistinguishiability\\nfor time series data. In this work, we propose adversarial examples in the\\nWasserstein space for time series data for the first time and utilize\\nWasserstein distance to bound the perturbation between normal examples and\\nadversarial examples. We introduce Wasserstein projected gradient descent\\n(WPGD), an adversarial attack method for perturbing univariant time series\\ndata. We leverage the closed-form solution of Wasserstein distance in the 1D\\nspace to calculate the projection step of WPGD efficiently with the gradient\\ndescent method. We further propose a two-step projection so that the search of\\nadversarial examples in the Wasserstein space is guided and constrained by\\nEuclidean norms to yield more effective and imperceptible perturbations. We\\nempirically evaluate the proposed attack on several time series datasets in the\\nhealthcare domain. Extensive results demonstrate that the Wasserstein attack is\\npowerful and can successfully attack most of the target classifiers with a high\\nattack success rate. To better study the nature of Wasserstein adversarial\\nexample, we evaluate a strong defense mechanism named Wasserstein smoothing for\\npotential certified robustness defense. Although the defense can achieve some\\naccuracy gain, it still has limitations in many cases and leaves space for\\ndeveloping a stronger certified robustness method to Wasserstein adversarial\\nexamples on univariant time series data.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2303.12357v1\"\n    },\n    {\n      \"title\": \"Lipschitz continuity of the Wasserstein projections in the convex order on the line\",\n      \"authors\": [\n        \"Benjamin Jourdain\",\n        \"William Margheriti\",\n        \"Gudmund Pammer\"\n      ],\n      \"published\": \"2022-08-22T22:17:12+00:00\",\n      \"summary\": \"Wasserstein projections in the convex order were first considered in the\\nframework of weak optimal transport, and found application in various problems\\nsuch as concentration inequalities and martingale optimal transport. In\\ndimension one, it is well-known that the set of probability measures with a\\ngiven mean is a lattice w.r.t. the convex order. Our main result is that,\\ncontrary to the minimum and maximum in the convex order, the Wasserstein\\nprojections are Lipschitz continuity w.r.t. the Wasserstein distance in\\ndimension one. Moreover, we provide examples that show sharpness of the\\nobtained bounds for the 1-Wasserstein distance.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2208.10635v1\"\n    },\n    {\n      \"title\": \"On a linear Gromov-Wasserstein distance\",\n      \"authors\": [\n        \"Florian Beier\",\n        \"Robert Beinert\",\n        \"Gabriele Steidl\"\n      ],\n      \"published\": \"2021-12-22T15:35:53+00:00\",\n      \"summary\": \"Gromov-Wasserstein distances are generalization of Wasserstein distances,\\nwhich are invariant under distance preserving transformations. Although a\\nsimplified version of optimal transport in Wasserstein spaces, called linear\\noptimal transport (LOT), was successfully used in practice, there does not\\nexist a notion of linear Gromov-Wasserstein distances so far. In this paper, we\\npropose a definition of linear Gromov-Wasserstein distances. We motivate our\\napproach by a generalized LOT model, which is based on barycentric projection\\nmaps of transport plans. Numerical examples illustrate that the linear\\nGromov-Wasserstein distances, similarly as LOT, can replace the expensive\\ncomputation of pairwise Gromov-Wasserstein distances in applications like shape\\nclassification.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2112.11964v4\"\n    }\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ArxivTool` with \n```json\n{\n  \"max_results\": 5,\n  \"query\": \"Wasserstein projections\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Found 5 paper(s) on arXiv\",\n  \"output\": [\n    {\n      \"title\": \"Tangential Wasserstein Projections\",\n      \"authors\": [\n        \"Florian Gunsilius\",\n        \"Meng Hsuan Hsieh\",\n        \"Myung Jin Lee\"\n      ],\n      \"published\": \"2022-07-29T14:59:58+00:00\",\n      \"summary\": \"We develop a notion of projections between sets of probability measures using\\nthe geometric properties of the 2-Wasserstein space. It is designed for general\\nmultivariate probability measures, is computationally efficient to implement,\\nand provides a unique solution in regular settings. The idea is to work on\\nregular tangent cones of the Wasserstein space using generalized geodesics. Its\\nstructure and computational properties make the method applicable in a variety\\nof settings, from causal inference to the analysis of object data. An\\napplication to estimating causal effects yields a generalization of the notion\\nof synthetic controls to multivariate data with individual-level heterogeneity,\\nas well as a way to estimate optimal weights jointly over all time periods.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2207.14727v2\"\n    },\n    {\n      \"title\": \"Tangential Fixpoint Iterations for Gromov-Wasserstein Barycenters\",\n      \"authors\": [\n        \"Florian Beier\",\n        \"Robert Beinert\"\n      ],\n      \"published\": \"2024-03-13T15:22:56+00:00\",\n      \"summary\": \"The Gromov-Wasserstein (GW) transport problem is a relaxation of classic\\noptimal transport, which seeks a transport between two measures while\\npreserving their internal geometry. Due to meeting this theoretical\\nunderpinning, it is a valuable tool for the analysis of objects that do not\\npossess a natural embedding or should be studied independently of it. Prime\\napplications can thus be found in e.g. shape matching, classification and\\ninterpolation tasks. To tackle the latter, one theoretically justified approach\\nis the employment of multi-marginal GW transport and GW barycenters, which are\\nFr\\\\'echet means with respect to the GW distance. However, because the\\ncomputation of GW itself already poses a quadratic and non-convex optimization\\nproblem, the determination of GW barycenters is a hard task and algorithms for\\ntheir computation are scarce. In this paper, we revisit a known procedure for\\nthe determination of Fr\\\\'echet means in Riemannian manifolds via tangential\\napproximations in the context of GW. We provide a characterization of\\nbarycenters in the GW tangent space, which ultimately gives rise to a fixpoint\\niteration for approximating GW barycenters using multi-marginal plans. We\\npropose a relaxation of this fixpoint iteration and show that it monotonously\\ndecreases the barycenter loss. In certain cases our proposed method naturally\\nprovides us with barycentric embeddings. The resulting algorithm is capable of\\nproducing qualitative shape interpolations between multiple 3d shapes with\\nsupport sizes of over thousands of points in reasonable time. In addition, we\\nverify our method on shape classification and multi-graph matching tasks.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2403.08612v1\"\n    },\n    {\n      \"title\": \"A smooth variational principle on Wasserstein space\",\n      \"authors\": [\n        \"Erhan Bayraktar\",\n        \"Ibrahim Ekren\",\n        \"Xin Zhang\"\n      ],\n      \"published\": \"2022-09-29T18:07:53+00:00\",\n      \"summary\": \"In this note, we provide a smooth variational principle on Wasserstein space\\nby constructing a smooth gauge-type function using the sliced Wasserstein\\ndistance. This function is a crucial tool for optimization problems and in\\nviscosity theory of PDEs on Wasserstein space.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2209.15028v2\"\n    },\n    {\n      \"title\": \"Stability and Finiteness of Wasserstein Spaces\",\n      \"authors\": [\n        \"Mohammad Alattar\"\n      ],\n      \"published\": \"2024-06-10T03:37:17+00:00\",\n      \"summary\": \"Under Gromov--Hausdorff convergence, and equivariant Gromov--Hausdorff\\nconvergence, we prove stability results of Wasserstein spaces over certain\\nclasses of singular and non-singular spaces. For example, we obtain an analogue\\nof Perelman's stability theorem on Wasserstein spaces.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2406.05998v1\"\n    },\n    {\n      \"title\": \"Absolutely continuous curves in extended Wasserstein-Orlicz spaces\",\n      \"authors\": [\n        \"Stefano Lisini\"\n      ],\n      \"published\": \"2014-02-28T17:34:32+00:00\",\n      \"summary\": \"In this paper we extend a previous result of the author [Lis07] of\\ncharacterization of absolutely continuous curves in Wasserstein spaces to a\\nmore general class of spaces: the spaces of probability measures endowed with\\nthe Wasserstein-Orlicz distance constructed on extended Polish spaces (in\\ngeneral non separable), recently considered in [AGS14]. An application to the\\ngeodesics of this Wasserstein-Orlicz space is also given.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/1402.7328v1\"\n    }\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ArxivTool` with \n```json\n{\n  \"max_results\": 5,\n  \"query\": \"Tangential Wasserstein space\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Found 5 paper(s) on arXiv\",\n  \"output\": [\n    {\n      \"title\": \"The Synthetic Control Method with Nonlinear Outcomes: Estimating the Impact of the 2019 Anti-Extradition Law Amendments Bill Protests on Hong Kong's Economy\",\n      \"authors\": [\n        \"Wei Tian\"\n      ],\n      \"published\": \"2023-06-03T00:25:53+00:00\",\n      \"summary\": \"The synthetic control estimator (Abadie et al., 2010) is asymptotically\\nunbiased assuming that the outcome is a linear function of the underlying\\npredictors and that the treated unit can be well approximated by the synthetic\\ncontrol before the treatment. When the outcome is nonlinear, the bias of the\\nsynthetic control estimator can be severe. In this paper, we provide conditions\\nfor the synthetic control estimator to be asymptotically unbiased when the\\noutcome is nonlinear, and propose a flexible and data-driven method to choose\\nthe synthetic control weights. Monte Carlo simulations show that compared with\\nthe competing methods, the nonlinear synthetic control method has similar or\\nbetter performance when the outcome is linear, and better performance when the\\noutcome is nonlinear, and that the confidence intervals have good coverage\\nprobabilities across settings. In the empirical application, we illustrate the\\nmethod by estimating the impact of the 2019 anti-extradition law amendments\\nbill protests on Hong Kong's economy, and find that the year-long protests\\nreduced real GDP per capita in Hong Kong by 11.27% in the first quarter of\\n2020, which was larger in magnitude than the economic decline during the 1997\\nAsian financial crisis or the 2008 global financial crisis.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2306.01967v1\"\n    },\n    {\n      \"title\": \"Synthetic Control As Online Linear Regression\",\n      \"authors\": [\n        \"Jiafeng Chen\"\n      ],\n      \"published\": \"2022-02-17T03:10:12+00:00\",\n      \"summary\": \"This paper notes a simple connection between synthetic control and online\\nlearning. Specifically, we recognize synthetic control as an instance of\\nFollow-The-Leader (FTL). Standard results in online convex optimization then\\nimply that, even when outcomes are chosen by an adversary, synthetic control\\npredictions of counterfactual outcomes for the treated unit perform almost as\\nwell as an oracle weighted average of control units' outcomes. Synthetic\\ncontrol on differenced data performs almost as well as oracle weighted\\ndifference-in-differences, potentially making it an attractive choice in\\npractice. We argue that this observation further supports the use of synthetic\\ncontrol estimators in comparative case studies.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2202.08426v2\"\n    },\n    {\n      \"title\": \"An Optimal Transport Approach to Estimating Causal Effects via Nonlinear Difference-in-Differences\",\n      \"authors\": [\n        \"William Torous\",\n        \"Florian Gunsilius\",\n        \"Philippe Rigollet\"\n      ],\n      \"published\": \"2021-08-12T17:08:32+00:00\",\n      \"summary\": \"We propose a nonlinear difference-in-differences method to estimate\\nmultivariate counterfactual distributions in classical treatment and control\\nstudy designs with observational data. Our approach sheds a new light on\\nexisting approaches like the changes-in-changes and the classical\\nsemiparametric difference-in-differences estimator and generalizes them to\\nsettings with multivariate heterogeneity in the outcomes. The main benefit of\\nthis extension is that it allows for arbitrary dependence and heterogeneity in\\nthe joint outcomes. We demonstrate its utility both on synthetic and real data.\\nIn particular, we revisit the classical Card \\\\& Krueger dataset, examining the\\neffect of a minimum wage increase on employment in fast food restaurants; a\\nreanalysis with our method reveals that restaurants tend to substitute\\nfull-time with part-time labor after a minimum wage increase at a faster pace.\\nA previous version of this work was entitled \\\"An optimal transport approach to\\ncausal inference.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2108.05858v2\"\n    },\n    {\n      \"title\": \"Estimating the effectiveness of permanent price reductions for competing products using multivariate Bayesian structural time series models\",\n      \"authors\": [\n        \"Fiammetta Menchetti\",\n        \"Iavor Bojinov\"\n      ],\n      \"published\": \"2020-06-22T14:03:55+00:00\",\n      \"summary\": \"The Florence branch of an Italian supermarket chain recently implemented a\\nstrategy that permanently lowered the price of numerous store brands in several\\nproduct categories. To quantify the impact of such a policy change, researchers\\noften use synthetic control methods for estimating causal effects when a subset\\nof units receive a single persistent treatment, and the rest are unaffected by\\nthe change. In our applications, however, competitor brands not assigned to\\ntreatment are likely impacted by the intervention because of substitution\\neffects; more broadly, this type of interference occurs whenever the treatment\\nassignment of one unit affects the outcome of another. This paper extends the\\nsynthetic control methods to accommodate partial interference, allowing\\ninterference within predefined groups but not between them. Focusing on a class\\nof causal estimands that capture the effect both on the treated and control\\nunits, we develop a multivariate Bayesian structural time series model for\\ngenerating synthetic controls that would have occurred in the absence of an\\nintervention enabling us to estimate our novel effects. In a simulation study,\\nwe explore our Bayesian procedure's empirical properties and show that it\\nachieves good frequentists coverage even when the model is misspecified. We use\\nour new methodology to make causal statements about the impact on sales of the\\naffected store brands and their direct competitors. Our proposed approach is\\nimplemented in the CausalMBSTS R package.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2006.12269v4\"\n    },\n    {\n      \"title\": \"Using Multiple Outcomes to Improve the Synthetic Control Method\",\n      \"authors\": [\n        \"Liyang Sun\",\n        \"Eli Ben-Michael\",\n        \"Avi Feller\"\n      ],\n      \"published\": \"2023-11-27T19:07:29+00:00\",\n      \"summary\": \"When there are multiple outcome series of interest, Synthetic Control\\nanalyses typically proceed by estimating separate weights for each outcome. In\\nthis paper, we instead propose estimating a common set of weights across\\noutcomes, by balancing either a vector of all outcomes or an index or average\\nof them. Under a low-rank factor model, we show that these approaches lead to\\nlower bias bounds than separate weights, and that averaging leads to further\\ngains when the number of outcomes grows. We illustrate this via a re-analysis\\nof the impact of the Flint water crisis on educational outcomes.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2311.16260v3\"\n    }\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ArxivTool` with \n```json\n{\n  \"query\": \"Synthetic control estimator multivariate outcomes\",\n  \"max_results\": 5\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ArxivTool', response={'result': {'status': 'success', 'message': 'Found 5 paper(s) on arXiv', 'output': [{'title': 'Fixed Support Tree-Sliced Wasserstein Barycenter', 'authors': ['Yuki Takezawa', 'Ryoma Sato', 'Zornitsa Kozareva', 'Sujith Ravi', 'Makoto Yamada'], 'published': '2021-09-08T04:50:33+00:00', 'summary': 'The Wasserstein barycenter has been widely studied in various fields,\\nincluding natural language processing, and computer vision. However, it\\nrequires a high computational cost to solve the Wasserstein barycenter problem\\nbecause the computation of the Wasserstein distance requires a quadratic time\\nwith respect to the number of supports. By contrast, the Wasserstein distance\\non a tree, called the tree-Wasserstein distance, can be computed in linear time\\nand allows for the fast comparison of a large number of distributions. In this\\nstudy, we propose a barycenter under the tree-Wasserstein distance, called the\\nfixed support tree-Wasserstein barycenter (FS-TWB) and its extension, called\\nthe fixed support tree-sliced Wasserstein barycenter (FS-TSWB). More\\nspecifically, we first show that the FS-TWB and FS-TSWB problems are convex\\noptimization problems and can be solved by using the projected subgradient\\ndescent. Moreover, we propose a more efficient algorithm to compute the\\nsubgradient and objective function value by using the properties of\\ntree-Wasserstein barycenter problems. Through real-world experiments, we show\\nthat, by using the proposed algorithm, the FS-TWB and FS-TSWB can be solved two\\norders of magnitude faster than the original Wasserstein barycenter.', 'pdf_url': 'http://arxiv.org/pdf/2109.03431v2'}, {'title': 'Inference for Projection-Based Wasserstein Distances on Finite Spaces', 'authors': ['Ryo Okano', 'Masaaki Imaizumi'], 'published': '2022-02-11T08:09:39+00:00', 'summary': 'The Wasserstein distance is a distance between two probability distributions\\nand has recently gained increasing popularity in statistics and machine\\nlearning, owing to its attractive properties. One important approach to\\nextending this distance is using low-dimensional projections of distributions\\nto avoid a high computational cost and the curse of dimensionality in empirical\\nestimation, such as the sliced Wasserstein or max-sliced Wasserstein distances.\\nDespite their practical success in machine learning tasks, the availability of\\nstatistical inferences for projection-based Wasserstein distances is limited\\nowing to the lack of distributional limit results. In this paper, we consider\\ndistances defined by integrating or maximizing Wasserstein distances between\\nlow-dimensional projections of two probability distributions. Then we derive\\nlimit distributions regarding these distances when the two distributions are\\nsupported on finite points. We also propose a bootstrap procedure to estimate\\nquantiles of limit distributions from data. This facilitates asymptotically\\nexact interval estimation and hypothesis testing for these distances. Our\\ntheoretical results are based on the arguments of Sommerfeld and Munk (2018)\\nfor deriving distributional limits regarding the original Wasserstein distance\\non finite spaces and the theory of sensitivity analysis in nonlinear\\nprogramming. Finally, we conduct numerical experiments to illustrate the\\ntheoretical results and demonstrate the applicability of our inferential\\nmethods to real data analysis.', 'pdf_url': 'http://arxiv.org/pdf/2202.05495v1'}, {'title': 'Wasserstein Adversarial Examples on Univariant Time Series Data', 'authors': ['Wenjie Wang', 'Li Xiong', 'Jian Lou'], 'published': '2023-03-22T07:50:15+00:00', 'summary': 'Adversarial examples are crafted by adding indistinguishable perturbations to\\nnormal examples in order to fool a well-trained deep learning model to\\nmisclassify. In the context of computer vision, this notion of\\nindistinguishability is typically bounded by $L_{\\\\infty}$ or other norms.\\nHowever, these norms are not appropriate for measuring indistinguishiability\\nfor time series data. In this work, we propose adversarial examples in the\\nWasserstein space for time series data for the first time and utilize\\nWasserstein distance to bound the perturbation between normal examples and\\nadversarial examples. We introduce Wasserstein projected gradient descent\\n(WPGD), an adversarial attack method for perturbing univariant time series\\ndata. We leverage the closed-form solution of Wasserstein distance in the 1D\\nspace to calculate the projection step of WPGD efficiently with the gradient\\ndescent method. We further propose a two-step projection so that the search of\\nadversarial examples in the Wasserstein space is guided and constrained by\\nEuclidean norms to yield more effective and imperceptible perturbations. We\\nempirically evaluate the proposed attack on several time series datasets in the\\nhealthcare domain. Extensive results demonstrate that the Wasserstein attack is\\npowerful and can successfully attack most of the target classifiers with a high\\nattack success rate. To better study the nature of Wasserstein adversarial\\nexample, we evaluate a strong defense mechanism named Wasserstein smoothing for\\npotential certified robustness defense. Although the defense can achieve some\\naccuracy gain, it still has limitations in many cases and leaves space for\\ndeveloping a stronger certified robustness method to Wasserstein adversarial\\nexamples on univariant time series data.', 'pdf_url': 'http://arxiv.org/pdf/2303.12357v1'}, {'title': 'Lipschitz continuity of the Wasserstein projections in the convex order on the line', 'authors': ['Benjamin Jourdain', 'William Margheriti', 'Gudmund Pammer'], 'published': '2022-08-22T22:17:12+00:00', 'summary': 'Wasserstein projections in the convex order were first considered in the\\nframework of weak optimal transport, and found application in various problems\\nsuch as concentration inequalities and martingale optimal transport. In\\ndimension one, it is well-known that the set of probability measures with a\\ngiven mean is a lattice w.r.t. the convex order. Our main result is that,\\ncontrary to the minimum and maximum in the convex order, the Wasserstein\\nprojections are Lipschitz continuity w.r.t. the Wasserstein distance in\\ndimension one. Moreover, we provide examples that show sharpness of the\\nobtained bounds for the 1-Wasserstein distance.', 'pdf_url': 'http://arxiv.org/pdf/2208.10635v1'}, {'title': 'On a linear Gromov-Wasserstein distance', 'authors': ['Florian Beier', 'Robert Beinert', 'Gabriele Steidl'], 'published': '2021-12-22T15:35:53+00:00', 'summary': 'Gromov-Wasserstein distances are generalization of Wasserstein distances,\\nwhich are invariant under distance preserving transformations. Although a\\nsimplified version of optimal transport in Wasserstein spaces, called linear\\noptimal transport (LOT), was successfully used in practice, there does not\\nexist a notion of linear Gromov-Wasserstein distances so far. In this paper, we\\npropose a definition of linear Gromov-Wasserstein distances. We motivate our\\napproach by a generalized LOT model, which is based on barycentric projection\\nmaps of transport plans. Numerical examples illustrate that the linear\\nGromov-Wasserstein distances, similarly as LOT, can replace the expensive\\ncomputation of pairwise Gromov-Wasserstein distances in applications like shape\\nclassification.', 'pdf_url': 'http://arxiv.org/pdf/2112.11964v4'}]}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ArxivTool', response={'result': {'status': 'success', 'message': 'Found 5 paper(s) on arXiv', 'output': [{'title': 'Tangential Wasserstein Projections', 'authors': ['Florian Gunsilius', 'Meng Hsuan Hsieh', 'Myung Jin Lee'], 'published': '2022-07-29T14:59:58+00:00', 'summary': 'We develop a notion of projections between sets of probability measures using\\nthe geometric properties of the 2-Wasserstein space. It is designed for general\\nmultivariate probability measures, is computationally efficient to implement,\\nand provides a unique solution in regular settings. The idea is to work on\\nregular tangent cones of the Wasserstein space using generalized geodesics. Its\\nstructure and computational properties make the method applicable in a variety\\nof settings, from causal inference to the analysis of object data. An\\napplication to estimating causal effects yields a generalization of the notion\\nof synthetic controls to multivariate data with individual-level heterogeneity,\\nas well as a way to estimate optimal weights jointly over all time periods.', 'pdf_url': 'http://arxiv.org/pdf/2207.14727v2'}, {'title': 'Tangential Fixpoint Iterations for Gromov-Wasserstein Barycenters', 'authors': ['Florian Beier', 'Robert Beinert'], 'published': '2024-03-13T15:22:56+00:00', 'summary': \"The Gromov-Wasserstein (GW) transport problem is a relaxation of classic\\noptimal transport, which seeks a transport between two measures while\\npreserving their internal geometry. Due to meeting this theoretical\\nunderpinning, it is a valuable tool for the analysis of objects that do not\\npossess a natural embedding or should be studied independently of it. Prime\\napplications can thus be found in e.g. shape matching, classification and\\ninterpolation tasks. To tackle the latter, one theoretically justified approach\\nis the employment of multi-marginal GW transport and GW barycenters, which are\\nFr\\\\'echet means with respect to the GW distance. However, because the\\ncomputation of GW itself already poses a quadratic and non-convex optimization\\nproblem, the determination of GW barycenters is a hard task and algorithms for\\ntheir computation are scarce. In this paper, we revisit a known procedure for\\nthe determination of Fr\\\\'echet means in Riemannian manifolds via tangential\\napproximations in the context of GW. We provide a characterization of\\nbarycenters in the GW tangent space, which ultimately gives rise to a fixpoint\\niteration for approximating GW barycenters using multi-marginal plans. We\\npropose a relaxation of this fixpoint iteration and show that it monotonously\\ndecreases the barycenter loss. In certain cases our proposed method naturally\\nprovides us with barycentric embeddings. The resulting algorithm is capable of\\nproducing qualitative shape interpolations between multiple 3d shapes with\\nsupport sizes of over thousands of points in reasonable time. In addition, we\\nverify our method on shape classification and multi-graph matching tasks.\", 'pdf_url': 'http://arxiv.org/pdf/2403.08612v1'}, {'title': 'A smooth variational principle on Wasserstein space', 'authors': ['Erhan Bayraktar', 'Ibrahim Ekren', 'Xin Zhang'], 'published': '2022-09-29T18:07:53+00:00', 'summary': 'In this note, we provide a smooth variational principle on Wasserstein space\\nby constructing a smooth gauge-type function using the sliced Wasserstein\\ndistance. This function is a crucial tool for optimization problems and in\\nviscosity theory of PDEs on Wasserstein space.', 'pdf_url': 'http://arxiv.org/pdf/2209.15028v2'}, {'title': 'Stability and Finiteness of Wasserstein Spaces', 'authors': ['Mohammad Alattar'], 'published': '2024-06-10T03:37:17+00:00', 'summary': \"Under Gromov--Hausdorff convergence, and equivariant Gromov--Hausdorff\\nconvergence, we prove stability results of Wasserstein spaces over certain\\nclasses of singular and non-singular spaces. For example, we obtain an analogue\\nof Perelman's stability theorem on Wasserstein spaces.\", 'pdf_url': 'http://arxiv.org/pdf/2406.05998v1'}, {'title': 'Absolutely continuous curves in extended Wasserstein-Orlicz spaces', 'authors': ['Stefano Lisini'], 'published': '2014-02-28T17:34:32+00:00', 'summary': 'In this paper we extend a previous result of the author [Lis07] of\\ncharacterization of absolutely continuous curves in Wasserstein spaces to a\\nmore general class of spaces: the spaces of probability measures endowed with\\nthe Wasserstein-Orlicz distance constructed on extended Polish spaces (in\\ngeneral non separable), recently considered in [AGS14]. An application to the\\ngeodesics of this Wasserstein-Orlicz space is also given.', 'pdf_url': 'http://arxiv.org/pdf/1402.7328v1'}]}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ArxivTool', response={'result': {'status': 'success', 'message': 'Found 5 paper(s) on arXiv', 'output': [{'title': \"The Synthetic Control Method with Nonlinear Outcomes: Estimating the Impact of the 2019 Anti-Extradition Law Amendments Bill Protests on Hong Kong's Economy\", 'authors': ['Wei Tian'], 'published': '2023-06-03T00:25:53+00:00', 'summary': \"The synthetic control estimator (Abadie et al., 2010) is asymptotically\\nunbiased assuming that the outcome is a linear function of the underlying\\npredictors and that the treated unit can be well approximated by the synthetic\\ncontrol before the treatment. When the outcome is nonlinear, the bias of the\\nsynthetic control estimator can be severe. In this paper, we provide conditions\\nfor the synthetic control estimator to be asymptotically unbiased when the\\noutcome is nonlinear, and propose a flexible and data-driven method to choose\\nthe synthetic control weights. Monte Carlo simulations show that compared with\\nthe competing methods, the nonlinear synthetic control method has similar or\\nbetter performance when the outcome is linear, and better performance when the\\noutcome is nonlinear, and that the confidence intervals have good coverage\\nprobabilities across settings. In the empirical application, we illustrate the\\nmethod by estimating the impact of the 2019 anti-extradition law amendments\\nbill protests on Hong Kong's economy, and find that the year-long protests\\nreduced real GDP per capita in Hong Kong by 11.27% in the first quarter of\\n2020, which was larger in magnitude than the economic decline during the 1997\\nAsian financial crisis or the 2008 global financial crisis.\", 'pdf_url': 'http://arxiv.org/pdf/2306.01967v1'}, {'title': 'Synthetic Control As Online Linear Regression', 'authors': ['Jiafeng Chen'], 'published': '2022-02-17T03:10:12+00:00', 'summary': \"This paper notes a simple connection between synthetic control and online\\nlearning. Specifically, we recognize synthetic control as an instance of\\nFollow-The-Leader (FTL). Standard results in online convex optimization then\\nimply that, even when outcomes are chosen by an adversary, synthetic control\\npredictions of counterfactual outcomes for the treated unit perform almost as\\nwell as an oracle weighted average of control units' outcomes. Synthetic\\ncontrol on differenced data performs almost as well as oracle weighted\\ndifference-in-differences, potentially making it an attractive choice in\\npractice. We argue that this observation further supports the use of synthetic\\ncontrol estimators in comparative case studies.\", 'pdf_url': 'http://arxiv.org/pdf/2202.08426v2'}, {'title': 'An Optimal Transport Approach to Estimating Causal Effects via Nonlinear Difference-in-Differences', 'authors': ['William Torous', 'Florian Gunsilius', 'Philippe Rigollet'], 'published': '2021-08-12T17:08:32+00:00', 'summary': 'We propose a nonlinear difference-in-differences method to estimate\\nmultivariate counterfactual distributions in classical treatment and control\\nstudy designs with observational data. Our approach sheds a new light on\\nexisting approaches like the changes-in-changes and the classical\\nsemiparametric difference-in-differences estimator and generalizes them to\\nsettings with multivariate heterogeneity in the outcomes. The main benefit of\\nthis extension is that it allows for arbitrary dependence and heterogeneity in\\nthe joint outcomes. We demonstrate its utility both on synthetic and real data.\\nIn particular, we revisit the classical Card \\\\& Krueger dataset, examining the\\neffect of a minimum wage increase on employment in fast food restaurants; a\\nreanalysis with our method reveals that restaurants tend to substitute\\nfull-time with part-time labor after a minimum wage increase at a faster pace.\\nA previous version of this work was entitled \"An optimal transport approach to\\ncausal inference.', 'pdf_url': 'http://arxiv.org/pdf/2108.05858v2'}, {'title': 'Estimating the effectiveness of permanent price reductions for competing products using multivariate Bayesian structural time series models', 'authors': ['Fiammetta Menchetti', 'Iavor Bojinov'], 'published': '2020-06-22T14:03:55+00:00', 'summary': \"The Florence branch of an Italian supermarket chain recently implemented a\\nstrategy that permanently lowered the price of numerous store brands in several\\nproduct categories. To quantify the impact of such a policy change, researchers\\noften use synthetic control methods for estimating causal effects when a subset\\nof units receive a single persistent treatment, and the rest are unaffected by\\nthe change. In our applications, however, competitor brands not assigned to\\ntreatment are likely impacted by the intervention because of substitution\\neffects; more broadly, this type of interference occurs whenever the treatment\\nassignment of one unit affects the outcome of another. This paper extends the\\nsynthetic control methods to accommodate partial interference, allowing\\ninterference within predefined groups but not between them. Focusing on a class\\nof causal estimands that capture the effect both on the treated and control\\nunits, we develop a multivariate Bayesian structural time series model for\\ngenerating synthetic controls that would have occurred in the absence of an\\nintervention enabling us to estimate our novel effects. In a simulation study,\\nwe explore our Bayesian procedure's empirical properties and show that it\\nachieves good frequentists coverage even when the model is misspecified. We use\\nour new methodology to make causal statements about the impact on sales of the\\naffected store brands and their direct competitors. Our proposed approach is\\nimplemented in the CausalMBSTS R package.\", 'pdf_url': 'http://arxiv.org/pdf/2006.12269v4'}, {'title': 'Using Multiple Outcomes to Improve the Synthetic Control Method', 'authors': ['Liyang Sun', 'Eli Ben-Michael', 'Avi Feller'], 'published': '2023-11-27T19:07:29+00:00', 'summary': 'When there are multiple outcome series of interest, Synthetic Control\\nanalyses typically proceed by estimating separate weights for each outcome. In\\nthis paper, we instead propose estimating a common set of weights across\\noutcomes, by balancing either a vector of all outcomes or an index or average\\nof them. Under a low-rank factor model, we show that these approaches lead to\\nlower bias bounds than separate weights, and that averaging leads to further\\ngains when the number of outcomes grows. We illustrate this via a re-analysis\\nof the impact of the Flint water crisis on educational outcomes.', 'pdf_url': 'http://arxiv.org/pdf/2311.16260v3'}]}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I have conducted research on the paper's context and novelty. Now, I will create three reviewer agents with specialized expertise to evaluate the paper from different perspectives.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"gemini-2.5-flash-preview-05-20\": {\n      \"description\": \"Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math\",\n      \"create_expense_cost\": 0,\n      \"invoke_expense_cost\": 0.15,\n      \"output_expense_cost\": 0.6\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'gemini-2.5-flash-preview-05-20': {'description': 'Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math', 'create_expense_cost': 0, 'invoke_expense_cost': 0.15, 'output_expense_cost': 0.6}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'WassersteinExpert', 'system_prompt': 'You are an expert in Wasserstein spaces and optimal transport theory. You will rigorously evaluate the theoretical soundness and mathematical rigor of the paper, focusing on the correctness of proofs, the validity of assumptions, and the novelty of the theoretical contributions.', 'base_model': 'gemini-2.5-flash-preview-05-20', 'description': 'Expert in Wasserstein spaces and optimal transport theory.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'Expert in causal inference and synthetic control methods.', 'system_prompt': 'You are an expert in causal inference and synthetic control methods. You will evaluate the application of the proposed method to causal inference, focusing on the appropriateness of the synthetic control estimator, the validity of the causal assumptions, and the practical relevance of the empirical results.', 'base_model': 'gemini-2.5-flash-preview-05-20', 'agent_name': 'CausalInferenceReviewer'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'description': 'Expert in statistical consistency and asymptotic properties of estimators.', 'system_prompt': 'You are an expert in statistical consistency and asymptotic properties of estimators. You will rigorously evaluate the statistical consistency results, focusing on the assumptions required for consistency, the rates of convergence, and the practical implications of the theoretical findings.', 'agent_name': 'StatisticalConsistencyAnalyst', 'base_model': 'gemini-2.5-flash-preview-05-20'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.9032088\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"agent_name\": \"WassersteinExpert\",\n  \"system_prompt\": \"You are an expert in Wasserstein spaces and optimal transport theory. You will rigorously evaluate the theoretical soundness and mathematical rigor of the paper, focusing on the correctness of proofs, the validity of assumptions, and the novelty of the theoretical contributions.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"description\": \"Expert in Wasserstein spaces and optimal transport theory.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.9032088\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"Expert in causal inference and synthetic control methods.\",\n  \"system_prompt\": \"You are an expert in causal inference and synthetic control methods. You will evaluate the application of the proposed method to causal inference, focusing on the appropriateness of the synthetic control estimator, the validity of the causal assumptions, and the practical relevance of the empirical results.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"agent_name\": \"CausalInferenceReviewer\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.9032088\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"description\": \"Expert in statistical consistency and asymptotic properties of estimators.\",\n  \"system_prompt\": \"You are an expert in statistical consistency and asymptotic properties of estimators. You will rigorously evaluate the statistical consistency results, focusing on the assumptions required for consistency, the rates of convergence, and the practical implications of the theoretical findings.\",\n  \"agent_name\": \"StatisticalConsistencyAnalyst\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.9032088}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.9032088}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.9032088}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'WassersteinExpert', 'prompt': \"Evaluate the theoretical soundness and mathematical rigor of the paper 'Tangential Wasserstein Projections', focusing on the correctness of proofs, the validity of assumptions, and the novelty of the theoretical contributions. Provide a detailed assessment of the proofs for Proposition 2.1, 2.2, and 3.1.\"}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': \"Evaluate the application of the proposed method in 'Tangential Wasserstein Projections' to causal inference, focusing on the appropriateness of the synthetic control estimator, the validity of the causal assumptions, and the practical relevance of the empirical results. Assess the strength of the evidence supporting the causal claims.\", 'agent_name': 'CausalInferenceReviewer'}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'StatisticalConsistencyAnalyst', 'prompt': \"Evaluate the statistical consistency results in 'Tangential Wasserstein Projections', focusing on the assumptions required for consistency, the rates of convergence, and the practical implications of the theoretical findings. Analyze the generality of Proposition 3.1 and Corollary 3.1.\"}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The paper \\\"Tangential Wasserstein Projections\\\" proposes a novel approach to dimensionality reduction in the space of probability measures $P_2(\\\\mathcal{X})$ equipped with the $W_2$ metric, focusing on projecting data onto \\\"tangential\\\" subspaces. This evaluation will assess its theoretical soundness, mathematical rigor, and the novelty of its contributions, with a detailed focus on Propositions 2.1, 2.2, and 3.1.\\n\\n## Overall Assessment\\n\\nThe core idea of \\\"Tangential Wasserstein Projections\\\" is sound and builds upon established concepts in optimal transport and the geometric structure of Wasserstein space, particularly the Otto calculus. By working with the tangent space identified as a Hilbert space of vector fields (or gradients of potentials), the authors effectively linearize the problem locally around a reference measure. This allows them to leverage standard Hilbert space techniques (like orthogonal projection and PCA) in a novel and principled way for Wasserstein data.\\n\\nThe mathematical rigor hinges critically on the precise definition and properties of the tangent space and the \\\"tangential projection\\\" itself. Assuming these foundational definitions are robustly established within the paper (which is common in the literature building on Otto's work), the subsequent propositions largely become applications of standard Hilbert space theory.\\n\\n### Core Concepts and Theoretical Framework\\n\\n1.  **Wasserstein Space $P_2(\\\\mathcal{X})$:** The paper assumes the standard definition of $P_2(\\\\mathcal{X})$, the space of probability measures with finite second moments on $\\\\mathcal{X} = \\\\mathbb{R}^d$, endowed with the $W_2$ metric. This is a well-established complete metric space.\\n2.  **Geodesics:** Geodesics in $P_2(\\\\mathcal{X})$ are known to be unique and can be characterized by optimal transport maps or displacement interpolation. For any two measures $\\\\mu_0, \\\\mu_1 \\\\in P_2(\\\\mathcal{X})$, there exists a unique optimal transport map $T: \\\\mathcal{X} \\\\to \\\\mathcal{X}$ such that $T_\\\\# \\\\mu_0 = \\\\mu_1$ and $T = id + \\\\nabla \\\\phi$ for some convex function $\\\\phi$ (Brenier's Theorem), assuming $\\\\mu_0$ is absolutely continuous.\\n3.  **Tangent Space $T_{\\\\mu_0}P_2(\\\\mathcal{X})$:** This is the cornerstone of the paper. Following the Otto calculus, the tangent space at a measure $\\\\mu_0$ is typically identified with the space of vector fields $v \\\\in L^2(\\\\mu_0; \\\\mathbb{R}^d)$ that are gradients of potentials, i.e., $v = \\\\nabla \\\\phi$ for some $\\\\phi \\\\in L^2(\\\\mu_0)$ with $\\\\int \\\\phi d\\\\mu_0 = 0$. This space, equipped with the $L^2(\\\\mu_0)$ inner product $\\\\langle v_1, v_2 \\\\rangle_{L^2(\\\\mu_0)} = \\\\int v_1 \\\\cdot v_2 d\\\\mu_0$, forms a Hilbert space.\\n    *   A geodesic from $\\\\mu_0$ to $\\\\mu_t$ can be represented as $\\\\mu_t = (id + t v)_\\\\# \\\\mu_0$ for small $t$, where $v = \\\\nabla \\\\phi$ is the initial velocity field. More generally, given $\\\\mu_0$ and $\\\\mu_1$, the \\\"initial velocity vector field\\\" $v_{\\\\mu_0 \\\\to \\\\mu_1}$ is given by $\\\\nabla \\\\phi$ where $T = id + \\\\nabla \\\\phi$ is the optimal transport map from $\\\\mu_0$ to $\\\\mu_1$. This identification is crucial.\\n4.  **Tangential Projection:** The paper defines a \\\"tangential projection\\\" not as a projection of a measure *onto a manifold of measures*, but rather as a projection *of the tangent vector* connecting a reference measure $\\\\mu_0$ to a given measure $\\\\mu_i$ *onto a linear subspace* $V$ within the tangent space $T_{\\\\mu_0}P_2(\\\\mathcal{X})$. The resulting \\\"projected measure\\\" is then obtained by pushing forward $\\\\mu_0$ using this projected tangent vector. This distinction is vital for understanding the simplicity and rigor of the proofs.\\n\\n## Detailed Assessment of Proofs\\n\\nThe proofs of Propositions 2.1, 2.2, and 3.1 are fundamentally correct, provided the underlying Hilbert space structure of the tangent space and the definition of the \\\"tangential projection\\\" are correctly set up and robust. They mostly rely on well-established properties of Hilbert spaces.\\n\\n### Proposition 2.1: Existence and Uniqueness of Tangential Projection\\n\\n**Statement (paraphrased):** Given a reference measure $\\\\mu_0$, a measure $\\\\mu \\\\in P_2(\\\\mathcal{X})$, and a closed linear subspace $V \\\\subseteq T_{\\\\mu_0}P_2(\\\\mathcal{X})$, there exists a unique measure $\\\\tilde{\\\\mu} \\\\in P_2(\\\\mathcal{X})$ such that $\\\\tilde{\\\\mu} = (id + \\\\tilde{v})_\\\\# \\\\mu_0$ for some $\\\\tilde{v} \\\\in V$, which minimizes $W_2(\\\\mu, (id+v)_\\\\# \\\\mu_0)$ over all $v \\\\in V$.\\n\\n**Proof Assessment:**\\n1.  **Identification of Tangent Vectors:** The proof hinges on identifying the tangent vector $v_{\\\\mu_0 \\\\to \\\\mu}$ for the geodesic from $\\\\mu_0$ to $\\\\mu$. This is $v_{\\\\mu_0 \\\\to \\\\mu} = \\\\nabla \\\\phi$, where $T = id + \\\\nabla \\\\phi$ is the optimal transport map from $\\\\mu_0$ to $\\\\mu$. This $v_{\\\\mu_0 \\\\to \\\\mu}$ is a unique element of $T_{\\\\mu_0}P_2(\\\\mathcal{X})$ (viewed as a space of gradients of $L^2(\\\\mu_0)$ functions, or more broadly, as $L^2(\\\\mu_0; \\\\mathbb{R}^d)$ vector fields).\\n2.  **Minimization Problem:** The problem is to find $\\\\tilde{v} \\\\in V$ that minimizes $W_2(\\\\mu, (id+v)_\\\\# \\\\mu_0)$.\\n    *   The key insight is that $W_2^2(\\\\mu_0, \\\\mu) = \\\\int \\\\|v_{\\\\mu_0 \\\\to \\\\mu}(x)\\\\|^2 d\\\\mu_0(x) = \\\\|v_{\\\\mu_0 \\\\to \\\\mu}\\\\|_{L^2(\\\\mu_0)}^2$. This identity holds when $v_{\\\\mu_0 \\\\to \\\\mu}$ is the initial velocity of the geodesic.\\n    *   The paper's definition of \\\"tangential projection\\\" implicitly means finding $\\\\tilde{v} \\\\in V$ such that $\\\\tilde{v}$ is the *orthogonal projection* of $v_{\\\\mu_0 \\\\to \\\\mu}$ onto $V$.\\n    *   In a Hilbert space (which $T_{\\\\mu_0}P_2(\\\\mathcal{X})$ with the $L^2(\\\\mu_0)$ norm is), for any element $x$ and any closed subspace $V$, there exists a unique $P_V(x) \\\\in V$ that minimizes $\\\\|x - y\\\\|$ for $y \\\\in V$. This is the orthogonal projection.\\n    *   Therefore, $\\\\tilde{v} = P_V(v_{\\\\mu_0 \\\\to \\\\mu})$ is uniquely determined.\\n3.  **Mapping Back to Measure:** Once $\\\\tilde{v}$ is found, the projected measure is $\\\\tilde{\\\\mu} = (id + \\\\tilde{v})_\\\\# \\\\mu_0$. The pushforward $(id + \\\\tilde{v})_\\\\# \\\\mu_0$ is well-defined. If $\\\\mu_0$ is absolutely continuous, then $(id + \\\\tilde{v})$ is an optimal transport map from $\\\\mu_0$ to $\\\\tilde{\\\\mu}$ if $\\\\tilde{v}$ is the gradient of a convex function (or potential). Given that $V$ is a subspace of elements like $\\\\nabla \\\\phi$, this mapping is well-behaved.\\n\\n**Conclusion:** The proof is correct. It leverages the isometric mapping between geodesics and initial velocity vectors in the tangent space and the standard result of orthogonal projection in Hilbert spaces. The existence and uniqueness are direct consequences of the Hilbert space structure of $T_{\\\\mu_0}P_2(\\\\mathcal{X})$.\\n\\n### Proposition 2.2: Linearity Property of Tangential Projections\\n\\n**Statement (paraphrased):** Let $\\\\mu_0 \\\\in P_2(\\\\mathcal{X})$ be a reference measure, and $V \\\\subseteq T_{\\\\mu_0}P_2(\\\\mathcal{X})$ be a closed linear subspace. If $v_1, v_2 \\\\in T_{\\\\mu_0}P_2(\\\\mathcal{X})$ are initial velocity vectors corresponding to geodesics from $\\\\mu_0$ to $\\\\mu_1$ and $\\\\mu_2$ respectively, then for any $\\\\alpha, \\\\beta \\\\in \\\\mathbb{R}$, the tangential projection of $\\\\alpha v_1 + \\\\beta v_2$ onto $V$ is $\\\\alpha P_V(v_1) + \\\\beta P_V(v_2)$.\\n\\n**Proof Assessment:**\\n1.  **Context:** This proposition is about the linearity of the *projection operator* $P_V$ acting on tangent vectors.\\n2.  **Hilbert Space Property:** As established, $T_{\\\\mu_0}P_2(\\\\mathcal{X})$ is a Hilbert space. In any Hilbert space, orthogonal projection onto a closed linear subspace is a linear operator. That is, $P_V(ax + by) = a P_V(x) + b P_V(y)$ for any scalars $a,b$ and vectors $x,y$ in the Hilbert space.\\n\\n**Conclusion:** The proof is correct and mathematically trivial, given the established Hilbert space structure of $T_{\\\\mu_0}P_2(\\\\mathcal{X})$ and the definition of tangential projection as an orthogonal projection of tangent vectors. The phrasing could be slightly misleading if one interprets it as linearity in the *measures* $\\\\mu_i$, but it clearly refers to linearity in the *tangent vectors* $v_i$.\\n\\n### Proposition 3.1: Optimal Tangential Projection (for TWPCA)\\n\\n**Statement (paraphrased):** Given a dataset of measures $\\\\{\\\\mu_i\\\\}_{i=1}^N$, a reference measure $\\\\mu_0$, let $v_i \\\\in T_{\\\\mu_0}P_2(\\\\mathcal{X})$ be the initial velocity vectors from $\\\\mu_0$ to $\\\\mu_i$. The Tangential Wasserstein Principal Component Analysis (TWPCA) procedure finds the optimal $k$-dimensional linear subspace $V_k \\\\subseteq T_{\\\\mu_0}P_2(\\\\mathcal{X})$ that minimizes the sum of squared tangential projection errors, i.e., $\\\\sum_{i=1}^N \\\\|v_i - P_{V_k}(v_i)\\\\|_{L^2(\\\\mu_0)}^2$. This optimal subspace $V_k$ is spanned by the $k$ eigenvectors corresponding to the largest eigenvalues of the empirical covariance operator of the tangent vectors $\\\\{v_i\\\\}_{i=1}^N$.\\n\\n**Proof Assessment:**\\n1.  **PCA in Hilbert Spaces:** This proposition is a direct and standard result from Principal Component Analysis theory applied to data points in a Hilbert space.\\n2.  **Objective Function:** The objective function $\\\\sum_{i=1}^N \\\\|v_i - P_{V_k}(v_i)\\\\|_{L^2(\\\\mu_0)}^2$ is precisely the standard reconstruction error criterion for PCA.\\n3.  **Optimal Subspace:** It is a well-known theorem that the $k$-dimensional subspace minimizing this reconstruction error is the one spanned by the $k$ eigenvectors corresponding to the largest eigenvalues of the empirical covariance operator (or matrix, if the space is finite-dimensional or represented by a Gram matrix).\\n    *   The empirical covariance operator $C_v$ for a set of vectors $\\\\{v_i\\\\}$ in a Hilbert space is defined such that $C_v(\\\\cdot) = \\\\frac{1}{N} \\\\sum_{i=1}^N \\\\langle v_i - \\\\bar{v}, \\\\cdot \\\\rangle (v_i - \\\\bar{v})$, where $\\\\bar{v} = \\\\frac{1}{N}\\\\sum v_i$. Its eigenvectors capture the directions of maximal variance.\\n\\n**Conclusion:** The proof is entirely correct. It is a direct application of the fundamental theorem of PCA in Hilbert spaces. The rigor relies on the correct identification of the tangent vectors as elements of a Hilbert space and the proper definition of the objective function.\\n\\n### Validity of Assumptions\\n\\n*   **Absolute Continuity of Measures:** The existence and uniqueness of the optimal transport map $T=id+\\\\nabla\\\\phi$ (and thus $v=\\\\nabla\\\\phi$) requires $\\\\mu_0$ to be absolutely continuous with respect to the Lebesgue measure. This is a standard assumption in this context.\\n*   **Finite Second Moments:** Measures in $P_2(\\\\mathcal{X})$ inherently have finite second moments, which is required for the $W_2$ metric to be finite.\\n*   **Choice of Reference Measure $\\\\mu_0$**: The theory relies on choosing a suitable $\\\\mu_0$. A common choice is the Fr\u00e9chet mean (Wasserstein barycenter) of the dataset, which minimizes $\\\\sum_{i=1}^N W_2^2(\\\\mu, \\\\mu_i)$. This choice is generally sound and provides a central point for linearization.\\n*   **Regularity of $\\\\mathcal{X}$:** $\\\\mathcal{X} = \\\\mathbb{R}^d$ ensures sufficient smoothness for optimal transport theory.\\n\\n## Novelty and Significance\\n\\nThe concept of \\\"Tangential Wasserstein Projections\\\" offers a genuinely novel approach to dimensionality reduction in Wasserstein space, distinguishing itself from prior methods:\\n\\n1.  **Intrinsic Geometry:** Unlike methods that embed measures into a Euclidean space (e.g., using AOT/LOT maps) or rely on barycentric coordinates, this approach works directly within the intrinsic Riemannian geometry of $P_2(\\\\mathcal{X})$ via its tangent space. This avoids potential distortions introduced by embedding procedures.\\n2.  **Linearization in the Tangent Space:** The core novelty lies in recognizing that while $P_2(\\\\mathcal{X})$ is a non-linear manifold, its tangent space at a given point $\\\\mu_0$ is a Hilbert space. This allows for direct application of powerful linear methods (like PCA) to the \\\"directions\\\" of variation between measures.\\n3.  **Clear Optimization Objective:** The TWPCA explicitly minimizes reconstruction error in the tangent space, providing a clear and interpretable objective function directly analogous to standard Euclidean PCA.\\n4.  **Computational Tractability (Conceptual):** While practical computation of $v_i = \\\\nabla \\\\phi_i$ can be challenging (requiring optimal transport map computation), the theoretical framework is clean and translates standard algorithms directly.\\n\\n**Comparison to existing methods:**\\n*   **Barycentric PCA/MDS:** These methods often project data onto a low-dimensional manifold *parameterized* by barycentric coordinates or perform MDS on $W_2$ distances. They don't explicitly utilize the tangent space.\\n*   **Amplitude and Phase OT (AOT/LOT):** These methods embed measures into $L^2$ spaces, effectively linearizing the problem. While effective, they introduce a specific choice of embedding and can be sensitive to it. Tangential projections offer an alternative, perhaps more direct, linearization.\\n\\n## Conclusion\\n\\nThe paper \\\"Tangential Wasserstein Projections\\\" presents a theoretically sound and mathematically rigorous framework for dimensionality reduction in Wasserstein space. The proofs for Propositions 2.1, 2.2, and 3.1 are correct, relying on the well-established Hilbert space structure of the tangent space to $P_2(\\\\mathcal{X})$ and standard results from functional analysis and PCA. The novelty lies in the elegant and principled way the authors leverage this local linearization via the tangent space to apply standard linear dimensionality reduction techniques directly to the intrinsic geometry of Wasserstein space. This approach has the potential to offer significant advantages over methods that rely on embedding or other approximations, providing a deeper connection to the underlying geometric structure of optimal transport. The practical implementation challenge of computing optimal transport maps and their gradients remains, but the theoretical foundation is robust.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.89998725\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"WassersteinExpert\",\n  \"prompt\": \"Evaluate the theoretical soundness and mathematical rigor of the paper 'Tangential Wasserstein Projections', focusing on the correctness of proofs, the validity of assumptions, and the novelty of the theoretical contributions. Provide a detailed assessment of the proofs for Proposition 2.1, 2.2, and 3.1.\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The 'Tangential Wasserstein Projections' (TWP) method, as its name suggests, is primarily a machine learning/statistical tool for representing or projecting complex data (often distributions) onto a manifold using Wasserstein geometry. Its application to causal inference, particularly within the framework of Synthetic Control Methods (SCM), is a novel and potentially powerful extension.\\n\\nLet's evaluate its suitability for causal inference:\\n\\n### 1. Appropriateness of the Synthetic Control Estimator (as modified/applied by TWP)\\n\\n*   **Standard SCM Recap:** The core idea of SCM (Abadie, Diamond, Hainmueller) is to estimate the causal effect of an intervention on a single treated unit by constructing a \\\"synthetic control\\\" unit. This synthetic unit is a weighted average of untreated control units chosen to closely match the treated unit's pre-intervention outcomes and relevant covariates. The weights are constrained to be non-negative and sum to one, ensuring the synthetic control remains an interpolation.\\n*   **TWP's Contribution to SCM:**\\n    *   **Handling Distributional Data:** The most significant potential contribution of TWP to SCM is its ability to directly compare and project *distributions*. Traditional SCM focuses on matching scalar time series (e.g., GDP, crime rates) and point estimates of covariates (e.g., population, demographics). If the \\\"characteristics\\\" of units relevant for confounding are themselves *distributions* (e.g., income distributions, age distributions, distributions of health outcomes within a region), then standard SCM struggles. TWP, by leveraging Wasserstein distance, is inherently designed to compare and project these complex distributional objects. This could revolutionize SCM for certain types of data.\\n    *   **Sophisticated Matching Criterion:** Instead of matching based on Euclidean distance in pre-treatment features or simple means, TWP could define \\\"closeness\\\" or \\\"match\\\" using a more nuanced Wasserstein metric. This might lead to more robust or accurate synthetic controls when the underlying data's geometry is best captured by optimal transport.\\n    *   **Beyond Simple Linear Combinations (Potentially):** While SCM typically involves linear combinations of control units, TWP's \\\"projection\\\" into a tangent space might allow for more complex relationships or feature transformations that are difficult to achieve with simple weighted averages. However, if the output is no longer a direct linear combination of *actual control units*, it might sacrifice some of the interpretability of standard SCM (where the weights explicitly tell you which donor units contribute how much).\\n*   **Limitations/Considerations:**\\n    *   **Computational Complexity:** Wasserstein distances, especially for high-dimensional distributions, can be computationally intensive. While 'tangential projections' are often used to simplify this, the computational burden will likely be higher than standard SCM, which relies on solving a simple quadratic program.\\n    *   **Interpretability:** The appeal of SCM is its intuitive interpretability \u2013 \\\"Country X behaved like a blend of California and New York.\\\" If the \\\"synthetic control\\\" becomes a projection in a abstract tangent space, it might lose some of this direct interpretability, making it harder to explain to policymakers or non-technical audiences.\\n    *   **Necessity:** For traditional SCM applications where data are scalar time series, TWP might be overkill. Its true value lies when the data *are* inherently distributional or high-dimensional in a way that standard SCM cannot capture.\\n\\n### 2. Validity of Causal Assumptions\\n\\nTWP, when applied to SCM, operates within the existing framework of SCM's causal assumptions. It does not fundamentally alter them but rather provides a more sophisticated tool to *fulfill* them, particularly the matching assumption.\\n\\n*   **Common Factor Model / No Unmeasured Confounding:** The core SCM assumption is that, conditional on observed pre-intervention outcomes and covariates, the treated unit's counterfactual outcome can be represented as a weighted average of control units. This implicitly assumes that all relevant confounders are either observed or their influence is captured by the pre-intervention trends. TWP enhances the *matching mechanism* by allowing for more granular (distributional) matching of observed confounders, thus potentially strengthening the argument that \\\"all relevant confounders have been controlled for *among the observed data*.\\\" However, it *cannot* address unmeasured confounding that is not reflected in the observed data.\\n*   **Stable Unit Treatment Value Assumption (SUTVA):** This assumption (no interference between units, consistent treatment) remains unchanged and is required. TWP does not impact SUTVA.\\n*   **Sufficient Control Units & Pre-Treatment Period:** Just like standard SCM, TWP requires a sufficiently rich donor pool of control units and a long enough pre-intervention period to establish a credible match. If the treated unit's distributional trajectory is unique, no method (including TWP) can construct a reliable counterfactual.\\n*   **Interpolation vs. Extrapolation:** SCM is strongest when the synthetic control is an interpolation of the control units. If the TWP leads to projections that are effectively extrapolating beyond the convex hull of the control unit distributions, the validity of the counterfactual outcome decreases significantly.\\n\\n### 3. Practical Relevance of the Empirical Results\\n\\nThe practical relevance hinges on the types of causal questions TWP can answer that traditional methods cannot, or answers significantly better.\\n\\n*   **Niche Applications:** TWP's primary practical relevance would be in settings where the \\\"state\\\" of a unit is best described by a *distribution* rather than a single number. Examples:\\n    *   Evaluating the impact of a policy on *income inequality* (distribution of income) rather than just average income.\\n    *   Assessing the effect of an intervention on *student performance distributions* (e.g., test score quartiles) across schools, not just average scores.\\n    *   Comparing the impact of a medical intervention on *patient symptom severity distributions* in hospitals.\\n    *   Analyzing the effect of environmental regulations on *air quality distributions* across cities.\\n*   **Real-world Applicability:** If the paper presents empirical results using synthetic or real-world datasets where units' characteristics are genuinely distributional, and TWP demonstrates superior performance (e.g., better pre-treatment fit in Wasserstein space, more plausible counterfactuals) compared to attempts to simplify these distributions to scalars for standard SCM, then its practical relevance is high.\\n*   **Usability and Tooling:** For TWP to be widely adopted, it needs to be packaged into user-friendly software or libraries, similar to `Synth` in R or `synth_runner` in Python. The complexity of Wasserstein geometry might make this more challenging.\\n\\n### 4. Strength of Evidence Supporting Causal Claims\\n\\nThe strength of evidence for causal claims derived using TWP in an SCM context depends on several factors:\\n\\n*   **Quality of Pre-Intervention Fit:** The first pillar of SCM's credibility is an excellent pre-intervention match. If TWP can demonstrate a significantly better match in the relevant distributional sense than standard methods (or methods that simply match moments of distributions), this strengthens the evidence. Visualizations showing the alignment of pre-treatment *distributions* (not just means) would be crucial.\\n*   **Placebo/Permutation Tests:** For a single treated unit study, the gold standard for inference in SCM is permutation tests or placebo tests. These involve applying the SCM procedure to untreated control units (placebos) to see if similar \\\"treatment effects\\\" are found by chance. A compelling result shows the treated unit's effect as an outlier in the distribution of placebo effects. The paper should ideally incorporate these to demonstrate statistical significance and rule out chance findings.\\n*   **Sensitivity Analyses:** Robustness checks, such as excluding certain control units, varying the pre-treatment period, or including different sets of covariates, are essential for any SCM application. TWP's results should be shown to be robust to these variations.\\n*   **Theoretical Guarantees:** Does the TWP method offer any theoretical guarantees or bounds on the estimation error, particularly when dealing with distributional data? Such guarantees would bolster confidence in the causal estimates.\\n*   **Comparison to Alternatives:** For the types of problems TWP is designed for, how does it compare to other methods that might try to handle distributional data (e.g., matching on kernel density estimates, or using functional data analysis in SCM)? Demonstrating superior performance against relevant alternatives strengthens the claim.\\n\\nIn conclusion, Tangential Wasserstein Projections offers a theoretically appealing extension to Synthetic Control Methods, particularly for causal inference problems where the characteristics and outcomes of units are best represented as *distributions*. Its appropriateness stems from its ability to leverage Wasserstein geometry for more sophisticated matching. The validity of causal assumptions remains largely consistent with standard SCM, though TWP allows for a more rigorous fulfillment of the 'no unmeasured confounding' assumption by enabling richer observed matching. The practical relevance is high for specific, complex datasets. The strength of causal evidence will depend heavily on the empirical implementation, particularly the quality of pre-treatment fit of *distributions*, rigorous inference procedures (like placebo tests), and robust sensitivity analyses. If applied judiciously to problems where its unique capabilities are truly needed, TWP could represent a significant methodological advance in causal inference.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.8991474\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Evaluate the application of the proposed method in 'Tangential Wasserstein Projections' to causal inference, focusing on the appropriateness of the synthetic control estimator, the validity of the causal assumptions, and the practical relevance of the empirical results. Assess the strength of the evidence supporting the causal claims.\",\n  \"agent_name\": \"CausalInferenceReviewer\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The paper \\\"Tangential Wasserstein Projections\\\" introduces a novel framework for dimensionality reduction and manifold learning in the space of probability measures, $\\\\mathcal{P}_2(\\\\mathbb{R}^D)$, equipped with the 2-Wasserstein metric. The core idea is to project measures onto a lower-dimensional tangent space defined at a specific point (a Fr\u00e9chet mean) on the manifold. The statistical consistency results, particularly Proposition 3.1 and Corollary 3.1, are crucial for establishing the theoretical validity of this empirical approach.\\n\\n### Summary of Key Consistency Results\\n\\nThe paper aims to estimate a low-dimensional manifold $\\\\mathcal{M}$ embedded in $\\\\mathcal{P}_2(\\\\mathbb{R}^D)$. When data are given as i.i.d. samples $X_1, \\\\ldots, X_n$ from a measure $\\\\nu$ residing on $\\\\mathcal{M}$, the goal is to estimate the tangent space $T_x$ at the true Fr\u00e9chet mean $x = \\\\mathbb{E}_{\\\\nu}[X]$ and to project empirical measures onto this estimated space.\\n\\n1.  **Proposition 3.1: Consistency of the Empirical Tangent Space.**\\n    *   **Statement:** This proposition asserts that the empirical tangent space $\\\\widehat{T}_{\\\\bar{x}_n}$ converges almost surely to the true tangent space $T_x$ as the sample size $n \\\\to \\\\infty$. Here, $\\\\bar{x}_n = \\\\mathbb{E}_{\\\\mu_n}[X]$ is the sample mean of the support points, and $\\\\widehat{T}_{\\\\bar{x}_n}$ is constructed based on the empirical measure $\\\\mu_n = \\\\frac{1}{n}\\\\sum_{i=1}^n \\\\delta_{X_i}$. The convergence is in a metric space of subspaces (specifically, a Fr\u00e9chet mean of subspaces metric defined in the paper's supplementary material).\\n\\n2.  **Corollary 3.1: Consistency of the Empirical Projection.**\\n    *   **Statement:** Building upon Proposition 3.1, this corollary states that the empirical projection $P_{\\\\bar{x}_n}(\\\\mu_n)$ converges almost surely to $x$ (the true Fr\u00e9chet mean on the manifold) as $n \\\\to \\\\infty$. This implies that the projection of the empirical measure onto the *estimated* tangent space converges to the true point on the manifold that the data is sampled around.\\n\\n### Analysis of Assumptions Required for Consistency\\n\\nThe consistency results rely on several significant assumptions, which are common in manifold learning theory but have specific implications in the Wasserstein space setting:\\n\\n1.  **A1: Smoothness of the Manifold $\\\\mathcal{M}$:** The manifold $\\\\mathcal{M}$ is assumed to be a \\\"closed embedded $d$-dimensional submanifold of $\\\\mathcal{P}_2(\\\\mathbb{R}^D)$.\\\" This is fundamental. If the manifold is not smooth, the concept of a unique, well-defined tangent space (as a vector space) is problematic. This is a standard assumption in differential geometry and manifold learning.\\n\\n2.  **A2: Smoothness of the Parametrization Map $F$:** The manifold is locally parametrized by a \\\"continuously differentiable map $F: U \\\\subset \\\\mathbb{R}^d \\\\to \\\\mathcal{P}_2(\\\\mathbb{R}^D)$.\\\" This ensures that the tangent vectors and the tangent space are well-defined and can be related to derivatives in the parameter space.\\n\\n3.  **A3: Data Distribution on the Manifold:** The underlying probability measure $\\\\nu$ from which the data $X_1, \\\\ldots, X_n$ are drawn is assumed to be a \\\"Borel probability measure on $\\\\mathcal{M}$.\\\" This is a strong theoretical assumption. It implies that the true data points *lie perfectly on the manifold* and are not subject to observation noise that would pull them off the manifold. This is a common idealization in theoretical manifold learning, but it is a significant limitation for practical applications where data is inherently noisy and only *approximately* on a manifold.\\n\\n4.  **A4: i.i.d. Sampling:** The data points $X_1, \\\\ldots, X_n$ are assumed to be independent and identically distributed (i.i.d.) draws from $\\\\nu$. This is a standard assumption for almost all statistical consistency results and allows for the application of laws of large numbers and central limit theorems.\\n\\n5.  **A5: Convergence of Empirical Mean:** It is assumed that $\\\\bar{x}_n = \\\\mathbb{E}_{\\\\mu_n}[X] \\\\to x = \\\\mathbb{E}_{\\\\nu}[X]$ almost surely. This is guaranteed by the Law of Large Numbers for i.i.d. samples, provided the first moment exists. The authors also use results on the convergence of empirical measures in Wasserstein space, e.g., $\\\\mu_n \\\\to \\\\nu$ in $W_2$ a.s.\\n\\n6.  **A6: Regularity of the Fr\u00e9chet Mean $x$ (for Tangent Space Definition):** Proposition 3.1 specifically requires that the true Fr\u00e9chet mean $x$ is \\\"non-atomic and $\\\\nabla \\\\log \\\\rho_x$ is Lipschitz for some representation $x = (\\\\rho_x, \\\\nabla \\\\phi_x)$.\\\" This is a critical assumption specific to the structure of Wasserstein space and the definition of the tangent space via optimal transport maps.\\n    *   **Non-atomic:** This means $x$ does not place probability mass on individual points. This is usually fine for continuous distributions.\\n    *   **Lipschitz gradient of log-density:** This is a strong regularity condition on the underlying measure $\\\\rho_x$ (the density of $x$ with respect to Lebesgue measure). It ensures that the displacement fields that define the tangent space are sufficiently well-behaved. Many common distributions (e.g., Gaussian, uniform on a compact set) satisfy this, but it may not hold for measures with sharp boundaries, singular components, or very rough densities. This significantly limits the generality of the type of probability measures to which the theory applies.\\n\\n7.  **A7: Uniqueness and Continuity of Projection (Implicit):** For Corollary 3.1, the existence, uniqueness, and continuity of the projection operator $P_x(\\\\cdot)$ are implicitly assumed or derived from the structure of the tangent spaces and the properties of Wasserstein geometry. While the tangent space is a Hilbert space, the projection onto it requires the space to be \\\"nice enough.\\\"\\n\\n### Rates of Convergence\\n\\nThe consistency results in Proposition 3.1 and Corollary 3.1 are stated as **almost sure convergence**. This is a strong form of convergence, meaning that the estimated quantities converge to their true counterparts for almost all possible sequences of random samples.\\n\\nHowever, the paper **does not provide explicit rates of convergence** (e.g., $O_p(n^{-1/2})$ or $O(n^{-1})$). This is a limitation. Knowing the rate of convergence is crucial for understanding:\\n*   **Efficiency:** How quickly the estimator approaches the true value as $n$ increases.\\n*   **Finite Sample Performance:** How large $n$ needs to be to achieve a certain level of accuracy in practice.\\n*   **Comparison:** Without rates, it's difficult to compare the statistical efficiency of TWP with other manifold learning methods, or even alternative ways of estimating the tangent space in Wasserstein space.\\n\\nThe absence of rates is common in initial theoretical developments for complex, non-Euclidean spaces like Wasserstein space, as deriving such bounds can be significantly more challenging than in Euclidean settings.\\n\\n### Generality of Proposition 3.1 and Corollary 3.1\\n\\nThe generality of these results can be assessed along several dimensions:\\n\\n1.  **Manifold Type:** The results apply to any \\\"closed embedded $d$-dimensional submanifold\\\" of $\\\\mathcal{P}_2(\\\\mathbb{R}^D)$. This is quite general within the paradigm of smooth manifold learning. It doesn't restrict to specific types of manifolds (e.g., geodesics, or specific statistical models).\\n\\n2.  **Space of Measures:** The results are specific to the 2-Wasserstein space $\\\\mathcal{P}_2(\\\\mathbb{R}^D)$. They do not generalize to other metric spaces, or even other optimal transport metrics (like $W_p$ for $p \\\\neq 2$), or other types of probability metric spaces (e.g., total variation, KL divergence). The Hilbertian structure of the tangent space in $W_2$ is key to the definitions and derivations.\\n\\n3.  **Type of Measures (A6 Critical):** This is where generality is most constrained. The assumption of \\\"non-atomic and $\\\\nabla \\\\log \\\\rho_x$ is Lipschitz\\\" for the true Fr\u00e9chet mean $x$ is highly restrictive.\\n    *   **Atomic Measures:** It generally excludes discrete probability measures or measures with atomic components, which are common in real-world applications (e.g., count data, categorical data represented as point masses).\\n    *   **Lack of Smoothness:** It also excludes measures whose densities are not sufficiently smooth. For instance, a uniform distribution on a complicated domain might not have a Lipschitz log-gradient at the boundary. This limits the applicability to \\\"well-behaved\\\" continuous measures.\\n\\n4.  **Nature of Data (A3 Critical):** The assumption that data points are drawn *from the manifold* itself is a strong theoretical idealization. In reality, data is usually observed with noise, meaning it lies *near* the manifold, not *on* it. The consistency results do not directly address the robustness of TWP to such noise or the consistency of estimating a manifold from noisy off-manifold data. This is a common theoretical gap in manifold learning that subsequent research often aims to fill (e.g., by considering principal curves/surfaces that approximate the data rather than assuming it's on a true manifold).\\n\\n### Practical Implications of the Theoretical Findings\\n\\n1.  **Theoretical Justification:** The consistency results provide a strong theoretical foundation for Tangential Wasserstein Projections. They indicate that, under ideal conditions, the empirical methods proposed will indeed converge to their true, underlying geometric counterparts. This is essential for building confidence in the method.\\n\\n2.  **Feasibility of Estimation:** They demonstrate that estimating the local geometry (tangent space) and performing projections in Wasserstein space using finite empirical samples is statistically feasible, at least asymptotically.\\n\\n3.  **Guidance for Algorithm Design:** The proofs hint at the necessary properties for the numerical algorithms (e.g., accurate estimation of Fr\u00e9chet means, robust computation of optimal transport maps).\\n\\n4.  **Limitations in Real-World Applications:**\\n    *   **Noise Robustness:** The most significant practical limitation is the assumption that data lies perfectly on the manifold. Real-world data is noisy. The current theory doesn't explain how TWP performs when data is off-manifold or how to adapt it for robust estimation in the presence of noise.\\n    *   **Verifying Assumptions:** It's often impractical or impossible to verify the \\\"Lipschitz gradient of log-density\\\" assumption for real empirical measures.\\n    *   **Computational Cost:** While consistency is proven, the actual computation of empirical Fr\u00e9chet means and optimal transport maps is notoriously expensive for high-dimensional measures and large sample sizes. The theoretical consistency doesn't guarantee computational tractability for practical use cases.\\n    *   **Finite Sample Performance:** Without explicit rates of convergence, practitioners have little guidance on how large a sample size $n$ needs to be to achieve a reasonable approximation of the true tangent space or projection.\\n    *   **Model Misspecification:** If the true data structure is not a smooth manifold in Wasserstein space (e.g., a union of manifolds, or simply a cloud of points without a discernible manifold structure), the theoretical guarantees do not apply.\\n\\nIn conclusion, 'Tangential Wasserstein Projections' provides significant theoretical groundwork for manifold learning in Wasserstein space, demonstrating the consistency of its core components under specific assumptions. The almost sure convergence is a strong theoretical result. However, the strong assumptions regarding data lying perfectly on the manifold and the regularity of measures, coupled with the absence of explicit convergence rates, suggest that further research is needed to bridge the gap between this elegant theory and its broader applicability to noisy, real-world data and computationally efficient implementations.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.89813025\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"agent_name\": \"StatisticalConsistencyAnalyst\",\n  \"prompt\": \"Evaluate the statistical consistency results in 'Tangential Wasserstein Projections', focusing on the assumptions required for consistency, the rates of convergence, and the practical implications of the theoretical findings. Analyze the generality of Proposition 3.1 and Corollary 3.1.\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The paper \"Tangential Wasserstein Projections\" proposes a novel approach to dimensionality reduction in the space of probability measures $P_2(\\\\mathcal{X})$ equipped with the $W_2$ metric, focusing on projecting data onto \"tangential\" subspaces. This evaluation will assess its theoretical soundness, mathematical rigor, and the novelty of its contributions, with a detailed focus on Propositions 2.1, 2.2, and 3.1.\\n\\n## Overall Assessment\\n\\nThe core idea of \"Tangential Wasserstein Projections\" is sound and builds upon established concepts in optimal transport and the geometric structure of Wasserstein space, particularly the Otto calculus. By working with the tangent space identified as a Hilbert space of vector fields (or gradients of potentials), the authors effectively linearize the problem locally around a reference measure. This allows them to leverage standard Hilbert space techniques (like orthogonal projection and PCA) in a novel and principled way for Wasserstein data.\\n\\nThe mathematical rigor hinges critically on the precise definition and properties of the tangent space and the \"tangential projection\" itself. Assuming these foundational definitions are robustly established within the paper (which is common in the literature building on Otto\\'s work), the subsequent propositions largely become applications of standard Hilbert space theory.\\n\\n### Core Concepts and Theoretical Framework\\n\\n1.  **Wasserstein Space $P_2(\\\\mathcal{X})$:** The paper assumes the standard definition of $P_2(\\\\mathcal{X})$, the space of probability measures with finite second moments on $\\\\mathcal{X} = \\\\mathbb{R}^d$, endowed with the $W_2$ metric. This is a well-established complete metric space.\\n2.  **Geodesics:** Geodesics in $P_2(\\\\mathcal{X})$ are known to be unique and can be characterized by optimal transport maps or displacement interpolation. For any two measures $\\\\mu_0, \\\\mu_1 \\\\in P_2(\\\\mathcal{X})$, there exists a unique optimal transport map $T: \\\\mathcal{X} \\\\to \\\\mathcal{X}$ such that $T_\\\\# \\\\mu_0 = \\\\mu_1$ and $T = id + \\\\nabla \\\\phi$ for some convex function $\\\\phi$ (Brenier\\'s Theorem), assuming $\\\\mu_0$ is absolutely continuous.\\n3.  **Tangent Space $T_{\\\\mu_0}P_2(\\\\mathcal{X})$:** This is the cornerstone of the paper. Following the Otto calculus, the tangent space at a measure $\\\\mu_0$ is typically identified with the space of vector fields $v \\\\in L^2(\\\\mu_0; \\\\mathbb{R}^d)$ that are gradients of potentials, i.e., $v = \\\\nabla \\\\phi$ for some $\\\\phi \\\\in L^2(\\\\mu_0)$ with $\\\\int \\\\phi d\\\\mu_0 = 0$. This space, equipped with the $L^2(\\\\mu_0)$ inner product $\\\\langle v_1, v_2 \\\\rangle_{L^2(\\\\mu_0)} = \\\\int v_1 \\\\cdot v_2 d\\\\mu_0$, forms a Hilbert space.\\n    *   A geodesic from $\\\\mu_0$ to $\\\\mu_t$ can be represented as $\\\\mu_t = (id + t v)_\\\\# \\\\mu_0$ for small $t$, where $v = \\\\nabla \\\\phi$ is the initial velocity field. More generally, given $\\\\mu_0$ and $\\\\mu_1$, the \"initial velocity vector field\" $v_{\\\\mu_0 \\\\to \\\\mu_1}$ is given by $\\\\nabla \\\\phi$ where $T = id + \\\\nabla \\\\phi$ is the optimal transport map from $\\\\mu_0$ to $\\\\mu_1$. This identification is crucial.\\n4.  **Tangential Projection:** The paper defines a \"tangential projection\" not as a projection of a measure *onto a manifold of measures*, but rather as a projection *of the tangent vector* connecting a reference measure $\\\\mu_0$ to a given measure $\\\\mu_i$ *onto a linear subspace* $V$ within the tangent space $T_{\\\\mu_0}P_2(\\\\mathcal{X})$. The resulting \"projected measure\" is then obtained by pushing forward $\\\\mu_0$ using this projected tangent vector. This distinction is vital for understanding the simplicity and rigor of the proofs.\\n\\n## Detailed Assessment of Proofs\\n\\nThe proofs of Propositions 2.1, 2.2, and 3.1 are fundamentally correct, provided the underlying Hilbert space structure of the tangent space and the definition of the \"tangential projection\" are correctly set up and robust. They mostly rely on well-established properties of Hilbert spaces.\\n\\n### Proposition 2.1: Existence and Uniqueness of Tangential Projection\\n\\n**Statement (paraphrased):** Given a reference measure $\\\\mu_0$, a measure $\\\\mu \\\\in P_2(\\\\mathcal{X})$, and a closed linear subspace $V \\\\subseteq T_{\\\\mu_0}P_2(\\\\mathcal{X})$, there exists a unique measure $\\\\tilde{\\\\mu} \\\\in P_2(\\\\mathcal{X})$ such that $\\\\tilde{\\\\mu} = (id + \\\\tilde{v})_\\\\# \\\\mu_0$ for some $\\\\tilde{v} \\\\in V$, which minimizes $W_2(\\\\mu, (id+v)_\\\\# \\\\mu_0)$ over all $v \\\\in V$.\\n\\n**Proof Assessment:**\\n1.  **Identification of Tangent Vectors:** The proof hinges on identifying the tangent vector $v_{\\\\mu_0 \\\\to \\\\mu}$ for the geodesic from $\\\\mu_0$ to $\\\\mu$. This is $v_{\\\\mu_0 \\\\to \\\\mu} = \\\\nabla \\\\phi$, where $T = id + \\\\nabla \\\\phi$ is the optimal transport map from $\\\\mu_0$ to $\\\\mu$. This $v_{\\\\mu_0 \\\\to \\\\mu}$ is a unique element of $T_{\\\\mu_0}P_2(\\\\mathcal{X})$ (viewed as a space of gradients of $L^2(\\\\mu_0)$ functions, or more broadly, as $L^2(\\\\mu_0; \\\\mathbb{R}^d)$ vector fields).\\n2.  **Minimization Problem:** The problem is to find $\\\\tilde{v} \\\\in V$ that minimizes $W_2(\\\\mu, (id+v)_\\\\# \\\\mu_0)$.\\n    *   The key insight is that $W_2^2(\\\\mu_0, \\\\mu) = \\\\int \\\\|v_{\\\\mu_0 \\\\to \\\\mu}(x)\\\\|^2 d\\\\mu_0(x) = \\\\|v_{\\\\mu_0 \\\\to \\\\mu}\\\\|_{L^2(\\\\mu_0)}^2$. This identity holds when $v_{\\\\mu_0 \\\\to \\\\mu}$ is the initial velocity of the geodesic.\\n    *   The paper\\'s definition of \"tangential projection\" implicitly means finding $\\\\tilde{v} \\\\in V$ such that $\\\\tilde{v}$ is the *orthogonal projection* of $v_{\\\\mu_0 \\\\to \\\\mu}$ onto $V$.\\n    *   In a Hilbert space (which $T_{\\\\mu_0}P_2(\\\\mathcal{X})$ with the $L^2(\\\\mu_0)$ norm is), for any element $x$ and any closed subspace $V$, there exists a unique $P_V(x) \\\\in V$ that minimizes $\\\\|x - y\\\\|$ for $y \\\\in V$. This is the orthogonal projection.\\n    *   Therefore, $\\\\tilde{v} = P_V(v_{\\\\mu_0 \\\\to \\\\mu})$ is uniquely determined.\\n3.  **Mapping Back to Measure:** Once $\\\\tilde{v}$ is found, the projected measure is $\\\\tilde{\\\\mu} = (id + \\\\tilde{v})_\\\\# \\\\mu_0$. The pushforward $(id + \\\\tilde{v})_\\\\# \\\\mu_0$ is well-defined. If $\\\\mu_0$ is absolutely continuous, then $(id + \\\\tilde{v})$ is an optimal transport map from $\\\\mu_0$ to $\\\\tilde{\\\\mu}$ if $\\\\tilde{v}$ is the gradient of a convex function (or potential). Given that $V$ is a subspace of elements like $\\\\nabla \\\\phi$, this mapping is well-behaved.\\n\\n**Conclusion:** The proof is correct. It leverages the isometric mapping between geodesics and initial velocity vectors in the tangent space and the standard result of orthogonal projection in Hilbert spaces. The existence and uniqueness are direct consequences of the Hilbert space structure of $T_{\\\\mu_0}P_2(\\\\mathcal{X})$.\\n\\n### Proposition 2.2: Linearity Property of Tangential Projections\\n\\n**Statement (paraphrased):** Let $\\\\mu_0 \\\\in P_2(\\\\mathcal{X})$ be a reference measure, and $V \\\\subseteq T_{\\\\mu_0}P_2(\\\\mathcal{X})$ be a closed linear subspace. If $v_1, v_2 \\\\in T_{\\\\mu_0}P_2(\\\\mathcal{X})$ are initial velocity vectors corresponding to geodesics from $\\\\mu_0$ to $\\\\mu_1$ and $\\\\mu_2$ respectively, then for any $\\\\alpha, \\\\beta \\\\in \\\\mathbb{R}$, the tangential projection of $\\\\alpha v_1 + \\\\beta v_2$ onto $V$ is $\\\\alpha P_V(v_1) + \\\\beta P_V(v_2)$.\\n\\n**Proof Assessment:**\\n1.  **Context:** This proposition is about the linearity of the *projection operator* $P_V$ acting on tangent vectors.\\n2.  **Hilbert Space Property:** As established, $T_{\\\\mu_0}P_2(\\\\mathcal{X})$ is a Hilbert space. In any Hilbert space, orthogonal projection onto a closed linear subspace is a linear operator. That is, $P_V(ax + by) = a P_V(x) + b P_V(y)$ for any scalars $a,b$ and vectors $x,y$ in the Hilbert space.\\n\\n**Conclusion:** The proof is correct and mathematically trivial, given the established Hilbert space structure of $T_{\\\\mu_0}P_2(\\\\mathcal{X})$ and the definition of tangential projection as an orthogonal projection of tangent vectors. The phrasing could be slightly misleading if one interprets it as linearity in the *measures* $\\\\mu_i$, but it clearly refers to linearity in the *tangent vectors* $v_i$.\\n\\n### Proposition 3.1: Optimal Tangential Projection (for TWPCA)\\n\\n**Statement (paraphrased):** Given a dataset of measures $\\\\{\\\\mu_i\\\\}_{i=1}^N$, a reference measure $\\\\mu_0$, let $v_i \\\\in T_{\\\\mu_0}P_2(\\\\mathcal{X})$ be the initial velocity vectors from $\\\\mu_0$ to $\\\\mu_i$. The Tangential Wasserstein Principal Component Analysis (TWPCA) procedure finds the optimal $k$-dimensional linear subspace $V_k \\\\subseteq T_{\\\\mu_0}P_2(\\\\mathcal{X})$ that minimizes the sum of squared tangential projection errors, i.e., $\\\\sum_{i=1}^N \\\\|v_i - P_{V_k}(v_i)\\\\|_{L^2(\\\\mu_0)}^2$. This optimal subspace $V_k$ is spanned by the $k$ eigenvectors corresponding to the largest eigenvalues of the empirical covariance operator of the tangent vectors $\\\\{v_i\\\\}_{i=1}^N$.\\n\\n**Proof Assessment:**\\n1.  **PCA in Hilbert Spaces:** This proposition is a direct and standard result from Principal Component Analysis theory applied to data points in a Hilbert space.\\n2.  **Objective Function:** The objective function $\\\\sum_{i=1}^N \\\\|v_i - P_{V_k}(v_i)\\\\|_{L^2(\\\\mu_0)}^2$ is precisely the standard reconstruction error criterion for PCA.\\n3.  **Optimal Subspace:** It is a well-known theorem that the $k$-dimensional subspace minimizing this reconstruction error is the one spanned by the $k$ eigenvectors corresponding to the largest eigenvalues of the empirical covariance operator (or matrix, if the space is finite-dimensional or represented by a Gram matrix).\\n    *   The empirical covariance operator $C_v$ for a set of vectors $\\\\{v_i\\\\}$ in a Hilbert space is defined such that $C_v(\\\\cdot) = \\\\frac{1}{N} \\\\sum_{i=1}^N \\\\langle v_i - \\\\bar{v}, \\\\cdot \\\\rangle (v_i - \\\\bar{v})$, where $\\\\bar{v} = \\\\frac{1}{N}\\\\sum v_i$. Its eigenvectors capture the directions of maximal variance.\\n\\n**Conclusion:** The proof is entirely correct. It is a direct application of the fundamental theorem of PCA in Hilbert spaces. The rigor relies on the correct identification of the tangent vectors as elements of a Hilbert space and the proper definition of the objective function.\\n\\n### Validity of Assumptions\\n\\n*   **Absolute Continuity of Measures:** The existence and uniqueness of the optimal transport map $T=id+\\\\nabla\\\\phi$ (and thus $v=\\\\nabla\\\\phi$) requires $\\\\mu_0$ to be absolutely continuous with respect to the Lebesgue measure. This is a standard assumption in this context.\\n*   **Finite Second Moments:** Measures in $P_2(\\\\mathcal{X})$ inherently have finite second moments, which is required for the $W_2$ metric to be finite.\\n*   **Choice of Reference Measure $\\\\mu_0$**: The theory relies on choosing a suitable $\\\\mu_0$. A common choice is the Fr\u00e9chet mean (Wasserstein barycenter) of the dataset, which minimizes $\\\\sum_{i=1}^N W_2^2(\\\\mu, \\\\mu_i)$. This choice is generally sound and provides a central point for linearization.\\n*   **Regularity of $\\\\mathcal{X}$:** $\\\\mathcal{X} = \\\\mathbb{R}^d$ ensures sufficient smoothness for optimal transport theory.\\n\\n## Novelty and Significance\\n\\nThe concept of \"Tangential Wasserstein Projections\" offers a genuinely novel approach to dimensionality reduction in Wasserstein space, distinguishing itself from prior methods:\\n\\n1.  **Intrinsic Geometry:** Unlike methods that embed measures into a Euclidean space (e.g., using AOT/LOT maps) or rely on barycentric coordinates, this approach works directly within the intrinsic Riemannian geometry of $P_2(\\\\mathcal{X})$ via its tangent space. This avoids potential distortions introduced by embedding procedures.\\n2.  **Linearization in the Tangent Space:** The core novelty lies in recognizing that while $P_2(\\\\mathcal{X})$ is a non-linear manifold, its tangent space at a given point $\\\\mu_0$ is a Hilbert space. This allows for direct application of powerful linear methods (like PCA) to the \"directions\" of variation between measures.\\n3.  **Clear Optimization Objective:** The TWPCA explicitly minimizes reconstruction error in the tangent space, providing a clear and interpretable objective function directly analogous to standard Euclidean PCA.\\n4.  **Computational Tractability (Conceptual):** While practical computation of $v_i = \\\\nabla \\\\phi_i$ can be challenging (requiring optimal transport map computation), the theoretical framework is clean and translates standard algorithms directly.\\n\\n**Comparison to existing methods:**\\n*   **Barycentric PCA/MDS:** These methods often project data onto a low-dimensional manifold *parameterized* by barycentric coordinates or perform MDS on $W_2$ distances. They don\\'t explicitly utilize the tangent space.\\n*   **Amplitude and Phase OT (AOT/LOT):** These methods embed measures into $L^2$ spaces, effectively linearizing the problem. While effective, they introduce a specific choice of embedding and can be sensitive to it. Tangential projections offer an alternative, perhaps more direct, linearization.\\n\\n## Conclusion\\n\\nThe paper \"Tangential Wasserstein Projections\" presents a theoretically sound and mathematically rigorous framework for dimensionality reduction in Wasserstein space. The proofs for Propositions 2.1, 2.2, and 3.1 are correct, relying on the well-established Hilbert space structure of the tangent space to $P_2(\\\\mathcal{X})$ and standard results from functional analysis and PCA. The novelty lies in the elegant and principled way the authors leverage this local linearization via the tangent space to apply standard linear dimensionality reduction techniques directly to the intrinsic geometry of Wasserstein space. This approach has the potential to offer significant advantages over methods that rely on embedding or other approximations, providing a deeper connection to the underlying geometric structure of optimal transport. The practical implementation challenge of computing optimal transport maps and their gradients remains, but the theoretical foundation is robust.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.89998725}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The \\'Tangential Wasserstein Projections\\' (TWP) method, as its name suggests, is primarily a machine learning/statistical tool for representing or projecting complex data (often distributions) onto a manifold using Wasserstein geometry. Its application to causal inference, particularly within the framework of Synthetic Control Methods (SCM), is a novel and potentially powerful extension.\\n\\nLet\\'s evaluate its suitability for causal inference:\\n\\n### 1. Appropriateness of the Synthetic Control Estimator (as modified/applied by TWP)\\n\\n*   **Standard SCM Recap:** The core idea of SCM (Abadie, Diamond, Hainmueller) is to estimate the causal effect of an intervention on a single treated unit by constructing a \"synthetic control\" unit. This synthetic unit is a weighted average of untreated control units chosen to closely match the treated unit\\'s pre-intervention outcomes and relevant covariates. The weights are constrained to be non-negative and sum to one, ensuring the synthetic control remains an interpolation.\\n*   **TWP\\'s Contribution to SCM:**\\n    *   **Handling Distributional Data:** The most significant potential contribution of TWP to SCM is its ability to directly compare and project *distributions*. Traditional SCM focuses on matching scalar time series (e.g., GDP, crime rates) and point estimates of covariates (e.g., population, demographics). If the \"characteristics\" of units relevant for confounding are themselves *distributions* (e.g., income distributions, age distributions, distributions of health outcomes within a region), then standard SCM struggles. TWP, by leveraging Wasserstein distance, is inherently designed to compare and project these complex distributional objects. This could revolutionize SCM for certain types of data.\\n    *   **Sophisticated Matching Criterion:** Instead of matching based on Euclidean distance in pre-treatment features or simple means, TWP could define \"closeness\" or \"match\" using a more nuanced Wasserstein metric. This might lead to more robust or accurate synthetic controls when the underlying data\\'s geometry is best captured by optimal transport.\\n    *   **Beyond Simple Linear Combinations (Potentially):** While SCM typically involves linear combinations of control units, TWP\\'s \"projection\" into a tangent space might allow for more complex relationships or feature transformations that are difficult to achieve with simple weighted averages. However, if the output is no longer a direct linear combination of *actual control units*, it might sacrifice some of the interpretability of standard SCM (where the weights explicitly tell you which donor units contribute how much).\\n*   **Limitations/Considerations:**\\n    *   **Computational Complexity:** Wasserstein distances, especially for high-dimensional distributions, can be computationally intensive. While \\'tangential projections\\' are often used to simplify this, the computational burden will likely be higher than standard SCM, which relies on solving a simple quadratic program.\\n    *   **Interpretability:** The appeal of SCM is its intuitive interpretability \u2013 \"Country X behaved like a blend of California and New York.\" If the \"synthetic control\" becomes a projection in a abstract tangent space, it might lose some of this direct interpretability, making it harder to explain to policymakers or non-technical audiences.\\n    *   **Necessity:** For traditional SCM applications where data are scalar time series, TWP might be overkill. Its true value lies when the data *are* inherently distributional or high-dimensional in a way that standard SCM cannot capture.\\n\\n### 2. Validity of Causal Assumptions\\n\\nTWP, when applied to SCM, operates within the existing framework of SCM\\'s causal assumptions. It does not fundamentally alter them but rather provides a more sophisticated tool to *fulfill* them, particularly the matching assumption.\\n\\n*   **Common Factor Model / No Unmeasured Confounding:** The core SCM assumption is that, conditional on observed pre-intervention outcomes and covariates, the treated unit\\'s counterfactual outcome can be represented as a weighted average of control units. This implicitly assumes that all relevant confounders are either observed or their influence is captured by the pre-intervention trends. TWP enhances the *matching mechanism* by allowing for more granular (distributional) matching of observed confounders, thus potentially strengthening the argument that \"all relevant confounders have been controlled for *among the observed data*.\" However, it *cannot* address unmeasured confounding that is not reflected in the observed data.\\n*   **Stable Unit Treatment Value Assumption (SUTVA):** This assumption (no interference between units, consistent treatment) remains unchanged and is required. TWP does not impact SUTVA.\\n*   **Sufficient Control Units & Pre-Treatment Period:** Just like standard SCM, TWP requires a sufficiently rich donor pool of control units and a long enough pre-intervention period to establish a credible match. If the treated unit\\'s distributional trajectory is unique, no method (including TWP) can construct a reliable counterfactual.\\n*   **Interpolation vs. Extrapolation:** SCM is strongest when the synthetic control is an interpolation of the control units. If the TWP leads to projections that are effectively extrapolating beyond the convex hull of the control unit distributions, the validity of the counterfactual outcome decreases significantly.\\n\\n### 3. Practical Relevance of the Empirical Results\\n\\nThe practical relevance hinges on the types of causal questions TWP can answer that traditional methods cannot, or answers significantly better.\\n\\n*   **Niche Applications:** TWP\\'s primary practical relevance would be in settings where the \"state\" of a unit is best described by a *distribution* rather than a single number. Examples:\\n    *   Evaluating the impact of a policy on *income inequality* (distribution of income) rather than just average income.\\n    *   Assessing the effect of an intervention on *student performance distributions* (e.g., test score quartiles) across schools, not just average scores.\\n    *   Comparing the impact of a medical intervention on *patient symptom severity distributions* in hospitals.\\n    *   Analyzing the effect of environmental regulations on *air quality distributions* across cities.\\n*   **Real-world Applicability:** If the paper presents empirical results using synthetic or real-world datasets where units\\' characteristics are genuinely distributional, and TWP demonstrates superior performance (e.g., better pre-treatment fit in Wasserstein space, more plausible counterfactuals) compared to attempts to simplify these distributions to scalars for standard SCM, then its practical relevance is high.\\n*   **Usability and Tooling:** For TWP to be widely adopted, it needs to be packaged into user-friendly software or libraries, similar to `Synth` in R or `synth_runner` in Python. The complexity of Wasserstein geometry might make this more challenging.\\n\\n### 4. Strength of Evidence Supporting Causal Claims\\n\\nThe strength of evidence for causal claims derived using TWP in an SCM context depends on several factors:\\n\\n*   **Quality of Pre-Intervention Fit:** The first pillar of SCM\\'s credibility is an excellent pre-intervention match. If TWP can demonstrate a significantly better match in the relevant distributional sense than standard methods (or methods that simply match moments of distributions), this strengthens the evidence. Visualizations showing the alignment of pre-treatment *distributions* (not just means) would be crucial.\\n*   **Placebo/Permutation Tests:** For a single treated unit study, the gold standard for inference in SCM is permutation tests or placebo tests. These involve applying the SCM procedure to untreated control units (placebos) to see if similar \"treatment effects\" are found by chance. A compelling result shows the treated unit\\'s effect as an outlier in the distribution of placebo effects. The paper should ideally incorporate these to demonstrate statistical significance and rule out chance findings.\\n*   **Sensitivity Analyses:** Robustness checks, such as excluding certain control units, varying the pre-treatment period, or including different sets of covariates, are essential for any SCM application. TWP\\'s results should be shown to be robust to these variations.\\n*   **Theoretical Guarantees:** Does the TWP method offer any theoretical guarantees or bounds on the estimation error, particularly when dealing with distributional data? Such guarantees would bolster confidence in the causal estimates.\\n*   **Comparison to Alternatives:** For the types of problems TWP is designed for, how does it compare to other methods that might try to handle distributional data (e.g., matching on kernel density estimates, or using functional data analysis in SCM)? Demonstrating superior performance against relevant alternatives strengthens the claim.\\n\\nIn conclusion, Tangential Wasserstein Projections offers a theoretically appealing extension to Synthetic Control Methods, particularly for causal inference problems where the characteristics and outcomes of units are best represented as *distributions*. Its appropriateness stems from its ability to leverage Wasserstein geometry for more sophisticated matching. The validity of causal assumptions remains largely consistent with standard SCM, though TWP allows for a more rigorous fulfillment of the \\'no unmeasured confounding\\' assumption by enabling richer observed matching. The practical relevance is high for specific, complex datasets. The strength of causal evidence will depend heavily on the empirical implementation, particularly the quality of pre-treatment fit of *distributions*, rigorous inference procedures (like placebo tests), and robust sensitivity analyses. If applied judiciously to problems where its unique capabilities are truly needed, TWP could represent a significant methodological advance in causal inference.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.8991474}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The paper \"Tangential Wasserstein Projections\" introduces a novel framework for dimensionality reduction and manifold learning in the space of probability measures, $\\\\mathcal{P}_2(\\\\mathbb{R}^D)$, equipped with the 2-Wasserstein metric. The core idea is to project measures onto a lower-dimensional tangent space defined at a specific point (a Fr\u00e9chet mean) on the manifold. The statistical consistency results, particularly Proposition 3.1 and Corollary 3.1, are crucial for establishing the theoretical validity of this empirical approach.\\n\\n### Summary of Key Consistency Results\\n\\nThe paper aims to estimate a low-dimensional manifold $\\\\mathcal{M}$ embedded in $\\\\mathcal{P}_2(\\\\mathbb{R}^D)$. When data are given as i.i.d. samples $X_1, \\\\ldots, X_n$ from a measure $\\\\nu$ residing on $\\\\mathcal{M}$, the goal is to estimate the tangent space $T_x$ at the true Fr\u00e9chet mean $x = \\\\mathbb{E}_{\\\\nu}[X]$ and to project empirical measures onto this estimated space.\\n\\n1.  **Proposition 3.1: Consistency of the Empirical Tangent Space.**\\n    *   **Statement:** This proposition asserts that the empirical tangent space $\\\\widehat{T}_{\\\\bar{x}_n}$ converges almost surely to the true tangent space $T_x$ as the sample size $n \\\\to \\\\infty$. Here, $\\\\bar{x}_n = \\\\mathbb{E}_{\\\\mu_n}[X]$ is the sample mean of the support points, and $\\\\widehat{T}_{\\\\bar{x}_n}$ is constructed based on the empirical measure $\\\\mu_n = \\\\frac{1}{n}\\\\sum_{i=1}^n \\\\delta_{X_i}$. The convergence is in a metric space of subspaces (specifically, a Fr\u00e9chet mean of subspaces metric defined in the paper\\'s supplementary material).\\n\\n2.  **Corollary 3.1: Consistency of the Empirical Projection.**\\n    *   **Statement:** Building upon Proposition 3.1, this corollary states that the empirical projection $P_{\\\\bar{x}_n}(\\\\mu_n)$ converges almost surely to $x$ (the true Fr\u00e9chet mean on the manifold) as $n \\\\to \\\\infty$. This implies that the projection of the empirical measure onto the *estimated* tangent space converges to the true point on the manifold that the data is sampled around.\\n\\n### Analysis of Assumptions Required for Consistency\\n\\nThe consistency results rely on several significant assumptions, which are common in manifold learning theory but have specific implications in the Wasserstein space setting:\\n\\n1.  **A1: Smoothness of the Manifold $\\\\mathcal{M}$:** The manifold $\\\\mathcal{M}$ is assumed to be a \"closed embedded $d$-dimensional submanifold of $\\\\mathcal{P}_2(\\\\mathbb{R}^D)$.\" This is fundamental. If the manifold is not smooth, the concept of a unique, well-defined tangent space (as a vector space) is problematic. This is a standard assumption in differential geometry and manifold learning.\\n\\n2.  **A2: Smoothness of the Parametrization Map $F$:** The manifold is locally parametrized by a \"continuously differentiable map $F: U \\\\subset \\\\mathbb{R}^d \\\\to \\\\mathcal{P}_2(\\\\mathbb{R}^D)$.\" This ensures that the tangent vectors and the tangent space are well-defined and can be related to derivatives in the parameter space.\\n\\n3.  **A3: Data Distribution on the Manifold:** The underlying probability measure $\\\\nu$ from which the data $X_1, \\\\ldots, X_n$ are drawn is assumed to be a \"Borel probability measure on $\\\\mathcal{M}$.\" This is a strong theoretical assumption. It implies that the true data points *lie perfectly on the manifold* and are not subject to observation noise that would pull them off the manifold. This is a common idealization in theoretical manifold learning, but it is a significant limitation for practical applications where data is inherently noisy and only *approximately* on a manifold.\\n\\n4.  **A4: i.i.d. Sampling:** The data points $X_1, \\\\ldots, X_n$ are assumed to be independent and identically distributed (i.i.d.) draws from $\\\\nu$. This is a standard assumption for almost all statistical consistency results and allows for the application of laws of large numbers and central limit theorems.\\n\\n5.  **A5: Convergence of Empirical Mean:** It is assumed that $\\\\bar{x}_n = \\\\mathbb{E}_{\\\\mu_n}[X] \\\\to x = \\\\mathbb{E}_{\\\\nu}[X]$ almost surely. This is guaranteed by the Law of Large Numbers for i.i.d. samples, provided the first moment exists. The authors also use results on the convergence of empirical measures in Wasserstein space, e.g., $\\\\mu_n \\\\to \\\\nu$ in $W_2$ a.s.\\n\\n6.  **A6: Regularity of the Fr\u00e9chet Mean $x$ (for Tangent Space Definition):** Proposition 3.1 specifically requires that the true Fr\u00e9chet mean $x$ is \"non-atomic and $\\\\nabla \\\\log \\\\rho_x$ is Lipschitz for some representation $x = (\\\\rho_x, \\\\nabla \\\\phi_x)$.\" This is a critical assumption specific to the structure of Wasserstein space and the definition of the tangent space via optimal transport maps.\\n    *   **Non-atomic:** This means $x$ does not place probability mass on individual points. This is usually fine for continuous distributions.\\n    *   **Lipschitz gradient of log-density:** This is a strong regularity condition on the underlying measure $\\\\rho_x$ (the density of $x$ with respect to Lebesgue measure). It ensures that the displacement fields that define the tangent space are sufficiently well-behaved. Many common distributions (e.g., Gaussian, uniform on a compact set) satisfy this, but it may not hold for measures with sharp boundaries, singular components, or very rough densities. This significantly limits the generality of the type of probability measures to which the theory applies.\\n\\n7.  **A7: Uniqueness and Continuity of Projection (Implicit):** For Corollary 3.1, the existence, uniqueness, and continuity of the projection operator $P_x(\\\\cdot)$ are implicitly assumed or derived from the structure of the tangent spaces and the properties of Wasserstein geometry. While the tangent space is a Hilbert space, the projection onto it requires the space to be \"nice enough.\"\\n\\n### Rates of Convergence\\n\\nThe consistency results in Proposition 3.1 and Corollary 3.1 are stated as **almost sure convergence**. This is a strong form of convergence, meaning that the estimated quantities converge to their true counterparts for almost all possible sequences of random samples.\\n\\nHowever, the paper **does not provide explicit rates of convergence** (e.g., $O_p(n^{-1/2})$ or $O(n^{-1})$). This is a limitation. Knowing the rate of convergence is crucial for understanding:\\n*   **Efficiency:** How quickly the estimator approaches the true value as $n$ increases.\\n*   **Finite Sample Performance:** How large $n$ needs to be to achieve a certain level of accuracy in practice.\\n*   **Comparison:** Without rates, it\\'s difficult to compare the statistical efficiency of TWP with other manifold learning methods, or even alternative ways of estimating the tangent space in Wasserstein space.\\n\\nThe absence of rates is common in initial theoretical developments for complex, non-Euclidean spaces like Wasserstein space, as deriving such bounds can be significantly more challenging than in Euclidean settings.\\n\\n### Generality of Proposition 3.1 and Corollary 3.1\\n\\nThe generality of these results can be assessed along several dimensions:\\n\\n1.  **Manifold Type:** The results apply to any \"closed embedded $d$-dimensional submanifold\" of $\\\\mathcal{P}_2(\\\\mathbb{R}^D)$. This is quite general within the paradigm of smooth manifold learning. It doesn\\'t restrict to specific types of manifolds (e.g., geodesics, or specific statistical models).\\n\\n2.  **Space of Measures:** The results are specific to the 2-Wasserstein space $\\\\mathcal{P}_2(\\\\mathbb{R}^D)$. They do not generalize to other metric spaces, or even other optimal transport metrics (like $W_p$ for $p \\\\neq 2$), or other types of probability metric spaces (e.g., total variation, KL divergence). The Hilbertian structure of the tangent space in $W_2$ is key to the definitions and derivations.\\n\\n3.  **Type of Measures (A6 Critical):** This is where generality is most constrained. The assumption of \"non-atomic and $\\\\nabla \\\\log \\\\rho_x$ is Lipschitz\" for the true Fr\u00e9chet mean $x$ is highly restrictive.\\n    *   **Atomic Measures:** It generally excludes discrete probability measures or measures with atomic components, which are common in real-world applications (e.g., count data, categorical data represented as point masses).\\n    *   **Lack of Smoothness:** It also excludes measures whose densities are not sufficiently smooth. For instance, a uniform distribution on a complicated domain might not have a Lipschitz log-gradient at the boundary. This limits the applicability to \"well-behaved\" continuous measures.\\n\\n4.  **Nature of Data (A3 Critical):** The assumption that data points are drawn *from the manifold* itself is a strong theoretical idealization. In reality, data is usually observed with noise, meaning it lies *near* the manifold, not *on* it. The consistency results do not directly address the robustness of TWP to such noise or the consistency of estimating a manifold from noisy off-manifold data. This is a common theoretical gap in manifold learning that subsequent research often aims to fill (e.g., by considering principal curves/surfaces that approximate the data rather than assuming it\\'s on a true manifold).\\n\\n### Practical Implications of the Theoretical Findings\\n\\n1.  **Theoretical Justification:** The consistency results provide a strong theoretical foundation for Tangential Wasserstein Projections. They indicate that, under ideal conditions, the empirical methods proposed will indeed converge to their true, underlying geometric counterparts. This is essential for building confidence in the method.\\n\\n2.  **Feasibility of Estimation:** They demonstrate that estimating the local geometry (tangent space) and performing projections in Wasserstein space using finite empirical samples is statistically feasible, at least asymptotically.\\n\\n3.  **Guidance for Algorithm Design:** The proofs hint at the necessary properties for the numerical algorithms (e.g., accurate estimation of Fr\u00e9chet means, robust computation of optimal transport maps).\\n\\n4.  **Limitations in Real-World Applications:**\\n    *   **Noise Robustness:** The most significant practical limitation is the assumption that data lies perfectly on the manifold. Real-world data is noisy. The current theory doesn\\'t explain how TWP performs when data is off-manifold or how to adapt it for robust estimation in the presence of noise.\\n    *   **Verifying Assumptions:** It\\'s often impractical or impossible to verify the \"Lipschitz gradient of log-density\" assumption for real empirical measures.\\n    *   **Computational Cost:** While consistency is proven, the actual computation of empirical Fr\u00e9chet means and optimal transport maps is notoriously expensive for high-dimensional measures and large sample sizes. The theoretical consistency doesn\\'t guarantee computational tractability for practical use cases.\\n    *   **Finite Sample Performance:** Without explicit rates of convergence, practitioners have little guidance on how large a sample size $n$ needs to be to achieve a reasonable approximation of the true tangent space or projection.\\n    *   **Model Misspecification:** If the true data structure is not a smooth manifold in Wasserstein space (e.g., a union of manifolds, or simply a cloud of points without a discernible manifold structure), the theoretical guarantees do not apply.\\n\\nIn conclusion, \\'Tangential Wasserstein Projections\\' provides significant theoretical groundwork for manifold learning in Wasserstein space, demonstrating the consistency of its core components under specific assumptions. The almost sure convergence is a strong theoretical result. However, the strong assumptions regarding data lying perfectly on the manifold and the regularity of measures, coupled with the absence of explicit convergence rates, suggest that further research is needed to bridge the gap between this elegant theory and its broader applicability to noisy, real-world data and computationally efficient implementations.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.89813025}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I have received the evaluations from the three reviewer agents. Now, I will synthesize their assessments into a comprehensive review and provide a final recommendation.\n\n**Synthesis of Reviewer Assessments:**\n\n*   **WassersteinExpert:** The agent confirms the theoretical soundness and mathematical rigor of the paper, highlighting the novelty of the approach in leveraging the tangent space of the Wasserstein space. The proofs for Propositions 2.1, 2.2, and 3.1 are deemed correct, relying on the Hilbert space structure of the tangent space.\n*   **CausalInferenceReviewer:** The agent acknowledges the potential of the method for causal inference, particularly in handling distributional data within the synthetic control framework. However, it raises concerns about computational complexity, interpretability, and the need for strong pre-intervention fit. The agent emphasizes that the method's practical relevance lies in applications where the data is inherently distributional.\n*   **StatisticalConsistencyAnalyst:** The agent provides a detailed analysis of the statistical consistency results, outlining the assumptions required for consistency and highlighting the limitations in terms of generality and the absence of explicit convergence rates. The agent points out the restrictive assumptions on the data and the measures, suggesting that further research is needed to bridge the gap between theory and real-world applications.\n\n**Overall Assessment:**\n\nThe paper \"Tangential Wasserstein Projections\" presents a novel and theoretically sound approach to dimensionality reduction in the Wasserstein space. The method leverages the tangent space structure to enable the application of linear techniques, offering potential advantages over existing methods. The application to causal inference via synthetic controls is promising, particularly for handling distributional data. However, the method's practical applicability is limited by computational complexity, strong assumptions, and the absence of explicit convergence rates.\n\n**Recommendation:**\n\nWhile the paper presents a significant theoretical contribution, the limitations in terms of practical applicability and the need for further research warrant a **Reject** recommendation. The authors should address the concerns raised by the reviewers, particularly regarding computational complexity, the restrictive assumptions, and the lack of explicit convergence rates, before the paper can be considered for acceptance.\n\n**FINAL DECISION: Reject**\n"}], "ground_truth": "Reject", "response_history": "I have received the evaluations from the three reviewer agents. Now, I will synthesize their assessments into a comprehensive review and provide a final recommendation.\n\n**Synthesis of Reviewer Assessments:**\n\n*   **WassersteinExpert:** The agent confirms the theoretical soundness and mathematical rigor of the paper, highlighting the novelty of the approach in leveraging the tangent space of the Wasserstein space. The proofs for Propositions 2.1, 2.2, and 3.1 are deemed correct, relying on the Hilbert space structure of the tangent space.\n*   **CausalInferenceReviewer:** The agent acknowledges the potential of the method for causal inference, particularly in handling distributional data within the synthetic control framework. However, it raises concerns about computational complexity, interpretability, and the need for strong pre-intervention fit. The agent emphasizes that the method's practical relevance lies in applications where the data is inherently distributional.\n*   **StatisticalConsistencyAnalyst:** The agent provides a detailed analysis of the statistical consistency results, outlining the assumptions required for consistency and highlighting the limitations in terms of generality and the absence of explicit convergence rates. The agent points out the restrictive assumptions on the data and the measures, suggesting that further research is needed to bridge the gap between theory and real-world applications.\n\n**Overall Assessment:**\n\nThe paper \"Tangential Wasserstein Projections\" presents a novel and theoretically sound approach to dimensionality reduction in the Wasserstein space. The method leverages the tangent space structure to enable the application of linear techniques, offering potential advantages over existing methods. The application to causal inference via synthetic controls is promising, particularly for handling distributional data. However, the method's practical applicability is limited by computational complexity, strong assumptions, and the absence of explicit convergence rates.\n\n**Recommendation:**\n\nWhile the paper presents a significant theoretical contribution, the limitations in terms of practical applicability and the need for further research warrant a **Reject** recommendation. The authors should address the concerns raised by the reviewers, particularly regarding computational complexity, the restrictive assumptions, and the lack of explicit convergence rates, before the paper can be considered for acceptance.\n\n**FINAL DECISION: Reject**\n", "elapsed_time": 158.4258680343628, "reviewer_agents": {"WassersteinExpert": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in Wasserstein spaces and optimal transport theory.", "system_prompt": "You are an expert in Wasserstein spaces and optimal transport theory. You will rigorously evaluate the theoretical soundness and mathematical rigor of the paper, focusing on the correctness of proofs, the validity of assumptions, and the novelty of the theoretical contributions.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "CausalInferenceReviewer": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in causal inference and synthetic control methods.", "system_prompt": "You are an expert in causal inference and synthetic control methods. You will evaluate the application of the proposed method to causal inference, focusing on the appropriateness of the synthetic control estimator, the validity of the causal assumptions, and the practical relevance of the empirical results.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "StatisticalConsistencyAnalyst": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in statistical consistency and asymptotic properties of estimators.", "system_prompt": "You are an expert in statistical consistency and asymptotic properties of estimators. You will rigorously evaluate the statistical consistency results, focusing on the assumptions required for consistency, the rates of convergence, and the practical implications of the theoretical findings.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}}}
