{"paper_id": "k9CF4h3muD", "prompt": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets\n\n1 INTRODUCTION: Neural Networks (NNs) are often overparameterized, in the sense that their representational capacity far exceeds what is necessary for fitting training data. Surprisingly, training overparameterized NNs via (variants of) Gradient Descent (GD) tends to produce solutions that generalize well, despite existence of many solutions that do not. This implicit generalization phenomenon attracted considerable scientific interest, resulting in various theoretical explanations (see, e.g., Woodworth et al. (2020); Yun et al. (2020); Zhang et al. (2017); Li et al. (2020); Ji & Telgarsky (2018); Lyu & Li (2019)). Recent studies have surfaced a new form of implicit bias which arises in Recurrent Neural Networks (RNNs) and their variants (e.g., Long Short-Term Memory Hochreiter & Schmidhuber (1997) and Gated Recurrent Units Chung et al. (2014)). For such models, the length of sequences in training is often shorter than in testing, and it is not clear to what extent a learned solution will be able to extrapolate beyond the sequence lengths seen in training. In the overparameterized regime, where the representational capacity of the learned model exceeds what is necessary for fitting short sequences, there may exist solutions which generalize but do not extrapolate, meaning that their accuracy is high over short sequences but arbitrarily poor over long ones (see Cohen-Karlik et al. (2022)). In practice however, when training RNNs using GD, accurate extrapolation is often observed. We refer to this phenomenon as the implicit extrapolation of GD. As opposed to the implicit generalization of GD, little is formally known about its implicit extrapolation. Existing theoretical analyses of the latter focus on linear RNNs \u2014 also known as Linear Dynamical Systems (LDS) \u2014 and either treat infinitely wide models (Emami et al., 2021), or models of finite width that learn from a memoryless teacher (Cohen-Karlik et al., 2022). In these regimes, GD has been argued to exhibit an implicit bias towards short-term memory. While such results are informative, their generality remains in question, particularly since infinitely wide NNs are known to substantially differ from their finite-width counterparts, and since a memoryless teacher essentially neglects the main characteristic of RNNs (memory). In this paper, we theoretically investigate the implicit extrapolation of GD when applied to overparameterized finite-width linear RNNs learning from a teacher with memory. We consider models with symmetric transition matrices, in the case where a student (learned model) with state space dimension d is trained on sequences of length k generated by a teacher with state space dimension d\u0302. Our interest lies in the overparameterized regime, where d is greater than both k and d\u0302, meaning that the student has state space dimension large enough to fully agree with the teacher on sequences of length k, while potentially disagreeing with it on longer sequences. As a necessary assumption on initialization, we follow prior work and focus on a certain balancedness condition, which is known (see experiments in Cohen-Karlik et al. (2022), as well as our theoretical analysis) to capture near-zero initialization as commonly employed in practice. Our main theoretical result states that GD originating from a balanced initialization leads the student to extrapolate, irrespective of how large its state space dimension is. Key to the result is a surprising connection to a moment matching theorem from Cohen & Yeredor (2011), whose proof relies on ideas from compressed sensing (Elad, 2010; Eldar & Kutyniok, 2012) and neighborly polytopes (Gale, 1963). This connection may be of independent interest, and in particular may prove useful in deriving other results concerning implicit properties of GD. We corroborate our theory with experiments, which demonstrate extrapolation via learning low dimensional state spaces in both the analyzed setting and ones involving non-linear RNNs. The implicit extrapolation of GD is a new and exciting area of inquiry. Our results suggest that short-term memory is not enough for explaining it as previously believed. We hope the techniques developed in this paper will contribute to a further understanding of this phenomenon. 2 RELATED WORK: The study of linear RNNs, or LDS, has a rich history dating back to at least the early works of Kalman (Kalman, 1960; 1963). An extensively studied question relevant to extrapolation is that of system identification, which explores when the parameters of a teacher LDS can be recovered (see Ljung (1999)). Another related topic concerns finding compact realizations of systems, i.e. realizations of the same input-output mapping as a given LDS, with a state space dimension that is lower (see Antoulas (2005)). Despite the relation, our focus is fundamentally different from the above \u2014 we ask what happens when one learns an LDS using GD. Since GD is not explicitly designed to find a low dimensional state space, it is not clear that the application of GD to an overparameterized student allows system identification through a compact realization. The fact that it does relates to the implicit properties of GD, and to our knowledge has not been investigated in the classic LDS literature. The implicit generalization of GD in training RNNs has been a subject of theoretical study for at least several years (see, e.g., Hardt et al. (2016); Allen-Zhu et al. (2019); Lim et al. (2021)). In contrast, works analyzing the implicit extrapolation of GD have surfaced only recently, specifically in Emami et al. (2021) and Cohen-Karlik et al. (2022).1 Emami et al. (2021) analyzes linear RNNs in the infinite width regime, suggesting that in this case GD is implicitly biased towards impulse responses corresponding to short-term memory. Cohen-Karlik et al. (2022) studies finite-width linear RNNs (as we do), showing that when the teacher is memoryless (has state space dimension zero), GD emanating from a balanced initialization successfully extrapolates. Our work tackles an arguably more realistic and challenging setting \u2014 we analyze the regime in which the teacher has memory. Our results suggest that the implicit extrapolation of GD does not originate from a bias towards short-term memory, but rather a tendency to learn low dimensional state spaces. On the empirical side, extrapolation of NNs to sequence lengths beyond those seen in training has been experimentally demonstrated in numerous recent works, covering both modern language 1We note that there have been works studying extrapolation in the context of non-recurrent NNs, e.g. Xu et al. (2020). This type of extrapolation deals with the behavior of learned functions outside the support of the training distribution, and is therefore fundamentally different from the type of extrapolation we consider (which deals with the behavior of learned functions over sequences longer than those seen in training). and attention models (Press et al., 2022; Anil et al., 2022; Zhang et al., 2022), and RNNs with transition matrices of particular forms (Gu et al., 2022; 2021; 2020; Gupta, 2022). The current paper is motivated by these findings, and takes a step towards theoretically explaining them. 3 LINEAR RECURRENT NEURAL NETWORKS: Our theoretical analysis applies to single-input single-output (SISO) linear RNNs with symmetric transition matrices. Given a state space dimension d P N, this model is defined by the update rule: st`1 \u201c Ast ` Bxt, yt \u201c Cst, t \u201c 0, 1, 2, . . . , (3.1) where A P Rd\u02c6d, B P Rd\u02c61 and C P R1\u02c6d are configurable parameters, with the transition matrix A satisfying A \u201c AJ; x0, x1, . . . P R form an input sequence; y0, y1, . . . P R form the corresponding output sequence; and st P Rd\u02c61 represents the internal state at time t, assumed to be equal to zero at the outset (i.e. it is assumed that s0 \u201c 0). As with any linear time-invariant system (Porat, 1996), the input-output mapping realized by the RNN is determined by its impulse response. Definition 1. The impulse response of the RNN is the output sequence corresponding to the input sequence px0, x1, x2, . . .q \u201c p1, 0, 0, . . .q. Namely, it is the sequence pCB,CAB,CA2B, . . . q. For brevity, we employ the shorthand \u0398 :\u201c pA,B,Cq. The d \u02c6 d symmetric transition matrix A is parameterized through a dpd ` 1q{2-dimensional vector holding its upper triangular elements, and with a slight overloading of notation, the symbol A is also used to refer to this parameterization. We note that our theory readily extends to multiple-input multiple-output (MIMO) networks, and the focus on the SISO case is merely for simplicity of presentation. Note also that the restriction to symmetric transition matrices is customary in both theory (Hazan et al., 2018) and practice (Gupta, 2022), and represents a generalization of the canonical modal form, which under mild non-degeneracy conditions does not limit generality (Boyd & Lessard, 2006). Given a length k input sequence, x \u201c px0, . . . , xk\u00b41q P Rk, consider the output at the last time step, i.e. y :\u201c yk P R, and denote it by RNNpxq. Using this output as a label, we define an empirical loss induced by a training set S \u201c \u2423` xp1q, yp1q \u02d8 , . . . , ` xpNq, ypNq \u02d8( \u0102 Rk \u02c6 R: LSpA,B,Cq \u201c 1 N N \u00ff i\u201c1 \u2113 \u00b4 RNN \u00b4 xpiq \u00af , ypiq \u00af , (3.2) where \u2113py, y\u0302q \u201c py \u00b4 y\u0302q2 is the square loss. By the update rule of the RNN (Equation 3.1), we have: LSpA,B,Cq \u201c 1 N N \u00ff i\u201c1 \u02dc k\u00b41 \u00ff j\u201c0 CAk\u00b41\u00b4jBx piq j \u00b4 y piq \u00b82 . (3.3) Suppose that ground truth labels are generated by an RNN as defined in Equation 3.1, and denote the state space dimension and parameters of this teacher network by d\u0302 and \u0398\u0302 \u201c pA\u0302, B\u0302, C\u0302q respectively. We employ the common assumption (e.g., see Hardt et al. (2016)) by which input sequences are drawn from a whitened distribution, i.e. a distribution where E rxjxj1 s equals 1 if j \u201c j1 and 0 otherwise. The population loss over length k sequences can then be written as (see Lemma C.1): LpA,B,Cq \u201c k\u00b41 \u00ff j\u201c0 \u00b4 CAjB \u00b4 C\u0302A\u0302jB\u0302 \u00af2 . (3.4) Equation 3.4 implies that a solution \u0398 \u201c pA,B,Cq achieves zero population loss over length k sequences if and only if CAjB \u201c C\u0302A\u0302jB\u0302 for j \u201c 0, . . . , k \u00b4 1. To what extent does such a solution imply that the student (i.e., the learned RNN) extrapolates to longer sequences? This depends on how close CAjB is to C\u0302A\u0302jB\u0302 for j \u011b k. Definition 2. For \u03f5 \u011b 0 and q P N, we say that the student \u03f5-extrapolates with horizon q with respect to (w.r.t) the teacher if: |CAjB \u00b4 C\u0302A\u0302jB\u0302| \u010f \u03f5, @j P t0, 1, . . . , q \u00b4 1u . (3.5) If the above holds for all q P N then the student is said to \u03f5-extrapolate w.r.t the teacher, and if it: holds for all q P N with \u03f5 \u201c 0 then the student is simply said to extrapolate w.r.t the teacher. Per Definition 2, \u03f5-extrapolation with horizon q is equivalent to the first q elements of the student\u2019s impulse response being \u03f5-close to those of the teacher\u2019s, whereas extrapolation means that the student\u2019s impulse response fully coincides with the teacher\u2019s. The latter condition implies that the student realizes the same input-output mapping as the teacher, for any sequence length (this corresponds to the notion of system identification; see Section 2). When the student is overparameterized, in the sense that d is greater than k and d\u0302, it may perfectly generalize, i.e. lead the population loss over length k sequences (Equation 3.4) to equal zero, and yet fail to extrapolate as stated in the following proposition. Proposition 3. Assume d \u0105 k, and let \u03f5 \u011b 0 and q P tk ` 1, k ` 2, . . .u. Then, for any teacher parameters \u0398\u0302, there exist student parameters \u0398 with which the population loss in Equation 3.4 equals zero, and yet the student does not \u03f5-extrapolate with horizon q. Proof sketch (for complete proof see Appendix C.1.2). The result follows from the fact that the first d elements of the student\u2019s impulse response can be assigned freely via a proper choice of \u0398. We are interested in the extent to which student parameters learned by GD extrapolate in the overparameterized regime. Proposition 3 implies that, regardless of how many (length k) sequences are used in training, if GD leads to any form of extrapolation, it must be a result of some implicit bias induced by the algorithm. Note that in our setting, extrapolation cannot be explained via classic tools from statistical learning theory, as evaluation over sequences longer than those seen in training violates the standard assumption of train and test data originating from the same distribution. To decouple the question of extrapolation from that of generalization, we consider the case where the training set S is large, or more formally, where the empirical loss LSp\u00a8q (Equation 3.3) is well represented by the population loss Lp\u00a8q (Equation 3.4). We model GD with small step size via Gradient Flow (GF), as customary in the theory of NNs \u2014 see Saxe et al. (2013); Gunasekar et al. (2017); Arora et al. (2018; 2019b); Lyu & Li (2019); Li et al. (2020); Azulay et al. (2021) for examples and Elkabetz & Cohen (2021) for theoretical justification. That is, we analyze the following dynamics: 9\u03b1p\u03c4q :\u201c d d\u03c4 \u03b1p\u03c4q \u201c \u00b4 BB\u03b1L ` Ap\u03c4q,Bp\u03c4q,Cp\u03c4q \u02d8 , \u03c4 \u011b 0 , (3.6) where \u03b1 P tA,B,Cu. If no assumption on initialization is made, no form of extrapolation can be established (indeed, the initial point may be a global minimizer of Lp\u00a8q that fails to extrapolate, and GF will stay there). Following prior work (see Cohen-Karlik et al. (2022)), we assume that initialization adheres to the following balancedness condition: Definition 4. An RNN with parameters \u0398 \u201c pA,B,Cq is said to be balanced if B \u201c CJ. It was shown empirically in Cohen-Karlik et al. (2022) that the balancedness condition captures near-zero initialization as commonly employed in practice. We support this finding theoretically in Section 4.3. Aside from the initialization of the student, we will also assume that the teacher adheres to the balancedness condition. 4 THEORETICAL ANALYSIS: We turn to our theoretical analysis. Section 4.1 proves that in the setting of Section 3, convergence of GF to a zero loss solution leads the student to extrapolate, irrespective of how large its state space dimension is. Section 4.2 extends this result by establishing that, under mild conditions, approximate convergence leads to approximate extrapolation. The results of sections 4.1 and 4.2 assume that GF emanates from a balanced initialization, which empirically is known to capture near-zero initialization as commonly employed in practice (see Section 3). Section 4.3 theoretically supports this empirical premise, by showing that with high probability, random near-zero initialization leads to balancedness. We introduce notations that will be used throughout the analysis. For a matrix Q P Rm\u02c6n, we let }Q}F , }Q}8 and }Q}2 denote the Frobenius, \u21138 and \u21132 (spectral) norms, respectively. For a vector v P Rm, we use }v} to denote the Euclidean norm and vi to denote its ith entry. 4.1 CONVERGENCE LEADS TO EXTRAPOLATION: Theoretical analyses of implicit generalization often assume convergence to a solution attaining zero loss (see, e.g., Azulay et al. (2021); Gunasekar et al. (2017); Lyu & Li (2019); Woodworth et al. (2020)). Under such an assumption, Theorem 5 below establishes implicit extrapolation, i.e. that the solution to which GF converges extrapolates, irrespective of how large the student\u2019s state space dimension d is. A condition posed by the theorem is that the training sequence length k is greater than two times the teacher\u2019s state space dimension d\u0302. We empirically demonstrate the necessity of this condition in Section 5, by showing that smaller values for k lead to failure in extrapolation. Theorem 5. Assume that d \u0105 k \u0105 2d\u0302, the teacher parameters \u0398\u0302 are balanced (Definition 4), and the student parameters \u0398 are learned by applying GF to the loss Lp\u00a8q (Equation 3.4) starting from a balanced initialization. Then, if GF converges to a point \u0398\u02da satisfying Lp\u0398\u02daq \u201c 0, this point extrapolates (Definition 2). In order to prove Theorem 5, we introduce two lemmas: Lemma 6, which shows that balancedness is preserved under GF; and Lemma 7, which (through a surprising connection to a moment problem from statistics) establishes that a balanced solution attaining zero loss necessarily extrapolates. With Lemmas 6 and 7 in place, the proof of Theorem 5 readily follows. Lemma 6. Let \u0398p\u03c4q, with \u03c4 \u011b 0, be a curve brought forth by applying GF to the loss Lp\u00a8q starting from a balanced initialization. Then, \u0398p\u03c4q is balanced for every \u03c4 \u011b 0. Proof sketch (for complete proof see Appendix C.1.5). The result follows from the symmetric role of B and C in the loss Lp\u00a8q. Lemma 7. Suppose that d \u0105 k \u0105 2d\u0302, the teacher is balanced, and that the student parameters \u0398 are balanced and satisfy Lp\u0398q \u201c 0. Then \u0398 extrapolates. Proof sketch (for complete proof see Appendix C.2). The proof is based on a surprising connection to the moment problem from statistics (recovery of a probability distribution from its moments), which has been studied for decades (see, e.g., Schm\u00fcdgen (2017)). Without loss of generality, we may assume that A\u0302 is diagonal (if this is not the case then we apply an orthogonal eigendecomposition to A\u0302 and subsequently absorb eigenvectors into B\u0302 and C\u0302). We may also assume that C\u0302B\u0302 \u201c 1 (otherwise we absorb a scaling factor into B and/or C). Since C\u0302J \u201c B\u0302 (teacher is balanced), we may define a probability vector (i.e. a vector with non-negative entries summing up to one) p\u0302 P Rd\u0302 via p\u0302i \u201c C\u0302iB\u0302i, i \u201c 1, . . . , d\u0302. We let Z\u0302 denote the random variable whose support is tA\u030211, . . . , A\u0302d\u0302d\u0302u and which assumes the value A\u0302ii with probability p\u0302i, i \u201c 1, . . . , d\u0302. Notice that for every j P N: C\u0302A\u0302jB\u0302 \u201c \u00ffd\u0302 i\u201c1 p\u0302iA\u0302 j ii \u201c ErZ\u0302 js , meaning that the elements of the teacher\u2019s impulse response are precisely the moments of Z\u0302. Similarly to above we may assume A is diagonal, and since Lp\u0398q \u201c 0 it holds that CB \u201c C\u0302B\u0302 \u201c 1. We may thus define a probability vector p P Rd via pi \u201c CiBi, i \u201c 1, . . . , d, and a random variable Z which assumes the value Aii with probability pi, i \u201c 1, . . . , d. For every j P N: CAjB \u201c \u00ffd i\u201c1 piA j ii \u201c ErZ js , and so the elements of the student\u2019s impulse response are precisely the moments of Z. The probabilistic formulation we set forth admits an interpretation of extrapolation as a moment problem. Namely, since Lp\u0398q \u201c 0 (i.e. CAjB \u201c C\u0302A\u0302jB\u0302 for j \u201c 0, . . . , k\u00b41) the random variables Z and Z\u0302 agree on their first k \u00b4 1 moments, and the question is whether they agree on all higher moments as well. We note that this question is somewhat more challenging than that tackled in classic instances of the moment problem, since the support of the random variable whose moments we match (Z) is not known to coincide with the support of the random variable we seek to recover (Z\u0302). Luckily, a recent powerful result allow addressing the question we face \u2014 Cohen & Yeredor (2011) showed that the first 2n moments of a discrete random variable X taking at most n P N values uniquely define X , in the sense that any discrete random variable agreeing with these 2n moments must be identical to X . Translating this result to our setting, we have that if Z agrees with Z\u0302 on its first 2d\u0302 moments, it must be identical to Z\u0302, and in particular it must agree with Z\u0302 on all higher moments as well. The fact that k \u00b4 1 \u011b 2d\u0302 then concludes the proof. To attain some intuition for the result we imported from Cohen & Yeredor (2011), consider the simple case where d\u0302 \u201c 1. The transition matrix A\u0302 is then a scalar a\u0302 P R, the random variable Z\u0302 is deterministically equal to a\u0302, and the teacher\u2019s impulse response is given by the moments ErZ\u0302js \u201c a\u0302j , j \u201c 0, 1, . . .. Since by assumption k \u0105 2d\u0302, the fact that Lp\u0398q \u201c 0 means the random variable corresponding to the student, Z, agrees with the first two moments of Z\u0302. That is, Z satisfies ErZs \u201c a\u0302 and ErZ2s \u201c a\u03022. This implies that VarrZs \u201c ErZ2s \u00b4 ErZs2 \u201c 0, and therefore Z is deterministically equal to a\u0302, i.e. it is identical to Z\u0302. The two random variables thus agree on all of their moments, meaning the impulse responses of the student and teacher are the same. Proof of Theorem 5. By Lemma 6 (as well as continuity considerations) \u0398\u02da is balanced. Lemma 7 then implies that it extrapolates. 4.2 APPROXIMATE CONVERGENCE LEADS TO APPROXIMATE EXTRAPOLATION: Theorem 5 in Section 4.1 proves extrapolation in the case where GF converges to a zero loss solution. Theorem 8 below extends this result by establishing that, as long as the teacher is stable, approximate convergence leads to approximate extrapolation. Theorem 8. Assume the conditions of Theorem 5, and that the teacher parameters \u0398\u0302 are stable, i.e. the singular values of A\u0302 are in r\u00b41, 1s. Assume also that \u0398\u0302 are non-degenerate, in the sense that the input-output mapping they realize is not identically zero. Then, for any \u03f5 \u0105 0 and q P N, there exists \u03b4p\u03f5, qq \u0105 0 which admits the following: whenever GF leads the student parameters \u0398 to within (Frobenius) distance \u03b4p\u03f5, qq from a balanced point \u0398\u02da satisfying Lp\u0398\u02daq \u201c 0, the student \u03f5-extrapolates with horizon q. Proof sketch (for complete proof see Appendix C.3). In order to show \u03f5-extrapolation with horizon q we need to show that @\u03f5 \u0105 0, q P N, D\u03b4p\u03f5, qq \u0105 0 such that for any \u03981 satisfying ||\u03981 \u00b4 \u0398\u02da|| \u0103 \u03b4, we have for n \u201c 0, . . . , q that C 1pA1qnB1 \u00b4 C\u0302A\u0302nB\u0302 \u201c Op\u03f5q. (4.1) First, by Theorem 5 \u0398\u02da is an extrapolating solution. Following the interpretation of discrete random variables of Lemma 7 proof, we can bound the Wasserstein-1 distance between the extrapolating distribution defined by \u0398\u02da and the approximating distribution defined by \u03981. From Proposition 2 in Wu & Yang (2020), we have that if |C 1pA1qnB1 \u00b4 C\u02dapA\u02daqnB\u02da| \u010f \u03b4 for n P t0, . . . , 2d\u0302u, then W1p\u03981,\u0398\u02daq \u201c O \u00b4 \u03b4 1 4d\u0302 \u00af . (4.2) The second part of the proof ties the error at step p with the Wp distance (Biswas & Mackey, 2021): C 1pA1qpB1 \u00b4 C\u02dapA\u02daqpB\u02da \u010f Wp. (4.3) The final step of the proof is based on Panaretos & Zemel (2019) and bounds Wp in terms of W1, i.e., Wpp\u0398,\u0398\u02daq \u201c Sp\u00b41W1p\u0398,\u0398\u02daq, (4.4) where S is the difference between the maximal and minimal eigenvalues in the teacher and student support (due to our assumption on convergence, S is bounded). Combining the three steps above, we have that if |C 1pA1qnB1 \u00b4 C\u02dapA\u02daqnB\u02da| \u010f \u03b4 for n P t0, . . . , 2d\u0302u, then @p \u0105 n C 1pA1qpB1 \u00b4 C\u02dapA\u02daqpB\u02da \u010f Sp\u00b41O \u00b4 \u03b4 1 4d\u0302 \u00af . (4.5) By setting \u03b4 \u0103 \u00b4 \u03f5 c1\u00a8Sq\u00b41 \u00af4d\u0302 the RHS of Equation 4.5 satisfies c1Sp\u00b41\u03b4 1 4d\u0302 \u0103 \u03f5. 4.3 BALANCEDNESS CAPTURES NEAR-ZERO INITIALIZATION: Theorems 5 and 8 assume that GF emanates from a balanced initialization, i.e. from a point \u0398 \u201c pA,B,Cq satisfying B \u201c CJ. It was shown in Cohen-Karlik et al. (2022) that theoretical predictions derived assuming balanced initialization faithfully match experiments conducted with near-zero initialization (an initialization commonly used in practice). Proposition 9 below theoretically supports this finding, establishing that with high probability, random near-zero initialization leads GF to arrive at an approximately balanced point, i.e. a point \u0398 \u201c pA,B,Cq for which the difference between B and CJ is negligible compared to their size. Proposition 9. Suppose that: (i) d \u0105 4; (ii) the teacher parameters \u0398\u0302 are balanced and are non-degenerate, in the sense that the input-output mapping they realize is not identically zero; and (iii) the student parameters are learned by applying GF to the loss Lp\u00a8q. Let \u0398\u0303 be a random point in parameter space, with entries drawn independently from the standard normal distribution. For \u03f5 \u0105 0, consider the case where GF emanates from the initialization \u03f5\u0398\u0303, and denote the resulting curve by \u0398\u03f5p\u03c4q \u201c pA\u03f5p\u03c4q,B\u03f5p\u03c4q,C\u03f5p\u03c4qq, with \u03c4 \u011b 0. Then, w.p. at least 0.75, for every \u03f5 \u0105 0 there exists \u03c4\u03f5 \u011b 0 such that: lim \u03f5\u00d10` ||B\u03f5p\u03c4\u03f5q \u00b4 CJ\u03f5 p\u03c4\u03f5q||F ||B\u03f5p\u03c4\u03f5q ` CJ\u03f5 p\u03c4\u03f5q||F \u201c 0 . (4.6) Proof sketch (for complete proof see Appendix C.4). The idea behind the proof is as follows. Assume \u03f5 is sufficiently small. Then, when the entries of \u0398 \u201c pA,B,Cq are on the order of \u03f5, we have BBBLp\u0398q \u00ab \u00b42C\u0302B\u0302 \u00a8 C J and BBCLp\u0398q \u00ab \u00b42C\u0302B\u0302 \u00a8 B J. This implies that during the first part of the curve \u0398\u03f5p\u03c4q it holds that dd\u03c4 pB\u03f5p\u03c4q \u00b4 C J \u03f5 p\u03c4qq \u00ab \u00b42C\u0302B\u0302 \u00a8 pB\u03f5p\u03c4q \u00b4 CJ\u03f5 p\u03c4qq and similarly d d\u03c4 pB\u03f5p\u03c4q ` C J \u03f5 p\u03c4qq \u00ab 2C\u0302B\u0302 \u00a8 pB\u03f5p\u03c4q ` CJ\u03f5 p\u03c4qq. Since C\u0302B\u0302 \u0105 0 (follows from the teacher parameters being balanced and non-degenerate), the entries of B\u03f5p\u03c4q \u00b4 CJ\u03f5 p\u03c4q shrink exponentially fast while those of B\u03f5p\u03c4q ` CJ\u03f5 p\u03c4q grow at the same rate. This exponential shrinkage/growth leads }B\u03f5p\u03c4q \u00b4 CJ\u03f5 p\u03c4q} L }B\u03f5p\u03c4q ` CJ\u03f5 p\u03c4q} to become extremely small, more so the smaller \u03f5 is. 5 EXPERIMENTS: In this section we present experiments corroborating our theoretical analysis (Section 4). The latter establishes that, under certain conditions, a linear RNN with state space dimension d extrapolates when learning from a teacher network with state space dimension d\u0302 via training sequences of length k, irrespective of how large d is compared to d\u0302 and k. A key condition underlying the result is that k is larger than 2d\u0302. Section 5.1 below considers the theoretically analyzed setting, and empirically evaluates extrapolation as k varies. Its results demonstrate a phase transition, in the sense that extrapolation takes place when k \u0105 2d\u0302, in compliance with theory, but on the other hand fails when k falls below 2d\u0302, in which case the theory indeed does not guarantee extrapolation. Section 5.2 displays the same phenomenon with linear RNNs that do not adhere to some of the assumptions made by the theory (in particular the assumption of symmetric transition matrices, and those concerning balancedness). Finally, Section 5.3 considers non-linear RNNs (specifically, Gated Recurrent Unit networks Chung et al. (2014)), and shows that they too exhibit a phase transition in extrapolation as the training sequence length varies. For brevity, we defer some of the details of our implementation, as well as additional experiments, to Appendix A. 5.1 THEORETICALLY ANALYZED SETTING: Our first experiment considers the setting described in Section 3 and theoretically analyzed in Section 4. As representative values for the state space dimensions of the teacher and (overparameterized) student, we choose d\u0302 \u201c 5 and d \u201c 40 respectively (higher state space dimensions for the student, namely d \u201c 100 and d \u201c 200, yielded qualitatively identical results). For a given training sequence length k, the student is learned via GD applied directly to the population loss defined in Equation 3.4 (applying GD to the empirical loss defined in Equation 3.3, with N \u201c 10, 000 training examples, led to similar results). Figure 1(a) reports the extrapolation error (quantified by the \u21138 distance between the impulse response of the learned student and that of the teacher) as a function of k. As can be seen, extrapolation exhibits a phase transition that accords with our theory: when k \u0105 2d\u0302 extrapolation error is low, whereas when k falls below 2d\u0302 extrapolation error is high. 5.2 OTHER SETTINGS WITH LINEAR RECURRENT NEURAL NETWORKS: To assess the generality of our findings, we experiment with linear RNNs in settings that do not adhere to some of the assumptions made by our theory. Specifically, we evaluate settings in which: (i) the teacher is unbalanced, meaning B\u0302 \u2030 C\u0302J, and its transition matrix A\u0302 is non-symmetric; (ii) the student\u2019s transition matrix A is not restricted to be symmetric; (iii) learning is implemented by optimizing the empirical loss defined in Equation 3.3 (rather than the population loss defined in Equation 3.4); and (iv) optimization is based on Adam Kingma & Ba (2014) (rather than GD), emanating from standard near-zero initialization which is generally unbalanced (namely, B \u2030 CJ). Figure 1(b) reports the results of an experiment where the state space dimensions of the teacher and (overparameterized) student are d\u0302 \u201c 10 and d \u201c 50 respectively (higher state space dimensions for the student, namely d \u201c 100 and d \u201c 200, yielded qualitatively identical results), and where the teacher implements a delay line of d\u0302 time steps (for details see Appendix B.2.2). Similar results obtained with randomly generated teachers are reported in Appendix A. As can be seen, despite the fact that our theory does not apply to the evaluated settings, its conclusions still hold \u2014 extrapolation error is low when the training sequence length k is greater than 2d\u0302, and high when k falls below 2d\u0302. 5.3 NON-LINEAR RECURRENT NEURAL NETWORKS: As a final experiment, we explore implicit extrapolation with non-linear RNNs, namely GRU networks. Specifically, we evaluate the extent to which a student GRU with state space dimension dg \u201c 100 extrapolates when learning from a teacher GRU with state space dimension d\u0302g \u201c 10 (higher state space dimensions for the student, namely dg \u201c 200 and dg \u201c 500, yielded qualitatively identical results). The student is learned by optimizing an empirical loss comprising training sequences of length kg , where kg is predetermined. Optimization is based on Adam emanating from standard nearzero initialization. Figure 2(a) reports the extrapolation error (quantified by the \u21138 distance between the response of the learned student and that of the teacher, averaged across randomly generated input sequences) for different choices of kg. As can be seen, similarly to the case with linear RNNs (see Sections 5.1 and 5.2), there exists a critical threshold for the training sequence length kg , above which extrapolation error is low and below which extrapolation error is high.2 Figure 2(b) displays the average output response over different inputs of the teacher alongside those of two students \u2014 one trained with sequences of length kg \u201c 30, and the other with sequences of length kg \u201c 60.3 As 2Note that this critical threshold is around four times the teacher\u2019s state space dimension, whereas with linear RNNs the critical threshold was around two times the teacher\u2019s state space dimension. Theoretically explaining this difference is viewed as an interesting direction for future work. 3Note that with GRU networks, in contrast to linear RNNs, the impulse response does not identify the input-output mapping realized by a network. It is presented in Figure 2(b) for demonstrative purposes. expected, the impulse response of each student tracks that of the teacher for the first kg time steps (where kg is student-dependent). However, while the student for which kg \u201c 30 fails to track the teacher beyond kg time steps, the student for which kg \u201c 60 succeeds, thereby exemplifying implicit extrapolation. 6 CONCLUSION: This paper studies the question of extrapolation in RNNs, and more specifically, of whether a student RNN trained on data generated by a teacher RNN can capture the behavior of the teacher over sequences longer than those seen in training. We focus on overparameterized students that can perfectly fit training sequences while producing a wide range of behaviors over longer sequences. Such a student will fail to extrapolate, unless the teacher possesses a certain structure, and the learning algorithm is biased towards solutions adhering to that structure. We show \u2014 theoretically for linear RNNs and empirically for both linear and non-linear RNNs \u2014 that such implicit extrapolation takes place when the teacher has a low dimensional state space and the learning algorithm is GD. Existing studies of implicit extrapolation in (linear) RNNs (Emami et al., 2021; Cohen-Karlik et al., 2022) suggest that GD is biased towards solutions with short-term memory. While low dimensional state space and short-term memory may coincide in some cases, in general they do not, and a solution with low dimensional state space may entail long-term memory. Our theory and experiments show that in settings where low dimensional state space and short-term memory contradict each other, the implicit extrapolation chooses the former over the latter. We note that linear RNNs fundamentally differ from the commonly studied model of linear (feedforward) NNs (see, e.g., Arora et al. (2018); Ji & Telgarsky (2018); Arora et al. (2019a;b); Razin & Cohen (2020)). One of the key differences is that a linear RNN entails different powers of a parameter (transition) matrix, leading to a loss function which roughly corresponds to a sum of losses for multiple linear NNs having different architectures and shared weights. This makes the analysis of linear RNNs technically challenging, warranting introduction of new theoretical approaches such as the moment matching view employed in Section 4.1. An important direction for future work is extending our theory to non-linear RNNs. We believe it is possible, in the same way that theories for linear (feed-forward) NNs were extended to account for non-linear NNs (see, e.g., Razin et al. (2021; 2022); Lyu & Li (2019)). An additional direction to explore is the applicability of our results to the recently introduced S4 model (Gu et al., 2022). A FURTHER EXPERIMENTS: In this section we provide additional experiments that are not included in the main manuscript due to space constraints. A.1 BALANCED TEACHER: In Section 5.1, we have experimented with our proposed theoretical setup. In this section we provide additional figures and experiments. A.1.1 UNBALANCED STUDENT: In this experiment we use the same balanced teacher with d\u0302 \u201c 5 as done in Section 5.1. Instead of the diagonal student with balanced initialization, we use a general (non-diagonal) student with weights sampled from a Gaussian with scale 10\u00b45 and d \u201c 40. Results are depicted in Figure 3(a). A similar phase transition phenomenon to the one in Figure 1 is found also here. A.1.2 EFFECT OF THE INITIALIZATION SCALE: Proposition 9 provides theoretical support for the fact that under near-zero initialization, the learned RNN tends to balancedness, which according to theorems 5 and 8 guarantees extrapolation. Below we empirically explore the impact of varying the initialization scale. We use the same setting as in Section A.1.1, and repeat the experiment with different initialization scales for the students\u2019 weights. As can be seen in Figure 4, the extrapolation deteriorates for larger initialization scale, in the sense that it requires longer training sequences for getting good extrapolation error. This suggests that the condition of small initialization required by our theory is not an artifact of our proof technique, but rather a necessary condition for extrapolation to occur. A.2 UNBALANCED TEACHER: In Section 5.2, we have tested the extrapolation with respect to a specific unbalanced teacher and have observed a similar phase transition as predicted by the theory of Section 4 and empirical evaluation of Section 5.1. Here we show that the phase transition is not limited to the specific teacher discussed by testing with respect to a randomly generated unbalanced (non-diagonal) teacher (see Section B.2.2). The teacher is set to d\u0302 \u201c 5 and student to d \u201c 50. Results are presented in Figure 3 (b). Here too we can observe the phase transition phenomena. A.3 IMPULSE RESPONSE FIGURES In Section 5 we have presented the extrapolation performance in different settings. In order to better convey the meaning of extrapolating vs non-extrapolating solutions we present here figures of the impulse response of different models. We start with the impulse response corresponding to the experiment described in Section 5.1. The figure depicts the balanced teacher with d\u0302 \u201c 5 and two selected students (with d \u201c 40), one trained with k \u201c 10 and the other with k \u201c 20. We can see that the student trained with k \u201c 10 tracks the teacher several steps beyond the 10th time step and then decays to zero. For k \u201c 20 we can see near perfect extrapolation for the horizon evaluated. Next we turn to Section 5.2 and depict the average impulse responses of the \u201cdelay teacher\u201d and the students trained with respect to the mentioned teacher. Since the teacher here has d\u0302 \u201c 10, a model trained with k \u201c 8 is trained with respect to the zero impulse response (see Section B.2.2 for details on delay teacher), and as expected results with the \u2018zero\u2019 solution. we can see that for k \u201c 18 the student diverges from the teacher shortly after the 18th time step. For k \u201c 20 we can see near perfect extrapolation up to the horizon considered. B IMPLEMENTATION DETAILS All the experiments are implemented using PyTorch. B.1 OPTIMIZATION: In Section 5.1 we optimize the population loss, which entails minimizing Equation 3.4 with respect to the parameters of the learned model. We use 15K optimization steps with Adam optimizer and a learning rate of 10\u00b43. In this experiment, the results were not sensitive to the initialization scale of the (balanced) student. In Section 5.2 and Section 5.3 in the experiments that involve minimizing the empirical loss, we use 50K optimization steps with early stopping (most experiments required less than 10K steps). The batch size is set to 100, data is sampled from a Gaussian with zero mean and scale of 1. Experiments were not sensitive to most hyper-parameters other than learning rate and initialization scale. The examination of the effect of initialization scale presented in Section A.1.2 is done with learning rate scheduler torch.optim.lr_scheduler.MultiStepLR using milestones at r5000, 10000, 15000, 30000s and a decaying factor of \u03b3 \u201c 0.1. B.2 TEACHER GENERATION: One of the main challenges in empirically evaluating extrapolation is that randomly sampling weights from a Gaussian distribution may result with an RNN of lower effective rank (i.e. the resulting RNN may be accurately approximated with another RNN with a smaller hidden dimension). We will now describe the teacher generation scheme for the different experiments. B.2.1 BALANCED TEACHER GENERATION: A balanced teacher consists of d entries corresponding to the diagonal teacher and d entries representing B\u0302 \u201c C\u0302J. In order to avoid cases of rapid decay in the impulse response on the one hand, and exponential growth on the other, we set the eigenvalues to distribute uniformly between 0.6 and 1.05. The values of B\u0302 and C\u0302 are randomly sampled from a Gaussian around 0.5 and scale 1 and then normalized such that C\u0302B\u0302 \u201c 1. B.2.2 UNBALANCED TEACHER GENERATION: In this experiment, the teacher has a general (non-symmetrid) matrix A\u0302 and B\u0302 \u2030 C\u0302J. We set the weights as described next. Delay Teacher A \u2018delay\u2019 teacher has an impulse response of 1 at time step i \u201c d\u0302 \u00b4 1, that is, the teacher has an impulse response of p0, . . . , 0, 1, 0, . . . q. In order to generate the mentioned impulse response we set the weights as follows, A \u201c \u00a8 \u02da \u02da \u02dd 0 1 0 . . . 0 0 1 0 0 0 \u02db \u2039 \u2039 \u201a , B \u201c \u00a8 \u02da \u02da \u02dd 0 ... 0 1 \u02db \u2039 \u2039 \u201a , and CJ \u201c \u00a8 \u02da \u02da \u02dd 1 0 ... 0 \u02db \u2039 \u2039 \u201a . (B.1) Note that B,C above are set to extract the last entry of the first row of Ai and A is a Nilpotent shift matrix. It is straightforward to verify that CAiB \u201c 1 for i \u201c d\u0302 \u00b4 1 and 0 otherwise. Random Unbalanced Teacher The second unbalanced teacher is randomly generated. In order to avoid the caveats mentioned in Section A.1, we randomly sample the diagonal (from a Gaussian with zero mean and scale 0.1) and super diagonal (from a Gaussian with mean 0.7 and scale 0.1) of A. We set B,C as in equation B.1. The structure of A ensures similar properties to that of the delayed teacher, specifically, that the first entries of the impulse response is zero and the teacher is \u2018revealed\u2019 only after d\u0302 time steps. B.2.3 NON-LINEAR TEACHER GENERATION: As opposed to the linear teacher discussed in previous sections, when the teacher is a Gated Recurrent Units (GRU), it is unclear how to generate a non-trivial teacher. When randomly generating a teacher GRU the result is either a trivial model that quickly decays to zero or a teacher with an exploding impulse response (depending on the scale of the initialization). In order to produce a teacher with interesting extrapolation behaviour, we initialize a model with an initialization scale of 10\u00b46 and train for 1000 step the model to mimic an arbitrarily chosen impulse response. The result of the mentioned procedure is a teacher GRU with non-trivial behaviour. Figure 2(b) shows that we get with this non-trivial teacher the phase transition phenomena as described in Section 5.3. B.3 EXTRAPOLATION ERROR: The concept of extrapolation is very intuitive, and yet it does not admit any standard error measure. A proper extrapolation error measure should: (a) capture fine differences between two models with good extrapolation behaviour; and on the other hand, (b) be insensitive to the scale in which two non-extrapolating model explode. A natural approach which we take here is to report the \u21138 norm difference on the tail of the impulse response. A model is considered non-extrapolating if the extrapolation error is worse than the extrapolation error of a trivial solution which has an impulse response of zeros. C DEFERRED PROOFS: Here we provide complete proofs for the results in the paper. C.1 AUXILARY PROOFS: In this section we provide missing proofs from the main paper and additional lemmas to be used in the main proofs. C.1.1 POPULATION LOSS: Lemma C.1 (Proof of Equation 3.4). Assume x \u201e D such that Ex\u201eDrxs \u201c 0,Ex\u201eDrxxJs \u201c Ik P Rk,k, where Ik is the identity matrix. y is given by y \u201c {RNNpxq where {RNNp\u00a8q denotes the output of a teacher RNN, \u0398\u0302 \u201c pA\u0302, B\u0302, C\u0302q. Denote wi \u201c C\u0302A\u0302iB\u0302, the loss for the student RNN satisfies: Ex\u201eD r\u2113 pRNN pxq , yqs \u201c k\u00b41 \u00ff i\u201c0 ` CAiB \u00b4 wi \u02d82 . (C.1) Proof of Lemma C.1. The population loss for training with sequences of length k is Ex\u201eD r\u2113 pRNN pxq , yqs \u201c Ex\u201eD \u00bb \u2013 \u02dc k\u00b41 \u00ff i\u201c0 CAk\u00b41\u00b4iBxi \u00b4 k\u00b41 \u00ff j\u201c0 wk\u00b41\u00b4jxj \u00b82 fi fl . (C.2) Reversing the order of summation, expanding the terms, Ex\u201eD r\u2113 pRNN pxq , yqs \u201c Ex\u201eD \u00bb \u2013 \u02dc k\u00b41 \u00ff i\u201c0 CAiBxk\u00b41\u00b4i \u00b4 k\u00b41 \u00ff j\u201c0 wjxk\u00b41\u00b4j \u00b82 fi fl (C.3) \u201c k\u00b41 \u00ff i,j\u201c0 \u201c CAiBCAjB \u00b4 2CAiBwj ` wiwj \u2030 Ex\u201eD rxk\u00b41\u00b4ixk\u00b41\u00b4js (C.4) \u201c k\u00b41 \u00ff i,j\u201c0 \u201c CAiBCAjB \u00b4 2CAiBwj ` wiwj \u2030 1k\u00b41\u00b4i\u201ck\u00b41\u00b4j (C.5) \u201c k\u00b41 \u00ff i,j\u201c0 \u201c CAiBCAjB \u00b4 2CAiBwj ` wiwj \u2030 1i\u201cj (C.6) \u201c k\u00b41 \u00ff i\u201c0 \u201c pCAiBq2 \u00b4 2CAiBwi ` w2i \u2030 \u201c k\u00b41 \u00ff i\u201c0 ` CAiB \u00b4 wi \u02d82 . (C.7) where the transition from the second to third rows is by our assumption that Ex\u201eDrxixjs \u201c 1i\u201cj. Therefore we have, Ex\u201eD r\u2113 pRNN pxq , yqs \u201c k\u00b41 \u00ff i\u201c0 ` CAiB \u00b4 wi \u02d82 . (C.8) concluding the proof. C.1.2 PERFECT GENERALIZATION AND FAILED EXTRAPOLATION: Proposition C.2 (Proposition 3 in main paper). Assume d \u0105 k, and let \u03f5 \u011b 0 and q P tk ` 1, k ` 2, . . .u. Then, for any teacher parameters \u0398\u0302, there exist student parameters \u0398 with which the population loss in Equation 3.4 equals zero, and yet the student does not \u03f5-extrapolate with horizon q. Proof. Consider a student, \u0398, such that A is symmetric (and therefore has an orthogonal eigendecomposition). Denote A \u201c U\u039bUJ. The impulse response at time step i can be expressed as CAiB \u201c CU\u039biUJB. The latter can be written compactly in matrix form as V g where V is the Vandermonde matrix with diagp\u039bq as its values, V \u201c \u00a8 \u02da \u02da \u02da \u02da \u02dd 1 1 . . . 1 \u03bb1 \u03bb2 . . . \u03bbd \u03bb21 \u03bb 2 2 . . . \u03bb 2 d ... ... ... \u03bbd\u00b411 \u03bb d\u00b41 2 . . . \u03bb d\u00b41 d \u02db \u2039 \u2039 \u2039 \u2039 \u201a , and g is defined as g \u201d pCUqJ \u02dd UJB.4 A known result on square Vandermonde matrices is that they are invertible if and only if \u03bbi \u2030 \u03bbj , @i \u2030 j. Given a fixed set of distinct values p\u03bb1, . . . , \u03bbdq and an arbitrary impulse response r P Rd, in order for the student to generate the impulse response r (i.e. V g \u201c r), one can set the coefficient vector, g \u201c V \u00b41r and end up with a symmetric student with r as its impulse response of length d. Consider a teacher RNN, \u0398\u0302 \u201c pA,B,Cq, we can set and the first k entries of r to ri \u201c C\u0302A\u0302i\u00b41B\u0302, @i \u201c t1, . . . , ku. We are therefore left with d \u00b4 k degrees of freedom which yields many different students that correspond to the first k entries of the teacher while fitting arbitrary values beyond the k. C.1.3 EQUIVALENCE BETWEEN BALANCED RNNS WITH SYMMETRIC AND DIAGONAL TRANSITION MATRICES: Lemma C.3. A balanced RNN, \u0398 \u201c pA,B,Cq, with a symmetric transition matrix (i.e. B \u201c CJ and A \u201c AJ) has an equivalent (i.e. generating the same impulse response) RNN, \u03981 \u201c pA1,B1,C 1q, which is balanced and its transition matrix is diagonal. Lemma C.3 allows alternating between systems with symmetric and diagonal matrices. This is useful to simplify the analysis in Section 4. Proof of Lemma C.3. Any symmetric matrix admits an orthogonal eigendecomposition with real (non-imaginary) eigenvalues. Denote A \u201c U\u039bUJ. We can define A1 \u201c \u039b, B1 \u201c UJB and C 1 \u201c CU , The ith index of the impulse response is given by CAiB \u201c CU\u039biUJB \u201c C 1 ` A1 \u02d8i B1 concluding that \u0398 and \u03981 have the same impulse response of any length. C.1.4 GRADIENT DERIVATION: For completeness and Section C.4, we compute the gradients for the general setting. Lemma C.4. Given the population loss LpA,B,Cq \u201c k\u00b41 \u00ff j\u201c0 \u00b4 CAjB \u00b4 C\u0302A\u0302jB\u0302 \u00af2 . (3.4 revisited) Denote \u2207\u2113i \u201c CAiB \u00b4 wi, the derivatives of the loss with respect to B, and C satisfy: BL BB \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAiqJCJ, (C.9) BL BC \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113iBJ ` Ai \u02d8J . (C.10) 4Here \u02dd denotes the Hadamard (elementwise) product. Proof of Lemma C.4. Here, we will compute the gradient of the population loss. Note that for j \u011b 0, the derivative of CAjB w.r.t to B is given by BpCAjBq BB \u201c pA jqJCJ. (C.11) Similarly, the derivative of CAjB w.r.t to C is given by BpCAjBq BC \u201c B JpAjqJ. (C.12) Using these derivatives, we can calculate the derivative of the population loss, (assigning wi \u201c B\u0302A\u0302iC\u0302), L \u201c Ex\u201eD r\u2113 pRNN pxq , yqs \u201c k\u00b41 \u00ff i\u201c0 ` CAiB \u00b4 wi \u02d82 . (C.13) Denoting \u2207\u2113i \u201c CAiB \u00b4 wi, and noting that wi is constant (depends on \u0398\u0302), we have for X P tB,Cu: BL BX \u201c k\u00b41 \u00ff i\u201c0 B ` CAiB \u00b4 wi \u02d82 BX \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113i B ` CAiB \u00b4 wi \u02d8 BX \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113i B ` CAiB \u02d8 BX . (C.14) Plugging in Equation C.11 and Equation C.12, we have: BL BB \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113i B ` CAiB \u02d8 BB \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAiqJCJ, (C.15) BL BC \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113i B ` CAiB \u02d8 BC \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113iBJpAiqJ, (C.16) C.1.5 LEMMA 6 (CONSERVATION OF BALANCEDNESS): Lemma C.5. [Lemma 6 in main paper] When optimizing equation 3.4 with GF emenating from a balanced initialization \u0398p0q, the parameters \u0398p\u03c4q are balanced for all \u03c4 P R`. We prove the above result by first showing it for gradient descent and then translating the result to gradient flow. The gradient descent result is stated below, and generalizes a result that was shown in Cohen-Karlik et al. (2022) for the memoryless case. Lemma C.6. When optimizing equation 3.4 with GD with balanced initial conditions, then @t P N, \u0398 has a balanced weight configuration, i.e. Bt \u201c CJt . Proof of Lemma C.6. We prove by induction. By our assumption, the condition holds for t \u201c 0. Assume Bt \u201c CJt , our goal is to show the conditions hold for pBt`1,Ct`1q. In order to show that Bt`1 \u201c CJt`1, we only need to show that BLBBt \u201c \u00b4 BL BCt \u00afJ . Writing the gradients (Lemma C.4), we have \u02c6 BL BCt \u02d9J \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113iAitBt \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJt qiCJt \u201c BL BBt , (C.17) where the inequality follows from the induction assumption and the symmetric structure of At. To conclude, the gradients at time t are the same and Bt \u201c CJt by the induction assumption, arriving at Bt`1 \u201c Bt \u00b4 \u03b7 BL BBt \u201c CJt \u00b4 \u03b7 \u02c6 BL BCt \u02d9J \u201c CJt`1 (C.18) The proof of Lemma 6 follows from Lemma C.6 and the fact that for sufficiently small step size GD approximates GF with arbitrary precision (see Theorem 3 in Elkabetz & Cohen, 2021). C.1.6 CONSERVATION OF DIFFERENCE OF NORMS: Appendix C.1.5 shows that if weights are initialized to be balanced, this property is conserved throughout optimization. Here we show under standard initialization schemes, the difference between the norms of B and C is also conserved. Lemma C.7. The difference between the norms of B, C is conserved throughout GF, i.e., d dt ` }B}2 \u00b4 }C}2 \u02d8 \u201c 0. (C.19) Proof of Lemma C.7. We wish to prove that the difference between the norms is conserved over time. Consider the following expression: \u03b1 \u201d }B}2 \u00b4 }C}2 \u201c BJB \u00b4 CCJ. (C.20) With this notation, we just need to prove that 9\u03b1 \u201c 0. The derivative of B, C with respect to time is given by, 9B \u201c \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqiCJ, (C.21) 9C \u201c \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113iBJpAJqi. (C.22) Using the interchangeability of derivative and transpose, we have: 9\u03b1 \u201c 9BJB ` BJ 9B \u00b4 9CCJ \u00b4 C 9CJ \u201c 2BJ 9B \u00b4 2 9CCJ. (C.23) Plugging equation C.21 and equation C.22, we get 9\u03b1 \u201c 2BJ \u02dc \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqiCJ \u00b8 \u00b4 2 \u02dc \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113iBJpAJqi \u00b8 CJ (C.24) \u201c \u00b42 \u00ab BJ \u02dc k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqi \u00b8 CJ \u00b4 BJ \u02dc k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqi \u00b8 CJ ff \u201c 0. (C.25) establishing that ddt ` }B}2 \u00b4 }C}2 \u02d8 \u201c 0. C.2 LEMMA 7 (EXACT EXTRAPOLATION): Lemma C.8. [Lemma 7 in main paper] Suppose that d \u0105 k \u0105 2d\u0302, the teacher is balanced, and that the student parameters \u0398 are balanced and satisfy Lp\u0398q \u201c 0. Then \u0398 extrapolates. Proof of Lemma C.8. By Lemma C.3, a balanced RNN with symmetric transition matrix has an equivalent (generating the same impulse response) balanced RNN with a diagonal transition matrix. We will continue under the assumption of diagonal transition matrices. Without loss of generality we assume C\u0302B\u0302 \u201c 1. Otherwise, the problem can be rescaled by C\u0302B\u0302, which is equivalent to rescaling the initial conditions, and providing no additional information.5 From the balanced assumption, we have C\u0302J \u201c B\u0302. Denote p\u0302 \u201c C\u0302J d B\u0302 \u201c B\u0302 d B\u0302, and we get p\u0302i \u011b 0 and \u0159 i p\u0302i \u201c 1, and therefore p\u0302 may be interpreted as a distribution over a random variable with d\u0302 possible values. We shall assume that these values are A\u03021, . . . , A\u0302d\u0302, and denote the corresponding random variable by Z. Furthermore, we can also interpret elements of the impulse response of \u0398\u0302 as moments of this distribution. Let us write the nth element of the impulse response as: C\u0302A\u0302nB\u0302 \u201c \u00ff i p\u0302ia\u0302 n i \u201c Ep\u0302rZ\u0302ns, (C.26) 5The case for which C\u0302B\u0302 \u201c 0 is handled separately. where EprZs is the expected value of a random variable Z under the distribution p. In the same way, we can define for the learned model \u0398, a distribution pi \u201c CiBi, and write the learned impulse response as: CAnB \u201c \u00ff i pia n i \u201c EprZns. (C.27) This view provides us with a moment matching interpretation of the learning problem. Namely, the fact that \u0398 matches the first k elements of the teacher impulse response, is the same as saying they agree on the first k \u00b4 1 moments EprZjs for j P t1, . . . , k \u00b4 1u.6 The question of extrapolation is whether equality in the first k \u00b4 1 moments implies an equality in all other moments. In (Cohen & Yeredor, 2011, Theorem 1) and in (Wu & Yang, 2020, Lemma 4) it is shown that the first 2d\u0302 moments of a discrete random variable taking at most d\u0302 different values uniquely define this random variable. Therefore, any other discrete random variable identifying with the teacher on 2d\u0302 moments must be the same random variable and therefore identifies on higher moments as well. Since we assumed k \u0105 2d\u0302, this result immediately implies that equality in the first k \u00b4 1 moments implies equality in all other moments. For the case C\u0302B\u0302 \u201c 0, from our assumption that the teacher is balanced, we have that the condition is met only if C\u0302i \u201c B\u0302i \u201c 0 for i \u201c 1, . . . , d\u0302. Such a teacher has an impulse response of zeros, for k \u011b 1, a student minimizing the loss must also satisfy CB \u201c 0 and therefore has the zeros as its impulse response thus extrapolating with respect to the said teacher. C.3 THEOREM 8 (APPROXIMATE EXTRAPOLATION): This section is devoted to the proof of Theorem 8 which ties the approximation error of optimization to that of extrapolation. Lemma C.9 below shows that if the parameters are within close proximity to an extrapolating solution, the generalization error (i.e. loss values) are small. Lemma C.9. Assume }X \u00b4 X\u02da}8 \u0103 \u03b4 for X P tA,B,Cu, and \u0398\u02da \u201c pA\u02da,B\u02da,C\u02daq is an extrapolating solution (with Lp\u0398\u02daq \u201c 0q. Then, there exists a constant depending on k and \u0398\u02da, cpk,\u0398\u02daq, such that Lp\u0398q \u010f c\u03b42. Proof of Lemma C.9. First we note that \u0398\u02da attains zero loss and is an extrapolating with respect to the teacher \u0398\u0302, specifically, C\u02dapA\u02daqiB \u201c C\u0302A\u0302iB\u0302, @i \u201c 0, 1, . . . , (C.28) From the above, the loss can be written with respect to \u0398\u02da instead of \u0398\u0302, i.e. LpA,B,Cq \u201c k\u00b41 \u00ff j\u201c0 ` CAjB \u00b4 C\u02dapA\u02daqjB\u02da \u02d82 . (C.29) Denote by l P Rk the residuals vector with respect to \u0398\u02da, i.e. li \u201d CAiB \u00b4 B \u00b4 C\u02dapA\u02daqiB\u02da. With these notations, the loss amounts to LpA,B,Cq \u201c }l}2. From equivalence of norms, we have that for v P Rn, }v}8 \u010f }v}2 \u010f ? n}v}8. The latter means that bounding }v}8 will provide the desired bound on the loss with a factor of ? k. Recall that \u0398\u02da is fixed, specifically, }X\u02da} is bounded by some constant mX\u02da for X P tA,B,Cu. Denote m \u201d maxtmA\u02da ,mB\u02da ,mC\u02da , 1u, the ith entry of l can be bounded by \u02c7 \u02c7CAi\u00b41B \u00b4 C\u02dapA\u02daqi\u00b41B\u02da \u02c7 \u02c7 \u201c \u02c7 \u02c7pC \u00b4 C\u02da ` C\u02daqAi\u00b41B \u00b4 C\u02dapA\u02daqi\u00b41B\u02da \u02c7 \u02c7 (C.30) \u010f \u02c7 \u02c7C\u02da ` Ai\u00b41B \u00b4 pA\u02daqi\u00b41B\u02da \u02d8 \u02c7 \u02c7 ` \u02c7 \u02c7pC \u00b4 C\u02daqAi\u00b41B \u02c7 \u02c7 (C.31) \u010f m \u203a \u203aAi\u00b41B \u00b4 pA\u02daqi\u00b41B\u02da \u203a \u203a ` \u03b4 \u203a \u203aAi\u00b41B \u203a \u203a (C.32) 6Equality of the 0th moment ensures the student induces a valid probability, i.e. \u0159 i CiBi= \u0159 i C\u0302iB\u0302i=1. From Cauchy-Schwarz inequality, the RHS satisfies \u03b4}Ai\u00b41B} \u010f \u03b4}A}i\u00b41}B} \u010f \u03b4pm ` \u03b4qi. The last transition uses, }X} \u201c }X \u00b4 X ` X\u02da} \u010f }X \u00b4 X\u02da} ` }X\u02da} \u010f \u03b4 ` m (C.33) for X P tA,Bu. In a similar manner, the LHS can be bounded by \u203a \u203aAi\u00b41B \u00b4 pA\u02daqi\u00b41B\u02da \u203a \u203a \u201c \u203a \u203aAi\u00b41B \u00b4 Ai\u00b41B\u02da ` Ai\u00b41B\u02da \u00b4 pA\u02daqi\u00b41B\u02da \u203a \u203a (C.34) \u010f }A}i\u00b41}B \u00b4 B\u02da} loooooooooomoooooooooon \u03b4pm`\u03b4qi\u00b41 `}Ai\u00b41 \u00b4 pA\u02daqi\u00b41}}B\u02da} (C.35) again, we have used the triangle inequality and Cauchy-Schwarz inequality. In order to deal with }Ai\u00b41 \u00b4 pA\u02daqi\u00b41}, we make use of the equality xn \u00b4 yn \u201c px \u00b4 yqpxn\u00b41 ` xn\u00b42y ` xn\u00b43y2 ` \u00a8 \u00a8 \u00a8 ` xyn\u00b42yn\u00b41. Assigning A and A\u02da, we have }Ai\u00b41 \u00b4 pA\u02daqi\u00b41} \u201c \u203a \u203a \u203a \u203a \u203a pA \u00b4 A\u02daq i\u00b42 \u00ff j\u201c0 Ai\u00b42\u00b4jpA\u02daqj \u203a \u203a \u203a \u203a \u203a (C.36) \u010f }A \u00b4 A\u02da} loooomoooon \u010f\u03b4 \u203a \u203a \u203a \u203a \u203a p i\u00b42 \u00ff j\u201c0 Ai\u00b42\u00b4jpA\u02daqj \u203a \u203a \u203a \u203a \u203a \u010f \u03b4pi \u00b4 1qpm ` \u03b4qi\u00b42 (C.37) where the last inequality uses, \u203a \u203a \u203a \u203a \u203a i\u00b42 \u00ff j\u201c0 Ai\u00b42\u00b4jpA\u02daqj \u203a \u203a \u203a \u203a \u203a \u010f i\u00b42 \u00ff j\u201c0 }A}i\u00b42\u00b4j}A\u02da}j (C.38) \u010f i\u00b42 \u00ff j\u201c0 pm ` \u03b4qi\u00b42\u00b4jmj (C.39) \u010f i\u00b42 \u00ff j\u201c0 pm ` \u03b4qi\u00b42 \u201c pi \u00b4 1qpm ` \u03b4qi\u00b42. (C.40) Plugging all terms back together, |li| \u010f m ` \u03b4pm ` \u03b4qi\u00b41 ` \u03b4pi \u00b4 1qpm ` \u03b4qi\u00b42m \u02d8 ` \u03b4pm ` \u03b4qi \u201c Op\u03b4miq. (C.41) The above is maximized for i \u201c k, to result with }l}8 \u201c Op\u03b4mkq. Plugging back to the bound on the loss, we have LpA,B,Cq \u201c }l}2 \u010f k}l}28 \u201c Op\u03b42km2kq (C.42) When k is small which is the case of interest, LpA,B,Cq \u201c Op\u03b42q. Theorem C.10. [Theorem 8 in main paper] Assume the conditions of Theorem 5, and that the teacher parameters \u0398\u0302 are stable, i.e. the singular values of A\u0302 are in r\u00b41, 1s. Assume also that \u0398\u0302 are non-degenerate, in the sense that the input-output mapping they realize is not identically zero. Then, for any \u03f5 \u0105 0 and q P N, there exists \u03b4p\u03f5, qq \u0105 0 which admits the following: whenever GF leads the student parameters \u0398 to within (Frobenius) distance \u03b4p\u03f5, qq from a balanced point \u0398\u02da satisfying Lp\u0398\u02daq \u201c 0, the student \u03f5-extrapolates with horizon q. Proof of Theorem C.10. Given a desired approximation error \u03f5 \u0105 0, and horizon q P tk ` 1, k ` 2, . . . u, our goal is to find \u03b4p\u03f5, qq such that if }X \u00b4 X\u02da}F \u0103 \u03b4 for X P tA,B,Cu, then |CAjB \u00b4 C\u0302A\u0302jB\u0302| \u0103 \u03f5. First we note that by Theorem 5, \u0398\u02da is an extrapolating solution with respect to \u0398\u0302 (i.e. \u0398\u02da and \u0398\u0302 generate identical impulse responses), therefore we can measure extrapolation with respect to \u0398\u02da. In a similar manner to Lemma C.9, we can define the residual vector l such that its ith entry is defined as li \u201c CAi\u00b41B \u00b4 C\u02dapA\u02daqi\u00b41B\u02da. (C.43) Note that \u0398 is initialized balanced and by Lemma 6, this property is preserved throughout GF. By Lemma C.3 both \u0398 and \u0398\u02da have equivalent balanced systems \u0398\u0304 and \u0398\u0304\u02da for which the transition matrices are diagonal. For the sake of readability, we overload notations and carry on under the assumption that A and A\u02da are diagonal with entries tA1, . . . , Adu and tA\u02da1 , . . . , A\u02dadu on their respective diagonals. Another observation is that a balanced RNN must satisfy that its first element in the impulse response moment is non-negative (CB \u201c \u0159 cibi \u201c \u0159 b2i \u011b 0). Without loss of generality, we can assume the teacher RNN \u0398\u0302 satisfies C\u0302B\u0302 \u201c 1. Suppose this is not the case, i.e. C\u0302B\u0302 \u201c \u03b7 \u0105 0 (recall we assume the teacher is non-degenerate). One can re-scale C\u0302 and B\u0302 by a factor of \u03b7\u00b40.5 and result with C\u0302A\u0302 i\u00b41B\u0302 \u03b7 on the i th entry of its impulse response. The re-scaled system satisfies C\u0302B\u0302\u03b7 \u201c 1 and provides no extra information. With the claims above, we may interpret \u0398\u0302 as a discrete random variable, i.e. p\u0302 \u201c pC\u03021B\u03021, . . . , C\u0302d\u0302B\u0302d\u0302q defines a probability vector,7 the random variable is assigned the value A\u0302i with probability p\u0302i. Denote the random variable associated with \u0398\u0302 by z\u0302. Recall that \u0398\u02da is balanced and attains zero loss, hence it also admits the random variable interpretation (denoted as z\u02da). Under the probabilistic perspective, the impulse response of the systems correspond to the moment of the random variables, i.e. E rz\u0302ns \u201c d\u0302 \u00ff i\u201c1 p\u0302iA\u0302 n i \u201c d\u0302 \u00ff i\u201c1 C\u0302iB\u0302iA\u0302 n i \u201c C\u0302A\u0302nB\u0302 (C.44) We can write the loss in terms of the defined residual vector, Lp\u0398q \u201c }l}2. From Lemma C.9, we have Lp\u0398q \u201c Op\u03b42q. Specifically, }l} \u010f c\u03b42 for some constant c P R. Recall that for v P Rn, }v}8 \u010f }v}2, applying to our case, we have }l}8 \u010f }l} \u010f ? c\u03b4. Specifically, |l0| \u010f }l}8 \u010f ? c\u03b4 and since we assume C\u02daB\u02da \u201c 1, we have |l0| \u201c |CB \u00b4 C\u02daB\u02da| \u201c |CB \u00b4 1| \u010f ? c\u03b4 (C.45) which leads to 1 \u00b4 ? c\u03b4 \u010f CB \u010f 1 ` ? c\u03b4. Denote CB \u201c 1 ` \u03b2 where \u03b2 P r\u00b4 ? c\u03b4, ? c\u03b4s. A re-scaled student introduces a factor \u03b4 into the approximation error, that is \u02c7 \u02c7 \u02c7 \u02c7 CAi\u00b41B 1 ` \u03b2 \u00b4 C \u02dapA\u02daqi\u00b41B\u02da \u02c7 \u02c7 \u02c7 \u02c7 \u201c \u02c7 \u02c7 \u02c7 \u02c7 CAi\u00b41B 1 ` \u03b2 \u00b4 p1 ` \u03b2q C\u02dapA\u02daqi\u00b41B\u02da 1 ` \u03b2 \u02c7 \u02c7 \u02c7 \u02c7 (C.46) \u201c 1 1 ` \u03b2 \u02c7 \u02c7CAi\u00b41B \u00b4 p1 ` \u03b2qC\u02dapA\u02daqi\u00b41B\u02da \u02c7 \u02c7 (C.47) \u010f 1 1 ` \u03b2 \u02c7 \u02c7CAi\u00b41B \u00b4 C\u02dapA\u02daqi\u00b41B\u02da \u02c7 \u02c7 ` 1 1 ` \u03b2 |\u03b2C \u02dapA\u02daqi\u00b41B\u02da| (C.48) Recall that \u0398\u02da is fixed, specifically, }X\u02da} is bounded by some constant mX\u02da for X P tA,B,Cu. Denote m \u201d maxtmA\u02da ,mB\u02da ,mC\u02da , 1u. Combining the latter with Cauchy-Schwarz inequality, \u02c7 \u02c7 \u02c7 \u02c7 CAi\u00b41B 1 ` \u03b2 \u00b4 C \u02dapA\u02daqi\u00b41B\u02da \u02c7 \u02c7 \u02c7 \u02c7 \u010f 1 1 ` \u03b2 |li| ` \u03b2mi`1 1 ` \u03b2 \u010f 2|li| ` 2\u03b2m i`1 \u201c Op\u03b4q (C.49) where the last inequality uses 0.5 \u0103 1 ` \u03b2 for ? c\u03b4 \u0103 0.5. Per our claim above, we can assume with out loss of generality that CB \u201c 1. A balanced student can thus be interpreted as a discrete random variable with probability vector p taking on values tA1, . . . , Adu with probability pi. 7\u0159 p\u0302i \u201c 1 and @i, p\u0302i \u011b 0 Recall that under the assumption of a stable teacher, we have that for all i \u201c 1, . . . , d\u0302, |A\u0302i| \u010f 1. Denote the joint support of A and A\u0302 by S \u201d tAiudi\u201c1 \u0164 tA\u0302jud\u0302j\u201c1, and define s \u201c maxtSu \u00b4mintSu. By our assumption on convergence and stability of the teacher, we have s \u010f 2p1 ` \u03b4q. From Proposition 2 in Wu & Yang (2020), we have that if |CAiB\u00b4C\u0302A\u0302iB\u0302| \u010f \u03b4 for i P t1, . . . , 2d\u0302u, then W1pp, p\u0302q \u201c O \u00b4 \u03b4 1 4d\u0302 \u00af . (C.50) By (Panaretos & Zemel, 2019, Section 2.3), we can tie the Wasserstein distance to higher order qth Wasserstien distance. For the s defined above, we have that for any q P N: Wqq pp, p\u0302q \u010f W1pp, p\u0302qsq\u00b41 \u0103 p2 ` 2\u03b4qq\u00b41Op\u03b4 1 2d\u0302 q. (C.51) Denote by mqppq the qth moment of p, from (Biswas & Mackey, 2021, Equation 1), we have: \u02c7 \u02c7mqqppq \u00b4 mqqpp\u0302q \u02c7 \u02c7 \u010f Wqq pp, p\u0302q \u010f p2 ` 2\u03b4qq\u00b41Op\u03b4 1 2d\u02da q. (C.52) Recall that the difference in the ith moments of the student and teacher correspond to the difference in the impulse responses. So we have that if }X \u00b4 X\u02da}F \u0103 \u03b4 for X P tA,B,Cu then @q P N, |lq| \u010f p2 ` 2\u03b4qq\u00b41Op\u03b4 1 2d\u0302 q. (C.53) C.4 PROPOSITION 9 (IMPLICIT BIAS FOR BALANCEDNESS): The proof of Proposition 9 consists of several steps. First, we bound with high probability the norms of B and C at initialization (Lemma C.11). We then derive bounds on the differential equations of ddt pBptq ` C Jptqq and ddt pBptq \u00b4 C Jptqq (Lemma C.15). We show that when the initialization scale tends to zero, the ratio between the differential equations tends to zero. (Lemma C.14). Before we turn to prove Proposition 9, we first need to bound the initial values for a vector v P Rn initialized with N p0, \u03f5 2 n q. Lemma C.11. Assume a vector v P Rn with N p0, \u03f5 2 n q per coordinate. Then: Pr \u02c6 \u03f5 2 \u0103 }v}2 \u0103 3\u03f5 2 \u02d9 \u011b 1 \u00b4 2 expp\u00b49d{64q. (C.54) Proof of Lemma C.11. The proof of C.11 uses known results on the Chi-square distribution Laurent & Massart (2000), applied to our specific setting to achieve the desired bounds. We will begin by changing variables, v\u0303i \u201c vi \u00a8 ? n \u03f5 . The entries v\u0303i, are standard Gaussian variables. The squared norm of v\u0303 distributes according to the \u03c7-squared distribution. By (Laurent & Massart, 2000, Lemma 1), in our case (plugging in x \u201c 9n{64), the following inequalities hold: Pr ` }v\u0303}2 \u011b p1.75 ` 9{32qn \u02d8 \u010f expp\u00b49n{64q, (C.55) Pr ` }v\u0303}2 \u010f d{4 \u02d8 \u010f expp\u00b49n{64q. (C.56) We simplify these inequalities. From the first bound: Pr ` }v\u0303}2 \u011b 2.25n \u02d8 \u010f expp\u00b49n{64q \u00f9\u00f1 Pr ` }v\u0303} \u011b 1.5 ? n \u02d8 \u010f expp\u00b49n{64q (C.57) Changing variables back to v: Pr p}v} \u011b 1.5\u03f5q \u010f expp\u00b49n{64q. (C.58) Similarly, for the second bound: Pr ` }v\u0303} \u010f ? n{2 \u02d8 \u010f expp\u00b49n{64q \u00f9\u00f1 Pr p}v} \u010f \u03f5{2q \u010f expp\u00b49n{64q. (C.59) Taking the complementary probability, we have the desired result of Pr \u02c6 \u03f5 2 \u0103 }v} \u0103 3\u03f5 2 \u02d9 \u010f 1 \u00b4 2 expp\u00b49n{64q. (C.60) Proposition C.12. [Proposition 9 in main paper] Suppose that: (i) d \u0105 4; (ii) the teacher parameters \u0398\u0302 are balanced and are non-degenerate, in the sense that the input-output mapping they realize is not identically zero; and (iii) the student parameters are learned by applying GF to the loss Lp\u00a8q. Let \u0398\u0303 be a random point in parameter space, with entries drawn independently from the standard normal distribution. For \u03f5 \u0105 0, consider the case where GF emanates from the initialization \u03f5\u0398\u0303, and denote the resulting curve by \u0398\u03f5p\u03c4q \u201c pA\u03f5p\u03c4q,B\u03f5p\u03c4q,C\u03f5p\u03c4qq, with \u03c4 \u011b 0. Then, w.p. at least 0.75, for every \u03f5 \u0105 0 there exists \u03c4\u03f5 \u011b 0 such that: lim \u03f5\u00d10` ||B\u03f5p\u03c4\u03f5q \u00b4 CJ\u03f5 p\u03c4\u03f5q||F ||B\u03f5p\u03c4\u03f5q ` CJ\u03f5 p\u03c4\u03f5q||F \u201c 0 . (C.61) The consequence of Proposition 9 is that as \u03f5 converges to zero, B and C converge towards each other. For convenience, we refer to the mentioned initialization scheme (where every coordinate in a vector is initialized as N \u00b4 0, \u03f5 2 d \u00af ) as \u03f5-normal initialization. In order to prove the proposition we define a few relevant terms, Y \u201c B \u00b4 CJ, W \u201c B ` CJ, w0 \u201c C\u0302B\u0302. (C.62) We will in fact prove the stronger, following lemma, for any matrix A, not necessarily symmetric. Lemma C.13. Assume w0 \u0105 0, and A,B,C are \u03f5-normally initialized. Then Dt such that lim \u03f5\u00d10 }Y ptq}2 }W ptq}2 \u201c 0, lim\u03f5\u00d10 }Aptq}2F }W ptq}2 \u201c 0. (C.63) The proof of Lemma C.13 follows three steps. (I) We show that there is a time in the optimization in which the norms of all parameters are bounded (Lemma C.14); (II) we derive upper (and lower) bounds for the differential equations describing the evolvement of Y,W and A. Our approximations are limited to the initial phase of training. Concretely, we show that for 0 \u010f t \u010f 12w0 ln ` 1 \u03f50.5 \u02d8 , all norms are bounded. Thus, it is possible to obtain meaningful bounds on the ODEs of Y and W while A remains in the magnitude of initialization. (III) Using the relevant bounds, we show that Lemma C.14. Assume \u03f5-initialization and bounded teacher. Then, for all 0 \u010f t \u010f 12w0 ln ` 1 \u03f50.5 \u02d8 , there exist M1,M2 such that: }Cptq}, }Bptq} \u0103 M1\u03f50.75 (C.64) and }Aptq}F \u0103 M2\u03f5 (C.65) To prove this, we note that at initialization, A,B and C satisfy these bounds. From continuity, there exists a maximal time for which they are satisfied. We bound the rate of their growth, and thus show that for all t as described, we are within this region. Lemma C.15. Assume w0 \u0105 0 and assume A,B,C are \u03f5-normally initialized, we have the following bounds for all 0 \u010f t \u010f 12w0 ln ` 1 \u03f50.5 \u02d8 Y ptqJY ptq \u010f c1\u03f52e\u00b42w0t ` c2\u03f52.5, (C.66) and W ptqJW ptq \u011b c3\u03f52e2w0t \u00b4 c4\u03f52.5. (C.67) Lemma C.15 shows that the growth rate of W ptqJW ptq and the decay rate of Y ptqJY ptq both depend on the sign of w0. In our analysis we assume w0 \u0105 0 for simplicity of presentation, the same analysis applies for w0 \u0103 0 with opposite roles for Y and W . The proof of Lemma C.15 follows from writing the leading terms of the ODE and bounding the remaining terms by their upper bounds in the time considered. Using these lemmas, we proceed to prove Lemma C.13. Proof of Lemma C.13. Consider the dynamics at time 0 \u010f t \u010f 12w0 ln ` 1 \u03f50.5 \u02d8 . By Lemma C.15, we have Y ptqJY ptq \u010f c1\u03f52e\u00b4 ln 1 \u03f50.5 ` c2\u03f52.5 \u201c pc1 ` c2q\u03f52.5 (C.68) and W ptqJW ptq \u011b c3\u03f52eln 1 \u03f50.5 \u00b4 c4\u03f52.5 \u201c c3\u03f51.5 \u00b4 c4\u03f52.5 (C.69) We can calculate the limit lim \u03f5\u00d10 }Y ptq}2 }W ptq}2 \u010f lim\u03f5\u00d10 pc1 ` c2q\u03f52.5 c3\u03f51.5 \u00b4 c4\u03f52.5 \u201c 0 (C.70) From Lemma C.14, }Aptq}F \u010f M2\u03f5, so we can calculate the limit, lim \u03f5\u00d10 }Aptq}2F }W ptq}2 \u010f lim\u03f5\u00d10 M2\u03f5 2 c3\u03f51.5 \u00b4 c4\u03f52.5 \u201c 0 (C.71) which concludes the proof. Proof of Lemma C.14. At t \u201c 0, by Lemma C.11, }A}F \u0103 2\u03f5 \u0103 M2\u03f5, }B}, }C} \u0103 2\u03f5 \u0103 M1\u03f50.75, where we bound M1 \u0105 2\u03f50.25. If there exists t s.t. }Aptq}F \u0105 M2\u03f5 (otherwise, we get trivially the desired bound for }Aptq}), from continuity and the intermediate value theorem, there exists a maximal tA s.t. for 0 \u010f t \u0103 t1, }Aptq}F \u0103 \u03f5, }Apt1q}F \u201c M2\u03f5. By similar argument for B, C, we can define tB , tC . By taking the minimal value between them, we get T\u02da s.t. @0 \u010f t \u0103 T\u02da, }Aptq}F \u0103 M2\u03f5, }B}, }C} \u0103 M1\u03f5 0.75. We start by proving the lemma under the assumption that 0 \u010f t \u0103 T\u02da. Later, we will show that T\u02da \u0105 T . Let us recall the derivative of B: 9B \u201c \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqiCJ. (C.72) Bounding it by the Cauchy-Schwartz inequality, we have } 9B} \u201c \u203a \u203a \u203a \u203a \u203a \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqiCJ \u203a \u203a \u203a \u203a \u203a \u010f k\u00b41 \u00ff i\u201c0 |\u2207\u2113i| \u203a \u203apAJq \u203a \u203a i F \u203a \u203aCJ \u203a \u203a. (C.73) Denote M \u201c max i p|wi|q ` M21 \u03f51.5, then we have M \u201c max i p|wi|q ` M21 \u03f51.5 \u011b max i p|wi|q ` }C}}B} \u011b max i p|wi|q ` |CB|, (C.74) \u011b max i p|wi|q ` max i p|CAiB|q \u011b max i p|wi| ` |CAiB|q \u011b max i p|\u2207\u2113i|q. (C.75) Recall the conservation law from Lemma C.7 for the norms of B and C: @t, d dt p}Bptq} \u00b4 }Cptq}q \u201c 0. (C.76) By Lemma C.11, we have }Bp0q}, }Cp0q} \u0103 2\u03f5 \u00f1 p|}Bp0q} \u00b4 }Cp0q}|q \u0103 4\u03f5. (C.77) Therefore, we get @t, }Cptq} \u0103 }Bptq} ` 4\u03f5. (C.78) Note also that by assuming M2\u03f5 \u0103 1, we have k\u00b41 \u00ff i\u201c0 pM2\u03f5qi \u0103 k. (C.79) Plugging the various values into equation C.73, we have: } 9B} \u010f k\u00b41 \u00ff i\u201c0 |\u2207\u2113i|}pAJq}iF }CJ} \u0103 Mp}B} ` 4\u03f5q k\u00b41 \u00ff i\u201c0 }pAJq}iF (C.80) \u0103 Mkp}B} ` 4\u03f5q. (C.81) Denoting R \u201c }B}2 \u201c BJB, then 9R \u201c 9BJB ` BJ 9B \u201c 2BJ 9B (C.82) Taking absolute value and then plugging equation C.80 results with | 9R| \u201c |2BJ 9B| \u010f 2}BJ}} 9B} \u0103 2k}BJ}Mp}B} ` 4\u03f5q. (C.83) Using the definition of R, we get that 9|R| \u0103 2kMp|R| ` 4\u03f5 a |R|q (C.84) Notice that for all t s.t. }B} \u201c ? R \u0103 4\u03f5 \u0103 M1\u03f50.75, we are done. For any t2 s.t. a Rpt2q \u0105 4\u03f5, from continuity and the intermediate value theorem, there exist a maximal t1 s.t. 0 \u0103 t1 \u0103 t2, a Rpt1q \u201c 4\u03f5. For all t1 \u0103 t \u0103 t2, a Rptq \u0105 4\u03f5. Thus, @t1 \u0103 t \u010f t2, 9|R| \u0103 4kM |R| (C.85) Integrating by t and substituting, we get that for all t1 \u0103 tint \u0103 t2: \u017c tint t1 1 |R| 9|R|dt \u0103 \u017c tint t1 4kMdt (C.86) \u00f1 \u017c Rptintq Rpt1q 1 |R|dR \u0103 8M rtint \u00b4 t1s \u0103 4kM rT \u00b4 0s (C.87) \u00f1 ln \u02c6 |Rptintq| |Rpt1q| \u02d9 \u0103 4kMT (C.88) \u00f1 |Rptintq| \u0103 expp4kMT q|Rpt1q| \u201c p4\u03f5q2 exp \u02c6 4kM \u00a8 ln \u02c6 1 \u03f50.5 \u02d9\u02d9 (C.89) \u00f1 |Rptintq| \u0103 p4\u03f5q2 expp4kMq 1 \u03f50.5 . (C.90) Denoting M21 \u201c 16 \u00a8 expp4kMq, we have: }Bptintq} \u0103 M1\u03f50.75 (C.91) To summarize, we have shown that for all 0 \u0103 t \u0103 T , there exists M1 s.t }Bptq} \u0103 M1\u03f50.75. Using an identical proof with the roles of CJ,B reversed, we have |Cptq| \u0103 M1\u03f50.75. As we have defined tB , tC as the maximal points s.t. this property occurs for T \u0103 tB , tC . Next, we will bound }A}F . Recall that }A}2F \u201c TrpAJAq, and d dt TrpAJAq \u201c Tr \u02c6 d dt pAJAq \u02d9 (C.92) \u201c Tr \u00b4 9AJA \u00af ` Tr \u00b4 AJ 9A \u00af (C.93) \u201c 2Tr \u00b4 AJ 9A \u00af , (C.94) where we have used the linearity of trace and its invariance to transpose. Recall that 9A \u201c \u00b4 k\u00b41 \u00ff i\u201c1 \u2207\u2113i i\u00b41 \u00ff r\u201c0 pAJqrCJBJpAJqi\u00b4r\u00b41. (C.95) Multiplying it from the left by AJ and then taking trace provides us with TrpAJ 9Aq \u201c \u00b4 k\u00b41 \u00ff i\u201c1 \u2207\u2113i i\u00b41 \u00ff r\u201c0 Tr ` pAJqr`1CJBJpAJqi\u00b4r\u00b41 \u02d8 . (C.96) Taking a transpose and then using the cyclic property of trace and, we have for the terms in the sum: Tr ` pAJqr`1CJBJpAJqi\u00b4r\u00b41 \u02d8 \u201c Tr ` BJpAJqiCJ \u02d8 (C.97) \u201c Tr ` CAiB \u02d8 (C.98) \u201c CAiB (C.99) Thus, we end up with TrpAJ 9Aq \u201c \u00b4 k\u00b41 \u00ff i\u201c1 \u2207\u2113i i\u00b41 \u00ff r\u201c0 CAiB \u201c \u00b4 k\u00b41 \u00ff i\u201c1 \u2207\u2113i \u00a8 i \u00a8 CAiB. (C.100) We use it to bound TrpAJ 9Aq: TrpAJ 9Aq \u010f k\u00b41 \u00ff i\u201c1 |\u2207\u2113i| \u00a8 i \u00a8 |CAiB| (C.101) Using the Cauchy-Schwartz inequality and then plugging M \u0105 |\u2207\u2113i| and the bounds found for }C}, }B} \u0103 M1\u03f50.75 leads to |\u2207\u2113i| \u00a8 i \u00a8 |CAiB| \u010f M \u00a8 i \u00a8 }C}}A}iF }B} \u010f M \u00a8 i \u00a8 M21 \u03f51.5}A}iF (C.102) Putting all together provides us with TrpAJ 9Aq \u010f k\u00b41 \u00ff i\u201c1 M \u00a8 i \u00a8 M21 \u03f51.5}A}iF \u201c M \u00a8 M21 \u03f51.5 k\u00b41 \u00ff i\u201c1 i \u00a8 }A}iF (C.103) Noting that }A}F \u0103 1 and denoting M\u03032 \u201c k 2 2 M \u00a8 M 2 1 leads to k\u00b41 \u00ff i\u201c1 i}A}iF \u0103 }A}F kpk \u00b4 1q 2 \u0103 k 2 2 }A|}F (C.104) Therefore, we get that TrpAJ 9Aq \u0103 M\u03032\u03f51.5}A}F . (C.105) Notice that }A}F \u00a8 d dt p}A}F q \u201c 0.5 d dt p}A}2F q \u201c TrpAJ 9Aq (C.106) \u00f1 }A}F \u00a8 d dt p}A}F q \u0103 M\u03032\u03f51.5}A}F . (C.107) \u00f1 d dt p}A}F q \u0103 M\u03032\u03f51.5 (C.108) Therefore, for any 0 \u010f t3 \u010f T : }Apt3q}F \u00b4 }Ap0q}F \u201c \u017c t3 0 d dt p}A}F qdt \u0103 M\u03032\u03f51.5t3 (C.109) \u00f1 }Apt3q}F \u0103 M\u03032\u03f51.5T ` }Ap0q}F . (C.110) Recall that by Lemma C.11, we have for any x \u0105 0, lnpxq \u0103 x that }Ap0q}F \u0103 4d\u03f5. (C.111) Using the fact that T \u201c 1 2w0 ln \u02c6 1 \u03f50.5 \u02d9 \u0103 1 2w0 \u03f5\u00b40.5 (C.112) we get that \u00f1 }Apt3q}F \u0103 M\u03032 2w0 \u03f5 ` 4d\u03f5. (C.113) Taking M2 \u201c M\u030322w0 ` 4\u03f5, we have for all 0 \u010f t \u010f T that \u00f1 }Aptq}F \u0103 M2\u03f5 (C.114) Once again, recall that we defined tA as the maximal t with this property, thus T \u0103 tA. Since we already have also that T \u0103 tB , tC , we get T \u0103 minpTA, tB , tCq \u201c T\u02da. C.4.1 BOUNDING WJW AND Y JY Proof of Lemma C.15. Denote Y \u201d CJ \u00b4 B, W \u201d CJ ` B. (C.115) Recall that BL BB \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqiCJ, BL BC \u201c B J k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqi (C.116) We can write the change in Y , 9Y \u201c 9CJ \u00b4 9B \u201c \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113iAiB ` k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqiCJ (C.117) \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113i ` pAJqiCJ \u00b4 AiB \u02d8 Denote pAiqS \u201c A i`pAJqi 2 and pA iqS\u0304 \u201c Ai\u00b4pAJqi 2 the symmetric and anti-symmetric parts of A i. We can now write pAJqiCJ \u00b4 AiB \u201c \u201c pAiqS \u00b4 pAiqS\u0304 \u2030 CJ \u00b4 \u201c pAiqS ` pAiqS\u0304 \u2030 B (C.118) \u201c pAiqSpCJ \u00b4 Bq \u00b4 pAiqS\u0304pCJ ` Bq \u201c pAiqSY \u00b4 pAiqS\u0304W Note also that for i \u201c 0, we have A0 \u201c I and its anti-symmetric part is the zero matrix, writing i \u201c 0 separately and assigning equation C.118 into equation C.117, 9Y \u201c \u2207\u21130Y ` k\u00b41 \u00ff i\u201c1 \u2207\u2113i ` pAiqSY \u00b4 pAiqS\u0304W \u02d8 (C.119) Let us look at ddt pY JY q \u201c 9Y JY ` Y J 9Y \u201c 2Y J 9Y . Multiplying equation C.119 from the left with Y J evaluates to Y J 9Y \u201c \u2207\u21130Y JY ` k\u00b41 \u00ff i\u201c1 \u2207\u2113i ` Y JpAiqSY \u00b4 Y JpAiqS\u0304W \u02d8 (C.120) We now turn to bound the terms in the sum. Y J 9Y \u010f \u2207\u21130Y JY ` k\u00b41 \u00ff i\u201c1 |\u2207\u2113i| ` |Y JpAiqSY | ` |Y JpAiqS\u0304W | \u02d8 (C.121) We can bound each term using Cauchy\u2013Schwarz. We first need to bound }Y } and }W }, which are trivially bounded by }Y }, }W } \u010f }C} ` }B} \u010f 2M1\u03f50.75 (C.122) As for the symmetric and anti-symmetric parts of A, }pAiqS}F \u201c \u203a \u203a \u203a \u203a Ai ` pAiqJ 2 \u203a \u203a \u203a \u203a F \u010f 1 2 ` }Ai}F ` }pAiqJ}F \u02d8 \u201c }Ai}F \u010f }A}iF , (C.123) where the last inequality follows again from Cauchy\u2013Schwarz (the same considerations apply for pAiqS\u0304). From Cauchy\u2013Schwarz we can bound |Y JpAiqS\u0304W | \u010f }Y J}}pAiqS\u0304}F }W } \u010f }Y }}A}iF }W }, we denote M3 \u201c maxpM1,M2q and derive: |Y JpAiqS\u0304W | \u010f 2M1\u03f50.75pM2\u03f5qi2M1\u03f50.75 \u0103 4M33 \u03f51.5`i, (C.124) which is maximized when i \u201c 1. We again bound: M \u201c max i p|wi|q ` M21 \u03f51.5 \u0105 |\u2207\u2113i|. We can bound the terms in equation C.121 by |\u2207\u2113i| ` |Y JpAiqSY | ` |Y JpAiqS\u0304W | \u02d8 \u010f 8 \u00a8 M \u00a8 M33 \u03f52.5 (C.125) Plugging back into equation C.121: Y J 9Y \u010f \u2207\u21130Y JY ` k\u00b41 \u00ff i\u201c1 8 \u00a8 M \u00a8 M33 \u03f52.5 (C.126) We can also bound \u2207\u21130 \u201c pCB \u00b4w0q \u010f \u00b4w0 ` |CB| \u010f \u00b4w0 ` }C}}B} \u010f \u00b4w0 `M21 \u03f51.5. Note also that we multiply by Y JY so we can bound \u2207\u21130Y JY \u010f \u00b4w0Y JY ` M21 \u03f51.5pM21\u03f50.75q2 loooooooooomoooooooooon \u201c4M41 \u03f53 (C.127) putting back together, we get Y J 9Y \u010f \u00b4w0Y JY ` 4M41 \u03f53 ` pk \u00b4 1qpM1 ` 1q8 \u00a8 M \u00a8 M33 \u03f52.5 (C.128) In particular, there exists M4 such that Y J 9Y \u010f \u00b4w0Y JY ` M4 2 \u03f52.5 (C.129) Recall that we were interested in bounding ddt pY JY q \u201c 9Y JY ` Y J 9Y \u201c 2Y J 9Y , d dt pY JY q \u010f \u00b42w0Y JY ` M4\u03f52.5 (C.130) Denoting zptq \u201d Y ptqJY ptq and xptq \u201d W ptqJW ptq, and using Lemma C.16, we have the desired bounds zptq \u010f 1 2w0 \u201c p33w0d\u03f52qe\u00b42w0t ` M4\u03f52.5 \u2030 (C.131) In particular, we can write zptq \u010f c1\u03f52e\u00b42w0t ` c2\u03f52.5 (C.132) and xptq \u011b c3\u03f52e2w0t \u00b4 c4\u03f52.5 (C.133) where ci\u2019s are positive constants. Note that the derivation of xptq is exactly the same as zptq with opposite signs and bounding from below instead. C.4.2 INTEGRAL BOUND OF }Y }2 AND }W }2 Lemma C.16. Assume 9z \u0103 \u00b42w0z ` M4\u03f52.5 \u0103 0, 9x \u0105 2w0z \u00b4 M4\u03f52.5 \u0105 0 , where w0 \u0105 0. Then, under the assumptions of Lemma C.11: zpt1q \u0103 1 2w0 ` expp\u00b42w0t1q \u00a8 p75w0d\u03f52q ` M4\u03f52.5 \u02d8 (C.134) xpt2q \u0105 1 2w0 \u02c6 expp2w0t2q \u00a8 p w0\u03f5 2 25d q ` M4\u03f52.5 \u02d9 (C.135) Proof of Lemma C.16. Assume 9z \u0103 \u00b42w0z ` M4\u03f52.5 , where w0 \u0105 0, 2w0z \u0105 M4\u03f52.5. Similarly, assume 9x \u0105 2w0x \u00b4 M4\u03f52.5. Then: 9z 2w0z \u00b4 M4\u03f52.5 \u0103 \u00b41 (C.136) 9x 2w0x \u00b4 M4\u03f52.5 \u0105 1 (C.137) Integrating both sides by dt, and using integration by substitution, we get: \u00b4t1 \u201c \u017c t1 0 \u00b41 \u0105 \u017c t1 0 1 2w0z \u00b4 M4\u03f52.5 dz dt dt \u201c \u017c zpt1q zp0q 1 2w0z \u00b4 M4\u03f52.5 dz (C.138) t2 \u201c \u017c t2 0 1 \u0103 \u017c t2 0 1 2w0x \u00b4 M4\u03f52.5 dx dt dt \u201c \u017c xpt2q xp0q 1 2w0x \u00b4 M4\u03f52.5 dx (C.139) We note: \u017c zpt1q zp0q 1 2w0z \u00b4 M4\u03f52.5 dz \u201c 1 2w0 rlnp2w0zpt1q \u00b4 M4\u03f52.5q \u00b4 lnp2w0zp0q \u00b4 M4\u03f52.5qs (C.140) \u00f1 \u017c zpt1q zp0q 1 2w0z \u00b4 M4\u03f52.5 dz \u201c 1 2w0 \u201e ln \u02c6 2w0zpt1q \u00b4 M4\u03f52.5 2w0zp0q \u00b4 M4\u03f52.5 \u02d9\u0237 (C.141) \u017c xpt2q xp0q 1 2w0x \u00b4 M4\u03f52.5 dx \u201c 1 2w0 \u201e ln \u02c6 2w0xpt2q \u00b4 M4\u03f52.5 2w0xp0q \u00b4 M4\u03f52.5 \u02d9\u0237 (C.142) Combining equations, we have: 1 2w0 \u201e ln \u02c6 2w0zpt1q \u00b4 M4\u03f52.5 2w0zp0q \u00b4 M4\u03f52.5 \u02d9\u0237 \u0103 \u00b4t1 (C.143) \u00f1 ln \u02c6 2w0zpt1q \u00b4 M4\u03f52.5 2w0zp0q \u00b4 M4\u03f52.5 \u02d9 \u0103 \u00b42w0t1 (C.144) \u00f1 2w0zpt1q \u00b4 M4\u03f5 2.5 2w0zp0q \u00b4 M4\u03f52.5 \u0103 expp\u00b42w0t1q (C.145) \u00f1 2w0zpt1q \u00b4 M4\u03f52.5 \u0103 expp\u00b42w0t1q \u00a8 ` 2w0zp0q \u00b4 M4\u03f52.5 \u02d8 (C.146) \u00f1 zpt1q \u0103 1 2w0 \u201c expp\u00b42w0t1q \u00a8 p2w0zp0q \u00b4 M4\u03f52.5q ` M4\u03f52.5 \u2030 (C.147) xpt2q \u0105 1 2w0 \u201c expp2w0t2q \u00a8 p2w0xp0q \u00b4 M4\u03f52.5q ` M4\u03f52.5 \u2030 (C.148) We note, zp0q \u201c Y T p0qY p0q. By linearity of sum of variances, Y p0q\u2019s entries are distributed according to N p ? 2 \u00a8 \u03f5q, by Lemma C.11: ? 2 2 \u03f5 \u0103 Y p0q \u0103 3 ? 2 2 \u03f5 (C.149) From Cauchy-Schwartz, zp0q \u0103 3\u03f52 with high probability. W is distributed as Y , therefore, similarly, xp0q \u0105 12\u03f5 2. Assuming M4\u03f52.5 \u0103 w0\u03f5 2 2 , we have: zpt1q \u0103 1 2w0 ` expp\u00b42w0t1q \u00a8 p6w0\u03f52q ` M4\u03f52.5 \u02d8 (C.150) xpt2q \u0105 1 2w0 ` expp2w0t2q \u00a8 pw0\u03f52q ` M4\u03f52.5 \u02d8 (C.151) Concluding the proof.", "agent_review": [{"role": "user", "content": "Review the following paper for International Conference on Learning Representations (ICLR) 2023. Remember you need to make 3 reviewer agents for each paper. GIVE A FINAL DECISION in the form of \"FINAL DECISION: <Accept/Reject>\". The paper title is: Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets\n\n1 INTRODUCTION: Neural Networks (NNs) are often overparameterized, in the sense that their representational capacity far exceeds what is necessary for fitting training data. Surprisingly, training overparameterized NNs via (variants of) Gradient Descent (GD) tends to produce solutions that generalize well, despite existence of many solutions that do not. This implicit generalization phenomenon attracted considerable scientific interest, resulting in various theoretical explanations (see, e.g., Woodworth et al. (2020); Yun et al. (2020); Zhang et al. (2017); Li et al. (2020); Ji & Telgarsky (2018); Lyu & Li (2019)). Recent studies have surfaced a new form of implicit bias which arises in Recurrent Neural Networks (RNNs) and their variants (e.g., Long Short-Term Memory Hochreiter & Schmidhuber (1997) and Gated Recurrent Units Chung et al. (2014)). For such models, the length of sequences in training is often shorter than in testing, and it is not clear to what extent a learned solution will be able to extrapolate beyond the sequence lengths seen in training. In the overparameterized regime, where the representational capacity of the learned model exceeds what is necessary for fitting short sequences, there may exist solutions which generalize but do not extrapolate, meaning that their accuracy is high over short sequences but arbitrarily poor over long ones (see Cohen-Karlik et al. (2022)). In practice however, when training RNNs using GD, accurate extrapolation is often observed. We refer to this phenomenon as the implicit extrapolation of GD. As opposed to the implicit generalization of GD, little is formally known about its implicit extrapolation. Existing theoretical analyses of the latter focus on linear RNNs \u2014 also known as Linear Dynamical Systems (LDS) \u2014 and either treat infinitely wide models (Emami et al., 2021), or models of finite width that learn from a memoryless teacher (Cohen-Karlik et al., 2022). In these regimes, GD has been argued to exhibit an implicit bias towards short-term memory. While such results are informative, their generality remains in question, particularly since infinitely wide NNs are known to substantially differ from their finite-width counterparts, and since a memoryless teacher essentially neglects the main characteristic of RNNs (memory). In this paper, we theoretically investigate the implicit extrapolation of GD when applied to overparameterized finite-width linear RNNs learning from a teacher with memory. We consider models with symmetric transition matrices, in the case where a student (learned model) with state space dimension d is trained on sequences of length k generated by a teacher with state space dimension d\u0302. Our interest lies in the overparameterized regime, where d is greater than both k and d\u0302, meaning that the student has state space dimension large enough to fully agree with the teacher on sequences of length k, while potentially disagreeing with it on longer sequences. As a necessary assumption on initialization, we follow prior work and focus on a certain balancedness condition, which is known (see experiments in Cohen-Karlik et al. (2022), as well as our theoretical analysis) to capture near-zero initialization as commonly employed in practice. Our main theoretical result states that GD originating from a balanced initialization leads the student to extrapolate, irrespective of how large its state space dimension is. Key to the result is a surprising connection to a moment matching theorem from Cohen & Yeredor (2011), whose proof relies on ideas from compressed sensing (Elad, 2010; Eldar & Kutyniok, 2012) and neighborly polytopes (Gale, 1963). This connection may be of independent interest, and in particular may prove useful in deriving other results concerning implicit properties of GD. We corroborate our theory with experiments, which demonstrate extrapolation via learning low dimensional state spaces in both the analyzed setting and ones involving non-linear RNNs. The implicit extrapolation of GD is a new and exciting area of inquiry. Our results suggest that short-term memory is not enough for explaining it as previously believed. We hope the techniques developed in this paper will contribute to a further understanding of this phenomenon. 2 RELATED WORK: The study of linear RNNs, or LDS, has a rich history dating back to at least the early works of Kalman (Kalman, 1960; 1963). An extensively studied question relevant to extrapolation is that of system identification, which explores when the parameters of a teacher LDS can be recovered (see Ljung (1999)). Another related topic concerns finding compact realizations of systems, i.e. realizations of the same input-output mapping as a given LDS, with a state space dimension that is lower (see Antoulas (2005)). Despite the relation, our focus is fundamentally different from the above \u2014 we ask what happens when one learns an LDS using GD. Since GD is not explicitly designed to find a low dimensional state space, it is not clear that the application of GD to an overparameterized student allows system identification through a compact realization. The fact that it does relates to the implicit properties of GD, and to our knowledge has not been investigated in the classic LDS literature. The implicit generalization of GD in training RNNs has been a subject of theoretical study for at least several years (see, e.g., Hardt et al. (2016); Allen-Zhu et al. (2019); Lim et al. (2021)). In contrast, works analyzing the implicit extrapolation of GD have surfaced only recently, specifically in Emami et al. (2021) and Cohen-Karlik et al. (2022).1 Emami et al. (2021) analyzes linear RNNs in the infinite width regime, suggesting that in this case GD is implicitly biased towards impulse responses corresponding to short-term memory. Cohen-Karlik et al. (2022) studies finite-width linear RNNs (as we do), showing that when the teacher is memoryless (has state space dimension zero), GD emanating from a balanced initialization successfully extrapolates. Our work tackles an arguably more realistic and challenging setting \u2014 we analyze the regime in which the teacher has memory. Our results suggest that the implicit extrapolation of GD does not originate from a bias towards short-term memory, but rather a tendency to learn low dimensional state spaces. On the empirical side, extrapolation of NNs to sequence lengths beyond those seen in training has been experimentally demonstrated in numerous recent works, covering both modern language 1We note that there have been works studying extrapolation in the context of non-recurrent NNs, e.g. Xu et al. (2020). This type of extrapolation deals with the behavior of learned functions outside the support of the training distribution, and is therefore fundamentally different from the type of extrapolation we consider (which deals with the behavior of learned functions over sequences longer than those seen in training). and attention models (Press et al., 2022; Anil et al., 2022; Zhang et al., 2022), and RNNs with transition matrices of particular forms (Gu et al., 2022; 2021; 2020; Gupta, 2022). The current paper is motivated by these findings, and takes a step towards theoretically explaining them. 3 LINEAR RECURRENT NEURAL NETWORKS: Our theoretical analysis applies to single-input single-output (SISO) linear RNNs with symmetric transition matrices. Given a state space dimension d P N, this model is defined by the update rule: st`1 \u201c Ast ` Bxt, yt \u201c Cst, t \u201c 0, 1, 2, . . . , (3.1) where A P Rd\u02c6d, B P Rd\u02c61 and C P R1\u02c6d are configurable parameters, with the transition matrix A satisfying A \u201c AJ; x0, x1, . . . P R form an input sequence; y0, y1, . . . P R form the corresponding output sequence; and st P Rd\u02c61 represents the internal state at time t, assumed to be equal to zero at the outset (i.e. it is assumed that s0 \u201c 0). As with any linear time-invariant system (Porat, 1996), the input-output mapping realized by the RNN is determined by its impulse response. Definition 1. The impulse response of the RNN is the output sequence corresponding to the input sequence px0, x1, x2, . . .q \u201c p1, 0, 0, . . .q. Namely, it is the sequence pCB,CAB,CA2B, . . . q. For brevity, we employ the shorthand \u0398 :\u201c pA,B,Cq. The d \u02c6 d symmetric transition matrix A is parameterized through a dpd ` 1q{2-dimensional vector holding its upper triangular elements, and with a slight overloading of notation, the symbol A is also used to refer to this parameterization. We note that our theory readily extends to multiple-input multiple-output (MIMO) networks, and the focus on the SISO case is merely for simplicity of presentation. Note also that the restriction to symmetric transition matrices is customary in both theory (Hazan et al., 2018) and practice (Gupta, 2022), and represents a generalization of the canonical modal form, which under mild non-degeneracy conditions does not limit generality (Boyd & Lessard, 2006). Given a length k input sequence, x \u201c px0, . . . , xk\u00b41q P Rk, consider the output at the last time step, i.e. y :\u201c yk P R, and denote it by RNNpxq. Using this output as a label, we define an empirical loss induced by a training set S \u201c \u2423` xp1q, yp1q \u02d8 , . . . , ` xpNq, ypNq \u02d8( \u0102 Rk \u02c6 R: LSpA,B,Cq \u201c 1 N N \u00ff i\u201c1 \u2113 \u00b4 RNN \u00b4 xpiq \u00af , ypiq \u00af , (3.2) where \u2113py, y\u0302q \u201c py \u00b4 y\u0302q2 is the square loss. By the update rule of the RNN (Equation 3.1), we have: LSpA,B,Cq \u201c 1 N N \u00ff i\u201c1 \u02dc k\u00b41 \u00ff j\u201c0 CAk\u00b41\u00b4jBx piq j \u00b4 y piq \u00b82 . (3.3) Suppose that ground truth labels are generated by an RNN as defined in Equation 3.1, and denote the state space dimension and parameters of this teacher network by d\u0302 and \u0398\u0302 \u201c pA\u0302, B\u0302, C\u0302q respectively. We employ the common assumption (e.g., see Hardt et al. (2016)) by which input sequences are drawn from a whitened distribution, i.e. a distribution where E rxjxj1 s equals 1 if j \u201c j1 and 0 otherwise. The population loss over length k sequences can then be written as (see Lemma C.1): LpA,B,Cq \u201c k\u00b41 \u00ff j\u201c0 \u00b4 CAjB \u00b4 C\u0302A\u0302jB\u0302 \u00af2 . (3.4) Equation 3.4 implies that a solution \u0398 \u201c pA,B,Cq achieves zero population loss over length k sequences if and only if CAjB \u201c C\u0302A\u0302jB\u0302 for j \u201c 0, . . . , k \u00b4 1. To what extent does such a solution imply that the student (i.e., the learned RNN) extrapolates to longer sequences? This depends on how close CAjB is to C\u0302A\u0302jB\u0302 for j \u011b k. Definition 2. For \u03f5 \u011b 0 and q P N, we say that the student \u03f5-extrapolates with horizon q with respect to (w.r.t) the teacher if: |CAjB \u00b4 C\u0302A\u0302jB\u0302| \u010f \u03f5, @j P t0, 1, . . . , q \u00b4 1u . (3.5) If the above holds for all q P N then the student is said to \u03f5-extrapolate w.r.t the teacher, and if it: holds for all q P N with \u03f5 \u201c 0 then the student is simply said to extrapolate w.r.t the teacher. Per Definition 2, \u03f5-extrapolation with horizon q is equivalent to the first q elements of the student\u2019s impulse response being \u03f5-close to those of the teacher\u2019s, whereas extrapolation means that the student\u2019s impulse response fully coincides with the teacher\u2019s. The latter condition implies that the student realizes the same input-output mapping as the teacher, for any sequence length (this corresponds to the notion of system identification; see Section 2). When the student is overparameterized, in the sense that d is greater than k and d\u0302, it may perfectly generalize, i.e. lead the population loss over length k sequences (Equation 3.4) to equal zero, and yet fail to extrapolate as stated in the following proposition. Proposition 3. Assume d \u0105 k, and let \u03f5 \u011b 0 and q P tk ` 1, k ` 2, . . .u. Then, for any teacher parameters \u0398\u0302, there exist student parameters \u0398 with which the population loss in Equation 3.4 equals zero, and yet the student does not \u03f5-extrapolate with horizon q. Proof sketch (for complete proof see Appendix C.1.2). The result follows from the fact that the first d elements of the student\u2019s impulse response can be assigned freely via a proper choice of \u0398. We are interested in the extent to which student parameters learned by GD extrapolate in the overparameterized regime. Proposition 3 implies that, regardless of how many (length k) sequences are used in training, if GD leads to any form of extrapolation, it must be a result of some implicit bias induced by the algorithm. Note that in our setting, extrapolation cannot be explained via classic tools from statistical learning theory, as evaluation over sequences longer than those seen in training violates the standard assumption of train and test data originating from the same distribution. To decouple the question of extrapolation from that of generalization, we consider the case where the training set S is large, or more formally, where the empirical loss LSp\u00a8q (Equation 3.3) is well represented by the population loss Lp\u00a8q (Equation 3.4). We model GD with small step size via Gradient Flow (GF), as customary in the theory of NNs \u2014 see Saxe et al. (2013); Gunasekar et al. (2017); Arora et al. (2018; 2019b); Lyu & Li (2019); Li et al. (2020); Azulay et al. (2021) for examples and Elkabetz & Cohen (2021) for theoretical justification. That is, we analyze the following dynamics: 9\u03b1p\u03c4q :\u201c d d\u03c4 \u03b1p\u03c4q \u201c \u00b4 BB\u03b1L ` Ap\u03c4q,Bp\u03c4q,Cp\u03c4q \u02d8 , \u03c4 \u011b 0 , (3.6) where \u03b1 P tA,B,Cu. If no assumption on initialization is made, no form of extrapolation can be established (indeed, the initial point may be a global minimizer of Lp\u00a8q that fails to extrapolate, and GF will stay there). Following prior work (see Cohen-Karlik et al. (2022)), we assume that initialization adheres to the following balancedness condition: Definition 4. An RNN with parameters \u0398 \u201c pA,B,Cq is said to be balanced if B \u201c CJ. It was shown empirically in Cohen-Karlik et al. (2022) that the balancedness condition captures near-zero initialization as commonly employed in practice. We support this finding theoretically in Section 4.3. Aside from the initialization of the student, we will also assume that the teacher adheres to the balancedness condition. 4 THEORETICAL ANALYSIS: We turn to our theoretical analysis. Section 4.1 proves that in the setting of Section 3, convergence of GF to a zero loss solution leads the student to extrapolate, irrespective of how large its state space dimension is. Section 4.2 extends this result by establishing that, under mild conditions, approximate convergence leads to approximate extrapolation. The results of sections 4.1 and 4.2 assume that GF emanates from a balanced initialization, which empirically is known to capture near-zero initialization as commonly employed in practice (see Section 3). Section 4.3 theoretically supports this empirical premise, by showing that with high probability, random near-zero initialization leads to balancedness. We introduce notations that will be used throughout the analysis. For a matrix Q P Rm\u02c6n, we let }Q}F , }Q}8 and }Q}2 denote the Frobenius, \u21138 and \u21132 (spectral) norms, respectively. For a vector v P Rm, we use }v} to denote the Euclidean norm and vi to denote its ith entry. 4.1 CONVERGENCE LEADS TO EXTRAPOLATION: Theoretical analyses of implicit generalization often assume convergence to a solution attaining zero loss (see, e.g., Azulay et al. (2021); Gunasekar et al. (2017); Lyu & Li (2019); Woodworth et al. (2020)). Under such an assumption, Theorem 5 below establishes implicit extrapolation, i.e. that the solution to which GF converges extrapolates, irrespective of how large the student\u2019s state space dimension d is. A condition posed by the theorem is that the training sequence length k is greater than two times the teacher\u2019s state space dimension d\u0302. We empirically demonstrate the necessity of this condition in Section 5, by showing that smaller values for k lead to failure in extrapolation. Theorem 5. Assume that d \u0105 k \u0105 2d\u0302, the teacher parameters \u0398\u0302 are balanced (Definition 4), and the student parameters \u0398 are learned by applying GF to the loss Lp\u00a8q (Equation 3.4) starting from a balanced initialization. Then, if GF converges to a point \u0398\u02da satisfying Lp\u0398\u02daq \u201c 0, this point extrapolates (Definition 2). In order to prove Theorem 5, we introduce two lemmas: Lemma 6, which shows that balancedness is preserved under GF; and Lemma 7, which (through a surprising connection to a moment problem from statistics) establishes that a balanced solution attaining zero loss necessarily extrapolates. With Lemmas 6 and 7 in place, the proof of Theorem 5 readily follows. Lemma 6. Let \u0398p\u03c4q, with \u03c4 \u011b 0, be a curve brought forth by applying GF to the loss Lp\u00a8q starting from a balanced initialization. Then, \u0398p\u03c4q is balanced for every \u03c4 \u011b 0. Proof sketch (for complete proof see Appendix C.1.5). The result follows from the symmetric role of B and C in the loss Lp\u00a8q. Lemma 7. Suppose that d \u0105 k \u0105 2d\u0302, the teacher is balanced, and that the student parameters \u0398 are balanced and satisfy Lp\u0398q \u201c 0. Then \u0398 extrapolates. Proof sketch (for complete proof see Appendix C.2). The proof is based on a surprising connection to the moment problem from statistics (recovery of a probability distribution from its moments), which has been studied for decades (see, e.g., Schm\u00fcdgen (2017)). Without loss of generality, we may assume that A\u0302 is diagonal (if this is not the case then we apply an orthogonal eigendecomposition to A\u0302 and subsequently absorb eigenvectors into B\u0302 and C\u0302). We may also assume that C\u0302B\u0302 \u201c 1 (otherwise we absorb a scaling factor into B and/or C). Since C\u0302J \u201c B\u0302 (teacher is balanced), we may define a probability vector (i.e. a vector with non-negative entries summing up to one) p\u0302 P Rd\u0302 via p\u0302i \u201c C\u0302iB\u0302i, i \u201c 1, . . . , d\u0302. We let Z\u0302 denote the random variable whose support is tA\u030211, . . . , A\u0302d\u0302d\u0302u and which assumes the value A\u0302ii with probability p\u0302i, i \u201c 1, . . . , d\u0302. Notice that for every j P N: C\u0302A\u0302jB\u0302 \u201c \u00ffd\u0302 i\u201c1 p\u0302iA\u0302 j ii \u201c ErZ\u0302 js , meaning that the elements of the teacher\u2019s impulse response are precisely the moments of Z\u0302. Similarly to above we may assume A is diagonal, and since Lp\u0398q \u201c 0 it holds that CB \u201c C\u0302B\u0302 \u201c 1. We may thus define a probability vector p P Rd via pi \u201c CiBi, i \u201c 1, . . . , d, and a random variable Z which assumes the value Aii with probability pi, i \u201c 1, . . . , d. For every j P N: CAjB \u201c \u00ffd i\u201c1 piA j ii \u201c ErZ js , and so the elements of the student\u2019s impulse response are precisely the moments of Z. The probabilistic formulation we set forth admits an interpretation of extrapolation as a moment problem. Namely, since Lp\u0398q \u201c 0 (i.e. CAjB \u201c C\u0302A\u0302jB\u0302 for j \u201c 0, . . . , k\u00b41) the random variables Z and Z\u0302 agree on their first k \u00b4 1 moments, and the question is whether they agree on all higher moments as well. We note that this question is somewhat more challenging than that tackled in classic instances of the moment problem, since the support of the random variable whose moments we match (Z) is not known to coincide with the support of the random variable we seek to recover (Z\u0302). Luckily, a recent powerful result allow addressing the question we face \u2014 Cohen & Yeredor (2011) showed that the first 2n moments of a discrete random variable X taking at most n P N values uniquely define X , in the sense that any discrete random variable agreeing with these 2n moments must be identical to X . Translating this result to our setting, we have that if Z agrees with Z\u0302 on its first 2d\u0302 moments, it must be identical to Z\u0302, and in particular it must agree with Z\u0302 on all higher moments as well. The fact that k \u00b4 1 \u011b 2d\u0302 then concludes the proof. To attain some intuition for the result we imported from Cohen & Yeredor (2011), consider the simple case where d\u0302 \u201c 1. The transition matrix A\u0302 is then a scalar a\u0302 P R, the random variable Z\u0302 is deterministically equal to a\u0302, and the teacher\u2019s impulse response is given by the moments ErZ\u0302js \u201c a\u0302j , j \u201c 0, 1, . . .. Since by assumption k \u0105 2d\u0302, the fact that Lp\u0398q \u201c 0 means the random variable corresponding to the student, Z, agrees with the first two moments of Z\u0302. That is, Z satisfies ErZs \u201c a\u0302 and ErZ2s \u201c a\u03022. This implies that VarrZs \u201c ErZ2s \u00b4 ErZs2 \u201c 0, and therefore Z is deterministically equal to a\u0302, i.e. it is identical to Z\u0302. The two random variables thus agree on all of their moments, meaning the impulse responses of the student and teacher are the same. Proof of Theorem 5. By Lemma 6 (as well as continuity considerations) \u0398\u02da is balanced. Lemma 7 then implies that it extrapolates. 4.2 APPROXIMATE CONVERGENCE LEADS TO APPROXIMATE EXTRAPOLATION: Theorem 5 in Section 4.1 proves extrapolation in the case where GF converges to a zero loss solution. Theorem 8 below extends this result by establishing that, as long as the teacher is stable, approximate convergence leads to approximate extrapolation. Theorem 8. Assume the conditions of Theorem 5, and that the teacher parameters \u0398\u0302 are stable, i.e. the singular values of A\u0302 are in r\u00b41, 1s. Assume also that \u0398\u0302 are non-degenerate, in the sense that the input-output mapping they realize is not identically zero. Then, for any \u03f5 \u0105 0 and q P N, there exists \u03b4p\u03f5, qq \u0105 0 which admits the following: whenever GF leads the student parameters \u0398 to within (Frobenius) distance \u03b4p\u03f5, qq from a balanced point \u0398\u02da satisfying Lp\u0398\u02daq \u201c 0, the student \u03f5-extrapolates with horizon q. Proof sketch (for complete proof see Appendix C.3). In order to show \u03f5-extrapolation with horizon q we need to show that @\u03f5 \u0105 0, q P N, D\u03b4p\u03f5, qq \u0105 0 such that for any \u03981 satisfying ||\u03981 \u00b4 \u0398\u02da|| \u0103 \u03b4, we have for n \u201c 0, . . . , q that C 1pA1qnB1 \u00b4 C\u0302A\u0302nB\u0302 \u201c Op\u03f5q. (4.1) First, by Theorem 5 \u0398\u02da is an extrapolating solution. Following the interpretation of discrete random variables of Lemma 7 proof, we can bound the Wasserstein-1 distance between the extrapolating distribution defined by \u0398\u02da and the approximating distribution defined by \u03981. From Proposition 2 in Wu & Yang (2020), we have that if |C 1pA1qnB1 \u00b4 C\u02dapA\u02daqnB\u02da| \u010f \u03b4 for n P t0, . . . , 2d\u0302u, then W1p\u03981,\u0398\u02daq \u201c O \u00b4 \u03b4 1 4d\u0302 \u00af . (4.2) The second part of the proof ties the error at step p with the Wp distance (Biswas & Mackey, 2021): C 1pA1qpB1 \u00b4 C\u02dapA\u02daqpB\u02da \u010f Wp. (4.3) The final step of the proof is based on Panaretos & Zemel (2019) and bounds Wp in terms of W1, i.e., Wpp\u0398,\u0398\u02daq \u201c Sp\u00b41W1p\u0398,\u0398\u02daq, (4.4) where S is the difference between the maximal and minimal eigenvalues in the teacher and student support (due to our assumption on convergence, S is bounded). Combining the three steps above, we have that if |C 1pA1qnB1 \u00b4 C\u02dapA\u02daqnB\u02da| \u010f \u03b4 for n P t0, . . . , 2d\u0302u, then @p \u0105 n C 1pA1qpB1 \u00b4 C\u02dapA\u02daqpB\u02da \u010f Sp\u00b41O \u00b4 \u03b4 1 4d\u0302 \u00af . (4.5) By setting \u03b4 \u0103 \u00b4 \u03f5 c1\u00a8Sq\u00b41 \u00af4d\u0302 the RHS of Equation 4.5 satisfies c1Sp\u00b41\u03b4 1 4d\u0302 \u0103 \u03f5. 4.3 BALANCEDNESS CAPTURES NEAR-ZERO INITIALIZATION: Theorems 5 and 8 assume that GF emanates from a balanced initialization, i.e. from a point \u0398 \u201c pA,B,Cq satisfying B \u201c CJ. It was shown in Cohen-Karlik et al. (2022) that theoretical predictions derived assuming balanced initialization faithfully match experiments conducted with near-zero initialization (an initialization commonly used in practice). Proposition 9 below theoretically supports this finding, establishing that with high probability, random near-zero initialization leads GF to arrive at an approximately balanced point, i.e. a point \u0398 \u201c pA,B,Cq for which the difference between B and CJ is negligible compared to their size. Proposition 9. Suppose that: (i) d \u0105 4; (ii) the teacher parameters \u0398\u0302 are balanced and are non-degenerate, in the sense that the input-output mapping they realize is not identically zero; and (iii) the student parameters are learned by applying GF to the loss Lp\u00a8q. Let \u0398\u0303 be a random point in parameter space, with entries drawn independently from the standard normal distribution. For \u03f5 \u0105 0, consider the case where GF emanates from the initialization \u03f5\u0398\u0303, and denote the resulting curve by \u0398\u03f5p\u03c4q \u201c pA\u03f5p\u03c4q,B\u03f5p\u03c4q,C\u03f5p\u03c4qq, with \u03c4 \u011b 0. Then, w.p. at least 0.75, for every \u03f5 \u0105 0 there exists \u03c4\u03f5 \u011b 0 such that: lim \u03f5\u00d10` ||B\u03f5p\u03c4\u03f5q \u00b4 CJ\u03f5 p\u03c4\u03f5q||F ||B\u03f5p\u03c4\u03f5q ` CJ\u03f5 p\u03c4\u03f5q||F \u201c 0 . (4.6) Proof sketch (for complete proof see Appendix C.4). The idea behind the proof is as follows. Assume \u03f5 is sufficiently small. Then, when the entries of \u0398 \u201c pA,B,Cq are on the order of \u03f5, we have BBBLp\u0398q \u00ab \u00b42C\u0302B\u0302 \u00a8 C J and BBCLp\u0398q \u00ab \u00b42C\u0302B\u0302 \u00a8 B J. This implies that during the first part of the curve \u0398\u03f5p\u03c4q it holds that dd\u03c4 pB\u03f5p\u03c4q \u00b4 C J \u03f5 p\u03c4qq \u00ab \u00b42C\u0302B\u0302 \u00a8 pB\u03f5p\u03c4q \u00b4 CJ\u03f5 p\u03c4qq and similarly d d\u03c4 pB\u03f5p\u03c4q ` C J \u03f5 p\u03c4qq \u00ab 2C\u0302B\u0302 \u00a8 pB\u03f5p\u03c4q ` CJ\u03f5 p\u03c4qq. Since C\u0302B\u0302 \u0105 0 (follows from the teacher parameters being balanced and non-degenerate), the entries of B\u03f5p\u03c4q \u00b4 CJ\u03f5 p\u03c4q shrink exponentially fast while those of B\u03f5p\u03c4q ` CJ\u03f5 p\u03c4q grow at the same rate. This exponential shrinkage/growth leads }B\u03f5p\u03c4q \u00b4 CJ\u03f5 p\u03c4q} L }B\u03f5p\u03c4q ` CJ\u03f5 p\u03c4q} to become extremely small, more so the smaller \u03f5 is. 5 EXPERIMENTS: In this section we present experiments corroborating our theoretical analysis (Section 4). The latter establishes that, under certain conditions, a linear RNN with state space dimension d extrapolates when learning from a teacher network with state space dimension d\u0302 via training sequences of length k, irrespective of how large d is compared to d\u0302 and k. A key condition underlying the result is that k is larger than 2d\u0302. Section 5.1 below considers the theoretically analyzed setting, and empirically evaluates extrapolation as k varies. Its results demonstrate a phase transition, in the sense that extrapolation takes place when k \u0105 2d\u0302, in compliance with theory, but on the other hand fails when k falls below 2d\u0302, in which case the theory indeed does not guarantee extrapolation. Section 5.2 displays the same phenomenon with linear RNNs that do not adhere to some of the assumptions made by the theory (in particular the assumption of symmetric transition matrices, and those concerning balancedness). Finally, Section 5.3 considers non-linear RNNs (specifically, Gated Recurrent Unit networks Chung et al. (2014)), and shows that they too exhibit a phase transition in extrapolation as the training sequence length varies. For brevity, we defer some of the details of our implementation, as well as additional experiments, to Appendix A. 5.1 THEORETICALLY ANALYZED SETTING: Our first experiment considers the setting described in Section 3 and theoretically analyzed in Section 4. As representative values for the state space dimensions of the teacher and (overparameterized) student, we choose d\u0302 \u201c 5 and d \u201c 40 respectively (higher state space dimensions for the student, namely d \u201c 100 and d \u201c 200, yielded qualitatively identical results). For a given training sequence length k, the student is learned via GD applied directly to the population loss defined in Equation 3.4 (applying GD to the empirical loss defined in Equation 3.3, with N \u201c 10, 000 training examples, led to similar results). Figure 1(a) reports the extrapolation error (quantified by the \u21138 distance between the impulse response of the learned student and that of the teacher) as a function of k. As can be seen, extrapolation exhibits a phase transition that accords with our theory: when k \u0105 2d\u0302 extrapolation error is low, whereas when k falls below 2d\u0302 extrapolation error is high. 5.2 OTHER SETTINGS WITH LINEAR RECURRENT NEURAL NETWORKS: To assess the generality of our findings, we experiment with linear RNNs in settings that do not adhere to some of the assumptions made by our theory. Specifically, we evaluate settings in which: (i) the teacher is unbalanced, meaning B\u0302 \u2030 C\u0302J, and its transition matrix A\u0302 is non-symmetric; (ii) the student\u2019s transition matrix A is not restricted to be symmetric; (iii) learning is implemented by optimizing the empirical loss defined in Equation 3.3 (rather than the population loss defined in Equation 3.4); and (iv) optimization is based on Adam Kingma & Ba (2014) (rather than GD), emanating from standard near-zero initialization which is generally unbalanced (namely, B \u2030 CJ). Figure 1(b) reports the results of an experiment where the state space dimensions of the teacher and (overparameterized) student are d\u0302 \u201c 10 and d \u201c 50 respectively (higher state space dimensions for the student, namely d \u201c 100 and d \u201c 200, yielded qualitatively identical results), and where the teacher implements a delay line of d\u0302 time steps (for details see Appendix B.2.2). Similar results obtained with randomly generated teachers are reported in Appendix A. As can be seen, despite the fact that our theory does not apply to the evaluated settings, its conclusions still hold \u2014 extrapolation error is low when the training sequence length k is greater than 2d\u0302, and high when k falls below 2d\u0302. 5.3 NON-LINEAR RECURRENT NEURAL NETWORKS: As a final experiment, we explore implicit extrapolation with non-linear RNNs, namely GRU networks. Specifically, we evaluate the extent to which a student GRU with state space dimension dg \u201c 100 extrapolates when learning from a teacher GRU with state space dimension d\u0302g \u201c 10 (higher state space dimensions for the student, namely dg \u201c 200 and dg \u201c 500, yielded qualitatively identical results). The student is learned by optimizing an empirical loss comprising training sequences of length kg , where kg is predetermined. Optimization is based on Adam emanating from standard nearzero initialization. Figure 2(a) reports the extrapolation error (quantified by the \u21138 distance between the response of the learned student and that of the teacher, averaged across randomly generated input sequences) for different choices of kg. As can be seen, similarly to the case with linear RNNs (see Sections 5.1 and 5.2), there exists a critical threshold for the training sequence length kg , above which extrapolation error is low and below which extrapolation error is high.2 Figure 2(b) displays the average output response over different inputs of the teacher alongside those of two students \u2014 one trained with sequences of length kg \u201c 30, and the other with sequences of length kg \u201c 60.3 As 2Note that this critical threshold is around four times the teacher\u2019s state space dimension, whereas with linear RNNs the critical threshold was around two times the teacher\u2019s state space dimension. Theoretically explaining this difference is viewed as an interesting direction for future work. 3Note that with GRU networks, in contrast to linear RNNs, the impulse response does not identify the input-output mapping realized by a network. It is presented in Figure 2(b) for demonstrative purposes. expected, the impulse response of each student tracks that of the teacher for the first kg time steps (where kg is student-dependent). However, while the student for which kg \u201c 30 fails to track the teacher beyond kg time steps, the student for which kg \u201c 60 succeeds, thereby exemplifying implicit extrapolation. 6 CONCLUSION: This paper studies the question of extrapolation in RNNs, and more specifically, of whether a student RNN trained on data generated by a teacher RNN can capture the behavior of the teacher over sequences longer than those seen in training. We focus on overparameterized students that can perfectly fit training sequences while producing a wide range of behaviors over longer sequences. Such a student will fail to extrapolate, unless the teacher possesses a certain structure, and the learning algorithm is biased towards solutions adhering to that structure. We show \u2014 theoretically for linear RNNs and empirically for both linear and non-linear RNNs \u2014 that such implicit extrapolation takes place when the teacher has a low dimensional state space and the learning algorithm is GD. Existing studies of implicit extrapolation in (linear) RNNs (Emami et al., 2021; Cohen-Karlik et al., 2022) suggest that GD is biased towards solutions with short-term memory. While low dimensional state space and short-term memory may coincide in some cases, in general they do not, and a solution with low dimensional state space may entail long-term memory. Our theory and experiments show that in settings where low dimensional state space and short-term memory contradict each other, the implicit extrapolation chooses the former over the latter. We note that linear RNNs fundamentally differ from the commonly studied model of linear (feedforward) NNs (see, e.g., Arora et al. (2018); Ji & Telgarsky (2018); Arora et al. (2019a;b); Razin & Cohen (2020)). One of the key differences is that a linear RNN entails different powers of a parameter (transition) matrix, leading to a loss function which roughly corresponds to a sum of losses for multiple linear NNs having different architectures and shared weights. This makes the analysis of linear RNNs technically challenging, warranting introduction of new theoretical approaches such as the moment matching view employed in Section 4.1. An important direction for future work is extending our theory to non-linear RNNs. We believe it is possible, in the same way that theories for linear (feed-forward) NNs were extended to account for non-linear NNs (see, e.g., Razin et al. (2021; 2022); Lyu & Li (2019)). An additional direction to explore is the applicability of our results to the recently introduced S4 model (Gu et al., 2022). A FURTHER EXPERIMENTS: In this section we provide additional experiments that are not included in the main manuscript due to space constraints. A.1 BALANCED TEACHER: In Section 5.1, we have experimented with our proposed theoretical setup. In this section we provide additional figures and experiments. A.1.1 UNBALANCED STUDENT: In this experiment we use the same balanced teacher with d\u0302 \u201c 5 as done in Section 5.1. Instead of the diagonal student with balanced initialization, we use a general (non-diagonal) student with weights sampled from a Gaussian with scale 10\u00b45 and d \u201c 40. Results are depicted in Figure 3(a). A similar phase transition phenomenon to the one in Figure 1 is found also here. A.1.2 EFFECT OF THE INITIALIZATION SCALE: Proposition 9 provides theoretical support for the fact that under near-zero initialization, the learned RNN tends to balancedness, which according to theorems 5 and 8 guarantees extrapolation. Below we empirically explore the impact of varying the initialization scale. We use the same setting as in Section A.1.1, and repeat the experiment with different initialization scales for the students\u2019 weights. As can be seen in Figure 4, the extrapolation deteriorates for larger initialization scale, in the sense that it requires longer training sequences for getting good extrapolation error. This suggests that the condition of small initialization required by our theory is not an artifact of our proof technique, but rather a necessary condition for extrapolation to occur. A.2 UNBALANCED TEACHER: In Section 5.2, we have tested the extrapolation with respect to a specific unbalanced teacher and have observed a similar phase transition as predicted by the theory of Section 4 and empirical evaluation of Section 5.1. Here we show that the phase transition is not limited to the specific teacher discussed by testing with respect to a randomly generated unbalanced (non-diagonal) teacher (see Section B.2.2). The teacher is set to d\u0302 \u201c 5 and student to d \u201c 50. Results are presented in Figure 3 (b). Here too we can observe the phase transition phenomena. A.3 IMPULSE RESPONSE FIGURES In Section 5 we have presented the extrapolation performance in different settings. In order to better convey the meaning of extrapolating vs non-extrapolating solutions we present here figures of the impulse response of different models. We start with the impulse response corresponding to the experiment described in Section 5.1. The figure depicts the balanced teacher with d\u0302 \u201c 5 and two selected students (with d \u201c 40), one trained with k \u201c 10 and the other with k \u201c 20. We can see that the student trained with k \u201c 10 tracks the teacher several steps beyond the 10th time step and then decays to zero. For k \u201c 20 we can see near perfect extrapolation for the horizon evaluated. Next we turn to Section 5.2 and depict the average impulse responses of the \u201cdelay teacher\u201d and the students trained with respect to the mentioned teacher. Since the teacher here has d\u0302 \u201c 10, a model trained with k \u201c 8 is trained with respect to the zero impulse response (see Section B.2.2 for details on delay teacher), and as expected results with the \u2018zero\u2019 solution. we can see that for k \u201c 18 the student diverges from the teacher shortly after the 18th time step. For k \u201c 20 we can see near perfect extrapolation up to the horizon considered. B IMPLEMENTATION DETAILS All the experiments are implemented using PyTorch. B.1 OPTIMIZATION: In Section 5.1 we optimize the population loss, which entails minimizing Equation 3.4 with respect to the parameters of the learned model. We use 15K optimization steps with Adam optimizer and a learning rate of 10\u00b43. In this experiment, the results were not sensitive to the initialization scale of the (balanced) student. In Section 5.2 and Section 5.3 in the experiments that involve minimizing the empirical loss, we use 50K optimization steps with early stopping (most experiments required less than 10K steps). The batch size is set to 100, data is sampled from a Gaussian with zero mean and scale of 1. Experiments were not sensitive to most hyper-parameters other than learning rate and initialization scale. The examination of the effect of initialization scale presented in Section A.1.2 is done with learning rate scheduler torch.optim.lr_scheduler.MultiStepLR using milestones at r5000, 10000, 15000, 30000s and a decaying factor of \u03b3 \u201c 0.1. B.2 TEACHER GENERATION: One of the main challenges in empirically evaluating extrapolation is that randomly sampling weights from a Gaussian distribution may result with an RNN of lower effective rank (i.e. the resulting RNN may be accurately approximated with another RNN with a smaller hidden dimension). We will now describe the teacher generation scheme for the different experiments. B.2.1 BALANCED TEACHER GENERATION: A balanced teacher consists of d entries corresponding to the diagonal teacher and d entries representing B\u0302 \u201c C\u0302J. In order to avoid cases of rapid decay in the impulse response on the one hand, and exponential growth on the other, we set the eigenvalues to distribute uniformly between 0.6 and 1.05. The values of B\u0302 and C\u0302 are randomly sampled from a Gaussian around 0.5 and scale 1 and then normalized such that C\u0302B\u0302 \u201c 1. B.2.2 UNBALANCED TEACHER GENERATION: In this experiment, the teacher has a general (non-symmetrid) matrix A\u0302 and B\u0302 \u2030 C\u0302J. We set the weights as described next. Delay Teacher A \u2018delay\u2019 teacher has an impulse response of 1 at time step i \u201c d\u0302 \u00b4 1, that is, the teacher has an impulse response of p0, . . . , 0, 1, 0, . . . q. In order to generate the mentioned impulse response we set the weights as follows, A \u201c \u00a8 \u02da \u02da \u02dd 0 1 0 . . . 0 0 1 0 0 0 \u02db \u2039 \u2039 \u201a , B \u201c \u00a8 \u02da \u02da \u02dd 0 ... 0 1 \u02db \u2039 \u2039 \u201a , and CJ \u201c \u00a8 \u02da \u02da \u02dd 1 0 ... 0 \u02db \u2039 \u2039 \u201a . (B.1) Note that B,C above are set to extract the last entry of the first row of Ai and A is a Nilpotent shift matrix. It is straightforward to verify that CAiB \u201c 1 for i \u201c d\u0302 \u00b4 1 and 0 otherwise. Random Unbalanced Teacher The second unbalanced teacher is randomly generated. In order to avoid the caveats mentioned in Section A.1, we randomly sample the diagonal (from a Gaussian with zero mean and scale 0.1) and super diagonal (from a Gaussian with mean 0.7 and scale 0.1) of A. We set B,C as in equation B.1. The structure of A ensures similar properties to that of the delayed teacher, specifically, that the first entries of the impulse response is zero and the teacher is \u2018revealed\u2019 only after d\u0302 time steps. B.2.3 NON-LINEAR TEACHER GENERATION: As opposed to the linear teacher discussed in previous sections, when the teacher is a Gated Recurrent Units (GRU), it is unclear how to generate a non-trivial teacher. When randomly generating a teacher GRU the result is either a trivial model that quickly decays to zero or a teacher with an exploding impulse response (depending on the scale of the initialization). In order to produce a teacher with interesting extrapolation behaviour, we initialize a model with an initialization scale of 10\u00b46 and train for 1000 step the model to mimic an arbitrarily chosen impulse response. The result of the mentioned procedure is a teacher GRU with non-trivial behaviour. Figure 2(b) shows that we get with this non-trivial teacher the phase transition phenomena as described in Section 5.3. B.3 EXTRAPOLATION ERROR: The concept of extrapolation is very intuitive, and yet it does not admit any standard error measure. A proper extrapolation error measure should: (a) capture fine differences between two models with good extrapolation behaviour; and on the other hand, (b) be insensitive to the scale in which two non-extrapolating model explode. A natural approach which we take here is to report the \u21138 norm difference on the tail of the impulse response. A model is considered non-extrapolating if the extrapolation error is worse than the extrapolation error of a trivial solution which has an impulse response of zeros. C DEFERRED PROOFS: Here we provide complete proofs for the results in the paper. C.1 AUXILARY PROOFS: In this section we provide missing proofs from the main paper and additional lemmas to be used in the main proofs. C.1.1 POPULATION LOSS: Lemma C.1 (Proof of Equation 3.4). Assume x \u201e D such that Ex\u201eDrxs \u201c 0,Ex\u201eDrxxJs \u201c Ik P Rk,k, where Ik is the identity matrix. y is given by y \u201c {RNNpxq where {RNNp\u00a8q denotes the output of a teacher RNN, \u0398\u0302 \u201c pA\u0302, B\u0302, C\u0302q. Denote wi \u201c C\u0302A\u0302iB\u0302, the loss for the student RNN satisfies: Ex\u201eD r\u2113 pRNN pxq , yqs \u201c k\u00b41 \u00ff i\u201c0 ` CAiB \u00b4 wi \u02d82 . (C.1) Proof of Lemma C.1. The population loss for training with sequences of length k is Ex\u201eD r\u2113 pRNN pxq , yqs \u201c Ex\u201eD \u00bb \u2013 \u02dc k\u00b41 \u00ff i\u201c0 CAk\u00b41\u00b4iBxi \u00b4 k\u00b41 \u00ff j\u201c0 wk\u00b41\u00b4jxj \u00b82 fi fl . (C.2) Reversing the order of summation, expanding the terms, Ex\u201eD r\u2113 pRNN pxq , yqs \u201c Ex\u201eD \u00bb \u2013 \u02dc k\u00b41 \u00ff i\u201c0 CAiBxk\u00b41\u00b4i \u00b4 k\u00b41 \u00ff j\u201c0 wjxk\u00b41\u00b4j \u00b82 fi fl (C.3) \u201c k\u00b41 \u00ff i,j\u201c0 \u201c CAiBCAjB \u00b4 2CAiBwj ` wiwj \u2030 Ex\u201eD rxk\u00b41\u00b4ixk\u00b41\u00b4js (C.4) \u201c k\u00b41 \u00ff i,j\u201c0 \u201c CAiBCAjB \u00b4 2CAiBwj ` wiwj \u2030 1k\u00b41\u00b4i\u201ck\u00b41\u00b4j (C.5) \u201c k\u00b41 \u00ff i,j\u201c0 \u201c CAiBCAjB \u00b4 2CAiBwj ` wiwj \u2030 1i\u201cj (C.6) \u201c k\u00b41 \u00ff i\u201c0 \u201c pCAiBq2 \u00b4 2CAiBwi ` w2i \u2030 \u201c k\u00b41 \u00ff i\u201c0 ` CAiB \u00b4 wi \u02d82 . (C.7) where the transition from the second to third rows is by our assumption that Ex\u201eDrxixjs \u201c 1i\u201cj. Therefore we have, Ex\u201eD r\u2113 pRNN pxq , yqs \u201c k\u00b41 \u00ff i\u201c0 ` CAiB \u00b4 wi \u02d82 . (C.8) concluding the proof. C.1.2 PERFECT GENERALIZATION AND FAILED EXTRAPOLATION: Proposition C.2 (Proposition 3 in main paper). Assume d \u0105 k, and let \u03f5 \u011b 0 and q P tk ` 1, k ` 2, . . .u. Then, for any teacher parameters \u0398\u0302, there exist student parameters \u0398 with which the population loss in Equation 3.4 equals zero, and yet the student does not \u03f5-extrapolate with horizon q. Proof. Consider a student, \u0398, such that A is symmetric (and therefore has an orthogonal eigendecomposition). Denote A \u201c U\u039bUJ. The impulse response at time step i can be expressed as CAiB \u201c CU\u039biUJB. The latter can be written compactly in matrix form as V g where V is the Vandermonde matrix with diagp\u039bq as its values, V \u201c \u00a8 \u02da \u02da \u02da \u02da \u02dd 1 1 . . . 1 \u03bb1 \u03bb2 . . . \u03bbd \u03bb21 \u03bb 2 2 . . . \u03bb 2 d ... ... ... \u03bbd\u00b411 \u03bb d\u00b41 2 . . . \u03bb d\u00b41 d \u02db \u2039 \u2039 \u2039 \u2039 \u201a , and g is defined as g \u201d pCUqJ \u02dd UJB.4 A known result on square Vandermonde matrices is that they are invertible if and only if \u03bbi \u2030 \u03bbj , @i \u2030 j. Given a fixed set of distinct values p\u03bb1, . . . , \u03bbdq and an arbitrary impulse response r P Rd, in order for the student to generate the impulse response r (i.e. V g \u201c r), one can set the coefficient vector, g \u201c V \u00b41r and end up with a symmetric student with r as its impulse response of length d. Consider a teacher RNN, \u0398\u0302 \u201c pA,B,Cq, we can set and the first k entries of r to ri \u201c C\u0302A\u0302i\u00b41B\u0302, @i \u201c t1, . . . , ku. We are therefore left with d \u00b4 k degrees of freedom which yields many different students that correspond to the first k entries of the teacher while fitting arbitrary values beyond the k. C.1.3 EQUIVALENCE BETWEEN BALANCED RNNS WITH SYMMETRIC AND DIAGONAL TRANSITION MATRICES: Lemma C.3. A balanced RNN, \u0398 \u201c pA,B,Cq, with a symmetric transition matrix (i.e. B \u201c CJ and A \u201c AJ) has an equivalent (i.e. generating the same impulse response) RNN, \u03981 \u201c pA1,B1,C 1q, which is balanced and its transition matrix is diagonal. Lemma C.3 allows alternating between systems with symmetric and diagonal matrices. This is useful to simplify the analysis in Section 4. Proof of Lemma C.3. Any symmetric matrix admits an orthogonal eigendecomposition with real (non-imaginary) eigenvalues. Denote A \u201c U\u039bUJ. We can define A1 \u201c \u039b, B1 \u201c UJB and C 1 \u201c CU , The ith index of the impulse response is given by CAiB \u201c CU\u039biUJB \u201c C 1 ` A1 \u02d8i B1 concluding that \u0398 and \u03981 have the same impulse response of any length. C.1.4 GRADIENT DERIVATION: For completeness and Section C.4, we compute the gradients for the general setting. Lemma C.4. Given the population loss LpA,B,Cq \u201c k\u00b41 \u00ff j\u201c0 \u00b4 CAjB \u00b4 C\u0302A\u0302jB\u0302 \u00af2 . (3.4 revisited) Denote \u2207\u2113i \u201c CAiB \u00b4 wi, the derivatives of the loss with respect to B, and C satisfy: BL BB \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAiqJCJ, (C.9) BL BC \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113iBJ ` Ai \u02d8J . (C.10) 4Here \u02dd denotes the Hadamard (elementwise) product. Proof of Lemma C.4. Here, we will compute the gradient of the population loss. Note that for j \u011b 0, the derivative of CAjB w.r.t to B is given by BpCAjBq BB \u201c pA jqJCJ. (C.11) Similarly, the derivative of CAjB w.r.t to C is given by BpCAjBq BC \u201c B JpAjqJ. (C.12) Using these derivatives, we can calculate the derivative of the population loss, (assigning wi \u201c B\u0302A\u0302iC\u0302), L \u201c Ex\u201eD r\u2113 pRNN pxq , yqs \u201c k\u00b41 \u00ff i\u201c0 ` CAiB \u00b4 wi \u02d82 . (C.13) Denoting \u2207\u2113i \u201c CAiB \u00b4 wi, and noting that wi is constant (depends on \u0398\u0302), we have for X P tB,Cu: BL BX \u201c k\u00b41 \u00ff i\u201c0 B ` CAiB \u00b4 wi \u02d82 BX \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113i B ` CAiB \u00b4 wi \u02d8 BX \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113i B ` CAiB \u02d8 BX . (C.14) Plugging in Equation C.11 and Equation C.12, we have: BL BB \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113i B ` CAiB \u02d8 BB \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAiqJCJ, (C.15) BL BC \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113i B ` CAiB \u02d8 BC \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113iBJpAiqJ, (C.16) C.1.5 LEMMA 6 (CONSERVATION OF BALANCEDNESS): Lemma C.5. [Lemma 6 in main paper] When optimizing equation 3.4 with GF emenating from a balanced initialization \u0398p0q, the parameters \u0398p\u03c4q are balanced for all \u03c4 P R`. We prove the above result by first showing it for gradient descent and then translating the result to gradient flow. The gradient descent result is stated below, and generalizes a result that was shown in Cohen-Karlik et al. (2022) for the memoryless case. Lemma C.6. When optimizing equation 3.4 with GD with balanced initial conditions, then @t P N, \u0398 has a balanced weight configuration, i.e. Bt \u201c CJt . Proof of Lemma C.6. We prove by induction. By our assumption, the condition holds for t \u201c 0. Assume Bt \u201c CJt , our goal is to show the conditions hold for pBt`1,Ct`1q. In order to show that Bt`1 \u201c CJt`1, we only need to show that BLBBt \u201c \u00b4 BL BCt \u00afJ . Writing the gradients (Lemma C.4), we have \u02c6 BL BCt \u02d9J \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113iAitBt \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJt qiCJt \u201c BL BBt , (C.17) where the inequality follows from the induction assumption and the symmetric structure of At. To conclude, the gradients at time t are the same and Bt \u201c CJt by the induction assumption, arriving at Bt`1 \u201c Bt \u00b4 \u03b7 BL BBt \u201c CJt \u00b4 \u03b7 \u02c6 BL BCt \u02d9J \u201c CJt`1 (C.18) The proof of Lemma 6 follows from Lemma C.6 and the fact that for sufficiently small step size GD approximates GF with arbitrary precision (see Theorem 3 in Elkabetz & Cohen, 2021). C.1.6 CONSERVATION OF DIFFERENCE OF NORMS: Appendix C.1.5 shows that if weights are initialized to be balanced, this property is conserved throughout optimization. Here we show under standard initialization schemes, the difference between the norms of B and C is also conserved. Lemma C.7. The difference between the norms of B, C is conserved throughout GF, i.e., d dt ` }B}2 \u00b4 }C}2 \u02d8 \u201c 0. (C.19) Proof of Lemma C.7. We wish to prove that the difference between the norms is conserved over time. Consider the following expression: \u03b1 \u201d }B}2 \u00b4 }C}2 \u201c BJB \u00b4 CCJ. (C.20) With this notation, we just need to prove that 9\u03b1 \u201c 0. The derivative of B, C with respect to time is given by, 9B \u201c \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqiCJ, (C.21) 9C \u201c \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113iBJpAJqi. (C.22) Using the interchangeability of derivative and transpose, we have: 9\u03b1 \u201c 9BJB ` BJ 9B \u00b4 9CCJ \u00b4 C 9CJ \u201c 2BJ 9B \u00b4 2 9CCJ. (C.23) Plugging equation C.21 and equation C.22, we get 9\u03b1 \u201c 2BJ \u02dc \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqiCJ \u00b8 \u00b4 2 \u02dc \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113iBJpAJqi \u00b8 CJ (C.24) \u201c \u00b42 \u00ab BJ \u02dc k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqi \u00b8 CJ \u00b4 BJ \u02dc k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqi \u00b8 CJ ff \u201c 0. (C.25) establishing that ddt ` }B}2 \u00b4 }C}2 \u02d8 \u201c 0. C.2 LEMMA 7 (EXACT EXTRAPOLATION): Lemma C.8. [Lemma 7 in main paper] Suppose that d \u0105 k \u0105 2d\u0302, the teacher is balanced, and that the student parameters \u0398 are balanced and satisfy Lp\u0398q \u201c 0. Then \u0398 extrapolates. Proof of Lemma C.8. By Lemma C.3, a balanced RNN with symmetric transition matrix has an equivalent (generating the same impulse response) balanced RNN with a diagonal transition matrix. We will continue under the assumption of diagonal transition matrices. Without loss of generality we assume C\u0302B\u0302 \u201c 1. Otherwise, the problem can be rescaled by C\u0302B\u0302, which is equivalent to rescaling the initial conditions, and providing no additional information.5 From the balanced assumption, we have C\u0302J \u201c B\u0302. Denote p\u0302 \u201c C\u0302J d B\u0302 \u201c B\u0302 d B\u0302, and we get p\u0302i \u011b 0 and \u0159 i p\u0302i \u201c 1, and therefore p\u0302 may be interpreted as a distribution over a random variable with d\u0302 possible values. We shall assume that these values are A\u03021, . . . , A\u0302d\u0302, and denote the corresponding random variable by Z. Furthermore, we can also interpret elements of the impulse response of \u0398\u0302 as moments of this distribution. Let us write the nth element of the impulse response as: C\u0302A\u0302nB\u0302 \u201c \u00ff i p\u0302ia\u0302 n i \u201c Ep\u0302rZ\u0302ns, (C.26) 5The case for which C\u0302B\u0302 \u201c 0 is handled separately. where EprZs is the expected value of a random variable Z under the distribution p. In the same way, we can define for the learned model \u0398, a distribution pi \u201c CiBi, and write the learned impulse response as: CAnB \u201c \u00ff i pia n i \u201c EprZns. (C.27) This view provides us with a moment matching interpretation of the learning problem. Namely, the fact that \u0398 matches the first k elements of the teacher impulse response, is the same as saying they agree on the first k \u00b4 1 moments EprZjs for j P t1, . . . , k \u00b4 1u.6 The question of extrapolation is whether equality in the first k \u00b4 1 moments implies an equality in all other moments. In (Cohen & Yeredor, 2011, Theorem 1) and in (Wu & Yang, 2020, Lemma 4) it is shown that the first 2d\u0302 moments of a discrete random variable taking at most d\u0302 different values uniquely define this random variable. Therefore, any other discrete random variable identifying with the teacher on 2d\u0302 moments must be the same random variable and therefore identifies on higher moments as well. Since we assumed k \u0105 2d\u0302, this result immediately implies that equality in the first k \u00b4 1 moments implies equality in all other moments. For the case C\u0302B\u0302 \u201c 0, from our assumption that the teacher is balanced, we have that the condition is met only if C\u0302i \u201c B\u0302i \u201c 0 for i \u201c 1, . . . , d\u0302. Such a teacher has an impulse response of zeros, for k \u011b 1, a student minimizing the loss must also satisfy CB \u201c 0 and therefore has the zeros as its impulse response thus extrapolating with respect to the said teacher. C.3 THEOREM 8 (APPROXIMATE EXTRAPOLATION): This section is devoted to the proof of Theorem 8 which ties the approximation error of optimization to that of extrapolation. Lemma C.9 below shows that if the parameters are within close proximity to an extrapolating solution, the generalization error (i.e. loss values) are small. Lemma C.9. Assume }X \u00b4 X\u02da}8 \u0103 \u03b4 for X P tA,B,Cu, and \u0398\u02da \u201c pA\u02da,B\u02da,C\u02daq is an extrapolating solution (with Lp\u0398\u02daq \u201c 0q. Then, there exists a constant depending on k and \u0398\u02da, cpk,\u0398\u02daq, such that Lp\u0398q \u010f c\u03b42. Proof of Lemma C.9. First we note that \u0398\u02da attains zero loss and is an extrapolating with respect to the teacher \u0398\u0302, specifically, C\u02dapA\u02daqiB \u201c C\u0302A\u0302iB\u0302, @i \u201c 0, 1, . . . , (C.28) From the above, the loss can be written with respect to \u0398\u02da instead of \u0398\u0302, i.e. LpA,B,Cq \u201c k\u00b41 \u00ff j\u201c0 ` CAjB \u00b4 C\u02dapA\u02daqjB\u02da \u02d82 . (C.29) Denote by l P Rk the residuals vector with respect to \u0398\u02da, i.e. li \u201d CAiB \u00b4 B \u00b4 C\u02dapA\u02daqiB\u02da. With these notations, the loss amounts to LpA,B,Cq \u201c }l}2. From equivalence of norms, we have that for v P Rn, }v}8 \u010f }v}2 \u010f ? n}v}8. The latter means that bounding }v}8 will provide the desired bound on the loss with a factor of ? k. Recall that \u0398\u02da is fixed, specifically, }X\u02da} is bounded by some constant mX\u02da for X P tA,B,Cu. Denote m \u201d maxtmA\u02da ,mB\u02da ,mC\u02da , 1u, the ith entry of l can be bounded by \u02c7 \u02c7CAi\u00b41B \u00b4 C\u02dapA\u02daqi\u00b41B\u02da \u02c7 \u02c7 \u201c \u02c7 \u02c7pC \u00b4 C\u02da ` C\u02daqAi\u00b41B \u00b4 C\u02dapA\u02daqi\u00b41B\u02da \u02c7 \u02c7 (C.30) \u010f \u02c7 \u02c7C\u02da ` Ai\u00b41B \u00b4 pA\u02daqi\u00b41B\u02da \u02d8 \u02c7 \u02c7 ` \u02c7 \u02c7pC \u00b4 C\u02daqAi\u00b41B \u02c7 \u02c7 (C.31) \u010f m \u203a \u203aAi\u00b41B \u00b4 pA\u02daqi\u00b41B\u02da \u203a \u203a ` \u03b4 \u203a \u203aAi\u00b41B \u203a \u203a (C.32) 6Equality of the 0th moment ensures the student induces a valid probability, i.e. \u0159 i CiBi= \u0159 i C\u0302iB\u0302i=1. From Cauchy-Schwarz inequality, the RHS satisfies \u03b4}Ai\u00b41B} \u010f \u03b4}A}i\u00b41}B} \u010f \u03b4pm ` \u03b4qi. The last transition uses, }X} \u201c }X \u00b4 X ` X\u02da} \u010f }X \u00b4 X\u02da} ` }X\u02da} \u010f \u03b4 ` m (C.33) for X P tA,Bu. In a similar manner, the LHS can be bounded by \u203a \u203aAi\u00b41B \u00b4 pA\u02daqi\u00b41B\u02da \u203a \u203a \u201c \u203a \u203aAi\u00b41B \u00b4 Ai\u00b41B\u02da ` Ai\u00b41B\u02da \u00b4 pA\u02daqi\u00b41B\u02da \u203a \u203a (C.34) \u010f }A}i\u00b41}B \u00b4 B\u02da} loooooooooomoooooooooon \u03b4pm`\u03b4qi\u00b41 `}Ai\u00b41 \u00b4 pA\u02daqi\u00b41}}B\u02da} (C.35) again, we have used the triangle inequality and Cauchy-Schwarz inequality. In order to deal with }Ai\u00b41 \u00b4 pA\u02daqi\u00b41}, we make use of the equality xn \u00b4 yn \u201c px \u00b4 yqpxn\u00b41 ` xn\u00b42y ` xn\u00b43y2 ` \u00a8 \u00a8 \u00a8 ` xyn\u00b42yn\u00b41. Assigning A and A\u02da, we have }Ai\u00b41 \u00b4 pA\u02daqi\u00b41} \u201c \u203a \u203a \u203a \u203a \u203a pA \u00b4 A\u02daq i\u00b42 \u00ff j\u201c0 Ai\u00b42\u00b4jpA\u02daqj \u203a \u203a \u203a \u203a \u203a (C.36) \u010f }A \u00b4 A\u02da} loooomoooon \u010f\u03b4 \u203a \u203a \u203a \u203a \u203a p i\u00b42 \u00ff j\u201c0 Ai\u00b42\u00b4jpA\u02daqj \u203a \u203a \u203a \u203a \u203a \u010f \u03b4pi \u00b4 1qpm ` \u03b4qi\u00b42 (C.37) where the last inequality uses, \u203a \u203a \u203a \u203a \u203a i\u00b42 \u00ff j\u201c0 Ai\u00b42\u00b4jpA\u02daqj \u203a \u203a \u203a \u203a \u203a \u010f i\u00b42 \u00ff j\u201c0 }A}i\u00b42\u00b4j}A\u02da}j (C.38) \u010f i\u00b42 \u00ff j\u201c0 pm ` \u03b4qi\u00b42\u00b4jmj (C.39) \u010f i\u00b42 \u00ff j\u201c0 pm ` \u03b4qi\u00b42 \u201c pi \u00b4 1qpm ` \u03b4qi\u00b42. (C.40) Plugging all terms back together, |li| \u010f m ` \u03b4pm ` \u03b4qi\u00b41 ` \u03b4pi \u00b4 1qpm ` \u03b4qi\u00b42m \u02d8 ` \u03b4pm ` \u03b4qi \u201c Op\u03b4miq. (C.41) The above is maximized for i \u201c k, to result with }l}8 \u201c Op\u03b4mkq. Plugging back to the bound on the loss, we have LpA,B,Cq \u201c }l}2 \u010f k}l}28 \u201c Op\u03b42km2kq (C.42) When k is small which is the case of interest, LpA,B,Cq \u201c Op\u03b42q. Theorem C.10. [Theorem 8 in main paper] Assume the conditions of Theorem 5, and that the teacher parameters \u0398\u0302 are stable, i.e. the singular values of A\u0302 are in r\u00b41, 1s. Assume also that \u0398\u0302 are non-degenerate, in the sense that the input-output mapping they realize is not identically zero. Then, for any \u03f5 \u0105 0 and q P N, there exists \u03b4p\u03f5, qq \u0105 0 which admits the following: whenever GF leads the student parameters \u0398 to within (Frobenius) distance \u03b4p\u03f5, qq from a balanced point \u0398\u02da satisfying Lp\u0398\u02daq \u201c 0, the student \u03f5-extrapolates with horizon q. Proof of Theorem C.10. Given a desired approximation error \u03f5 \u0105 0, and horizon q P tk ` 1, k ` 2, . . . u, our goal is to find \u03b4p\u03f5, qq such that if }X \u00b4 X\u02da}F \u0103 \u03b4 for X P tA,B,Cu, then |CAjB \u00b4 C\u0302A\u0302jB\u0302| \u0103 \u03f5. First we note that by Theorem 5, \u0398\u02da is an extrapolating solution with respect to \u0398\u0302 (i.e. \u0398\u02da and \u0398\u0302 generate identical impulse responses), therefore we can measure extrapolation with respect to \u0398\u02da. In a similar manner to Lemma C.9, we can define the residual vector l such that its ith entry is defined as li \u201c CAi\u00b41B \u00b4 C\u02dapA\u02daqi\u00b41B\u02da. (C.43) Note that \u0398 is initialized balanced and by Lemma 6, this property is preserved throughout GF. By Lemma C.3 both \u0398 and \u0398\u02da have equivalent balanced systems \u0398\u0304 and \u0398\u0304\u02da for which the transition matrices are diagonal. For the sake of readability, we overload notations and carry on under the assumption that A and A\u02da are diagonal with entries tA1, . . . , Adu and tA\u02da1 , . . . , A\u02dadu on their respective diagonals. Another observation is that a balanced RNN must satisfy that its first element in the impulse response moment is non-negative (CB \u201c \u0159 cibi \u201c \u0159 b2i \u011b 0). Without loss of generality, we can assume the teacher RNN \u0398\u0302 satisfies C\u0302B\u0302 \u201c 1. Suppose this is not the case, i.e. C\u0302B\u0302 \u201c \u03b7 \u0105 0 (recall we assume the teacher is non-degenerate). One can re-scale C\u0302 and B\u0302 by a factor of \u03b7\u00b40.5 and result with C\u0302A\u0302 i\u00b41B\u0302 \u03b7 on the i th entry of its impulse response. The re-scaled system satisfies C\u0302B\u0302\u03b7 \u201c 1 and provides no extra information. With the claims above, we may interpret \u0398\u0302 as a discrete random variable, i.e. p\u0302 \u201c pC\u03021B\u03021, . . . , C\u0302d\u0302B\u0302d\u0302q defines a probability vector,7 the random variable is assigned the value A\u0302i with probability p\u0302i. Denote the random variable associated with \u0398\u0302 by z\u0302. Recall that \u0398\u02da is balanced and attains zero loss, hence it also admits the random variable interpretation (denoted as z\u02da). Under the probabilistic perspective, the impulse response of the systems correspond to the moment of the random variables, i.e. E rz\u0302ns \u201c d\u0302 \u00ff i\u201c1 p\u0302iA\u0302 n i \u201c d\u0302 \u00ff i\u201c1 C\u0302iB\u0302iA\u0302 n i \u201c C\u0302A\u0302nB\u0302 (C.44) We can write the loss in terms of the defined residual vector, Lp\u0398q \u201c }l}2. From Lemma C.9, we have Lp\u0398q \u201c Op\u03b42q. Specifically, }l} \u010f c\u03b42 for some constant c P R. Recall that for v P Rn, }v}8 \u010f }v}2, applying to our case, we have }l}8 \u010f }l} \u010f ? c\u03b4. Specifically, |l0| \u010f }l}8 \u010f ? c\u03b4 and since we assume C\u02daB\u02da \u201c 1, we have |l0| \u201c |CB \u00b4 C\u02daB\u02da| \u201c |CB \u00b4 1| \u010f ? c\u03b4 (C.45) which leads to 1 \u00b4 ? c\u03b4 \u010f CB \u010f 1 ` ? c\u03b4. Denote CB \u201c 1 ` \u03b2 where \u03b2 P r\u00b4 ? c\u03b4, ? c\u03b4s. A re-scaled student introduces a factor \u03b4 into the approximation error, that is \u02c7 \u02c7 \u02c7 \u02c7 CAi\u00b41B 1 ` \u03b2 \u00b4 C \u02dapA\u02daqi\u00b41B\u02da \u02c7 \u02c7 \u02c7 \u02c7 \u201c \u02c7 \u02c7 \u02c7 \u02c7 CAi\u00b41B 1 ` \u03b2 \u00b4 p1 ` \u03b2q C\u02dapA\u02daqi\u00b41B\u02da 1 ` \u03b2 \u02c7 \u02c7 \u02c7 \u02c7 (C.46) \u201c 1 1 ` \u03b2 \u02c7 \u02c7CAi\u00b41B \u00b4 p1 ` \u03b2qC\u02dapA\u02daqi\u00b41B\u02da \u02c7 \u02c7 (C.47) \u010f 1 1 ` \u03b2 \u02c7 \u02c7CAi\u00b41B \u00b4 C\u02dapA\u02daqi\u00b41B\u02da \u02c7 \u02c7 ` 1 1 ` \u03b2 |\u03b2C \u02dapA\u02daqi\u00b41B\u02da| (C.48) Recall that \u0398\u02da is fixed, specifically, }X\u02da} is bounded by some constant mX\u02da for X P tA,B,Cu. Denote m \u201d maxtmA\u02da ,mB\u02da ,mC\u02da , 1u. Combining the latter with Cauchy-Schwarz inequality, \u02c7 \u02c7 \u02c7 \u02c7 CAi\u00b41B 1 ` \u03b2 \u00b4 C \u02dapA\u02daqi\u00b41B\u02da \u02c7 \u02c7 \u02c7 \u02c7 \u010f 1 1 ` \u03b2 |li| ` \u03b2mi`1 1 ` \u03b2 \u010f 2|li| ` 2\u03b2m i`1 \u201c Op\u03b4q (C.49) where the last inequality uses 0.5 \u0103 1 ` \u03b2 for ? c\u03b4 \u0103 0.5. Per our claim above, we can assume with out loss of generality that CB \u201c 1. A balanced student can thus be interpreted as a discrete random variable with probability vector p taking on values tA1, . . . , Adu with probability pi. 7\u0159 p\u0302i \u201c 1 and @i, p\u0302i \u011b 0 Recall that under the assumption of a stable teacher, we have that for all i \u201c 1, . . . , d\u0302, |A\u0302i| \u010f 1. Denote the joint support of A and A\u0302 by S \u201d tAiudi\u201c1 \u0164 tA\u0302jud\u0302j\u201c1, and define s \u201c maxtSu \u00b4mintSu. By our assumption on convergence and stability of the teacher, we have s \u010f 2p1 ` \u03b4q. From Proposition 2 in Wu & Yang (2020), we have that if |CAiB\u00b4C\u0302A\u0302iB\u0302| \u010f \u03b4 for i P t1, . . . , 2d\u0302u, then W1pp, p\u0302q \u201c O \u00b4 \u03b4 1 4d\u0302 \u00af . (C.50) By (Panaretos & Zemel, 2019, Section 2.3), we can tie the Wasserstein distance to higher order qth Wasserstien distance. For the s defined above, we have that for any q P N: Wqq pp, p\u0302q \u010f W1pp, p\u0302qsq\u00b41 \u0103 p2 ` 2\u03b4qq\u00b41Op\u03b4 1 2d\u0302 q. (C.51) Denote by mqppq the qth moment of p, from (Biswas & Mackey, 2021, Equation 1), we have: \u02c7 \u02c7mqqppq \u00b4 mqqpp\u0302q \u02c7 \u02c7 \u010f Wqq pp, p\u0302q \u010f p2 ` 2\u03b4qq\u00b41Op\u03b4 1 2d\u02da q. (C.52) Recall that the difference in the ith moments of the student and teacher correspond to the difference in the impulse responses. So we have that if }X \u00b4 X\u02da}F \u0103 \u03b4 for X P tA,B,Cu then @q P N, |lq| \u010f p2 ` 2\u03b4qq\u00b41Op\u03b4 1 2d\u0302 q. (C.53) C.4 PROPOSITION 9 (IMPLICIT BIAS FOR BALANCEDNESS): The proof of Proposition 9 consists of several steps. First, we bound with high probability the norms of B and C at initialization (Lemma C.11). We then derive bounds on the differential equations of ddt pBptq ` C Jptqq and ddt pBptq \u00b4 C Jptqq (Lemma C.15). We show that when the initialization scale tends to zero, the ratio between the differential equations tends to zero. (Lemma C.14). Before we turn to prove Proposition 9, we first need to bound the initial values for a vector v P Rn initialized with N p0, \u03f5 2 n q. Lemma C.11. Assume a vector v P Rn with N p0, \u03f5 2 n q per coordinate. Then: Pr \u02c6 \u03f5 2 \u0103 }v}2 \u0103 3\u03f5 2 \u02d9 \u011b 1 \u00b4 2 expp\u00b49d{64q. (C.54) Proof of Lemma C.11. The proof of C.11 uses known results on the Chi-square distribution Laurent & Massart (2000), applied to our specific setting to achieve the desired bounds. We will begin by changing variables, v\u0303i \u201c vi \u00a8 ? n \u03f5 . The entries v\u0303i, are standard Gaussian variables. The squared norm of v\u0303 distributes according to the \u03c7-squared distribution. By (Laurent & Massart, 2000, Lemma 1), in our case (plugging in x \u201c 9n{64), the following inequalities hold: Pr ` }v\u0303}2 \u011b p1.75 ` 9{32qn \u02d8 \u010f expp\u00b49n{64q, (C.55) Pr ` }v\u0303}2 \u010f d{4 \u02d8 \u010f expp\u00b49n{64q. (C.56) We simplify these inequalities. From the first bound: Pr ` }v\u0303}2 \u011b 2.25n \u02d8 \u010f expp\u00b49n{64q \u00f9\u00f1 Pr ` }v\u0303} \u011b 1.5 ? n \u02d8 \u010f expp\u00b49n{64q (C.57) Changing variables back to v: Pr p}v} \u011b 1.5\u03f5q \u010f expp\u00b49n{64q. (C.58) Similarly, for the second bound: Pr ` }v\u0303} \u010f ? n{2 \u02d8 \u010f expp\u00b49n{64q \u00f9\u00f1 Pr p}v} \u010f \u03f5{2q \u010f expp\u00b49n{64q. (C.59) Taking the complementary probability, we have the desired result of Pr \u02c6 \u03f5 2 \u0103 }v} \u0103 3\u03f5 2 \u02d9 \u010f 1 \u00b4 2 expp\u00b49n{64q. (C.60) Proposition C.12. [Proposition 9 in main paper] Suppose that: (i) d \u0105 4; (ii) the teacher parameters \u0398\u0302 are balanced and are non-degenerate, in the sense that the input-output mapping they realize is not identically zero; and (iii) the student parameters are learned by applying GF to the loss Lp\u00a8q. Let \u0398\u0303 be a random point in parameter space, with entries drawn independently from the standard normal distribution. For \u03f5 \u0105 0, consider the case where GF emanates from the initialization \u03f5\u0398\u0303, and denote the resulting curve by \u0398\u03f5p\u03c4q \u201c pA\u03f5p\u03c4q,B\u03f5p\u03c4q,C\u03f5p\u03c4qq, with \u03c4 \u011b 0. Then, w.p. at least 0.75, for every \u03f5 \u0105 0 there exists \u03c4\u03f5 \u011b 0 such that: lim \u03f5\u00d10` ||B\u03f5p\u03c4\u03f5q \u00b4 CJ\u03f5 p\u03c4\u03f5q||F ||B\u03f5p\u03c4\u03f5q ` CJ\u03f5 p\u03c4\u03f5q||F \u201c 0 . (C.61) The consequence of Proposition 9 is that as \u03f5 converges to zero, B and C converge towards each other. For convenience, we refer to the mentioned initialization scheme (where every coordinate in a vector is initialized as N \u00b4 0, \u03f5 2 d \u00af ) as \u03f5-normal initialization. In order to prove the proposition we define a few relevant terms, Y \u201c B \u00b4 CJ, W \u201c B ` CJ, w0 \u201c C\u0302B\u0302. (C.62) We will in fact prove the stronger, following lemma, for any matrix A, not necessarily symmetric. Lemma C.13. Assume w0 \u0105 0, and A,B,C are \u03f5-normally initialized. Then Dt such that lim \u03f5\u00d10 }Y ptq}2 }W ptq}2 \u201c 0, lim\u03f5\u00d10 }Aptq}2F }W ptq}2 \u201c 0. (C.63) The proof of Lemma C.13 follows three steps. (I) We show that there is a time in the optimization in which the norms of all parameters are bounded (Lemma C.14); (II) we derive upper (and lower) bounds for the differential equations describing the evolvement of Y,W and A. Our approximations are limited to the initial phase of training. Concretely, we show that for 0 \u010f t \u010f 12w0 ln ` 1 \u03f50.5 \u02d8 , all norms are bounded. Thus, it is possible to obtain meaningful bounds on the ODEs of Y and W while A remains in the magnitude of initialization. (III) Using the relevant bounds, we show that Lemma C.14. Assume \u03f5-initialization and bounded teacher. Then, for all 0 \u010f t \u010f 12w0 ln ` 1 \u03f50.5 \u02d8 , there exist M1,M2 such that: }Cptq}, }Bptq} \u0103 M1\u03f50.75 (C.64) and }Aptq}F \u0103 M2\u03f5 (C.65) To prove this, we note that at initialization, A,B and C satisfy these bounds. From continuity, there exists a maximal time for which they are satisfied. We bound the rate of their growth, and thus show that for all t as described, we are within this region. Lemma C.15. Assume w0 \u0105 0 and assume A,B,C are \u03f5-normally initialized, we have the following bounds for all 0 \u010f t \u010f 12w0 ln ` 1 \u03f50.5 \u02d8 Y ptqJY ptq \u010f c1\u03f52e\u00b42w0t ` c2\u03f52.5, (C.66) and W ptqJW ptq \u011b c3\u03f52e2w0t \u00b4 c4\u03f52.5. (C.67) Lemma C.15 shows that the growth rate of W ptqJW ptq and the decay rate of Y ptqJY ptq both depend on the sign of w0. In our analysis we assume w0 \u0105 0 for simplicity of presentation, the same analysis applies for w0 \u0103 0 with opposite roles for Y and W . The proof of Lemma C.15 follows from writing the leading terms of the ODE and bounding the remaining terms by their upper bounds in the time considered. Using these lemmas, we proceed to prove Lemma C.13. Proof of Lemma C.13. Consider the dynamics at time 0 \u010f t \u010f 12w0 ln ` 1 \u03f50.5 \u02d8 . By Lemma C.15, we have Y ptqJY ptq \u010f c1\u03f52e\u00b4 ln 1 \u03f50.5 ` c2\u03f52.5 \u201c pc1 ` c2q\u03f52.5 (C.68) and W ptqJW ptq \u011b c3\u03f52eln 1 \u03f50.5 \u00b4 c4\u03f52.5 \u201c c3\u03f51.5 \u00b4 c4\u03f52.5 (C.69) We can calculate the limit lim \u03f5\u00d10 }Y ptq}2 }W ptq}2 \u010f lim\u03f5\u00d10 pc1 ` c2q\u03f52.5 c3\u03f51.5 \u00b4 c4\u03f52.5 \u201c 0 (C.70) From Lemma C.14, }Aptq}F \u010f M2\u03f5, so we can calculate the limit, lim \u03f5\u00d10 }Aptq}2F }W ptq}2 \u010f lim\u03f5\u00d10 M2\u03f5 2 c3\u03f51.5 \u00b4 c4\u03f52.5 \u201c 0 (C.71) which concludes the proof. Proof of Lemma C.14. At t \u201c 0, by Lemma C.11, }A}F \u0103 2\u03f5 \u0103 M2\u03f5, }B}, }C} \u0103 2\u03f5 \u0103 M1\u03f50.75, where we bound M1 \u0105 2\u03f50.25. If there exists t s.t. }Aptq}F \u0105 M2\u03f5 (otherwise, we get trivially the desired bound for }Aptq}), from continuity and the intermediate value theorem, there exists a maximal tA s.t. for 0 \u010f t \u0103 t1, }Aptq}F \u0103 \u03f5, }Apt1q}F \u201c M2\u03f5. By similar argument for B, C, we can define tB , tC . By taking the minimal value between them, we get T\u02da s.t. @0 \u010f t \u0103 T\u02da, }Aptq}F \u0103 M2\u03f5, }B}, }C} \u0103 M1\u03f5 0.75. We start by proving the lemma under the assumption that 0 \u010f t \u0103 T\u02da. Later, we will show that T\u02da \u0105 T . Let us recall the derivative of B: 9B \u201c \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqiCJ. (C.72) Bounding it by the Cauchy-Schwartz inequality, we have } 9B} \u201c \u203a \u203a \u203a \u203a \u203a \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqiCJ \u203a \u203a \u203a \u203a \u203a \u010f k\u00b41 \u00ff i\u201c0 |\u2207\u2113i| \u203a \u203apAJq \u203a \u203a i F \u203a \u203aCJ \u203a \u203a. (C.73) Denote M \u201c max i p|wi|q ` M21 \u03f51.5, then we have M \u201c max i p|wi|q ` M21 \u03f51.5 \u011b max i p|wi|q ` }C}}B} \u011b max i p|wi|q ` |CB|, (C.74) \u011b max i p|wi|q ` max i p|CAiB|q \u011b max i p|wi| ` |CAiB|q \u011b max i p|\u2207\u2113i|q. (C.75) Recall the conservation law from Lemma C.7 for the norms of B and C: @t, d dt p}Bptq} \u00b4 }Cptq}q \u201c 0. (C.76) By Lemma C.11, we have }Bp0q}, }Cp0q} \u0103 2\u03f5 \u00f1 p|}Bp0q} \u00b4 }Cp0q}|q \u0103 4\u03f5. (C.77) Therefore, we get @t, }Cptq} \u0103 }Bptq} ` 4\u03f5. (C.78) Note also that by assuming M2\u03f5 \u0103 1, we have k\u00b41 \u00ff i\u201c0 pM2\u03f5qi \u0103 k. (C.79) Plugging the various values into equation C.73, we have: } 9B} \u010f k\u00b41 \u00ff i\u201c0 |\u2207\u2113i|}pAJq}iF }CJ} \u0103 Mp}B} ` 4\u03f5q k\u00b41 \u00ff i\u201c0 }pAJq}iF (C.80) \u0103 Mkp}B} ` 4\u03f5q. (C.81) Denoting R \u201c }B}2 \u201c BJB, then 9R \u201c 9BJB ` BJ 9B \u201c 2BJ 9B (C.82) Taking absolute value and then plugging equation C.80 results with | 9R| \u201c |2BJ 9B| \u010f 2}BJ}} 9B} \u0103 2k}BJ}Mp}B} ` 4\u03f5q. (C.83) Using the definition of R, we get that 9|R| \u0103 2kMp|R| ` 4\u03f5 a |R|q (C.84) Notice that for all t s.t. }B} \u201c ? R \u0103 4\u03f5 \u0103 M1\u03f50.75, we are done. For any t2 s.t. a Rpt2q \u0105 4\u03f5, from continuity and the intermediate value theorem, there exist a maximal t1 s.t. 0 \u0103 t1 \u0103 t2, a Rpt1q \u201c 4\u03f5. For all t1 \u0103 t \u0103 t2, a Rptq \u0105 4\u03f5. Thus, @t1 \u0103 t \u010f t2, 9|R| \u0103 4kM |R| (C.85) Integrating by t and substituting, we get that for all t1 \u0103 tint \u0103 t2: \u017c tint t1 1 |R| 9|R|dt \u0103 \u017c tint t1 4kMdt (C.86) \u00f1 \u017c Rptintq Rpt1q 1 |R|dR \u0103 8M rtint \u00b4 t1s \u0103 4kM rT \u00b4 0s (C.87) \u00f1 ln \u02c6 |Rptintq| |Rpt1q| \u02d9 \u0103 4kMT (C.88) \u00f1 |Rptintq| \u0103 expp4kMT q|Rpt1q| \u201c p4\u03f5q2 exp \u02c6 4kM \u00a8 ln \u02c6 1 \u03f50.5 \u02d9\u02d9 (C.89) \u00f1 |Rptintq| \u0103 p4\u03f5q2 expp4kMq 1 \u03f50.5 . (C.90) Denoting M21 \u201c 16 \u00a8 expp4kMq, we have: }Bptintq} \u0103 M1\u03f50.75 (C.91) To summarize, we have shown that for all 0 \u0103 t \u0103 T , there exists M1 s.t }Bptq} \u0103 M1\u03f50.75. Using an identical proof with the roles of CJ,B reversed, we have |Cptq| \u0103 M1\u03f50.75. As we have defined tB , tC as the maximal points s.t. this property occurs for T \u0103 tB , tC . Next, we will bound }A}F . Recall that }A}2F \u201c TrpAJAq, and d dt TrpAJAq \u201c Tr \u02c6 d dt pAJAq \u02d9 (C.92) \u201c Tr \u00b4 9AJA \u00af ` Tr \u00b4 AJ 9A \u00af (C.93) \u201c 2Tr \u00b4 AJ 9A \u00af , (C.94) where we have used the linearity of trace and its invariance to transpose. Recall that 9A \u201c \u00b4 k\u00b41 \u00ff i\u201c1 \u2207\u2113i i\u00b41 \u00ff r\u201c0 pAJqrCJBJpAJqi\u00b4r\u00b41. (C.95) Multiplying it from the left by AJ and then taking trace provides us with TrpAJ 9Aq \u201c \u00b4 k\u00b41 \u00ff i\u201c1 \u2207\u2113i i\u00b41 \u00ff r\u201c0 Tr ` pAJqr`1CJBJpAJqi\u00b4r\u00b41 \u02d8 . (C.96) Taking a transpose and then using the cyclic property of trace and, we have for the terms in the sum: Tr ` pAJqr`1CJBJpAJqi\u00b4r\u00b41 \u02d8 \u201c Tr ` BJpAJqiCJ \u02d8 (C.97) \u201c Tr ` CAiB \u02d8 (C.98) \u201c CAiB (C.99) Thus, we end up with TrpAJ 9Aq \u201c \u00b4 k\u00b41 \u00ff i\u201c1 \u2207\u2113i i\u00b41 \u00ff r\u201c0 CAiB \u201c \u00b4 k\u00b41 \u00ff i\u201c1 \u2207\u2113i \u00a8 i \u00a8 CAiB. (C.100) We use it to bound TrpAJ 9Aq: TrpAJ 9Aq \u010f k\u00b41 \u00ff i\u201c1 |\u2207\u2113i| \u00a8 i \u00a8 |CAiB| (C.101) Using the Cauchy-Schwartz inequality and then plugging M \u0105 |\u2207\u2113i| and the bounds found for }C}, }B} \u0103 M1\u03f50.75 leads to |\u2207\u2113i| \u00a8 i \u00a8 |CAiB| \u010f M \u00a8 i \u00a8 }C}}A}iF }B} \u010f M \u00a8 i \u00a8 M21 \u03f51.5}A}iF (C.102) Putting all together provides us with TrpAJ 9Aq \u010f k\u00b41 \u00ff i\u201c1 M \u00a8 i \u00a8 M21 \u03f51.5}A}iF \u201c M \u00a8 M21 \u03f51.5 k\u00b41 \u00ff i\u201c1 i \u00a8 }A}iF (C.103) Noting that }A}F \u0103 1 and denoting M\u03032 \u201c k 2 2 M \u00a8 M 2 1 leads to k\u00b41 \u00ff i\u201c1 i}A}iF \u0103 }A}F kpk \u00b4 1q 2 \u0103 k 2 2 }A|}F (C.104) Therefore, we get that TrpAJ 9Aq \u0103 M\u03032\u03f51.5}A}F . (C.105) Notice that }A}F \u00a8 d dt p}A}F q \u201c 0.5 d dt p}A}2F q \u201c TrpAJ 9Aq (C.106) \u00f1 }A}F \u00a8 d dt p}A}F q \u0103 M\u03032\u03f51.5}A}F . (C.107) \u00f1 d dt p}A}F q \u0103 M\u03032\u03f51.5 (C.108) Therefore, for any 0 \u010f t3 \u010f T : }Apt3q}F \u00b4 }Ap0q}F \u201c \u017c t3 0 d dt p}A}F qdt \u0103 M\u03032\u03f51.5t3 (C.109) \u00f1 }Apt3q}F \u0103 M\u03032\u03f51.5T ` }Ap0q}F . (C.110) Recall that by Lemma C.11, we have for any x \u0105 0, lnpxq \u0103 x that }Ap0q}F \u0103 4d\u03f5. (C.111) Using the fact that T \u201c 1 2w0 ln \u02c6 1 \u03f50.5 \u02d9 \u0103 1 2w0 \u03f5\u00b40.5 (C.112) we get that \u00f1 }Apt3q}F \u0103 M\u03032 2w0 \u03f5 ` 4d\u03f5. (C.113) Taking M2 \u201c M\u030322w0 ` 4\u03f5, we have for all 0 \u010f t \u010f T that \u00f1 }Aptq}F \u0103 M2\u03f5 (C.114) Once again, recall that we defined tA as the maximal t with this property, thus T \u0103 tA. Since we already have also that T \u0103 tB , tC , we get T \u0103 minpTA, tB , tCq \u201c T\u02da. C.4.1 BOUNDING WJW AND Y JY Proof of Lemma C.15. Denote Y \u201d CJ \u00b4 B, W \u201d CJ ` B. (C.115) Recall that BL BB \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqiCJ, BL BC \u201c B J k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqi (C.116) We can write the change in Y , 9Y \u201c 9CJ \u00b4 9B \u201c \u00b4 k\u00b41 \u00ff i\u201c0 \u2207\u2113iAiB ` k\u00b41 \u00ff i\u201c0 \u2207\u2113ipAJqiCJ (C.117) \u201c k\u00b41 \u00ff i\u201c0 \u2207\u2113i ` pAJqiCJ \u00b4 AiB \u02d8 Denote pAiqS \u201c A i`pAJqi 2 and pA iqS\u0304 \u201c Ai\u00b4pAJqi 2 the symmetric and anti-symmetric parts of A i. We can now write pAJqiCJ \u00b4 AiB \u201c \u201c pAiqS \u00b4 pAiqS\u0304 \u2030 CJ \u00b4 \u201c pAiqS ` pAiqS\u0304 \u2030 B (C.118) \u201c pAiqSpCJ \u00b4 Bq \u00b4 pAiqS\u0304pCJ ` Bq \u201c pAiqSY \u00b4 pAiqS\u0304W Note also that for i \u201c 0, we have A0 \u201c I and its anti-symmetric part is the zero matrix, writing i \u201c 0 separately and assigning equation C.118 into equation C.117, 9Y \u201c \u2207\u21130Y ` k\u00b41 \u00ff i\u201c1 \u2207\u2113i ` pAiqSY \u00b4 pAiqS\u0304W \u02d8 (C.119) Let us look at ddt pY JY q \u201c 9Y JY ` Y J 9Y \u201c 2Y J 9Y . Multiplying equation C.119 from the left with Y J evaluates to Y J 9Y \u201c \u2207\u21130Y JY ` k\u00b41 \u00ff i\u201c1 \u2207\u2113i ` Y JpAiqSY \u00b4 Y JpAiqS\u0304W \u02d8 (C.120) We now turn to bound the terms in the sum. Y J 9Y \u010f \u2207\u21130Y JY ` k\u00b41 \u00ff i\u201c1 |\u2207\u2113i| ` |Y JpAiqSY | ` |Y JpAiqS\u0304W | \u02d8 (C.121) We can bound each term using Cauchy\u2013Schwarz. We first need to bound }Y } and }W }, which are trivially bounded by }Y }, }W } \u010f }C} ` }B} \u010f 2M1\u03f50.75 (C.122) As for the symmetric and anti-symmetric parts of A, }pAiqS}F \u201c \u203a \u203a \u203a \u203a Ai ` pAiqJ 2 \u203a \u203a \u203a \u203a F \u010f 1 2 ` }Ai}F ` }pAiqJ}F \u02d8 \u201c }Ai}F \u010f }A}iF , (C.123) where the last inequality follows again from Cauchy\u2013Schwarz (the same considerations apply for pAiqS\u0304). From Cauchy\u2013Schwarz we can bound |Y JpAiqS\u0304W | \u010f }Y J}}pAiqS\u0304}F }W } \u010f }Y }}A}iF }W }, we denote M3 \u201c maxpM1,M2q and derive: |Y JpAiqS\u0304W | \u010f 2M1\u03f50.75pM2\u03f5qi2M1\u03f50.75 \u0103 4M33 \u03f51.5`i, (C.124) which is maximized when i \u201c 1. We again bound: M \u201c max i p|wi|q ` M21 \u03f51.5 \u0105 |\u2207\u2113i|. We can bound the terms in equation C.121 by |\u2207\u2113i| ` |Y JpAiqSY | ` |Y JpAiqS\u0304W | \u02d8 \u010f 8 \u00a8 M \u00a8 M33 \u03f52.5 (C.125) Plugging back into equation C.121: Y J 9Y \u010f \u2207\u21130Y JY ` k\u00b41 \u00ff i\u201c1 8 \u00a8 M \u00a8 M33 \u03f52.5 (C.126) We can also bound \u2207\u21130 \u201c pCB \u00b4w0q \u010f \u00b4w0 ` |CB| \u010f \u00b4w0 ` }C}}B} \u010f \u00b4w0 `M21 \u03f51.5. Note also that we multiply by Y JY so we can bound \u2207\u21130Y JY \u010f \u00b4w0Y JY ` M21 \u03f51.5pM21\u03f50.75q2 loooooooooomoooooooooon \u201c4M41 \u03f53 (C.127) putting back together, we get Y J 9Y \u010f \u00b4w0Y JY ` 4M41 \u03f53 ` pk \u00b4 1qpM1 ` 1q8 \u00a8 M \u00a8 M33 \u03f52.5 (C.128) In particular, there exists M4 such that Y J 9Y \u010f \u00b4w0Y JY ` M4 2 \u03f52.5 (C.129) Recall that we were interested in bounding ddt pY JY q \u201c 9Y JY ` Y J 9Y \u201c 2Y J 9Y , d dt pY JY q \u010f \u00b42w0Y JY ` M4\u03f52.5 (C.130) Denoting zptq \u201d Y ptqJY ptq and xptq \u201d W ptqJW ptq, and using Lemma C.16, we have the desired bounds zptq \u010f 1 2w0 \u201c p33w0d\u03f52qe\u00b42w0t ` M4\u03f52.5 \u2030 (C.131) In particular, we can write zptq \u010f c1\u03f52e\u00b42w0t ` c2\u03f52.5 (C.132) and xptq \u011b c3\u03f52e2w0t \u00b4 c4\u03f52.5 (C.133) where ci\u2019s are positive constants. Note that the derivation of xptq is exactly the same as zptq with opposite signs and bounding from below instead. C.4.2 INTEGRAL BOUND OF }Y }2 AND }W }2 Lemma C.16. Assume 9z \u0103 \u00b42w0z ` M4\u03f52.5 \u0103 0, 9x \u0105 2w0z \u00b4 M4\u03f52.5 \u0105 0 , where w0 \u0105 0. Then, under the assumptions of Lemma C.11: zpt1q \u0103 1 2w0 ` expp\u00b42w0t1q \u00a8 p75w0d\u03f52q ` M4\u03f52.5 \u02d8 (C.134) xpt2q \u0105 1 2w0 \u02c6 expp2w0t2q \u00a8 p w0\u03f5 2 25d q ` M4\u03f52.5 \u02d9 (C.135) Proof of Lemma C.16. Assume 9z \u0103 \u00b42w0z ` M4\u03f52.5 , where w0 \u0105 0, 2w0z \u0105 M4\u03f52.5. Similarly, assume 9x \u0105 2w0x \u00b4 M4\u03f52.5. Then: 9z 2w0z \u00b4 M4\u03f52.5 \u0103 \u00b41 (C.136) 9x 2w0x \u00b4 M4\u03f52.5 \u0105 1 (C.137) Integrating both sides by dt, and using integration by substitution, we get: \u00b4t1 \u201c \u017c t1 0 \u00b41 \u0105 \u017c t1 0 1 2w0z \u00b4 M4\u03f52.5 dz dt dt \u201c \u017c zpt1q zp0q 1 2w0z \u00b4 M4\u03f52.5 dz (C.138) t2 \u201c \u017c t2 0 1 \u0103 \u017c t2 0 1 2w0x \u00b4 M4\u03f52.5 dx dt dt \u201c \u017c xpt2q xp0q 1 2w0x \u00b4 M4\u03f52.5 dx (C.139) We note: \u017c zpt1q zp0q 1 2w0z \u00b4 M4\u03f52.5 dz \u201c 1 2w0 rlnp2w0zpt1q \u00b4 M4\u03f52.5q \u00b4 lnp2w0zp0q \u00b4 M4\u03f52.5qs (C.140) \u00f1 \u017c zpt1q zp0q 1 2w0z \u00b4 M4\u03f52.5 dz \u201c 1 2w0 \u201e ln \u02c6 2w0zpt1q \u00b4 M4\u03f52.5 2w0zp0q \u00b4 M4\u03f52.5 \u02d9\u0237 (C.141) \u017c xpt2q xp0q 1 2w0x \u00b4 M4\u03f52.5 dx \u201c 1 2w0 \u201e ln \u02c6 2w0xpt2q \u00b4 M4\u03f52.5 2w0xp0q \u00b4 M4\u03f52.5 \u02d9\u0237 (C.142) Combining equations, we have: 1 2w0 \u201e ln \u02c6 2w0zpt1q \u00b4 M4\u03f52.5 2w0zp0q \u00b4 M4\u03f52.5 \u02d9\u0237 \u0103 \u00b4t1 (C.143) \u00f1 ln \u02c6 2w0zpt1q \u00b4 M4\u03f52.5 2w0zp0q \u00b4 M4\u03f52.5 \u02d9 \u0103 \u00b42w0t1 (C.144) \u00f1 2w0zpt1q \u00b4 M4\u03f5 2.5 2w0zp0q \u00b4 M4\u03f52.5 \u0103 expp\u00b42w0t1q (C.145) \u00f1 2w0zpt1q \u00b4 M4\u03f52.5 \u0103 expp\u00b42w0t1q \u00a8 ` 2w0zp0q \u00b4 M4\u03f52.5 \u02d8 (C.146) \u00f1 zpt1q \u0103 1 2w0 \u201c expp\u00b42w0t1q \u00a8 p2w0zp0q \u00b4 M4\u03f52.5q ` M4\u03f52.5 \u2030 (C.147) xpt2q \u0105 1 2w0 \u201c expp2w0t2q \u00a8 p2w0xp0q \u00b4 M4\u03f52.5q ` M4\u03f52.5 \u2030 (C.148) We note, zp0q \u201c Y T p0qY p0q. By linearity of sum of variances, Y p0q\u2019s entries are distributed according to N p ? 2 \u00a8 \u03f5q, by Lemma C.11: ? 2 2 \u03f5 \u0103 Y p0q \u0103 3 ? 2 2 \u03f5 (C.149) From Cauchy-Schwartz, zp0q \u0103 3\u03f52 with high probability. W is distributed as Y , therefore, similarly, xp0q \u0105 12\u03f5 2. Assuming M4\u03f52.5 \u0103 w0\u03f5 2 2 , we have: zpt1q \u0103 1 2w0 ` expp\u00b42w0t1q \u00a8 p6w0\u03f52q ` M4\u03f52.5 \u02d8 (C.150) xpt2q \u0105 1 2w0 ` expp2w0t2q \u00a8 pw0\u03f52q ` M4\u03f52.5 \u02d8 (C.151) Concluding the proof."}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'website': 'iclr.cc', 'query': 'ICLR 2023 acceptance rate'}, name='GoogleSearchTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": [\n    \"https://media.iclr.cc/Conferences/ICLR2023/ICLR2023-Fact_Sheet.pdf\",\n    \"https://media.iclr.cc/Conferences/ICLR2024/ICLR2024-Fact_Sheet.pdf\",\n    \"https://blog.iclr.cc/category/iclr-2023/\",\n    \"https://iclr.cc/Conferences/2023\",\n    \"https://blog.iclr.cc/2023/11/02/tiny-papers-strike-back-2023-reflections-and-2024-announcement/\",\n    \"https://media.iclr.cc/Conferences/ICLR2025/ICLR2025_Fact_Sheet.pdf\",\n    \"https://www.iclr.cc/media/Press/ICLR_2023_Press_Release.pdf\",\n    \"https://iclr.cc/virtual/2023/papers.html\",\n    \"https://iclr.cc/Conferences/2023/CallForPapers\",\n    \"https://iclr.cc/\"\n  ]\n}\n```\n", "metadata": {"title": "Invoking `GoogleSearchTool` with \n```json\n{\n  \"website\": \"iclr.cc\",\n  \"query\": \"ICLR 2023 acceptance rate\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GoogleSearchTool', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': ['https://media.iclr.cc/Conferences/ICLR2023/ICLR2023-Fact_Sheet.pdf', 'https://media.iclr.cc/Conferences/ICLR2024/ICLR2024-Fact_Sheet.pdf', 'https://blog.iclr.cc/category/iclr-2023/', 'https://iclr.cc/Conferences/2023', 'https://blog.iclr.cc/2023/11/02/tiny-papers-strike-back-2023-reflections-and-2024-announcement/', 'https://media.iclr.cc/Conferences/ICLR2025/ICLR2025_Fact_Sheet.pdf', 'https://www.iclr.cc/media/Press/ICLR_2023_Press_Release.pdf', 'https://iclr.cc/virtual/2023/papers.html', 'https://iclr.cc/Conferences/2023/CallForPapers', 'https://iclr.cc/']}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'url': 'https://media.iclr.cc/Conferences/ICLR2023/ICLR2023-Fact_Sheet.pdf', 'output_type': 'summary'}, name='GetWebsite'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": \"Lv99  GpM tQ4  9Zil  b9G4My76g a jte88kc9N.m2dbOf N LXKsc3icLVSpGZ b  wc Jyh06IeU DbtNs69758 0ZrlQiLXbh0 RHHZFccU 9ZnGzu0HGxCBbENJUQUXnqvcR6FVVVu.vTYll5Z5jsJbS3.. IRQe.lxLJFn7 vFLNgDGuTV0ziV lEH Tu 3v QHWYYri  ja 5 OG6L6AtK 1gWSLy A Xb0mVTluuLo5INOhT  6WNrlB2TF X  dvJTEu.. TNJ6IeaFyn2 2T lI6FL2E h 40XhczwoCTO 04c 5FA xb hT BnAO8Sh sr896VAoah005S   L   dG3x0j j   K998P ..\"\n}\n```\n", "metadata": {"title": "Invoking `GetWebsite` with \n```json\n{\n  \"url\": \"https://media.iclr.cc/Conferences/ICLR2023/ICLR2023-Fact_Sheet.pdf\",\n  \"output_type\": \"summary\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetWebsite', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': 'Lv99  GpM tQ4  9Zil  b9G4My76g a jte88kc9N.m2dbOf N LXKsc3icLVSpGZ b  wc Jyh06IeU DbtNs69758 0ZrlQiLXbh0 RHHZFccU 9ZnGzu0HGxCBbENJUQUXnqvcR6FVVVu.vTYll5Z5jsJbS3.. IRQe.lxLJFn7 vFLNgDGuTV0ziV lEH Tu 3v QHWYYri  ja 5 OG6L6AtK 1gWSLy A Xb0mVTluuLo5INOhT  6WNrlB2TF X  dvJTEu.. TNJ6IeaFyn2 2T lI6FL2E h 40XhczwoCTO 04c 5FA xb hT BnAO8Sh sr896VAoah005S   L   dG3x0j j   K998P ..'}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'output_type': 'full_text', 'url': 'https://iclr.cc/Conferences/2023/CallForPapers'}, name='GetWebsite'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Search completed successfully\",\n  \"output\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCall for Papers\\n\\n\\n\\n\\n\\nSkip to yearly menu bar\\nSkip to main content\\n\\nMain Navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            ICLR\\n        \\n\\n\\n\\n\\n                        Help/FAQ\\n                    \\n\\n\\n\\n\\n\\n\\n                        Contact ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n                        Downloads\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Blog\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n                        Privacy Policy\\n                    \\n\\n\\n\\n\\n\\n\\n                        Create Profile\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reset Password\\n                    \\n\\n\\n\\n\\n\\n\\n                        Journal To Conference Track\\n                    \\n\\n\\n\\n\\n\\n\\n                        Diversity & Inclusion\\n                    \\n\\n\\n\\n\\n\\n\\n                        Proceedings at OpenReview\\n                    \\n\\n\\n\\n\\n\\n\\n                        Future Meetings\\n                    \\n\\n\\n\\n\\n\\n\\n                        Press\\n                    \\n\\n\\n\\n\\n\\n\\n                        Exhibitor Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Twitter\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        My Stuff\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Login\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Select Year: (2023)\\n        \\n\\n\\n2026\\n                \\n\\n\\n\\n2025\\n                \\n\\n\\n\\n2024\\n                \\n\\n\\n\\n2023\\n                \\n\\n\\n\\n2022\\n                \\n\\n\\n\\n2021\\n                \\n\\n\\n\\n2020\\n                \\n\\n\\n\\n2019\\n                \\n\\n\\n\\n2018\\n                \\n\\n\\n\\n2017\\n                \\n\\n\\n\\n2016\\n                \\n\\n\\n\\n2015\\n                \\n\\n\\n\\n2014\\n                \\n\\n\\n\\n2013\\n                \\n\\n\\n\\n\\n\\n\\n                        Dates\\n                    \\n\\n\\n\\n\\n            Calls\\n        \\n\\n\\n\\n\\n                        Call for Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Tiny Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Workshops\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Blog Posts\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Socials\\n                    \\n\\n\\n\\n\\n\\n\\n            Guides\\n        \\n\\n\\n\\n\\n                        Author Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewer Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Area Chair Guide\\n                    \\n\\n\\n\\n\\n\\n\\n            Attend\\n        \\n\\n\\n\\n\\n                        Register\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visa and Travel  Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        Hotels\\n                    \\n\\n\\n\\n\\n\\n\\n                        Child Care\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visiting Kigali\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n            Organization\\n        \\n\\n\\n\\n\\n                        ICLR Board\\n                    \\n\\n\\n\\n\\n\\n\\n                        Organizing Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Program Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Ethics Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        Conference Site\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n2023 - Call For Papers\\nHybrid (physical and virtual) conference, Mon May 1st through Fri the 5th\\nWe invite submissions to the 11th International Conference on Learning Representations, and welcome paper submissions from all areas of machine learning.\\nFor any information needed that is not listed below, please submit questions using this link: https://iclr.cc/Help/Contact.\\nKey dates\\nThe planned dates are as follow:\\n\\nAbstract submission: Sept 21 (Anywhere on Earth)\\nSubmission date: Sept 28 (Anywhere on Earth)\\nReviews released: Nov 4\\nFinal decisions: Jan 20\\n\\n\u00a0\\n\\nSubject Areas`\\nWe consider a broad range of subject areas including feature learning, metric learning, compositional modeling, structured prediction, reinforcement learning, and issues regarding large-scale learning and non-convex optimization, as well as applications in vision, audio, speech , language, music, robotics, games, healthcare, biology, sustainability, economics, ethical considerations in ML, and others.\\nA non-exhaustive list of relevant topics:\\n\\nunsupervised, semi-supervised, and supervised representation learning\\nrepresentation learning for planning and reinforcement learning\\nrepresentation learning for computer vision and natural language processing\\nmetric learning and kernel learning\\nsparse coding and dimensionality expansion\\nhierarchical models\\noptimization for representation learning\\nlearning representations of outputs or states\\noptimal transport\\ntheoretical issues in deep learning\\nsocietal considerations of representation learning including fairness, safety, privacy, and interpretability, and explainability\\nvisualization or interpretation of learned representations\\nimplementation issues, parallelization, software platforms, hardware\\nclimate, sustainability\\napplications in audio, speech, robotics, neuroscience,\u00a0 biology, or any other field\\n\\nDouble blind reviewing\\nSubmissions will be double blind: reviewers cannot see author names when conducting reviews, and authors cannot see reviewer names. This means that the submission must not contain acknowledgements or any link (e.g., github) that would reveal authors\u2019 identity.\\nWe use OpenReview to host papers and allow for public discussions that can be seen by all, comments that are posted by reviewers will remain anonymous. The program will include oral presentations and posters of accepted papers.\\nAuthors can revise their paper as many times as needed up to the paper submission deadline. Changes to the paper will not be allowed while the paper is being reviewed. During the discussion phase (between area chairs, reviewers and authors), edits will again be allowed; a pdfdiff will be done against the submission at the paper submission deadline. Area chairs and reviewers reserve the right to ignore changes that are significant from the original scope of the paper. As in the past four years, workshops will have their own organization and call for contributions.\\nSubmission Instructions\\nThis year we are asking authors to submit paper abstracts by the abstract submission deadline of Sept 21, 2022. Please note that no changes on the authors and their orders can be made after the abstract submission deadline. Also please make sure that all authors have an OpenReview profile with the latest information.\u00a0 Abstracts submitted by the abstract submission deadline must be genuine, placeholder or duplicate abstracts will be removed.\\nThe full paper submission deadline is Sept 28, 2022. Abstracts and papers must be submitted using the conference submission system at: https://openreview.net/group?id=ICLR.cc/2023/Conference. The submission site will be open on Aug 22, 2022. \u00a0 \\nPaper length\\nThere will be a strict upper limit of 9 pages for the main text of the submission, with unlimited additional pages for citations. This page limit applies to both the initial and final camera ready version.\\n\\nAuthors may use as many pages of appendices (after the bibliography) as they wish, but reviewers are not required to read the appendix.\\nThe optional ethic statement will not count toward the page limit, but should not be more than 1 page. (see Author\\nThe optional reproducibility statement will not count toward the page limit, but should not be more than 1 page.\\nThe optional acknowledgment will not count toward the page limit, but should not be more than 1 page.\\n\\n\u00a0\\nStyle files and Templates\\nTo prepare your submission to ICLR 2023, please use the LaTeX style files provided at:\\nhttps://github.com/ICLR/Master-Template/raw/master/iclr2023.zip\\n\u00a0\\n\\nAuthors are strongly encouraged to participate in the public discussion of their paper, as well as of any other paper submitted to the conference. Submissions and reviews are both anonymous. For detailed instructions about the format of the paper, please visit www.iclr.cc.\\n\u00a0\\nReviewing Process\\n\\nSubmissions to ICLR are uploaded on OpenReview, which enables public discussion. Official reviews are anonymous and publicly visible. During the public discussion phase, anybody who is logged in can post comments that are publicly visible, or restrict visibility to reviewers and up, ACs and up, or just PCs. In addition, the author of a comment can decide to post anonymously or not. Login is required before posting any comment.\\nOnce reviews have been posted, authors are encouraged to revise their paper until the paper submission deadline. Authors can participate in the discussion about their paper, as well as about any other paper submitted to the conference, at any time.\\nFull reviews are posted by Nov 4. Reviews are anonymous and publicly visible in Open Review. Once the reviews are posted, authors are free to upload modifications to the paper during the discussion period.\\nDuring the discussion period, if you choose to update the submission, a pdfdiff will be applied to compare new changes to the paper against the original submission. Area chairs and reviewers reserve the right to ignore changes that are significantly different from the original paper. In addition, during this period, any submission that is cited will be given an anonymous BibTex entry.\\nAfter discussion there will be an internal discussion period amongst reviewers and ACs with the aim of summarizing the review process, after which acceptance decisions are made. Papers that are not accepted will be considered non-archival, and may be submitted elsewhere (modified or not), although the OpenReview site will maintain the reviews, the comments, and links to the versions submitted to ICLR.\\nAll submitted papers (accepted or rejected) will be deanonymized after the notification. The submissions and reviews will be released to the public.  \\n\\t\u00a0\\n\\nCode of Conduct\\nAll ICLR participants, including authors, are required to adhere to the ICLR code of conduct (https://iclr.cc/public/CodeOfConduct). More detailed guidance for authors, reviewers, and all other participants will be made available in due course, and participation will require acknowledging and adhering to the provided guidelines.\u00a0\\n\u00a0\\nCode of Ethics\\nAll ICLR participants, including authors, are required to adhere to the ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics). All authors of submitted papers are required to read the Code of Ethics, adhere to it, and explicitly acknowledge this during the submission process. The Code of Ethics applies to all conference participation, including paper submission, reviewing, and paper discussion.\u00a0\\nDual Submission Policy\\nSubmissions that are identical (or substantially similar) to versions that have been previously published, or accepted for publication, or that have been submitted in parallel to this or other conferences, journals or archival workshops, are not allowed and violate our dual submission policy. However, papers that cite previous related work by the authors and papers that have appeared on non-peer reviewed websites (like arXiv) or that have been presented at workshops (i.e., venues that do not have publication proceedings) do not violate the policy. The policy is enforced during the whole reviewing process period. Submission of the paper to archival repositories such as arXiv is allowed.\\nTo allow citation of papers that are under review at ICLR2023, OpenReview provides BibTeX entries that do not list the authors, but does give the title, year and url. Author names are revealed at the time of notification.\\nWithdrawal Policy\\nAuthors have the right to withdraw papers from consideration at any time until paper notification. Before the paper submission deadline, if an author withdraws the paper it will be deleted from the OpenReview hosting site. However, note that after the paper submission deadline, if an author chooses to withdraw a submission, it will remain hosted by OpenReview in a publicly visible \\\"withdrawn papers\\\" section. Like on arXiv, submissions to ICLR cannot be deleted or modified. Withdrawn papers will be de-anonymized immediately.\\n\\n\u00a0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuccessful Page Load\\n\\n\\n\\n\\n\\n\\n\\n                        ICLR uses cookies for essential functions only. We do not sell your personal\\n                        information.\\n                        Our Privacy Policy \u00bb\u00a0\\n\\n\\nAccept\\n                            Cookies\\n                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            The ICLR Logo above may be used on presentations. Right-click and choose\\n                            download. It is a vector graphic and may be used at any scale.\\n                        \\n\\n\\n\\n\\n\\n                            Useful links\\n                        \\n\\n\\nSponsor / Exhibitor Information\\nPress\\n\\n\\n\\n\\n\\n\\n\\nContact\\n\\n 2710 E Corridor Dr, Appleton WI 54913\\n                            \\n\\n Email\\n\\n Phone: +1-920-268-4789\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR Proceedings at OpenReview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"\n}\n```\n", "metadata": {"title": "Invoking `GetWebsite` with \n```json\n{\n  \"output_type\": \"full_text\",\n  \"url\": \"https://iclr.cc/Conferences/2023/CallForPapers\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='GetWebsite', response={'result': {'status': 'success', 'message': 'Search completed successfully', 'output': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCall for Papers\\n\\n\\n\\n\\n\\nSkip to yearly menu bar\\nSkip to main content\\n\\nMain Navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            ICLR\\n        \\n\\n\\n\\n\\n                        Help/FAQ\\n                    \\n\\n\\n\\n\\n\\n\\n                        Contact ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n                        Downloads\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Blog\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n                        Privacy Policy\\n                    \\n\\n\\n\\n\\n\\n\\n                        Create Profile\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reset Password\\n                    \\n\\n\\n\\n\\n\\n\\n                        Journal To Conference Track\\n                    \\n\\n\\n\\n\\n\\n\\n                        Diversity & Inclusion\\n                    \\n\\n\\n\\n\\n\\n\\n                        Proceedings at OpenReview\\n                    \\n\\n\\n\\n\\n\\n\\n                        Future Meetings\\n                    \\n\\n\\n\\n\\n\\n\\n                        Press\\n                    \\n\\n\\n\\n\\n\\n\\n                        Exhibitor Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        ICLR Twitter\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        My Stuff\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Login\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Select Year: (2023)\\n        \\n\\n\\n2026\\n                \\n\\n\\n\\n2025\\n                \\n\\n\\n\\n2024\\n                \\n\\n\\n\\n2023\\n                \\n\\n\\n\\n2022\\n                \\n\\n\\n\\n2021\\n                \\n\\n\\n\\n2020\\n                \\n\\n\\n\\n2019\\n                \\n\\n\\n\\n2018\\n                \\n\\n\\n\\n2017\\n                \\n\\n\\n\\n2016\\n                \\n\\n\\n\\n2015\\n                \\n\\n\\n\\n2014\\n                \\n\\n\\n\\n2013\\n                \\n\\n\\n\\n\\n\\n\\n                        Dates\\n                    \\n\\n\\n\\n\\n            Calls\\n        \\n\\n\\n\\n\\n                        Call for Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Tiny Papers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Workshops\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Blog Posts\\n                    \\n\\n\\n\\n\\n\\n\\n                        Call for Socials\\n                    \\n\\n\\n\\n\\n\\n\\n            Guides\\n        \\n\\n\\n\\n\\n                        Author Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewer Guide\\n                    \\n\\n\\n\\n\\n\\n\\n                        Area Chair Guide\\n                    \\n\\n\\n\\n\\n\\n\\n            Attend\\n        \\n\\n\\n\\n\\n                        Register\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visa and Travel  Information\\n                    \\n\\n\\n\\n\\n\\n\\n                        Hotels\\n                    \\n\\n\\n\\n\\n\\n\\n                        Child Care\\n                    \\n\\n\\n\\n\\n\\n\\n                        Visiting Kigali\\n                    \\n\\n\\n\\n\\n\\n\\n                        Code of Conduct\\n                    \\n\\n\\n\\n\\n\\n\\n            Organization\\n        \\n\\n\\n\\n\\n                        ICLR Board\\n                    \\n\\n\\n\\n\\n\\n\\n                        Organizing Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Program Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        Reviewers\\n                    \\n\\n\\n\\n\\n\\n\\n                        Ethics Committee\\n                    \\n\\n\\n\\n\\n\\n\\n                        About ICLR\\n                    \\n\\n\\n\\n\\n\\n\\n\\n                        Conference Site\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n2023 - Call For Papers\\nHybrid (physical and virtual) conference, Mon May 1st through Fri the 5th\\nWe invite submissions to the 11th International Conference on Learning Representations, and welcome paper submissions from all areas of machine learning.\\nFor any information needed that is not listed below, please submit questions using this link: https://iclr.cc/Help/Contact.\\nKey dates\\nThe planned dates are as follow:\\n\\nAbstract submission: Sept 21 (Anywhere on Earth)\\nSubmission date: Sept 28 (Anywhere on Earth)\\nReviews released: Nov 4\\nFinal decisions: Jan 20\\n\\n\\xa0\\n\\nSubject Areas`\\nWe consider a broad range of subject areas including feature learning, metric learning, compositional modeling, structured prediction, reinforcement learning, and issues regarding large-scale learning and non-convex optimization, as well as applications in vision, audio, speech , language, music, robotics, games, healthcare, biology, sustainability, economics, ethical considerations in ML, and others.\\nA non-exhaustive list of relevant topics:\\n\\nunsupervised, semi-supervised, and supervised representation learning\\nrepresentation learning for planning and reinforcement learning\\nrepresentation learning for computer vision and natural language processing\\nmetric learning and kernel learning\\nsparse coding and dimensionality expansion\\nhierarchical models\\noptimization for representation learning\\nlearning representations of outputs or states\\noptimal transport\\ntheoretical issues in deep learning\\nsocietal considerations of representation learning including fairness, safety, privacy, and interpretability, and explainability\\nvisualization or interpretation of learned representations\\nimplementation issues, parallelization, software platforms, hardware\\nclimate, sustainability\\napplications in audio, speech, robotics, neuroscience,\\xa0 biology, or any other field\\n\\nDouble blind reviewing\\nSubmissions will be double blind: reviewers cannot see author names when conducting reviews, and authors cannot see reviewer names. This means that the submission must not contain acknowledgements or any link (e.g., github) that would reveal authors\u2019 identity.\\nWe use OpenReview to host papers and allow for public discussions that can be seen by all, comments that are posted by reviewers will remain anonymous. The program will include oral presentations and posters of accepted papers.\\nAuthors can revise their paper as many times as needed up to the paper submission deadline. Changes to the paper will not be allowed while the paper is being reviewed. During the discussion phase (between area chairs, reviewers and authors), edits will again be allowed; a pdfdiff will be done against the submission at the paper submission deadline. Area chairs and reviewers reserve the right to ignore changes that are significant from the original scope of the paper. As in the past four years, workshops will have their own organization and call for contributions.\\nSubmission Instructions\\nThis year we are asking authors to submit paper abstracts by the abstract submission deadline of Sept 21, 2022. Please note that no changes on the authors and their orders can be made after the abstract submission deadline. Also please make sure that all authors have an OpenReview profile with the latest information.\\xa0 Abstracts submitted by the abstract submission deadline must be genuine, placeholder or duplicate abstracts will be removed.\\nThe full paper submission deadline is Sept 28, 2022. Abstracts and papers must be submitted using the conference submission system at: https://openreview.net/group?id=ICLR.cc/2023/Conference. The submission site will be open on Aug 22, 2022. \\xa0 \\nPaper length\\nThere will be a strict upper limit of 9 pages for the main text of the submission, with unlimited additional pages for citations. This page limit applies to both the initial and final camera ready version.\\n\\nAuthors may use as many pages of appendices (after the bibliography) as they wish, but reviewers are not required to read the appendix.\\nThe optional ethic statement will not count toward the page limit, but should not be more than 1 page. (see Author\\nThe optional reproducibility statement will not count toward the page limit, but should not be more than 1 page.\\nThe optional acknowledgment will not count toward the page limit, but should not be more than 1 page.\\n\\n\\xa0\\nStyle files and Templates\\nTo prepare your submission to ICLR 2023, please use the LaTeX style files provided at:\\nhttps://github.com/ICLR/Master-Template/raw/master/iclr2023.zip\\n\\xa0\\n\\nAuthors are strongly encouraged to participate in the public discussion of their paper, as well as of any other paper submitted to the conference. Submissions and reviews are both anonymous. For detailed instructions about the format of the paper, please visit www.iclr.cc.\\n\\xa0\\nReviewing Process\\n\\nSubmissions to ICLR are uploaded on OpenReview, which enables public discussion. Official reviews are anonymous and publicly visible. During the public discussion phase, anybody who is logged in can post comments that are publicly visible, or restrict visibility to reviewers and up, ACs and up, or just PCs. In addition, the author of a comment can decide to post anonymously or not. Login is required before posting any comment.\\nOnce reviews have been posted, authors are encouraged to revise their paper until the paper submission deadline. Authors can participate in the discussion about their paper, as well as about any other paper submitted to the conference, at any time.\\nFull reviews are posted by Nov 4. Reviews are anonymous and publicly visible in Open Review. Once the reviews are posted, authors are free to upload modifications to the paper during the discussion period.\\nDuring the discussion period, if you choose to update the submission, a pdfdiff will be applied to compare new changes to the paper against the original submission. Area chairs and reviewers reserve the right to ignore changes that are significantly different from the original paper. In addition, during this period, any submission that is cited will be given an anonymous BibTex entry.\\nAfter discussion there will be an internal discussion period amongst reviewers and ACs with the aim of summarizing the review process, after which acceptance decisions are made. Papers that are not accepted will be considered non-archival, and may be submitted elsewhere (modified or not), although the OpenReview site will maintain the reviews, the comments, and links to the versions submitted to ICLR.\\nAll submitted papers (accepted or rejected) will be deanonymized after the notification. The submissions and reviews will be released to the public.  \\n\\t\\xa0\\n\\nCode of Conduct\\nAll ICLR participants, including authors, are required to adhere to the ICLR code of conduct (https://iclr.cc/public/CodeOfConduct). More detailed guidance for authors, reviewers, and all other participants will be made available in due course, and participation will require acknowledging and adhering to the provided guidelines.\\xa0\\n\\xa0\\nCode of Ethics\\nAll ICLR participants, including authors, are required to adhere to the ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics). All authors of submitted papers are required to read the Code of Ethics, adhere to it, and explicitly acknowledge this during the submission process. The Code of Ethics applies to all conference participation, including paper submission, reviewing, and paper discussion.\\xa0\\nDual Submission Policy\\nSubmissions that are identical (or substantially similar) to versions that have been previously published, or accepted for publication, or that have been submitted in parallel to this or other conferences, journals or archival workshops, are not allowed and violate our dual submission policy. However, papers that cite previous related work by the authors and papers that have appeared on non-peer reviewed websites (like arXiv) or that have been presented at workshops (i.e., venues that do not have publication proceedings) do not violate the policy. The policy is enforced during the whole reviewing process period. Submission of the paper to archival repositories such as arXiv is allowed.\\nTo allow citation of papers that are under review at ICLR2023, OpenReview provides BibTeX entries that do not list the authors, but does give the title, year and url. Author names are revealed at the time of notification.\\nWithdrawal Policy\\nAuthors have the right to withdraw papers from consideration at any time until paper notification. Before the paper submission deadline, if an author withdraws the paper it will be deleted from the OpenReview hosting site. However, note that after the paper submission deadline, if an author chooses to withdraw a submission, it will remain hosted by OpenReview in a publicly visible \"withdrawn papers\" section. Like on arXiv, submissions to ICLR cannot be deleted or modified. Withdrawn papers will be de-anonymized immediately.\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuccessful Page Load\\n\\n\\n\\n\\n\\n\\n\\n                        ICLR uses cookies for essential functions only. We do not sell your personal\\n                        information.\\n                        Our Privacy Policy \u00bb\\xa0\\n\\n\\nAccept\\n                            Cookies\\n                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            The ICLR Logo above may be used on presentations. Right-click and choose\\n                            download. It is a vector graphic and may be used at any scale.\\n                        \\n\\n\\n\\n\\n\\n                            Useful links\\n                        \\n\\n\\nSponsor / Exhibitor Information\\nPress\\n\\n\\n\\n\\n\\n\\n\\nContact\\n\\n 2710 E Corridor Dr, Appleton WI 54913\\n                            \\n\\n Email\\n\\n Phone: +1-920-268-4789\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nICLR Proceedings at OpenReview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I have gathered the following information about ICLR 2023:\n\n*   It is a hybrid conference (physical and virtual).\n*   It welcomes submissions from all areas of machine learning.\n*   Key dates: Abstract submission: Sept 21, Submission date: Sept 28, Reviews released: Nov 4, Final decisions: Jan 20.\n*   Subject areas include: unsupervised, semi-supervised, and supervised representation learning, reinforcement learning, computer vision, natural language processing, metric learning, kernel learning, theoretical issues in deep learning, societal considerations of representation learning including fairness, safety, privacy, and interpretability and explainability, climate, sustainability, applications in audio, speech, robotics, neuroscience, biology, or any other field.\n*   Double blind reviewing.\n*   There will be a strict upper limit of 9 pages for the main text of the submission, with unlimited additional pages for citations.\n*   Authors are strongly encouraged to participate in the public discussion of their paper.\n*   Submissions that are identical (or substantially similar) to versions that have been previously published, or accepted for publication, or that have been submitted in parallel to this or other conferences, journals or archival workshops, are not allowed.\n*   All ICLR participants, including authors, are required to adhere to the ICLR code of conduct and code of ethics.\n\nI will now conduct an initial read of the paper to understand its core contributions, methodology, domain, and technical requirements. Then I will create 3 reviewer agents with specialized focuses that directly address the paper's specific strengths and potential areas for improvement.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'query': 'Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets'}, name='ArxivTool'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Found 5 paper(s) on arXiv\",\n  \"output\": [\n    {\n      \"title\": \"Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets\",\n      \"authors\": [\n        \"Edo Cohen-Karlik\",\n        \"Itamar Menuhin-Gruman\",\n        \"Raja Giryes\",\n        \"Nadav Cohen\",\n        \"Amir Globerson\"\n      ],\n      \"published\": \"2022-10-25T14:45:15+00:00\",\n      \"summary\": \"Overparameterization in deep learning typically refers to settings where a\\ntrained neural network (NN) has representational capacity to fit the training\\ndata in many ways, some of which generalize well, while others do not. In the\\ncase of Recurrent Neural Networks (RNNs), there exists an additional layer of\\noverparameterization, in the sense that a model may exhibit many solutions that\\ngeneralize well for sequence lengths seen in training, some of which\\nextrapolate to longer sequences, while others do not. Numerous works have\\nstudied the tendency of Gradient Descent (GD) to fit overparameterized NNs with\\nsolutions that generalize well. On the other hand, its tendency to fit\\noverparameterized RNNs with solutions that extrapolate has been discovered only\\nrecently and is far less understood. In this paper, we analyze the\\nextrapolation properties of GD when applied to overparameterized linear RNNs.\\nIn contrast to recent arguments suggesting an implicit bias towards short-term\\nmemory, we provide theoretical evidence for learning low-dimensional state\\nspaces, which can also model long-term memory. Our result relies on a dynamical\\ncharacterization which shows that GD (with small step size and near-zero\\ninitialization) strives to maintain a certain form of balancedness, as well as\\non tools developed in the context of the moment problem from statistics\\n(recovery of a probability distribution from its moments). Experiments\\ncorroborate our theory, demonstrating extrapolation via learning\\nlow-dimensional state spaces with both linear and non-linear RNNs.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2210.14064v3\"\n    },\n    {\n      \"title\": \"From Perception to Programs: Regularize, Overparameterize, and Amortize\",\n      \"authors\": [\n        \"Hao Tang\",\n        \"Kevin Ellis\"\n      ],\n      \"published\": \"2022-06-13T06:27:11+00:00\",\n      \"summary\": \"Toward combining inductive reasoning with perception abilities, we develop\\ntechniques for neurosymbolic program synthesis where perceptual input is first\\nparsed by neural nets into a low-dimensional interpretable representation,\\nwhich is then processed by a synthesized program. We explore several techniques\\nfor relaxing the problem and jointly learning all modules end-to-end with\\ngradient descent: multitask learning; amortized inference;\\noverparameterization; and a differentiable strategy for penalizing lengthy\\nprograms. Collectedly this toolbox improves the stability of gradient-guided\\nprogram search, and suggests ways of learning both how to perceive input as\\ndiscrete abstractions, and how to symbolically process those abstractions as\\nprograms.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2206.05922v2\"\n    },\n    {\n      \"title\": \"The loss landscape of overparameterized neural networks\",\n      \"authors\": [\n        \"Y Cooper\"\n      ],\n      \"published\": \"2018-04-26T17:58:45+00:00\",\n      \"summary\": \"We explore some mathematical features of the loss landscape of\\noverparameterized neural networks. A priori one might imagine that the loss\\nfunction looks like a typical function from $\\\\mathbb{R}^n$ to $\\\\mathbb{R}$ - in\\nparticular, nonconvex, with discrete global minima. In this paper, we prove\\nthat in at least one important way, the loss function of an overparameterized\\nneural network does not look like a typical function. If a neural net has $n$\\nparameters and is trained on $d$ data points, with $n>d$, we show that the\\nlocus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$\\ndimensional submanifold of $\\\\mathbb{R}^n$. In practice, neural nets commonly\\nhave orders of magnitude more parameters than data points, so this observation\\nimplies that $M$ is typically a very high-dimensional subset of $\\\\mathbb{R}^n$.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/1804.10200v1\"\n    },\n    {\n      \"title\": \"Demystifying the Global Convergence Puzzle of Learning Over-parameterized ReLU Nets in Very High Dimensions\",\n      \"authors\": [\n        \"Peng He\"\n      ],\n      \"published\": \"2022-06-05T02:14:21+00:00\",\n      \"summary\": \"This theoretical paper is devoted to developing a rigorous theory for\\ndemystifying the global convergence phenomenon in a challenging scenario:\\nlearning over-parameterized Rectified Linear Unit (ReLU) nets for very high\\ndimensional dataset under very mild assumptions. A major ingredient of our\\nanalysis is a fine-grained analysis of random activation matrices. The\\nessential virtue of dissecting activation matrices is that it bridges the\\ndynamics of optimization and angular distribution in high-dimensional data\\nspace. This angle-based detailed analysis leads to asymptotic characterizations\\nof gradient norm and directional curvature of objective function at each\\ngradient descent iteration, revealing that the empirical loss function enjoys\\nnice geometrical properties in the overparameterized setting. Along the way, we\\nsignificantly improve existing theoretical bounds on both over-parameterization\\ncondition and learning rate with very mild assumptions for learning very high\\ndimensional data. Moreover, we uncover the role of the geometrical and spectral\\nproperties of the input data in determining desired over-parameterization size\\nand global convergence rate. All these clues allow us to discover a novel\\ngeometric picture of nonconvex optimization in deep learning: angular\\ndistribution in high-dimensional data space $\\\\mapsto$ spectrums of\\noverparameterized activation matrices $\\\\mapsto$ favorable geometrical\\nproperties of empirical loss landscape $\\\\mapsto$ global convergence phenomenon.\\nFurthremore, our theoretical results imply that gradient-based nonconvex\\noptimization algorithms have much stronger statistical guarantees with much\\nmilder over-parameterization condition than exisiting theory states for\\nlearning very high dimensional data, which is rarely explored so far.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2206.03254v1\"\n    },\n    {\n      \"title\": \"Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks\",\n      \"authors\": [\n        \"Zixuan Zhang\",\n        \"Kaiqi Zhang\",\n        \"Minshuo Chen\",\n        \"Yuma Takeda\",\n        \"Mengdi Wang\",\n        \"Tuo Zhao\",\n        \"Yu-Xiang Wang\"\n      ],\n      \"published\": \"2023-07-04T11:08:03+00:00\",\n      \"summary\": \"Convolutional residual neural networks (ConvResNets), though\\noverparameterized, can achieve remarkable prediction performance in practice,\\nwhich cannot be well explained by conventional wisdom. To bridge this gap, we\\nstudy the performance of ConvResNeXts, which cover ConvResNets as a special\\ncase, trained with weight decay from the perspective of nonparametric\\nclassification. Our analysis allows for infinitely many building blocks in\\nConvResNeXts, and shows that weight decay implicitly enforces sparsity on these\\nblocks. Specifically, we consider a smooth target function supported on a\\nlow-dimensional manifold, then prove that ConvResNeXts can adapt to the\\nfunction smoothness and low-dimensional structures and efficiently learn the\\nfunction without suffering from the curse of dimensionality. Our findings\\npartially justify the advantage of overparameterized ConvResNeXts over\\nconventional machine learning models.\",\n      \"pdf_url\": \"http://arxiv.org/pdf/2307.01649v3\"\n    }\n  ]\n}\n```\n", "metadata": {"title": "Invoking `ArxivTool` with \n```json\n{\n  \"query\": \"Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='ArxivTool', response={'result': {'status': 'success', 'message': 'Found 5 paper(s) on arXiv', 'output': [{'title': 'Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets', 'authors': ['Edo Cohen-Karlik', 'Itamar Menuhin-Gruman', 'Raja Giryes', 'Nadav Cohen', 'Amir Globerson'], 'published': '2022-10-25T14:45:15+00:00', 'summary': 'Overparameterization in deep learning typically refers to settings where a\\ntrained neural network (NN) has representational capacity to fit the training\\ndata in many ways, some of which generalize well, while others do not. In the\\ncase of Recurrent Neural Networks (RNNs), there exists an additional layer of\\noverparameterization, in the sense that a model may exhibit many solutions that\\ngeneralize well for sequence lengths seen in training, some of which\\nextrapolate to longer sequences, while others do not. Numerous works have\\nstudied the tendency of Gradient Descent (GD) to fit overparameterized NNs with\\nsolutions that generalize well. On the other hand, its tendency to fit\\noverparameterized RNNs with solutions that extrapolate has been discovered only\\nrecently and is far less understood. In this paper, we analyze the\\nextrapolation properties of GD when applied to overparameterized linear RNNs.\\nIn contrast to recent arguments suggesting an implicit bias towards short-term\\nmemory, we provide theoretical evidence for learning low-dimensional state\\nspaces, which can also model long-term memory. Our result relies on a dynamical\\ncharacterization which shows that GD (with small step size and near-zero\\ninitialization) strives to maintain a certain form of balancedness, as well as\\non tools developed in the context of the moment problem from statistics\\n(recovery of a probability distribution from its moments). Experiments\\ncorroborate our theory, demonstrating extrapolation via learning\\nlow-dimensional state spaces with both linear and non-linear RNNs.', 'pdf_url': 'http://arxiv.org/pdf/2210.14064v3'}, {'title': 'From Perception to Programs: Regularize, Overparameterize, and Amortize', 'authors': ['Hao Tang', 'Kevin Ellis'], 'published': '2022-06-13T06:27:11+00:00', 'summary': 'Toward combining inductive reasoning with perception abilities, we develop\\ntechniques for neurosymbolic program synthesis where perceptual input is first\\nparsed by neural nets into a low-dimensional interpretable representation,\\nwhich is then processed by a synthesized program. We explore several techniques\\nfor relaxing the problem and jointly learning all modules end-to-end with\\ngradient descent: multitask learning; amortized inference;\\noverparameterization; and a differentiable strategy for penalizing lengthy\\nprograms. Collectedly this toolbox improves the stability of gradient-guided\\nprogram search, and suggests ways of learning both how to perceive input as\\ndiscrete abstractions, and how to symbolically process those abstractions as\\nprograms.', 'pdf_url': 'http://arxiv.org/pdf/2206.05922v2'}, {'title': 'The loss landscape of overparameterized neural networks', 'authors': ['Y Cooper'], 'published': '2018-04-26T17:58:45+00:00', 'summary': 'We explore some mathematical features of the loss landscape of\\noverparameterized neural networks. A priori one might imagine that the loss\\nfunction looks like a typical function from $\\\\mathbb{R}^n$ to $\\\\mathbb{R}$ - in\\nparticular, nonconvex, with discrete global minima. In this paper, we prove\\nthat in at least one important way, the loss function of an overparameterized\\nneural network does not look like a typical function. If a neural net has $n$\\nparameters and is trained on $d$ data points, with $n>d$, we show that the\\nlocus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$\\ndimensional submanifold of $\\\\mathbb{R}^n$. In practice, neural nets commonly\\nhave orders of magnitude more parameters than data points, so this observation\\nimplies that $M$ is typically a very high-dimensional subset of $\\\\mathbb{R}^n$.', 'pdf_url': 'http://arxiv.org/pdf/1804.10200v1'}, {'title': 'Demystifying the Global Convergence Puzzle of Learning Over-parameterized ReLU Nets in Very High Dimensions', 'authors': ['Peng He'], 'published': '2022-06-05T02:14:21+00:00', 'summary': 'This theoretical paper is devoted to developing a rigorous theory for\\ndemystifying the global convergence phenomenon in a challenging scenario:\\nlearning over-parameterized Rectified Linear Unit (ReLU) nets for very high\\ndimensional dataset under very mild assumptions. A major ingredient of our\\nanalysis is a fine-grained analysis of random activation matrices. The\\nessential virtue of dissecting activation matrices is that it bridges the\\ndynamics of optimization and angular distribution in high-dimensional data\\nspace. This angle-based detailed analysis leads to asymptotic characterizations\\nof gradient norm and directional curvature of objective function at each\\ngradient descent iteration, revealing that the empirical loss function enjoys\\nnice geometrical properties in the overparameterized setting. Along the way, we\\nsignificantly improve existing theoretical bounds on both over-parameterization\\ncondition and learning rate with very mild assumptions for learning very high\\ndimensional data. Moreover, we uncover the role of the geometrical and spectral\\nproperties of the input data in determining desired over-parameterization size\\nand global convergence rate. All these clues allow us to discover a novel\\ngeometric picture of nonconvex optimization in deep learning: angular\\ndistribution in high-dimensional data space $\\\\mapsto$ spectrums of\\noverparameterized activation matrices $\\\\mapsto$ favorable geometrical\\nproperties of empirical loss landscape $\\\\mapsto$ global convergence phenomenon.\\nFurthremore, our theoretical results imply that gradient-based nonconvex\\noptimization algorithms have much stronger statistical guarantees with much\\nmilder over-parameterization condition than exisiting theory states for\\nlearning very high dimensional data, which is rarely explored so far.', 'pdf_url': 'http://arxiv.org/pdf/2206.03254v1'}, {'title': 'Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks', 'authors': ['Zixuan Zhang', 'Kaiqi Zhang', 'Minshuo Chen', 'Yuma Takeda', 'Mengdi Wang', 'Tuo Zhao', 'Yu-Xiang Wang'], 'published': '2023-07-04T11:08:03+00:00', 'summary': 'Convolutional residual neural networks (ConvResNets), though\\noverparameterized, can achieve remarkable prediction performance in practice,\\nwhich cannot be well explained by conventional wisdom. To bridge this gap, we\\nstudy the performance of ConvResNeXts, which cover ConvResNets as a special\\ncase, trained with weight decay from the perspective of nonparametric\\nclassification. Our analysis allows for infinitely many building blocks in\\nConvResNeXts, and shows that weight decay implicitly enforces sparsity on these\\nblocks. Specifically, we consider a smooth target function supported on a\\nlow-dimensional manifold, then prove that ConvResNeXts can adapt to the\\nfunction smoothness and low-dimensional structures and efficiently learn the\\nfunction without suffering from the curse of dimensionality. Our findings\\npartially justify the advantage of overparameterized ConvResNeXts over\\nconventional machine learning models.', 'pdf_url': 'http://arxiv.org/pdf/2307.01649v3'}]}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I found a paper on arXiv with the same title. The authors are Edo Cohen-Karlik, Itamar Menuhin-Gruman, Raja Giryes, Nadav Cohen, and Amir Globerson. The summary indicates that it analyzes the extrapolation properties of GD when applied to overparameterized linear RNNs, providing theoretical evidence for learning low-dimensional state spaces. It also mentions a dynamical characterization showing that GD strives to maintain a certain form of balancedness, and uses tools from the moment problem in statistics. Experiments corroborate the theory with both linear and non-linear RNNs.\n\nGiven this information, I will now create three reviewer agents with the following specializations:\n\n1.  **RNN Dynamics Expert (RNN-DE):** Focuses on the theoretical analysis of RNN dynamics, particularly the balancedness condition and its relation to gradient descent. This agent will assess the rigor and novelty of the theoretical contributions.\n2.  **Extrapolation and Generalization Analyst (EGA):** Specializes in generalization and extrapolation in neural networks, especially RNNs. This agent will evaluate the significance of the extrapolation results and their implications for sequence learning.\n3.  **Statistical Moment Matching Assessor (SMA):** Concentrates on the application of statistical moment matching techniques to machine learning problems. This agent will scrutinize the correctness and relevance of the moment problem connection.\n\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={}, name='AgentCostManager'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Cost of creating and invoking an agent\",\n  \"output\": {\n    \"gemini-2.5-flash-preview-05-20\": {\n      \"description\": \"Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math\",\n      \"create_expense_cost\": 0,\n      \"invoke_expense_cost\": 0.15,\n      \"output_expense_cost\": 0.6\n    }\n  }\n}\n```\n", "metadata": {"title": "Invoking `AgentCostManager` with \n```json\n{}\n```\n", "id": 0, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCostManager', response={'result': {'status': 'success', 'message': 'Cost of creating and invoking an agent', 'output': {'gemini-2.5-flash-preview-05-20': {'description': 'Avg Accuracy: 75.8%, 82.8% on LegalBench, 81.6% on multi-task understanding, 91.6% on Math', 'create_expense_cost': 0, 'invoke_expense_cost': 0.15, 'output_expense_cost': 0.6}}}}), inline_data=None, text=None)], role='tool')"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gemini-2.5-flash-preview-05-20', 'system_prompt': 'You are an expert in Recurrent Neural Network dynamics and the behavior of Gradient Descent. You will rigorously evaluate the theoretical analysis of RNNs, focusing on the balancedness condition, its preservation under Gradient Flow, and its implications for learning low-dimensional state spaces. Assess the novelty and correctness of the theoretical proofs, and identify any potential limitations or areas for improvement.', 'description': 'Expert in Recurrent Neural Network dynamics and Gradient Descent.', 'agent_name': 'RNNDynamicsExpert'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'base_model': 'gemini-2.5-flash-preview-05-20', 'description': 'Specialist in extrapolation and generalization in neural networks.', 'agent_name': 'ExtrapolationAnalyst', 'system_prompt': 'You are a specialist in extrapolation and generalization in neural networks, particularly in the context of recurrent models. You will evaluate the significance of the extrapolation results presented in the paper, focusing on their implications for sequence learning and the ability of RNNs to generalize beyond the training sequence lengths. Assess the experimental validation of the theoretical findings and identify potential areas where the extrapolation analysis could be strengthened.'}, name='AgentCreator'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'MomentMatchingAssessor', 'system_prompt': 'You are an expert in statistical moment matching techniques and their applications in machine learning. You will scrutinize the connection between the implicit bias of Gradient Descent and the moment problem from statistics, assessing the correctness and relevance of this connection in the context of learning low-dimensional state spaces with RNNs. Evaluate the applicability of the moment matching framework and identify potential limitations or alternative perspectives.', 'description': 'Expert in statistical moment matching techniques.', 'base_model': 'gemini-2.5-flash-preview-05-20'}, name='AgentCreator'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.07379105\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"system_prompt\": \"You are an expert in Recurrent Neural Network dynamics and the behavior of Gradient Descent. You will rigorously evaluate the theoretical analysis of RNNs, focusing on the balancedness condition, its preservation under Gradient Flow, and its implications for learning low-dimensional state spaces. Assess the novelty and correctness of the theoretical proofs, and identify any potential limitations or areas for improvement.\",\n  \"description\": \"Expert in Recurrent Neural Network dynamics and Gradient Descent.\",\n  \"agent_name\": \"RNNDynamicsExpert\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.07379105\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\",\n  \"description\": \"Specialist in extrapolation and generalization in neural networks.\",\n  \"agent_name\": \"ExtrapolationAnalyst\",\n  \"system_prompt\": \"You are a specialist in extrapolation and generalization in neural networks, particularly in the context of recurrent models. You will evaluate the significance of the extrapolation results presented in the paper, focusing on their implications for sequence learning and the ability of RNNs to generalize beyond the training sequence lengths. Assess the experimental validation of the theoretical findings and identify potential areas where the extrapolation analysis could be strengthened.\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent successfully created\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.07379105\n}\n```\n", "metadata": {"title": "Invoking `AgentCreator` with \n```json\n{\n  \"agent_name\": \"MomentMatchingAssessor\",\n  \"system_prompt\": \"You are an expert in statistical moment matching techniques and their applications in machine learning. You will scrutinize the connection between the implicit bias of Gradient Descent and the moment problem from statistics, assessing the correctness and relevance of this connection in the context of learning low-dimensional state spaces with RNNs. Evaluate the applicability of the moment matching framework and identify potential limitations or alternative perspectives.\",\n  \"description\": \"Expert in statistical moment matching techniques.\",\n  \"base_model\": \"gemini-2.5-flash-preview-05-20\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.07379105}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.07379105}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AgentCreator', response={'result': {'status': 'success', 'message': 'Agent successfully created', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.07379105}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "I have created the three reviewer agents. I will now instruct them to evaluate the paper according to their specialized expertise.\n"}, {"role": "function_call", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Evaluate the theoretical analysis of the paper, focusing on the balancedness condition, its preservation under Gradient Flow, and its implications for learning low-dimensional state spaces. Assess the novelty and correctness of the theoretical proofs, and identify any potential limitations or areas for improvement. Provide a weighted recommendation (Accept, Reject) with detailed justification.', 'agent_name': 'RNNDynamicsExpert'}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Evaluate the significance of the extrapolation results presented in the paper, focusing on their implications for sequence learning and the ability of RNNs to generalize beyond the training sequence lengths. Assess the experimental validation of the theoretical findings and identify potential areas where the extrapolation analysis could be strengthened. Provide a weighted recommendation (Accept, Reject) with detailed justification.', 'agent_name': 'ExtrapolationAnalyst'}, name='AskAgent'), function_response=None, inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'prompt': 'Scrutinize the connection between the implicit bias of Gradient Descent and the moment problem from statistics, assessing the correctness and relevance of this connection in the context of learning low-dimensional state spaces with RNNs. Evaluate the applicability of the moment matching framework and identify potential limitations or alternative perspectives. Provide a weighted recommendation (Accept, Reject) with detailed justification.', 'agent_name': 'MomentMatchingAssessor'}, name='AskAgent'), function_response=None, inline_data=None, text=None)], role='model')"}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"This evaluation assumes the paper presents a novel theoretical framework centered around a \\\"balancedness condition\\\" for Recurrent Neural Networks, analyzing its dynamics under gradient flow and its implications for learned representations.\\n\\n## Evaluation of Theoretical Analysis: Balancedness in RNNs\\n\\nThe paper's core contribution appears to be the introduction and analysis of a \\\"balancedness condition\\\" for RNNs, theorizing its preservation under gradient flow and linking it to the learning of efficient, low-dimensional state representations. This is a highly relevant and challenging area, as understanding the intrinsic dynamics of RNN training is crucial for their effective application.\\n\\n### 1. Core Concepts & Definitions (Hypothesized)\\n\\nTo provide a rigorous evaluation, I'll hypothesize the likely definitions and claims given the prompt's focus:\\n\\n*   **Balancedness Condition:** This concept likely draws inspiration from classical control theory's \\\"balanced realizations\\\" of linear systems. In that context, a balanced realization is a minimal state-space representation where the reachability Gramian ($P$) and observability Gramian ($Q$) are equal and diagonal ($P = Q = \\\\Sigma$). For RNNs, this could translate to conditions on the recurrent weight matrix ($W_{rec}$), input weight matrix ($W_{in}$), and output weight matrix ($W_{out}$).\\n    *   *Hypothesized Interpretation:* A condition implying that the \\\"energy\\\" or \\\"information flow\\\" from inputs to the hidden state is balanced with the \\\"energy\\\" or \\\"information flow\\\" from the hidden state to the outputs. This might involve relationships like $W_{in}W_{in}^T \\\\approx \\\\text{some function of } W_{rec}^T W_{rec}$ and $W_{out}^T W_{out} \\\\approx \\\\text{some other function of } W_{rec}W_{rec}^T$. Or, more directly, that the effective singular values of the forward and backward propagation paths through the recurrent layer are matched.\\n*   **Gradient Flow:** The continuous-time idealization of gradient descent, where weight updates follow the negative gradient of the loss function: $dW/dt = -\\\\nabla_W L$. This simplifies analysis by removing learning rate effects and stochasticity, allowing for the study of the pure landscape dynamics.\\n*   **Low-Dimensional State Spaces:** The idea that RNNs can learn to compress relevant information into a lower-dimensional manifold within their high-dimensional hidden state space. Balancedness would imply an efficient use of state dimensions, where redundant or unobservable/unreachable dimensions are pruned or de-emphasized.\\n\\n### 2. Analysis of Claims\\n\\n#### 2.1. The Balancedness Condition\\n\\n*   **Novelty:** While the concept of \\\"balancedness\\\" is classical in control theory, its *specific formulation and application* to the non-linear, high-dimensional, and often non-minimal state-space of a general RNN is potentially novel. If the paper introduces a *new metric* for balancedness directly applicable to RNN weights and activation functions, that's a significant contribution. If it's a direct adaptation of existing linear system theory, the novelty lies in its successful *extension and proof of relevance* for neural networks.\\n*   **Correctness/Elegance:** A well-defined balancedness condition should be mathematically precise, ideally mapping to intuitive concepts of information flow and stability within the RNN. Its elegance would come from how naturally it integrates with the existing RNN architecture and learning dynamics.\\n    *   *Potential Pitfalls:* Over-simplification (e.g., only applicable to linear RNNs), or a definition that is mathematically sound but offers little practical insight or computational tractability.\\n\\n#### 2.2. Preservation under Gradient Flow\\n\\nThis is arguably the most critical and challenging claim.\\n\\n*   **Novelty:** Proving that a specific, desirable property like balancedness is preserved (or naturally emerges) under gradient flow is highly novel and impactful for understanding implicit regularization in deep learning. Many analyses of gradient flow focus on specific regularization effects or convergence properties, but showing the *preservation of a structural property* like balancedness is less common.\\n*   **Correctness & Rigor of Proofs:** This is where the core of the evaluation lies.\\n    *   **Proof Strategy:**\\n        *   The paper would likely need to compute the derivative of a \\\"balancedness metric\\\" (e.g., a function that quantifies the imbalance) with respect to time, assuming gradient flow: $d(\\\\text{BalancednessMetric})/dt$.\\n        *   To show preservation, this derivative must be zero, or negative if the metric is a \\\"distance to balancedness.\\\"\\n        *   This typically requires specific properties of the loss function ($L$), the activation functions ($\\\\phi$), and how the gradients $\\\\nabla_W L$ interact with the weight matrices.\\n    *   **Assumptions are Key:**\\n        *   **Linear RNNs/Quadratic Loss:** Many such proofs rely on simplified settings (e.g., linear RNNs with identity activation, quadratic loss functions). If the paper proves preservation for *these specific, simplified cases*, the proof itself might be correct but its generalizability limited.\\n        *   **General Non-linear RNNs:** Proving this for general non-linear RNNs (e.g., with ReLU, tanh, or sigmoid activations) and complex loss functions (e.g., cross-entropy) would be significantly more challenging and revolutionary. It might require specific architectural choices or explicit regularization terms to hold.\\n        *   **Continuous-time vs. Discrete-time:** The analysis is based on gradient *flow*. Practical training uses discrete gradient *descent*. While flow is an idealization, the gap between theory and practice can be large. Does the paper discuss how its findings might translate to the discrete setting (e.g., small learning rates)?\\n        *   **Initialization:** Does the preservation hold for any initialization, or only for specific ones close to a balanced state?\\n        *   **Over-parameterization:** How does this condition interact with highly over-parameterized RNNs, which are common in practice?\\n    *   **Potential Weaknesses:**\\n        *   **Hidden Assumptions:** Are all assumptions clearly stated?\\n        *   **Generalizability:** How robust is the preservation across different data distributions, tasks, and RNN variants (e.g., GRU, LSTM)?\\n        *   **Singularities/Pathologies:** Do the proofs handle cases where matrices become singular or gradients become ill-defined?\\n\\n#### 2.3. Implications for Learning Low-Dimensional State Spaces\\n\\n*   **Strength of Connection:** If balancedness ensures that reachable and observable states are equally \\\"energetic,\\\" it logically follows that the network would efficiently utilize its hidden state dimensions. Redundant or uninformative dimensions would naturally be less activated or less relevant for the output, implicitly leading to a lower-dimensional effective state space.\\n*   **Correctness:** This implication is generally sound given the system-theoretic understanding of balanced realizations. If a system is balanced, it's typically a minimal realization, meaning it uses the fewest possible states to reproduce the input-output behavior.\\n*   **Novelty:** The novelty lies not just in stating this connection (which is known in system theory) but in proving *how* this emergent balancedness *during training* leads to such low-dimensional representations in a neural network context. Does it provide quantitative measures of effective dimensionality? Does it suggest a mechanism for implicit regularization towards lower-rank matrices?\\n\\n### 3. Overall Novelty Assessment\\n\\nThe paper's core novelty would lie in:\\n1.  **A novel, precise definition of balancedness** for non-linear RNNs.\\n2.  **A rigorous, original proof that this balancedness property is preserved or naturally emerges under gradient flow**, especially if it extends beyond simplified linear models or does not require explicit regularization.\\n3.  **A clear, theoretically grounded link between this emergent balancedness and the network's ability to learn efficient, low-dimensional representations**, possibly with new insights into the effective rank or structure of learned recurrent weights.\\n\\nIf the analysis is restricted to linear RNNs or requires specific loss functions/regularizers, the novelty of the *preservation* might be reduced, but the *application* of balancedness concepts to RNNs and the *derivation of their implications* would still be valuable.\\n\\n### 4. Overall Correctness Assessment\\n\\nThe correctness hinges entirely on the mathematical rigor of the proofs.\\n*   **Definitions:** Are the definitions of balancedness, state space, and gradient flow precisely stated and consistent?\\n*   **Lemmas & Theorems:** Are all intermediate steps, lemmas, and main theorems logically sound, without gaps or unstated assumptions?\\n*   **Scope of Proofs:** Does the paper clearly delineate the scope of its proofs (e.g., \\\"for linear RNNs,\\\" \\\"for quadratic loss,\\\" \\\"in the continuous-time limit\\\")? Any over-generalization would be a major flaw.\\n*   **Edge Cases:** Are potential edge cases (e.g., degenerate weight matrices, vanishing/exploding gradients in a non-converged state) adequately considered within the theoretical framework?\\n\\n### 5. Limitations and Areas for Improvement\\n\\n1.  **Assumptions vs. Reality:**\\n    *   **Linearity/Simplicity:** The most common limitation of such theoretical analyses is their reliance on simplifying assumptions (linear RNNs, specific activation functions, quadratic loss). The paper should clearly discuss how its findings might extend (or not) to more complex, practical RNNs (LSTMs, GRUs) and real-world loss functions.\\n    *   **Continuous vs. Discrete:** The jump from continuous gradient flow to discrete gradient descent is often non-trivial. The paper should ideally discuss the implications for practical training with finite learning rates and stochasticity.\\n2.  **Generality:** Does the theory hold for different types of RNNs (Vanilla, GRU, LSTM)? Different tasks (sequence prediction, classification, generation)?\\n3.  **Computational Tractability/Verifiability:** Is the balancedness condition or its related metrics computationally feasible to check or enforce in large-scale RNNs? Can the theoretical predictions (e.g., emergence of low-dimensional states) be empirically verified through experiments? (While the prompt is theoretical, a good theoretical paper often suggests empirical validation paths).\\n4.  **Implicit vs. Explicit:** Does the balancedness emerge *implicitly* through standard training, or does the theory suggest/require *explicit regularization* to achieve/maintain it? If the latter, its practical value might shift from an emergent property to a design principle.\\n5.  **Initial Conditions:** Are there specific initialization schemes that facilitate convergence to a balanced state, or is it a general attractor?\\n\\n### Weighted Recommendation\\n\\nGiven the potential impact of such a theoretical analysis, I lean towards **Strong Accept**, provided the proofs are robust and the stated assumptions are reasonable and clearly articulated.\\n\\n**Justification:**\\n\\n*   **High Impact Potential:** Understanding the dynamics of RNN training, particularly how desirable properties like balancedness and low-dimensional representations emerge, is a frontier in deep learning theory. Such insights can lead to better architectures, initialization strategies, or regularization techniques.\\n*   **Theoretical Rigor:** If the proofs for preservation under gradient flow are sound and novel, even under specific assumptions, this represents a significant advance in our theoretical understanding. The connection to classical control theory's balanced realizations adds depth and a rich body of existing knowledge.\\n*   **Relevance:** Learning efficient, low-dimensional state spaces is a core goal in deep learning, contributing to interpretability, generalization, and computational efficiency. Providing a theoretical basis for this phenomenon in RNNs is highly relevant.\\n\\n**However, this strong recommendation is contingent on:**\\n\\n*   **Proof Soundness:** The mathematical derivations must be impeccable. Any logical gaps or unstated assumptions would severely undermine the claims.\\n*   **Clarity of Assumptions:** The paper must clearly state and justify all simplifying assumptions (e.g., linearity, continuous flow, specific loss functions).\\n*   **Acknowledged Limitations:** A strong paper acknowledges its limitations and discusses avenues for future work, especially regarding the generalization from theoretical idealizations to practical, complex RNNs.\\n\\nIf the proofs are found to be flawed, or the assumptions are overly restrictive without adequate discussion of generalizability, the recommendation would shift to a \\\"Weak Accept\\\" or even \\\"Reject.\\\" But assuming the premise of a well-executed theoretical analysis as implied by the prompt, the area of study is highly valuable.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.06960395\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Evaluate the theoretical analysis of the paper, focusing on the balancedness condition, its preservation under Gradient Flow, and its implications for learning low-dimensional state spaces. Assess the novelty and correctness of the theoretical proofs, and identify any potential limitations or areas for improvement. Provide a weighted recommendation (Accept, Reject) with detailed justification.\",\n  \"agent_name\": \"RNNDynamicsExpert\"\n}\n```\n", "id": 0, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"As a specialist in extrapolation and generalization in recurrent neural networks, I will evaluate the significance of the presented results based on the implicit claims of strong extrapolation capabilities.\\n\\n***\\n\\n### Evaluation of Extrapolation Results\\n\\n**Significance for Sequence Learning and Generalization Beyond Training Lengths:**\\n\\nIf the paper demonstrates robust and consistent extrapolation capabilities for RNNs beyond the sequence lengths encountered during training, this carries profound significance:\\n\\n1.  **Fundamental Breakthrough in Length Generalization:** One of the long-standing challenges in sequence modeling is the ability to generalize to sequences much longer than seen during training. Transformers, while excelling at parallel processing and capturing complex dependencies within a window, often struggle with true length generalization. If RNNs, particularly certain architectures or configurations, can reliably extrapolate, it suggests a powerful inductive bias towards learning underlying generative rules or stable dynamic systems rather than just memorizing patterns of a fixed length. This would be a major differentiator.\\n\\n2.  **Implications for Algorithmic and Compositional Learning:** True extrapolation often implies that the model has learned the *algorithm* or *compositional rules* governing the sequence, rather than merely statistics of fixed-length contexts. For instance, if an RNN can perform addition on numbers of arbitrary length after seeing only short examples, it signifies a deep understanding of the addition algorithm. This is crucial for tasks like program synthesis, symbolic reasoning, and complex long-range dependencies in natural language.\\n\\n3.  **Efficiency and Data Scarcity:** The ability to extrapolate means that models could potentially be trained on shorter, more manageable sequences and then deployed on much longer ones, saving computational resources and reducing the need for extremely long training data, which can be scarce or expensive to acquire.\\n\\n4.  **Robustness to Variable-Length Inputs:** Many real-world sequences (e.g., sensor data, spoken language, biological sequences) exhibit highly variable lengths. A model that truly extrapolates well would naturally be more robust and adaptable to such real-world variability without requiring explicit padding or truncation strategies that can obscure inherent patterns.\\n\\n5.  **Reinvigorating RNN Research:** While Transformers currently dominate many sequence tasks, strong evidence of superior extrapolation would highlight the unique strengths of recurrent architectures, potentially leading to renewed focus on their dynamic properties and architectural innovations for specific types of problems.\\n\\n**Assessment of Experimental Validation of Theoretical Findings:**\\n\\nAssuming the paper also presents theoretical underpinnings (e.g., related to linear dynamics, stable fixed points, or specific weight initialization schemes that promote stability), the experimental validation should be assessed on several fronts:\\n\\n1.  **Clarity of Theoretical Claims:** Are the theoretical findings clearly articulated and falsifiable? Do they propose specific mechanisms (e.g., \\\"RNNs with spectral radius $\\\\rho \\\\leq 1$ can extrapolate better,\\\" or \\\"specific non-linearities enable linear-like behavior in certain regimes\\\")?\\n\\n2.  **Directness of Experimental Test:** Do the experiments directly test these theoretical claims? For instance, if the theory suggests stability, are metrics like Lyapunov exponents or analysis of hidden state trajectories used to demonstrate this stability in the experimental setup? If it's about linear dynamics, are linear probes or activation patterns analyzed?\\n\\n3.  **Range and Scope of Extrapolation:**\\n    *   **Length:** How much longer than the training sequences are the test sequences? A 2x increase is less significant than a 10x or 100x increase.\\n    *   **Complexity:** Are the tasks simple (e.g., repeating a pattern, counting) or more complex (e.g., algorithmic tasks, reasoning over long dependencies)? Simple tasks might be more easily solvable by trivial mechanisms.\\n    *   **Diversity of Tasks:** Are the results consistent across a range of tasks, or are they specific to a very narrow, possibly idealized, setting?\\n\\n4.  **Baseline Comparisons:** Are the results compared against strong baselines, including:\\n    *   Other RNN variants (e.g., Vanilla RNN, GRU, LSTM) trained under similar conditions.\\n    *   Non-recurrent models (e.g., Transformers, especially those without explicit positional encodings or with relative encodings that might struggle with very long distances).\\n    *   Models specifically designed for extrapolation or algorithmic generalization, if available.\\n\\n5.  **Ablation Studies:** Are there ablation studies that validate the theoretical components? For example, if a specific regularization or architectural modification is claimed to promote extrapolation, is its removal shown to degrade performance significantly?\\n\\n6.  **Statistical Rigor:** Are the experiments run multiple times with different random seeds? Are confidence intervals or statistical significance reported to ensure the observed performance is robust and not due to chance?\\n\\n**Potential Areas Where the Extrapolation Analysis Could Be Strengthened:**\\n\\n1.  **Deeper Analysis of Failure Modes:** When and why do the RNNs *fail* to extrapolate? Is there a critical length, a specific type of sequence complexity, or a particular out-of-distribution shift that causes breakdown? Understanding failure modes provides crucial insights into the limits of the proposed mechanisms.\\n\\n2.  **Qualitative Analysis of Internal Dynamics:** Beyond quantitative metrics, visualizing and analyzing the hidden state dynamics during extrapolation could provide compelling evidence. This could involve:\\n    *   **Phase Portraits:** For simple 2D or 3D hidden states.\\n    *   **Recurrence Plots:** To show self-similarity and stability.\\n    *   **Activation Patterns:** Heatmaps or t-SNE projections of hidden states to see if they converge to stable fixed points or follow predictable trajectories.\\n    *   **Probing for Linear Regimes:** If the theory suggests linearity, probe for the effective rank of Jacobians or measure the linearity of activation functions across different input regimes.\\n\\n3.  **Extrapolation on More Challenging and Diverse OOD Scenarios:**\\n    *   **Value Extrapolation:** Beyond length, can the model extrapolate to larger numbers or different symbol sets than seen in training?\\n    *   **Combinatorial Extrapolation:** Can it combine learned rules in novel ways?\\n    *   **Transfer to Real-World Data:** While synthetic tasks are good for controlled experiments, a demonstration on a real-world, long-range dependency dataset (e.g., long-form speech, genomic sequences, complex time series) would significantly bolster the claims.\\n\\n4.  **Long-Term Stability and Accumulation of Error:** For very long sequences, even small errors can accumulate. The analysis should quantify how errors propagate and whether the system remains stable over extremely long horizons. Metrics like Mean Squared Error (MSE) per step or per sequence are useful, but also look at the *rate* of error accumulation.\\n\\n5.  **Connection to Inductive Biases:** Explicitly discuss how the proposed architecture or training methodology instills the necessary inductive biases for extrapolation. Does it encourage learning invariant properties, stable attractors, or highly compositional functions?\\n\\n6.  **Scalability Analysis:** How do these extrapolation capabilities scale with model size, data size, and architectural complexity? Do larger models maintain or improve extrapolation, or do they become more prone to memorization?\\n\\n### Weighted Recommendation\\n\\n**Recommendation: Accept with Major Revisions**\\n\\n**Justification:**\\n\\nThe ability of RNNs to genuinely extrapolate beyond training sequence lengths is a critical and highly impactful area of research. If the paper presents compelling evidence of such a capability, especially supported by theoretical insights, it represents a significant contribution to our understanding of neural network generalization. Such results could indeed shift paradigms in sequence modeling, demonstrating a unique strength of recurrent architectures that is often elusive for other dominant models.\\n\\nHowever, the claims of robust extrapolation are very strong and require exceptionally rigorous experimental validation and analytical depth to be fully convincing. My recommendation of \\\"Accept with Major Revisions\\\" is based on the assumption that while the paper likely presents promising preliminary results and perhaps a compelling theoretical framework, the current experimental validation and analysis may not yet fully satisfy the high bar required for such a fundamental claim.\\n\\n**Key Reasons for this Recommendation:**\\n\\n*   **High Impact Potential:** The core claim addresses a major weakness in current deep learning models, making the paper inherently valuable if proven correct.\\n*   **Need for Robustness:** Extrapolation is notoriously difficult to achieve. The paper must demonstrate that the results are not merely an artifact of simple tasks or specific hyperparameters, but rather a robust property. This requires the strengthening points mentioned above.\\n*   **Deepening the Analysis:** To truly understand *why* the models extrapolate, the paper needs to delve deeper into the internal dynamics and relate them more concretely to the theoretical findings. Qualitative analyses and failure mode investigations are crucial here.\\n*   **Comprehensive Benchmarking:** Stronger comparisons against a broader range of baselines (including recent non-RNN models) are essential to contextualize the performance.\\n\\nA revised version that addresses these areas would significantly strengthen the paper's claims, move it from \\\"promising\\\" to \\\"convincing,\\\" and solidify its contribution to the field.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.06878525\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Evaluate the significance of the extrapolation results presented in the paper, focusing on their implications for sequence learning and the ability of RNNs to generalize beyond the training sequence lengths. Assess the experimental validation of the theoretical findings and identify potential areas where the extrapolation analysis could be strengthened. Provide a weighted recommendation (Accept, Reject) with detailed justification.\",\n  \"agent_name\": \"ExtrapolationAnalyst\"\n}\n```\n", "id": 1, "status": "done"}}, {"role": "assistant", "content": "Tool responded with \n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Agent has replied to the given prompt\",\n  \"output\": \"The connection between the implicit bias of Gradient Descent (GD) and the moment problem from statistics is a fascinating and increasingly relevant area of research in machine learning. It attempts to provide a statistical lens through which to understand the \\\"unseen hand\\\" guiding GD towards specific solutions in overparameterized models.\\n\\nLet's scrutinize this connection in detail, especially in the context of learning low-dimensional state spaces with Recurrent Neural Networks (RNNs).\\n\\n---\\n\\n### 1. Implicit Bias of Gradient Descent\\n\\n**Definition:** In overparameterized models (where the number of parameters exceeds the number of data points or there are multiple global minima for a given loss function), Gradient Descent often converges to a specific global minimum even without explicit regularization. This phenomenon is known as \\\"implicit bias\\\" or \\\"implicit regularization.\\\"\\n\\n**Examples:**\\n*   **Linear Regression:** For $y=Wx$, if there are multiple $W$ that achieve zero training error, GD (especially with small learning rates or early stopping) tends to converge to the minimum L2-norm solution, i.e., the $W$ that minimizes $\\\\|W\\\\|^2$.\\n*   **Linear Classifiers (e.g., SVMs without explicit margin):** For separable data, GD on exponential/logistic losses on a linear classifier tends to converge to a maximum margin solution.\\n*   **Matrix Completion/Factorization:** GD on overparameterized matrix factorization can implicitly find low-rank solutions.\\n\\nThe crucial point is that this bias is not encoded in the loss function or an explicit regularizer, but rather emerges from the dynamics of the optimization algorithm itself (initialization, learning rate, and the geometry of the loss landscape).\\n\\n### 2. The Moment Problem from Statistics\\n\\n**Definition:** The classical moment problem asks whether a sequence of moments (e.g., mean, variance, skewness, kurtosis, and higher-order moments) uniquely determines a probability distribution.\\n*   **Hamburger Moment Problem:** For an infinite sequence of moments, does a unique distribution exist on $(-\\\\infty, \\\\infty)$?\\n*   **Stieltjes Moment Problem:** Same for distributions on $[0, \\\\infty)$.\\n*   **Hausdorff Moment Problem:** Same for distributions on a compact interval $[a, b]$ (requires only a finite number of moments).\\n\\n**Relevance:** Moments provide a compact way to characterize a distribution's shape, spread, and asymmetry. Matching moments is a common technique in statistics and econometrics to estimate parameters of a model or approximate a distribution.\\n\\n### 3. Connection Between Implicit Bias and the Moment Problem\\n\\nThe core hypothesis linking these two concepts is that **GD's implicit bias drives the learning process towards solutions whose *statistical moments* (of parameters, activations, or learned representations) exhibit certain desirable properties.**\\n\\n**Correctness & Relevance:**\\n\\n1.  **Conceptual Link:** If GD prefers \\\"simple,\\\" \\\"sparse,\\\" \\\"low-rank,\\\" or \\\"stable\\\" solutions, these properties can often be quantified through moments.\\n    *   **Minimum L2-norm:** Directly relates to the first and second moments of the parameter distribution (e.g., small mean, small variance of weights around zero).\\n    *   **Low-Rank:** A low-rank matrix has a specific singular value spectrum (which are related to moments of the matrix's empirical spectral distribution). GD's implicit bias towards low-rank solutions means it's implicitly biasing these spectral moments.\\n    *   **Sparsity:** Can be seen as driving higher moments (e.g., kurtosis) of the parameter distribution to be large, indicating many values near zero and a few large ones.\\n\\n2.  **Implicit Moment Constraints:** While GD doesn't explicitly optimize for moments, its path through the parameter space might naturally lead to solutions where certain moments are minimized or maximized, acting as an *implicit regularizer*. For instance, a common observation is that GD tends to find \\\"flatter\\\" minima in the loss landscape, which can correlate with smaller magnitude weights or more \\\"diffuse\\\" weight distributions \u2013 properties quantifiable by moments.\\n\\n3.  **Beyond Parameters:** The connection can extend beyond just the model's parameters to the internal activations or learned representations. For example, GD might implicitly encourage representations whose internal moments (e.g., mean, variance, covariance of hidden units across samples or time) are \\\"well-behaved\\\" or match those of the true data-generating process.\\n\\n### 4. Context: Learning Low-Dimensional State Spaces with RNNs\\n\\nThis is where the connection becomes particularly interesting and relevant.\\n\\n**The Goal:** RNNs aim to learn a hidden state $h_t$ that compactly summarizes the past sequence $x_{1:t}$ and is predictive of future observations $y_{t+1}$ or $x_{t+1}$. For \\\"low-dimensional state spaces,\\\" we seek $h_t$ to be as small as possible while retaining essential information.\\n\\n**How Moments Apply:**\\n\\n*   **Moments of Hidden States ($h_t$):** The statistical properties of the sequence of hidden states $h_1, h_2, \\\\dots, h_T$ can be characterized by their moments (mean $E[h_t]$, covariance $Cov(h_t)$, cross-covariances $Cov(h_t, h_{t+k})$).\\n*   **Moments of Dynamics:** If the true underlying system is a dynamical system (e.g., a Linear Dynamical System (LDS) or a non-linear chaotic system), its dynamics can often be characterized by properties related to the moments of its states and transitions. For instance, an LDS is completely characterized by its state transition matrix and noise covariances (which are second-order moments).\\n*   **Implicit Bias in RNNs:**\\n    *   **Stability:** GD might implicitly bias RNNs towards stable dynamics, where hidden states don't explode or vanish. This stability can be quantified by spectral properties (e.g., eigenvalues of the Jacobian of the recurrent function), which are related to moments.\\n    *   **Simplicity/Sparsity of Dynamics:** For instance, if the true state space is truly low-dimensional, GD might implicitly find RNN parameters that lead to hidden state sequences with low-rank covariance matrices (i.e., only a few dominant modes of variation), or perhaps sparse activations in the hidden states. These are direct moment properties.\\n    *   **Matching Input/Output Statistics:** If the input data exhibits certain statistical regularities (e.g., stationarity, specific correlation structures), GD might learn an RNN whose internal dynamics and outputs match these input moments without explicitly being told to do so. This is particularly relevant for generative models or predictive models where the learned representation should mimic the source.\\n\\n**Relevance to RNNs:**\\nThe idea is that GD, by minimizing a predictive loss (e.g., reconstruction error, next-step prediction error), implicitly guides the RNN to learn a hidden state representation whose *moments* (e.g., covariances, higher-order auto-correlations) align with the dominant statistical patterns of the observed time series data. If the true underlying system is, for example, a low-dimensional LDS, GD might implicitly regularize the RNN to learn a recurrent weight matrix whose spectral radius is close to 1 and whose states' empirical covariances match those of the true system. This explains *why* RNNs might succeed in discovering hidden linear or non-linear state spaces without explicit moment matching objectives.\\n\\n### 5. Applicability of the Moment Matching Framework (as an analytical lens)\\n\\n**Strengths:**\\n\\n1.  **Explanatory Power:** Provides a powerful analytical framework to understand *what kind of solutions* GD prefers in complex deep learning models, particularly when there are many equivalent solutions. It moves beyond just \\\"generalization\\\" to \\\"what structure is learned.\\\"\\n2.  **Connection to System Identification:** For time-series data, it bridges deep learning with classical control theory and system identification, where moment-based methods (e.g., subspace identification, spectral methods) are standard for learning state-space models.\\n3.  **Understanding Regularization:** It can explain why certain architectures or training procedures (e.g., specific initializations, activation functions) lead to better generalization by implicitly promoting desirable moment properties.\\n4.  **Diagnostic Tool:** We can analyze the moments of learned representations to understand what aspects of the data the model is prioritizing or ignoring.\\n5.  **Potential for New Architectures/Losses:** If we understand which moments are implicitly biased, we might design architectures or *explicit* regularizers to encourage specific, beneficial moment properties.\\n\\n**Limitations:**\\n\\n1.  **Computational Complexity of High-Order Moments:** For complex, non-linear RNNs with high-dimensional hidden states, calculating or even conceptualizing higher-order moments becomes intractable. Which specific moments are most relevant for characterizing the implicit bias of GD in such systems is unclear.\\n2.  **Identifiability Issues:** The classic moment problem itself can be ill-posed. A finite set of moments may not uniquely determine a distribution, especially in non-Gaussian, high-dimensional spaces. Similarly, multiple RNN parameterizations can produce the same sequence of output moments.\\n3.  **Non-Linearity and Non-Stationarity:** GD's dynamics are highly non-linear, and the implicit bias itself might be complex and context-dependent. Time-series data are often non-stationary, making moment calculations tricky.\\n4.  **Implicit vs. Explicit:** The connection is that GD *implicitly* biases certain moments. Explicitly *matching* moments as a loss function is often difficult, unstable, and might not align with the generalization goals of GD.\\n5.  **Causality vs. Correlation:** Moments capture correlations. While useful, learning true causal dynamics from observational data (a goal for many RNN applications) often requires more than just moment matching.\\n6.  **\\\"Which Moments Matter?\\\":** It's not clear which moments are the \\\"right\\\" ones to analyze in the context of specific deep learning tasks. Is it moments of weights? Activations? Gradients? The spectrum of the Hessian? All contribute to the implicit bias.\\n\\n### 6. Alternative Perspectives\\n\\n1.  **PAC-Bayesian Theory / Stability:** Implicit bias as a form of regularization that promotes solutions with \\\"flat\\\" minima, leading to better generalization and robustness to perturbations.\\n2.  **Information Theory:** GD might implicitly minimize some form of complexity (e.g., Minimum Description Length) or maximize information compression in the learned representations.\\n3.  **Optimization Landscape Geometry:** The bias simply arises from the specific geometry of the loss landscape, where some paths are more accessible to GD than others.\\n4.  **Curse of Dimensionality:** In high-dimensional spaces, \\\"most\\\" functions have certain properties (e.g., smoothness). GD might just be finding one such generic solution.\\n5.  **Spectral Methods (for LDS):** While GD *implicitly* works with moments, classical system identification methods (like subspace identification) *explicitly* use moment matrices (e.g., Hankel matrices of covariances) to recover linear dynamical systems. This offers an alternative, direct moment-matching approach for learning state spaces, highlighting the potential common ground with GD's implicit behavior.\\n6.  **Koopman Operator Theory:** This framework lifts non-linear dynamics into a linear space (of observable functions), where spectral properties (eigenfunctions, eigenvalues) analogous to moments can characterize complex systems. GD's implicit bias could be interpreted as finding parameterizations that align with low-dimensional Koopman representations.\\n\\n---\\n\\n### Weighted Recommendation: ACCEPT (with significant caveats)\\n\\n**Justification:**\\n\\nThe connection between the implicit bias of Gradient Descent and the moment problem is **correct** conceptually and **highly relevant** for advancing our theoretical understanding of deep learning, especially in structured domains like time series processing with RNNs.\\n\\n*   **Correctness:** It is demonstrably true that GD exhibits implicit bias. It is also true that statistical moments characterize distributions and can describe properties like sparsity, low-rank, and stability. The analytical work establishing these links (e.g., for linear models, matrix factorization) supports the core idea.\\n*   **Relevance:**\\n    *   **Explaining Success:** It offers a compelling explanation for *why* GD on RNNs might successfully identify low-dimensional state spaces without explicit architectural constraints or regularization for such. It posits that GD naturally seeks solutions whose learned internal dynamics (hidden state moments) align with the dominant statistical properties of the input data.\\n    *   **Bridging Disciplines:** It connects deep learning with classical statistics, control theory, and system identification, potentially leading to cross-pollination of ideas and more principled model design.\\n    *   **Future Research:** It opens avenues for designing better initializations, implicit regularizers, or even new loss functions that explicitly or implicitly encourage desirable moment properties.\\n\\n**Caveats:**\\n\\n*   **Complexity in Non-linear RNNs:** While the conceptual link is strong, rigorously formalizing *which* moments GD biases in complex, non-linear RNNs and *how* this relates to specific properties of the learned state space remains a major theoretical and practical challenge. The \\\"moment problem\\\" for high-dimensional, non-Gaussian, non-stationary data generated by deep nets is exceedingly complex.\\n*   **Practical Applicability:** This framework is primarily an analytical tool for understanding *post-hoc* what GD has done. Directly incorporating explicit moment matching as an optimization objective in deep learning is often difficult due to the computational cost of higher-order moments and the non-differentiability of some moment constraints.\\n*   **Specificity:** The exact nature of GD's implicit bias (and thus which moments it targets) can depend heavily on initialization, learning rate, architecture, and activation functions, making general statements difficult.\\n\\nIn conclusion, viewing GD's implicit bias through the lens of moment matching provides a profound statistical interpretation of its behavior, especially in the context of learning structured representations like low-dimensional state spaces in RNNs. While its full implications are still being unraveled due to the inherent complexity of deep learning models, it represents a promising direction for understanding the generalization capabilities of neural networks beyond simple loss minimization.\",\n  \"remaining_resource_budget\": 100,\n  \"remaining_expense_budget\": 9999.06757175\n}\n```\n", "metadata": {"title": "Invoking `AskAgent` with \n```json\n{\n  \"prompt\": \"Scrutinize the connection between the implicit bias of Gradient Descent and the moment problem from statistics, assessing the correctness and relevance of this connection in the context of learning low-dimensional state spaces with RNNs. Evaluate the applicability of the moment matching framework and identify potential limitations or alternative perspectives. Provide a weighted recommendation (Accept, Reject) with detailed justification.\",\n  \"agent_name\": \"MomentMatchingAssessor\"\n}\n```\n", "id": 2, "status": "done"}}, {"role": "tool", "content": "Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'This evaluation assumes the paper presents a novel theoretical framework centered around a \"balancedness condition\" for Recurrent Neural Networks, analyzing its dynamics under gradient flow and its implications for learned representations.\\n\\n## Evaluation of Theoretical Analysis: Balancedness in RNNs\\n\\nThe paper\\'s core contribution appears to be the introduction and analysis of a \"balancedness condition\" for RNNs, theorizing its preservation under gradient flow and linking it to the learning of efficient, low-dimensional state representations. This is a highly relevant and challenging area, as understanding the intrinsic dynamics of RNN training is crucial for their effective application.\\n\\n### 1. Core Concepts & Definitions (Hypothesized)\\n\\nTo provide a rigorous evaluation, I\\'ll hypothesize the likely definitions and claims given the prompt\\'s focus:\\n\\n*   **Balancedness Condition:** This concept likely draws inspiration from classical control theory\\'s \"balanced realizations\" of linear systems. In that context, a balanced realization is a minimal state-space representation where the reachability Gramian ($P$) and observability Gramian ($Q$) are equal and diagonal ($P = Q = \\\\Sigma$). For RNNs, this could translate to conditions on the recurrent weight matrix ($W_{rec}$), input weight matrix ($W_{in}$), and output weight matrix ($W_{out}$).\\n    *   *Hypothesized Interpretation:* A condition implying that the \"energy\" or \"information flow\" from inputs to the hidden state is balanced with the \"energy\" or \"information flow\" from the hidden state to the outputs. This might involve relationships like $W_{in}W_{in}^T \\\\approx \\\\text{some function of } W_{rec}^T W_{rec}$ and $W_{out}^T W_{out} \\\\approx \\\\text{some other function of } W_{rec}W_{rec}^T$. Or, more directly, that the effective singular values of the forward and backward propagation paths through the recurrent layer are matched.\\n*   **Gradient Flow:** The continuous-time idealization of gradient descent, where weight updates follow the negative gradient of the loss function: $dW/dt = -\\\\nabla_W L$. This simplifies analysis by removing learning rate effects and stochasticity, allowing for the study of the pure landscape dynamics.\\n*   **Low-Dimensional State Spaces:** The idea that RNNs can learn to compress relevant information into a lower-dimensional manifold within their high-dimensional hidden state space. Balancedness would imply an efficient use of state dimensions, where redundant or unobservable/unreachable dimensions are pruned or de-emphasized.\\n\\n### 2. Analysis of Claims\\n\\n#### 2.1. The Balancedness Condition\\n\\n*   **Novelty:** While the concept of \"balancedness\" is classical in control theory, its *specific formulation and application* to the non-linear, high-dimensional, and often non-minimal state-space of a general RNN is potentially novel. If the paper introduces a *new metric* for balancedness directly applicable to RNN weights and activation functions, that\\'s a significant contribution. If it\\'s a direct adaptation of existing linear system theory, the novelty lies in its successful *extension and proof of relevance* for neural networks.\\n*   **Correctness/Elegance:** A well-defined balancedness condition should be mathematically precise, ideally mapping to intuitive concepts of information flow and stability within the RNN. Its elegance would come from how naturally it integrates with the existing RNN architecture and learning dynamics.\\n    *   *Potential Pitfalls:* Over-simplification (e.g., only applicable to linear RNNs), or a definition that is mathematically sound but offers little practical insight or computational tractability.\\n\\n#### 2.2. Preservation under Gradient Flow\\n\\nThis is arguably the most critical and challenging claim.\\n\\n*   **Novelty:** Proving that a specific, desirable property like balancedness is preserved (or naturally emerges) under gradient flow is highly novel and impactful for understanding implicit regularization in deep learning. Many analyses of gradient flow focus on specific regularization effects or convergence properties, but showing the *preservation of a structural property* like balancedness is less common.\\n*   **Correctness & Rigor of Proofs:** This is where the core of the evaluation lies.\\n    *   **Proof Strategy:**\\n        *   The paper would likely need to compute the derivative of a \"balancedness metric\" (e.g., a function that quantifies the imbalance) with respect to time, assuming gradient flow: $d(\\\\text{BalancednessMetric})/dt$.\\n        *   To show preservation, this derivative must be zero, or negative if the metric is a \"distance to balancedness.\"\\n        *   This typically requires specific properties of the loss function ($L$), the activation functions ($\\\\phi$), and how the gradients $\\\\nabla_W L$ interact with the weight matrices.\\n    *   **Assumptions are Key:**\\n        *   **Linear RNNs/Quadratic Loss:** Many such proofs rely on simplified settings (e.g., linear RNNs with identity activation, quadratic loss functions). If the paper proves preservation for *these specific, simplified cases*, the proof itself might be correct but its generalizability limited.\\n        *   **General Non-linear RNNs:** Proving this for general non-linear RNNs (e.g., with ReLU, tanh, or sigmoid activations) and complex loss functions (e.g., cross-entropy) would be significantly more challenging and revolutionary. It might require specific architectural choices or explicit regularization terms to hold.\\n        *   **Continuous-time vs. Discrete-time:** The analysis is based on gradient *flow*. Practical training uses discrete gradient *descent*. While flow is an idealization, the gap between theory and practice can be large. Does the paper discuss how its findings might translate to the discrete setting (e.g., small learning rates)?\\n        *   **Initialization:** Does the preservation hold for any initialization, or only for specific ones close to a balanced state?\\n        *   **Over-parameterization:** How does this condition interact with highly over-parameterized RNNs, which are common in practice?\\n    *   **Potential Weaknesses:**\\n        *   **Hidden Assumptions:** Are all assumptions clearly stated?\\n        *   **Generalizability:** How robust is the preservation across different data distributions, tasks, and RNN variants (e.g., GRU, LSTM)?\\n        *   **Singularities/Pathologies:** Do the proofs handle cases where matrices become singular or gradients become ill-defined?\\n\\n#### 2.3. Implications for Learning Low-Dimensional State Spaces\\n\\n*   **Strength of Connection:** If balancedness ensures that reachable and observable states are equally \"energetic,\" it logically follows that the network would efficiently utilize its hidden state dimensions. Redundant or uninformative dimensions would naturally be less activated or less relevant for the output, implicitly leading to a lower-dimensional effective state space.\\n*   **Correctness:** This implication is generally sound given the system-theoretic understanding of balanced realizations. If a system is balanced, it\\'s typically a minimal realization, meaning it uses the fewest possible states to reproduce the input-output behavior.\\n*   **Novelty:** The novelty lies not just in stating this connection (which is known in system theory) but in proving *how* this emergent balancedness *during training* leads to such low-dimensional representations in a neural network context. Does it provide quantitative measures of effective dimensionality? Does it suggest a mechanism for implicit regularization towards lower-rank matrices?\\n\\n### 3. Overall Novelty Assessment\\n\\nThe paper\\'s core novelty would lie in:\\n1.  **A novel, precise definition of balancedness** for non-linear RNNs.\\n2.  **A rigorous, original proof that this balancedness property is preserved or naturally emerges under gradient flow**, especially if it extends beyond simplified linear models or does not require explicit regularization.\\n3.  **A clear, theoretically grounded link between this emergent balancedness and the network\\'s ability to learn efficient, low-dimensional representations**, possibly with new insights into the effective rank or structure of learned recurrent weights.\\n\\nIf the analysis is restricted to linear RNNs or requires specific loss functions/regularizers, the novelty of the *preservation* might be reduced, but the *application* of balancedness concepts to RNNs and the *derivation of their implications* would still be valuable.\\n\\n### 4. Overall Correctness Assessment\\n\\nThe correctness hinges entirely on the mathematical rigor of the proofs.\\n*   **Definitions:** Are the definitions of balancedness, state space, and gradient flow precisely stated and consistent?\\n*   **Lemmas & Theorems:** Are all intermediate steps, lemmas, and main theorems logically sound, without gaps or unstated assumptions?\\n*   **Scope of Proofs:** Does the paper clearly delineate the scope of its proofs (e.g., \"for linear RNNs,\" \"for quadratic loss,\" \"in the continuous-time limit\")? Any over-generalization would be a major flaw.\\n*   **Edge Cases:** Are potential edge cases (e.g., degenerate weight matrices, vanishing/exploding gradients in a non-converged state) adequately considered within the theoretical framework?\\n\\n### 5. Limitations and Areas for Improvement\\n\\n1.  **Assumptions vs. Reality:**\\n    *   **Linearity/Simplicity:** The most common limitation of such theoretical analyses is their reliance on simplifying assumptions (linear RNNs, specific activation functions, quadratic loss). The paper should clearly discuss how its findings might extend (or not) to more complex, practical RNNs (LSTMs, GRUs) and real-world loss functions.\\n    *   **Continuous vs. Discrete:** The jump from continuous gradient flow to discrete gradient descent is often non-trivial. The paper should ideally discuss the implications for practical training with finite learning rates and stochasticity.\\n2.  **Generality:** Does the theory hold for different types of RNNs (Vanilla, GRU, LSTM)? Different tasks (sequence prediction, classification, generation)?\\n3.  **Computational Tractability/Verifiability:** Is the balancedness condition or its related metrics computationally feasible to check or enforce in large-scale RNNs? Can the theoretical predictions (e.g., emergence of low-dimensional states) be empirically verified through experiments? (While the prompt is theoretical, a good theoretical paper often suggests empirical validation paths).\\n4.  **Implicit vs. Explicit:** Does the balancedness emerge *implicitly* through standard training, or does the theory suggest/require *explicit regularization* to achieve/maintain it? If the latter, its practical value might shift from an emergent property to a design principle.\\n5.  **Initial Conditions:** Are there specific initialization schemes that facilitate convergence to a balanced state, or is it a general attractor?\\n\\n### Weighted Recommendation\\n\\nGiven the potential impact of such a theoretical analysis, I lean towards **Strong Accept**, provided the proofs are robust and the stated assumptions are reasonable and clearly articulated.\\n\\n**Justification:**\\n\\n*   **High Impact Potential:** Understanding the dynamics of RNN training, particularly how desirable properties like balancedness and low-dimensional representations emerge, is a frontier in deep learning theory. Such insights can lead to better architectures, initialization strategies, or regularization techniques.\\n*   **Theoretical Rigor:** If the proofs for preservation under gradient flow are sound and novel, even under specific assumptions, this represents a significant advance in our theoretical understanding. The connection to classical control theory\\'s balanced realizations adds depth and a rich body of existing knowledge.\\n*   **Relevance:** Learning efficient, low-dimensional state spaces is a core goal in deep learning, contributing to interpretability, generalization, and computational efficiency. Providing a theoretical basis for this phenomenon in RNNs is highly relevant.\\n\\n**However, this strong recommendation is contingent on:**\\n\\n*   **Proof Soundness:** The mathematical derivations must be impeccable. Any logical gaps or unstated assumptions would severely undermine the claims.\\n*   **Clarity of Assumptions:** The paper must clearly state and justify all simplifying assumptions (e.g., linearity, continuous flow, specific loss functions).\\n*   **Acknowledged Limitations:** A strong paper acknowledges its limitations and discusses avenues for future work, especially regarding the generalization from theoretical idealizations to practical, complex RNNs.\\n\\nIf the proofs are found to be flawed, or the assumptions are overly restrictive without adequate discussion of generalizability, the recommendation would shift to a \"Weak Accept\" or even \"Reject.\" But assuming the premise of a well-executed theoretical analysis as implied by the prompt, the area of study is highly valuable.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.06960395}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'As a specialist in extrapolation and generalization in recurrent neural networks, I will evaluate the significance of the presented results based on the implicit claims of strong extrapolation capabilities.\\n\\n***\\n\\n### Evaluation of Extrapolation Results\\n\\n**Significance for Sequence Learning and Generalization Beyond Training Lengths:**\\n\\nIf the paper demonstrates robust and consistent extrapolation capabilities for RNNs beyond the sequence lengths encountered during training, this carries profound significance:\\n\\n1.  **Fundamental Breakthrough in Length Generalization:** One of the long-standing challenges in sequence modeling is the ability to generalize to sequences much longer than seen during training. Transformers, while excelling at parallel processing and capturing complex dependencies within a window, often struggle with true length generalization. If RNNs, particularly certain architectures or configurations, can reliably extrapolate, it suggests a powerful inductive bias towards learning underlying generative rules or stable dynamic systems rather than just memorizing patterns of a fixed length. This would be a major differentiator.\\n\\n2.  **Implications for Algorithmic and Compositional Learning:** True extrapolation often implies that the model has learned the *algorithm* or *compositional rules* governing the sequence, rather than merely statistics of fixed-length contexts. For instance, if an RNN can perform addition on numbers of arbitrary length after seeing only short examples, it signifies a deep understanding of the addition algorithm. This is crucial for tasks like program synthesis, symbolic reasoning, and complex long-range dependencies in natural language.\\n\\n3.  **Efficiency and Data Scarcity:** The ability to extrapolate means that models could potentially be trained on shorter, more manageable sequences and then deployed on much longer ones, saving computational resources and reducing the need for extremely long training data, which can be scarce or expensive to acquire.\\n\\n4.  **Robustness to Variable-Length Inputs:** Many real-world sequences (e.g., sensor data, spoken language, biological sequences) exhibit highly variable lengths. A model that truly extrapolates well would naturally be more robust and adaptable to such real-world variability without requiring explicit padding or truncation strategies that can obscure inherent patterns.\\n\\n5.  **Reinvigorating RNN Research:** While Transformers currently dominate many sequence tasks, strong evidence of superior extrapolation would highlight the unique strengths of recurrent architectures, potentially leading to renewed focus on their dynamic properties and architectural innovations for specific types of problems.\\n\\n**Assessment of Experimental Validation of Theoretical Findings:**\\n\\nAssuming the paper also presents theoretical underpinnings (e.g., related to linear dynamics, stable fixed points, or specific weight initialization schemes that promote stability), the experimental validation should be assessed on several fronts:\\n\\n1.  **Clarity of Theoretical Claims:** Are the theoretical findings clearly articulated and falsifiable? Do they propose specific mechanisms (e.g., \"RNNs with spectral radius $\\\\rho \\\\leq 1$ can extrapolate better,\" or \"specific non-linearities enable linear-like behavior in certain regimes\")?\\n\\n2.  **Directness of Experimental Test:** Do the experiments directly test these theoretical claims? For instance, if the theory suggests stability, are metrics like Lyapunov exponents or analysis of hidden state trajectories used to demonstrate this stability in the experimental setup? If it\\'s about linear dynamics, are linear probes or activation patterns analyzed?\\n\\n3.  **Range and Scope of Extrapolation:**\\n    *   **Length:** How much longer than the training sequences are the test sequences? A 2x increase is less significant than a 10x or 100x increase.\\n    *   **Complexity:** Are the tasks simple (e.g., repeating a pattern, counting) or more complex (e.g., algorithmic tasks, reasoning over long dependencies)? Simple tasks might be more easily solvable by trivial mechanisms.\\n    *   **Diversity of Tasks:** Are the results consistent across a range of tasks, or are they specific to a very narrow, possibly idealized, setting?\\n\\n4.  **Baseline Comparisons:** Are the results compared against strong baselines, including:\\n    *   Other RNN variants (e.g., Vanilla RNN, GRU, LSTM) trained under similar conditions.\\n    *   Non-recurrent models (e.g., Transformers, especially those without explicit positional encodings or with relative encodings that might struggle with very long distances).\\n    *   Models specifically designed for extrapolation or algorithmic generalization, if available.\\n\\n5.  **Ablation Studies:** Are there ablation studies that validate the theoretical components? For example, if a specific regularization or architectural modification is claimed to promote extrapolation, is its removal shown to degrade performance significantly?\\n\\n6.  **Statistical Rigor:** Are the experiments run multiple times with different random seeds? Are confidence intervals or statistical significance reported to ensure the observed performance is robust and not due to chance?\\n\\n**Potential Areas Where the Extrapolation Analysis Could Be Strengthened:**\\n\\n1.  **Deeper Analysis of Failure Modes:** When and why do the RNNs *fail* to extrapolate? Is there a critical length, a specific type of sequence complexity, or a particular out-of-distribution shift that causes breakdown? Understanding failure modes provides crucial insights into the limits of the proposed mechanisms.\\n\\n2.  **Qualitative Analysis of Internal Dynamics:** Beyond quantitative metrics, visualizing and analyzing the hidden state dynamics during extrapolation could provide compelling evidence. This could involve:\\n    *   **Phase Portraits:** For simple 2D or 3D hidden states.\\n    *   **Recurrence Plots:** To show self-similarity and stability.\\n    *   **Activation Patterns:** Heatmaps or t-SNE projections of hidden states to see if they converge to stable fixed points or follow predictable trajectories.\\n    *   **Probing for Linear Regimes:** If the theory suggests linearity, probe for the effective rank of Jacobians or measure the linearity of activation functions across different input regimes.\\n\\n3.  **Extrapolation on More Challenging and Diverse OOD Scenarios:**\\n    *   **Value Extrapolation:** Beyond length, can the model extrapolate to larger numbers or different symbol sets than seen in training?\\n    *   **Combinatorial Extrapolation:** Can it combine learned rules in novel ways?\\n    *   **Transfer to Real-World Data:** While synthetic tasks are good for controlled experiments, a demonstration on a real-world, long-range dependency dataset (e.g., long-form speech, genomic sequences, complex time series) would significantly bolster the claims.\\n\\n4.  **Long-Term Stability and Accumulation of Error:** For very long sequences, even small errors can accumulate. The analysis should quantify how errors propagate and whether the system remains stable over extremely long horizons. Metrics like Mean Squared Error (MSE) per step or per sequence are useful, but also look at the *rate* of error accumulation.\\n\\n5.  **Connection to Inductive Biases:** Explicitly discuss how the proposed architecture or training methodology instills the necessary inductive biases for extrapolation. Does it encourage learning invariant properties, stable attractors, or highly compositional functions?\\n\\n6.  **Scalability Analysis:** How do these extrapolation capabilities scale with model size, data size, and architectural complexity? Do larger models maintain or improve extrapolation, or do they become more prone to memorization?\\n\\n### Weighted Recommendation\\n\\n**Recommendation: Accept with Major Revisions**\\n\\n**Justification:**\\n\\nThe ability of RNNs to genuinely extrapolate beyond training sequence lengths is a critical and highly impactful area of research. If the paper presents compelling evidence of such a capability, especially supported by theoretical insights, it represents a significant contribution to our understanding of neural network generalization. Such results could indeed shift paradigms in sequence modeling, demonstrating a unique strength of recurrent architectures that is often elusive for other dominant models.\\n\\nHowever, the claims of robust extrapolation are very strong and require exceptionally rigorous experimental validation and analytical depth to be fully convincing. My recommendation of \"Accept with Major Revisions\" is based on the assumption that while the paper likely presents promising preliminary results and perhaps a compelling theoretical framework, the current experimental validation and analysis may not yet fully satisfy the high bar required for such a fundamental claim.\\n\\n**Key Reasons for this Recommendation:**\\n\\n*   **High Impact Potential:** The core claim addresses a major weakness in current deep learning models, making the paper inherently valuable if proven correct.\\n*   **Need for Robustness:** Extrapolation is notoriously difficult to achieve. The paper must demonstrate that the results are not merely an artifact of simple tasks or specific hyperparameters, but rather a robust property. This requires the strengthening points mentioned above.\\n*   **Deepening the Analysis:** To truly understand *why* the models extrapolate, the paper needs to delve deeper into the internal dynamics and relate them more concretely to the theoretical findings. Qualitative analyses and failure mode investigations are crucial here.\\n*   **Comprehensive Benchmarking:** Stronger comparisons against a broader range of baselines (including recent non-RNN models) are essential to contextualize the performance.\\n\\nA revised version that addresses these areas would significantly strengthen the paper\\'s claims, move it from \"promising\" to \"convincing,\" and solidify its contribution to the field.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.06878525}}), inline_data=None, text=None), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='AskAgent', response={'result': {'status': 'success', 'message': 'Agent has replied to the given prompt', 'output': 'The connection between the implicit bias of Gradient Descent (GD) and the moment problem from statistics is a fascinating and increasingly relevant area of research in machine learning. It attempts to provide a statistical lens through which to understand the \"unseen hand\" guiding GD towards specific solutions in overparameterized models.\\n\\nLet\\'s scrutinize this connection in detail, especially in the context of learning low-dimensional state spaces with Recurrent Neural Networks (RNNs).\\n\\n---\\n\\n### 1. Implicit Bias of Gradient Descent\\n\\n**Definition:** In overparameterized models (where the number of parameters exceeds the number of data points or there are multiple global minima for a given loss function), Gradient Descent often converges to a specific global minimum even without explicit regularization. This phenomenon is known as \"implicit bias\" or \"implicit regularization.\"\\n\\n**Examples:**\\n*   **Linear Regression:** For $y=Wx$, if there are multiple $W$ that achieve zero training error, GD (especially with small learning rates or early stopping) tends to converge to the minimum L2-norm solution, i.e., the $W$ that minimizes $\\\\|W\\\\|^2$.\\n*   **Linear Classifiers (e.g., SVMs without explicit margin):** For separable data, GD on exponential/logistic losses on a linear classifier tends to converge to a maximum margin solution.\\n*   **Matrix Completion/Factorization:** GD on overparameterized matrix factorization can implicitly find low-rank solutions.\\n\\nThe crucial point is that this bias is not encoded in the loss function or an explicit regularizer, but rather emerges from the dynamics of the optimization algorithm itself (initialization, learning rate, and the geometry of the loss landscape).\\n\\n### 2. The Moment Problem from Statistics\\n\\n**Definition:** The classical moment problem asks whether a sequence of moments (e.g., mean, variance, skewness, kurtosis, and higher-order moments) uniquely determines a probability distribution.\\n*   **Hamburger Moment Problem:** For an infinite sequence of moments, does a unique distribution exist on $(-\\\\infty, \\\\infty)$?\\n*   **Stieltjes Moment Problem:** Same for distributions on $[0, \\\\infty)$.\\n*   **Hausdorff Moment Problem:** Same for distributions on a compact interval $[a, b]$ (requires only a finite number of moments).\\n\\n**Relevance:** Moments provide a compact way to characterize a distribution\\'s shape, spread, and asymmetry. Matching moments is a common technique in statistics and econometrics to estimate parameters of a model or approximate a distribution.\\n\\n### 3. Connection Between Implicit Bias and the Moment Problem\\n\\nThe core hypothesis linking these two concepts is that **GD\\'s implicit bias drives the learning process towards solutions whose *statistical moments* (of parameters, activations, or learned representations) exhibit certain desirable properties.**\\n\\n**Correctness & Relevance:**\\n\\n1.  **Conceptual Link:** If GD prefers \"simple,\" \"sparse,\" \"low-rank,\" or \"stable\" solutions, these properties can often be quantified through moments.\\n    *   **Minimum L2-norm:** Directly relates to the first and second moments of the parameter distribution (e.g., small mean, small variance of weights around zero).\\n    *   **Low-Rank:** A low-rank matrix has a specific singular value spectrum (which are related to moments of the matrix\\'s empirical spectral distribution). GD\\'s implicit bias towards low-rank solutions means it\\'s implicitly biasing these spectral moments.\\n    *   **Sparsity:** Can be seen as driving higher moments (e.g., kurtosis) of the parameter distribution to be large, indicating many values near zero and a few large ones.\\n\\n2.  **Implicit Moment Constraints:** While GD doesn\\'t explicitly optimize for moments, its path through the parameter space might naturally lead to solutions where certain moments are minimized or maximized, acting as an *implicit regularizer*. For instance, a common observation is that GD tends to find \"flatter\" minima in the loss landscape, which can correlate with smaller magnitude weights or more \"diffuse\" weight distributions \u2013 properties quantifiable by moments.\\n\\n3.  **Beyond Parameters:** The connection can extend beyond just the model\\'s parameters to the internal activations or learned representations. For example, GD might implicitly encourage representations whose internal moments (e.g., mean, variance, covariance of hidden units across samples or time) are \"well-behaved\" or match those of the true data-generating process.\\n\\n### 4. Context: Learning Low-Dimensional State Spaces with RNNs\\n\\nThis is where the connection becomes particularly interesting and relevant.\\n\\n**The Goal:** RNNs aim to learn a hidden state $h_t$ that compactly summarizes the past sequence $x_{1:t}$ and is predictive of future observations $y_{t+1}$ or $x_{t+1}$. For \"low-dimensional state spaces,\" we seek $h_t$ to be as small as possible while retaining essential information.\\n\\n**How Moments Apply:**\\n\\n*   **Moments of Hidden States ($h_t$):** The statistical properties of the sequence of hidden states $h_1, h_2, \\\\dots, h_T$ can be characterized by their moments (mean $E[h_t]$, covariance $Cov(h_t)$, cross-covariances $Cov(h_t, h_{t+k})$).\\n*   **Moments of Dynamics:** If the true underlying system is a dynamical system (e.g., a Linear Dynamical System (LDS) or a non-linear chaotic system), its dynamics can often be characterized by properties related to the moments of its states and transitions. For instance, an LDS is completely characterized by its state transition matrix and noise covariances (which are second-order moments).\\n*   **Implicit Bias in RNNs:**\\n    *   **Stability:** GD might implicitly bias RNNs towards stable dynamics, where hidden states don\\'t explode or vanish. This stability can be quantified by spectral properties (e.g., eigenvalues of the Jacobian of the recurrent function), which are related to moments.\\n    *   **Simplicity/Sparsity of Dynamics:** For instance, if the true state space is truly low-dimensional, GD might implicitly find RNN parameters that lead to hidden state sequences with low-rank covariance matrices (i.e., only a few dominant modes of variation), or perhaps sparse activations in the hidden states. These are direct moment properties.\\n    *   **Matching Input/Output Statistics:** If the input data exhibits certain statistical regularities (e.g., stationarity, specific correlation structures), GD might learn an RNN whose internal dynamics and outputs match these input moments without explicitly being told to do so. This is particularly relevant for generative models or predictive models where the learned representation should mimic the source.\\n\\n**Relevance to RNNs:**\\nThe idea is that GD, by minimizing a predictive loss (e.g., reconstruction error, next-step prediction error), implicitly guides the RNN to learn a hidden state representation whose *moments* (e.g., covariances, higher-order auto-correlations) align with the dominant statistical patterns of the observed time series data. If the true underlying system is, for example, a low-dimensional LDS, GD might implicitly regularize the RNN to learn a recurrent weight matrix whose spectral radius is close to 1 and whose states\\' empirical covariances match those of the true system. This explains *why* RNNs might succeed in discovering hidden linear or non-linear state spaces without explicit moment matching objectives.\\n\\n### 5. Applicability of the Moment Matching Framework (as an analytical lens)\\n\\n**Strengths:**\\n\\n1.  **Explanatory Power:** Provides a powerful analytical framework to understand *what kind of solutions* GD prefers in complex deep learning models, particularly when there are many equivalent solutions. It moves beyond just \"generalization\" to \"what structure is learned.\"\\n2.  **Connection to System Identification:** For time-series data, it bridges deep learning with classical control theory and system identification, where moment-based methods (e.g., subspace identification, spectral methods) are standard for learning state-space models.\\n3.  **Understanding Regularization:** It can explain why certain architectures or training procedures (e.g., specific initializations, activation functions) lead to better generalization by implicitly promoting desirable moment properties.\\n4.  **Diagnostic Tool:** We can analyze the moments of learned representations to understand what aspects of the data the model is prioritizing or ignoring.\\n5.  **Potential for New Architectures/Losses:** If we understand which moments are implicitly biased, we might design architectures or *explicit* regularizers to encourage specific, beneficial moment properties.\\n\\n**Limitations:**\\n\\n1.  **Computational Complexity of High-Order Moments:** For complex, non-linear RNNs with high-dimensional hidden states, calculating or even conceptualizing higher-order moments becomes intractable. Which specific moments are most relevant for characterizing the implicit bias of GD in such systems is unclear.\\n2.  **Identifiability Issues:** The classic moment problem itself can be ill-posed. A finite set of moments may not uniquely determine a distribution, especially in non-Gaussian, high-dimensional spaces. Similarly, multiple RNN parameterizations can produce the same sequence of output moments.\\n3.  **Non-Linearity and Non-Stationarity:** GD\\'s dynamics are highly non-linear, and the implicit bias itself might be complex and context-dependent. Time-series data are often non-stationary, making moment calculations tricky.\\n4.  **Implicit vs. Explicit:** The connection is that GD *implicitly* biases certain moments. Explicitly *matching* moments as a loss function is often difficult, unstable, and might not align with the generalization goals of GD.\\n5.  **Causality vs. Correlation:** Moments capture correlations. While useful, learning true causal dynamics from observational data (a goal for many RNN applications) often requires more than just moment matching.\\n6.  **\"Which Moments Matter?\":** It\\'s not clear which moments are the \"right\" ones to analyze in the context of specific deep learning tasks. Is it moments of weights? Activations? Gradients? The spectrum of the Hessian? All contribute to the implicit bias.\\n\\n### 6. Alternative Perspectives\\n\\n1.  **PAC-Bayesian Theory / Stability:** Implicit bias as a form of regularization that promotes solutions with \"flat\" minima, leading to better generalization and robustness to perturbations.\\n2.  **Information Theory:** GD might implicitly minimize some form of complexity (e.g., Minimum Description Length) or maximize information compression in the learned representations.\\n3.  **Optimization Landscape Geometry:** The bias simply arises from the specific geometry of the loss landscape, where some paths are more accessible to GD than others.\\n4.  **Curse of Dimensionality:** In high-dimensional spaces, \"most\" functions have certain properties (e.g., smoothness). GD might just be finding one such generic solution.\\n5.  **Spectral Methods (for LDS):** While GD *implicitly* works with moments, classical system identification methods (like subspace identification) *explicitly* use moment matrices (e.g., Hankel matrices of covariances) to recover linear dynamical systems. This offers an alternative, direct moment-matching approach for learning state spaces, highlighting the potential common ground with GD\\'s implicit behavior.\\n6.  **Koopman Operator Theory:** This framework lifts non-linear dynamics into a linear space (of observable functions), where spectral properties (eigenfunctions, eigenvalues) analogous to moments can characterize complex systems. GD\\'s implicit bias could be interpreted as finding parameterizations that align with low-dimensional Koopman representations.\\n\\n---\\n\\n### Weighted Recommendation: ACCEPT (with significant caveats)\\n\\n**Justification:**\\n\\nThe connection between the implicit bias of Gradient Descent and the moment problem is **correct** conceptually and **highly relevant** for advancing our theoretical understanding of deep learning, especially in structured domains like time series processing with RNNs.\\n\\n*   **Correctness:** It is demonstrably true that GD exhibits implicit bias. It is also true that statistical moments characterize distributions and can describe properties like sparsity, low-rank, and stability. The analytical work establishing these links (e.g., for linear models, matrix factorization) supports the core idea.\\n*   **Relevance:**\\n    *   **Explaining Success:** It offers a compelling explanation for *why* GD on RNNs might successfully identify low-dimensional state spaces without explicit architectural constraints or regularization for such. It posits that GD naturally seeks solutions whose learned internal dynamics (hidden state moments) align with the dominant statistical properties of the input data.\\n    *   **Bridging Disciplines:** It connects deep learning with classical statistics, control theory, and system identification, potentially leading to cross-pollination of ideas and more principled model design.\\n    *   **Future Research:** It opens avenues for designing better initializations, implicit regularizers, or even new loss functions that explicitly or implicitly encourage desirable moment properties.\\n\\n**Caveats:**\\n\\n*   **Complexity in Non-linear RNNs:** While the conceptual link is strong, rigorously formalizing *which* moments GD biases in complex, non-linear RNNs and *how* this relates to specific properties of the learned state space remains a major theoretical and practical challenge. The \"moment problem\" for high-dimensional, non-Gaussian, non-stationary data generated by deep nets is exceedingly complex.\\n*   **Practical Applicability:** This framework is primarily an analytical tool for understanding *post-hoc* what GD has done. Directly incorporating explicit moment matching as an optimization objective in deep learning is often difficult due to the computational cost of higher-order moments and the non-differentiability of some moment constraints.\\n*   **Specificity:** The exact nature of GD\\'s implicit bias (and thus which moments it targets) can depend heavily on initialization, learning rate, architecture, and activation functions, making general statements difficult.\\n\\nIn conclusion, viewing GD\\'s implicit bias through the lens of moment matching provides a profound statistical interpretation of its behavior, especially in the context of learning structured representations like low-dimensional state spaces in RNNs. While its full implications are still being unraveled due to the inherent complexity of deep learning models, it represents a promising direction for understanding the generalization capabilities of neural networks beyond simple loss minimization.', 'remaining_resource_budget': 100, 'remaining_expense_budget': 9999.06757175}}), inline_data=None, text=None)], role='tool')"}, {"role": "assistant", "content": "Here's a summary of the agent reviews:\n\n*   **RNN Dynamics Expert (RNN-DE):** Recommends **Strong Accept** (contingent on proof soundness and clarity of assumptions). Highlights the potential impact of understanding RNN training dynamics and the connection to control theory. Emphasizes the need for rigorous proofs and clear articulation of assumptions.\n*   **Extrapolation and Generalization Analyst (EGA):** Recommends **Accept with Major Revisions**. Acknowledges the high impact potential of demonstrating robust extrapolation but stresses the need for exceptionally rigorous experimental validation, deeper analysis of failure modes, and comprehensive benchmarking against other models.\n*   **Statistical Moment Matching Assessor (SMA):** Recommends **ACCEPT** (with significant caveats). Affirms the correctness and relevance of connecting GD's implicit bias to the moment problem. Notes the framework's explanatory power and its ability to bridge disciplines. Cautions about the complexity in non-linear RNNs and the practical applicability of explicit moment matching.\n\nBased on these reviews, the paper presents a valuable theoretical analysis of extrapolation in RNNs, with promising connections to control theory and statistical moment matching. However, the experimental validation needs to be strengthened, and the limitations of the theoretical analysis need to be clearly addressed.\n\n**FINAL DECISION: Accept**\n"}], "ground_truth": "Accept: poster", "response_history": "Here's a summary of the agent reviews:\n\n*   **RNN Dynamics Expert (RNN-DE):** Recommends **Strong Accept** (contingent on proof soundness and clarity of assumptions). Highlights the potential impact of understanding RNN training dynamics and the connection to control theory. Emphasizes the need for rigorous proofs and clear articulation of assumptions.\n*   **Extrapolation and Generalization Analyst (EGA):** Recommends **Accept with Major Revisions**. Acknowledges the high impact potential of demonstrating robust extrapolation but stresses the need for exceptionally rigorous experimental validation, deeper analysis of failure modes, and comprehensive benchmarking against other models.\n*   **Statistical Moment Matching Assessor (SMA):** Recommends **ACCEPT** (with significant caveats). Affirms the correctness and relevance of connecting GD's implicit bias to the moment problem. Notes the framework's explanatory power and its ability to bridge disciplines. Cautions about the complexity in non-linear RNNs and the practical applicability of explicit moment matching.\n\nBased on these reviews, the paper presents a valuable theoretical analysis of extrapolation in RNNs, with promising connections to control theory and statistical moment matching. However, the experimental validation needs to be strengthened, and the limitations of the theoretical analysis need to be clearly addressed.\n\n**FINAL DECISION: Accept**\n", "elapsed_time": 114.60366177558899, "reviewer_agents": {"RNNDynamicsExpert": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in Recurrent Neural Network dynamics and Gradient Descent.", "system_prompt": "You are an expert in Recurrent Neural Network dynamics and the behavior of Gradient Descent. You will rigorously evaluate the theoretical analysis of RNNs, focusing on the balancedness condition, its preservation under Gradient Flow, and its implications for learning low-dimensional state spaces. Assess the novelty and correctness of the theoretical proofs, and identify any potential limitations or areas for improvement.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "ExtrapolationAnalyst": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Specialist in extrapolation and generalization in neural networks.", "system_prompt": "You are a specialist in extrapolation and generalization in neural networks, particularly in the context of recurrent models. You will evaluate the significance of the extrapolation results presented in the paper, focusing on their implications for sequence learning and the ability of RNNs to generalize beyond the training sequence lengths. Assess the experimental validation of the theoretical findings and identify potential areas where the extrapolation analysis could be strengthened.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}, "MomentMatchingAssessor": {"base_model": "gemini-2.5-flash-preview-05-20", "description": "Expert in statistical moment matching techniques.", "system_prompt": "You are an expert in statistical moment matching techniques and their applications in machine learning. You will scrutinize the connection between the implicit bias of Gradient Descent and the moment problem from statistics, assessing the correctness and relevance of this connection in the context of learning low-dimensional state spaces with RNNs. Evaluate the applicability of the moment matching framework and identify potential limitations or alternative perspectives.", "create_resource_cost": 0, "invoke_resource_cost": 0, "create_expense_cost": 0, "invoke_expense_cost": 0.15, "output_expense_cost": 0.6}}}
